{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/main/TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "70c8ba82-60a0-42c8-ea95-9a05b2532732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=0a37d6509e1a19cd90a96ec9fa52cdd5fe024c624330e8a524626a449037f94c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "31c6c0e5-73a9-4ef1-cda1-81f1d7ff8269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-1-c5cb458fe86c>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TFBartForConditionalGeneration, BartTokenizer, T5Tokenizer, pipeline, TFT5ForConditionalGeneration\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect w/ HuggingFace HUB\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "9100de41ec6946c495c032b05d90913d",
            "40d035dada3347e2963c183d035d445c",
            "ecd31a14240f408b82b5224d97694ab7",
            "7afeab237ced4f01b70361e2a3ce7552",
            "5b7408314641483980402784564e62e6",
            "c4a0eef84a5e4061884e43d2a09da02a",
            "b114a3fe857949408305dde0c9436974",
            "f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "52801cc2a7a646109c4a7141ef768900",
            "db74bb5ffc4f4900b17aaf842a618019",
            "6deef8837ace4d94b5b803666ff4aa5a",
            "2e00c125b4d942368e702682002f94ff",
            "571b4b08342141c798092b6b23ab410a",
            "ca46452e63a34d99865718ab86726748",
            "493ec2745ddf4bf2bdce528219a1b309",
            "d3c8673d4bac474d9c5799e3ee02bf6b",
            "f44df68731c14e24914d1c8816c8edce"
          ]
        },
        "id": "wpR8O7wbdMdI",
        "outputId": "d0ef909e-6736-48c6-bf15-0c9b0cbe318a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9100de41ec6946c495c032b05d90913d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "89df2a05-5542-4ec6-cf03-020b9ee8dd4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "1I5H39lnQW9o",
        "outputId": "a0ba0b45-28c9-45dd-917f-c7f4ba6dcfab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source    paper_id  \\\n",
              "1262  Camera drones, a rapidly emerging technology, ...   BviYjfnIk   \n",
              "123   Achieving machine intelligence requires a smoo...   H1kMMmb0-   \n",
              "564   In this work, we address the problem of musica...  S1lvm305YQ   \n",
              "1249  Quantization of a neural network has an inhere...  SJx94o0qYX   \n",
              "931   Current end-to-end deep learning driving model...  B14rPj0qY7   \n",
              "\n",
              "                                                 target  \\\n",
              "1262  StarHopper is a novel touch screen interface f...   \n",
              "123   We use reinforcement learning to train an agen...   \n",
              "564   We present the TimbreTron, a pipeline for perf...   \n",
              "1249  precision highway; a generalized concept of hi...   \n",
              "931   we proposed a new self-driving model which is ...   \n",
              "\n",
              "                                                  title  number_words_target  \\\n",
              "1262  StarHopper: A Touch Interface for Remote Objec...                   97   \n",
              "123                                                 NaN                   30   \n",
              "564   TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pi...                   89   \n",
              "1249  Precision Highway for Ultra Low-precision Quan...                   70   \n",
              "931   RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDG...                  116   \n",
              "\n",
              "                                     extractive_summary  \n",
              "1262  A lab study shows that StarHopper offers an ef...  \n",
              "123   In this work, we address the question of creat...  \n",
              "564   We introduce TimbreTron, a method for musical ...  \n",
              "1249  In the experiments, our proposed method outper...  \n",
              "931   For example, though addressed the driving dire...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57574e49-0732-48a5-a0ee-9b4720fbf230\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>Camera drones, a rapidly emerging technology, ...</td>\n",
              "      <td>BviYjfnIk</td>\n",
              "      <td>StarHopper is a novel touch screen interface f...</td>\n",
              "      <td>StarHopper: A Touch Interface for Remote Objec...</td>\n",
              "      <td>97</td>\n",
              "      <td>A lab study shows that StarHopper offers an ef...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Achieving machine intelligence requires a smoo...</td>\n",
              "      <td>H1kMMmb0-</td>\n",
              "      <td>We use reinforcement learning to train an agen...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30</td>\n",
              "      <td>In this work, we address the question of creat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>In this work, we address the problem of musica...</td>\n",
              "      <td>S1lvm305YQ</td>\n",
              "      <td>We present the TimbreTron, a pipeline for perf...</td>\n",
              "      <td>TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pi...</td>\n",
              "      <td>89</td>\n",
              "      <td>We introduce TimbreTron, a method for musical ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>Quantization of a neural network has an inhere...</td>\n",
              "      <td>SJx94o0qYX</td>\n",
              "      <td>precision highway; a generalized concept of hi...</td>\n",
              "      <td>Precision Highway for Ultra Low-precision Quan...</td>\n",
              "      <td>70</td>\n",
              "      <td>In the experiments, our proposed method outper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>931</th>\n",
              "      <td>Current end-to-end deep learning driving model...</td>\n",
              "      <td>B14rPj0qY7</td>\n",
              "      <td>we proposed a new self-driving model which is ...</td>\n",
              "      <td>RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDG...</td>\n",
              "      <td>116</td>\n",
              "      <td>For example, though addressed the driving dire...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57574e49-0732-48a5-a0ee-9b4720fbf230')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-57574e49-0732-48a5-a0ee-9b4720fbf230 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-57574e49-0732-48a5-a0ee-9b4720fbf230');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-53a3a7de-ff60-4f9b-972a-fb8c4e15b761\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-53a3a7de-ff60-4f9b-972a-fb8c4e15b761')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-53a3a7de-ff60-4f9b-972a-fb8c4e15b761 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Achieving machine intelligence requires a smooth integration of perception and reasoning, yet models developed to date tend to specialize in one or the other; sophisticated manipulation of symbols acquired from rich perceptual spaces has so far proved elusive. Consider a visual arithmetic task, where the goal is to carry out simple arithmetical algorithms on digits presented under natural conditions (e.g. hand-written, placed randomly). We propose a two-tiered architecture for tackling this kind of problem. The lower tier consists of a heterogeneous collection of information processing modules, which can include pre-trained deep neural networks for locating and extracting characters from the image, as well as modules performing symbolic transformations on the representations extracted by perception. The higher tier consists of a controller, trained using reinforcement learning, which coordinates the modules in order to solve the high-level task. For instance, the controller may learn in what contexts to execute the perceptual networks and what symbolic transformations to apply to their outputs. The resulting model is able to solve a variety of tasks in the visual arithmetic domain,and has several advantages over standard, architecturally homogeneous feedforward networks including improved sample efficiency. Recent successes in machine learning have shown that difficult perceptual tasks can be tackled efficiently using deep neural networks BID18 . However, many challenging tasks may be most naturally solved by combining perception with symbol manipulation. The act of grading a question on a mathematics exam, for instance, requires both sophisticated perception (identifying discrete symbols rendered in many different writing styles) and complex symbol manipulation (confirming that the rendered symbols correspond to a correct answer to the given question). In this work, we address the question of creating machine learning systems that can be trained to solve such perceptuo-symbolic problems from a small number of examples. In particular, we consider, as a first step toward fullblown exam question grading, the visual arithmetic task, where the goal is to carry out basic arithmetic algorithms on hand-written digits embedded in an image, with the wrinkle that an additional symbol in the image specifies which of a handful of algorithms (e.g. max, min, +, *) should be performed on the provided digits. One straightforward approach to solving the visual arithmetic task with machine learning would be to formulate it as a simple classification problem, with the image as input, and an integer giving the correct answer to the arithmetic problem posed by the image as the label. A convolutional neural network (CNN; BID17 could then be trained via stochastic gradient descent to map from input images to correct answers. However, it is clear that there is a great deal of structure in the problem which is not being harnessed by this simple approach, and which would likely improve the sample efficiency of any learning algorithm that was able to exploit it. While the universal approximation theorem BID9 suggests that an architecturally homogeneous network such as a CNN should be able to solve any task when it is made large enough and given sufficient data, imposing model structure becomes important when one is aiming to capture human-like abilities of strong generalization and learning from small datasets BID16 .In particular, in this instance we would like to provide the learner with access to modules implementing information processing functions that are relevant for the task at hand -for example, modules that classify individual symbols in the image, or modules that perform symbolic computations on stored representations. However, it is not immediately clear how to include such modules in standard deep networks; the classifiers need to somehow be applied to the correct portion of the image, while the symbolic transformations need to be applied to the correct representations at the appropriate time and, moreover, will typically be non-differentiable, precluding the possibility of training via backpropogation. In this work we propose an approach that solves this type of task in two steps. First, the machine learning practitioner identifies a collection of modules, each performing an elementary information processing function that is predicted to be useful in the domain of interest, and assembles them into a designed information processing machine called an interface BID30 that is coupled to the external environment. Second, reinforcement learning (RL) is used to train a controller to make use of the interface; use of RL alleviates any need for the interface to be differentiable. For example, in this paper we make use of an interface for the visual arithmetic domain that contains: a discrete attention mechanism; three pre-trained perceptual neural networks that classify digits/classify arithmetic symbols/detect salient locations (respectively); several modules performing basic arithmetic operations on stored internal representations. Through the use of RL, a controller learns to sequentially combine these components to solve visual arithmetic tasks. We propose a novel recipe for constructing agents capable of solving complex tasks by sequentially combining provided information processing modules. The role of the system designer is limited to choosing a pool of modules and gathering training data in the form of input-output examples for the target task. A controller is then trained by RL to use the provided modules to solve tasks. We evaluate our approach on a family of visual arithmetic tasks wherein the agent is required to perform arithmetical reduction operations on handwritten digits in an image. Our experiments show that the proposed model can learn to solve tasks in this domain using significantly fewer training examples than unstructured feedforward networks. The remainder of the article is organized as follows. In Section 2 we describe our general approach and lay down the required technical machinery. In Section 3 we describe the visual arithmetic task domain in detail, and show how our approach may be applied there. In Section 4 we present empirical results demonstrating the advantages of our approach as it applies to visual arithmetic, before reviewing related work in Section 5 and concluding with a discussion in Section 6. Our approach makes use of standard reinforcement learning formalisms BID27 . The external world is modelled as a Partially Observable Markov Decision Process (POMDP), E. Each time step E is in a state s t , based upon which it emits an observation o t that is sent to the learning agent. The agent responds with an action a t , which causes the environment to emit a reward r t . Finally, the state is stochastically updated according to E's dynamics, s t+1 \\u223c P (\\u00b7|s t , a t ). This process repeats for T time steps. The agent is assumed to choose a t according to a parameterized policy that maps from observation-action histories to distributions over actions, i.e. a t \\u223c \\u03c0 \\u03b8 (\\u00b7|h t ), where h t = o 0 , a 0 , . . . , o t and \\u03b8 is a parameter vector. We make extensive use of the idea of an interface as proposed in BID30 . An interface is a designed, domain-specific machine that mediates a learning agent's interaction with the external world, providing a representation (observation and action spaces) which is intended to be more conducive to learning than the raw representation provided by the external world. In this work we formalize an interface as a POMDP I distinct from E, with its own state, observation and action spaces. The interface is assumed to be coupled to the external world in a particular way; each time step E sends an observation to I, which potentially alters its state, after which I emits its own observation to the agent. When the agent responds with an action, it is first processed by I, which once again has the opportunity to change its state, after which I sends an action to E. The agent may thus be regarded as interacting with a POMDP C comprised of the combination of E and I. C's observation and action spaces are the same as those of I, its state is the concatenation of the states of I and E, and its dynamics are determined by the nature of the coupling between I and E. BID30 learn to control interfaces in order to solve purely algorithmic tasks, such as copying lists of abstractly (rather than perceptually) represented digits. One of the main insights of the current work is that the idea of interfaces can be extended to tasks with rich perceptual domains by incorporating pre-trained deep networks to handle the perceptual components. We train controllers using the actor-critic algorithm (see e.g. BID27 BID2 BID20 BID26 ). We model the controller as a policy \\u03c0 \\u03b8 that is differentiable with respect to its parameters \\u03b8. Assume from the outset that the goal is to maximize the expected sum of discounted rewards when following \\u03c0 \\u03b8 : DISPLAYFORM0 where DISPLAYFORM1 is the probability of that trajectory under \\u03c0 \\u03b8 , and \\u03b3 \\u2208 (0, 1] is a discount factor. We look to maximize this objective using gradient ascent; however, it is not immediately clear how to compute \\u2207 \\u03b8 J(\\u03b8), since the probability of a trajectory P \\u03c0 \\u03b8 (\\u03c4 ) is a function of the environment dynamics, which are generally unknown. Fortunately, it can be shown that an unbiased estimate of \\u2207 \\u03b8 J(\\u03b8) can be obtained by differentiating a surrogate objective function that can be estimated from sample trajectories. Letting R t = T \\u22121 i=t \\u03b3 i\\u2212t r i , the surrogate objective is: DISPLAYFORM2 The standard REINFORCE algorithm BID29 consists in first sampling a batch of trajectories using \\u03c0 \\u03b8 , then forming an empirical estimate f (\\u03b8) of F(\\u03b8). \\u2207 \\u03b8 f (\\u03b8) is then computed, and the parameter vector \\u03b8 updated using standard gradient ascent or one of its more sophisticated counterparts (e.g. ADAM; BID13 ).The above gradient estimate is unbiased (i.e. E [\\u2207 \\u03b8 f (\\u03b8)] = \\u2207 \\u03b8 J(\\u03b8)) but can suffer from high variance. This variance can be somewhat reduced by the introduction of a baseline function b t (h) into Equation (2) : DISPLAYFORM3 It can be shown that including b t (h) does not bias the gradient estimate and may lower its variance if chosen appropriately. The value function for the current policy, et al., 2000) -however this will rarely be known. A typical compromise is to train a function V \\u03c9 (h), parameterized by a vector \\u03c9, to approximate V \\u03c0 \\u03b8 (h) at the same time as we are training \\u03c0 \\u03b8 . Specifically, this is achieved by minimizing a sample-based estimate of DISPLAYFORM4 DISPLAYFORM5 We employ two additional standard techniques BID20 . First, we have V \\u03c9 (h) share the majority of its parameters with \\u03c0 \\u03b8 , i.e. \\u03c9 = \\u03b8 . This allows the controller to learn useful representations even in the absence of reward, which can speed up learning when reward is sparse. Second, we include in the objective a term which encourages \\u03c0 \\u03b8 to have high entropy, thereby favouring exploration. Overall, given a batch of N sample trajectories from policy \\u03c0 \\u03b8 , we update \\u03b8 in the direction of the gradient of the following surrogate objective:1 N T k t as the first term does not provide a useful training signal for V \\u03b8 . We now describe the Visual Arithmetic task domain in detail, as well as the steps required to apply our approach there. We begin by describing the external environment E, before describing the interface I, and conclude the section with a specification of the manner in which E and I are coupled in order to produce the POMDP C with which the controller ultimately interacts. Tasks in the Visual Arithmetic domain can be cast as image classification problems. For each task, each input image consists of an (n, n) grid, where each grid cell is either blank or contains a digit or letter from the Extended MNIST dataset BID1 . Unless indicated otherwise, we use n = 2. The correct label corresponding to an input image is the integer that results from applying a specific (task-varying) reduction operation to the digits present in the image. We consider 5 tasks within this domain, grouped into two kinds. Changing the task may be regarded as changing the external environment E. Single Operation Tasks. In the first kind of task, each input image contains a randomly selected number of digits (2 or 3 unless otherwise stated) placed randomly on the grid, and the agent is required to output an answer that is a function of the both the specific task being performed and the digits displayed in the image. We consider 4 tasks of this kind: Sum, Product, Maximum and Minimum. Example input images are shown in FIG0 , Top Row. Combined Task. We next consider a task that combines the four Single Operation tasks. Each input example now contains a capital EMNIST letter in addition to 2-to-3 digits. This letter indicates which reduction operation should be performed on the digits: A indicates add/sum, M indicates multiplication/product, X indicates maximum, N indicates minimum. Example input images are shown in FIG0 , Bottom Row. Succeeding on this task requires being able to both carry out all the required arithmetic algorithms and being able to identify, for any given input instance, which of the possible algorithms should be executed. def update_interface(ExternalObs e, string action): if action == \\\"right\\\": fovea_x += 1 elif action == \\\"left\\\": fovea_x -= 1 elif action == \\\"down\\\": fovea_y += 1 elif action == \\\"up\\\": fovea_y -= 1 elif action == \\\"+\\\": store += digit elif action == \\\" * \\\": store * = digit elif action == \\\"max\\\": store = max(store, digit) elif action == \\\"min\\\": store = min(store, digit) elif action == \\\"+1\\\": store += 1 elif action == \\\"classify_op\\\": op = op_classifier(get_glimpse(e, fovea_x, fovea_y)) elif action == \\\"classify_digit\\\": digit = digit_classifier(get_glimpse(e, fovea_x, fovea_y)) elif action == \\\"update_salience\\\": salience_map = salience_detector(e, fovea_x, fovea_y) else:raise Exception(\\\"Invalid action\\\") obs = (fovea_x, fovea_y, store, op, digit, salience_map) return obs We now describe the interface I that is used to solve tasks in this domain. The first step is to identify information processing functions that we expect to be useful. We can immediately see that for Visual Arithmetic, it will be useful to have modules implementing the following functions:1. Detect and attend to salient locations in the image.2. Classify a digit or letter in the attended region.3. Manipulate symbols to produce an answer. We select modules to perform each of these functions and then assemble them into an interface which will be controlled by an agent trained via reinforcement learning. A single interface, depicted in FIG1 , is used to solve the various Visual Arithmetic tasks described in the previous section. This interface includes 3 pre-trained deep neural networks. Two of these are instances of LeNet (LeCun et al., 1998) , each consisting of two convolutional/max-pool layers followed by a fully-connected layer with 128 hidden units and RELU non-linearities. One of these LeNets, the op classifier, is pre-trained to classify capital letters from the EMNIST dataset. The other LeNet, the digit classifier, is pre-trained to classify EMNIST digits. The third network is the salience detector, a multilayer perceptron with 3 hidden layers of 100 units each and RELU non-linearities. The salience network is pre-trained to output a salience map when given as input scenes consisting of randomly scattered EMNIST characters (both letters and digits). In the Visual Arithmetic setting, E may be regarded as a degenerate POMDP which emits the same observation, the image containing the EMNIST letters/digits, every time step. I sends the contents of its store field (see FIG1 ) to E every time step as its action. During training, E responds to this action with a reward that depends on both the time step and whether the action sent to E corresponds to the correct answer to the arithmetic problem represented by the input image. Specifically, for all but the final time step, a reward of 0 is provided if the answer is correct, and \\u22121/T otherwise. On the final time step, a reward of 0 is provided if the answer is correct, and \\u22121 otherwise. Each episode runs for T = 30 time steps. At test time, no rewards are provided and the contents of the interface's store field on the final time step is taken as the agent's guess for the answer to the arithmetic problem posed by the input image. For the controller, we employ a Long Short-Term Memory (LSTM) BID8 with 128 hidden units. This network accepts observations provided by the interface (see FIG1 as input, and yields as output both \\u03c0 \\u03b8 (\\u00b7|h) (specifically a softmax distribution) from which an action is sampled, and V \\u03b8 (h) (which is only used during training). The weights of the LSTM are updated according to the actor-critic algorithm discussed in Section 2.3. In this section, we consider experiments applying our approach to the Visual Arithmetic domain. These experiments involve the high-level tasks described in Section 3.1. For all tasks, our reinforcement learning approach makes use of the interface described in Section 3.2 and the details provided in Section 3.3.Our experiments look primarily at how performance is influenced by the number of external environment training samples provided. For all sample sizes, training the controller with reinforcement learning requires many thousands of experiences, but all of those experiences operate on the small provided set of input-output training samples from the external environment. In other words, we assume the learner has access to a simulator for the interface, but not one for the external environment. We believe this to be a reasonable assumption given that the interface is designed by the machine learning practitioner and consists of a collection of information processing modules which will, in most cases, take the form of computer programs that can be executed as needed. We compare our approach against convolutional networks trained using cross-entropy loss, the de facto standard when applying deep learning to image classification tasks. These feedforward networks can be seen as interacting directly with the external environment (omitting the interface) and running for a single time step, i.e. T = 1. The particular feedforward architecture we experiment with is the LeNet (LeCun et al., 1998) , with either 32, 128 or 512 units in the fully-connected layer. Larger, more complex networks of course exist, but these will likely require much larger amounts of data to train, and here we are primarily concerned with performance at small sample sizes. For training the convolutional networks, we treat all tasks as classification problems with 101 classes. The first 100 classes correspond to the integers 0-99. All integers greater than or equal to 100 (e.g. when multiplying 3 digits) are subsumed under the 101st class. Experiments showing the sample efficiency of the candidate models on the Single Operation tasks are shown in FIG2 . Similar results for the Combined task are shown in FIG3 . In both cases, our reinforcement learning approach is able to leverage the inductive bias provided by the interface to achieve good performance at significantly smaller sample sizes than the relatively architecturally homogeneous feedforward convolutional networks. in cognitive science. Production systems date back to Newell and Simon's pioneering work on the study of high-level human problem solving, manifested most clearly in the General Problem Solver (GPS; BID23 . While production systems have fallen out of favour in mainstream AI, they still enjoy a strong following in the cognitive science community, forming the core of nearly every prominent cognitive architecture including ACT-R (Anderson, 2009), SOAR BID22 BID15 , and EPIC (Kieras and Meyer, 1997). However, the majority of these systems use hand-coded, symbolic controllers; we are not aware of any work that has applied recent advances in reinforcement learning to learn controllers for these systems for difficult tasks. A related body of work concerns recurrent neural networks applied to supervised learning problems. BID19 , for example, use reinforcement learning to train a recurrent network to control the location of an attentional window in order to classify images containing MNIST digits placed on cluttered backgrounds. Our approach may be regarded as providing a recipe for building similar kinds of models while placing greater emphasis on tasks with a difficult algorithmic components and the use of structured interfaces. Neural Abstract Machines. One obvious point of comparison to the current work is recent research on deep neural networks designed to learn to carry out algorithms on sequences of discrete symbols. Some of these frameworks, including the Differentiable Forth Interpreter BID25 and TerpreT (Gaunt et al., 2016) , achieve this by explicitly generating code, while others, including the Neural Turing Machine (NTM; , Neural Random-Access Machine (NRAM; BID14 , Neural Programmer (NP; BID21 , and Neural Programmer-Interpreter (NPI; BID24 avoid generating code and generally consist of a controller network that learns to perform actions using a differentiable external computational medium (i.e. a differentiable interface) in order to carry out an algorithm. Our approach is most similar to the latter category, the main difference being that we have elected not to require the external computational medium to be differentiable, which provides it with greater flexibility in terms of the components that can be included in the interface. In fact, our work is most similar to BID30 , which also uses reinforcement learning to learn algorithms, and from which we borrowed the idea of an interface, the main difference being that we have included deep networks in our interfaces in order to tackle tasks with non-trivial perceptual components. Visual Arithmetic. Past work has looked at learning arithmetic operations from visual input. BID10 train a multi-layer perceptron to map from images of two 7-digit numbers to an image of a number that is some task-specific function of the numbers in the input images. Specifically, they look at addition, subtraction, multiplication and Roman-Numeral addition. Howeover, they do not probe the sample efficiency of their method, and the digits are represented using a fixed computer font rather than being hand-written, making the perceptual portion of the task significantly easier. Gaunt et al. FORMULA0 addresses a task domain that is similar to Visual Arithmetic, and makes use of a differentiable code-generation method built on top of TerpreT. Their work has the advantage that their perceptual modules are learned rather than being pre-trained, but is perhaps less general since it requires all components to be differentiable. Moreover, we do not view our reliance on pre-trained modules as particularly problematic given the wide array of tasks deep networks have been used for. Indeed, we view our approach as a promising way to make further use of any trained neural network, especially as facilities for sharing neural weights mature and enter the mainstream. Additional work has focused more directly on the use of neural modules and adaptively choosing groups of modules to apply depending on the input. Endto-End Module Networks BID11 use reinforcement learning to train a recurrent neural network to lay out a feedforward neural network composed of elements of a stored library of neural modules (which are themselves learnable). Our work differs in that rather than having the layout of the network depend solely on the input, the module applied at each stage (i.e. the topology of the network) may depend on past module applications within the same episode, since a decision about which module to apply is made at every time step. Systems built using our framework can, for example, use their modules to gather information about the environment in order to decide which modules to apply at a later time, a feat that is not possible with Module Networks. In Deep Sequential Neural Networks (DSNN; BID3 ), each edge of a fixed directed acyclic graph (DAG) is a trainable neural module. Running the network on an input consists in moving through the DAG starting from the root while maintaining a feature vector which is repeatedly transformed by the neural modules associated with the traversed edges. At each node of the DAG, an outgoing edge is stochastically selected for traversal by a learned controller which takes the current features as input. This differs from our work, where each module may be applied many times rather than just once as is the case for the entirely feedforward DSNNs (where no module appears more than once in any path through the DAG connecting the input to the output). Finally, PathNet is a recent advance in the use of modular neural networks applied to transfer learning in RL BID4 ). An important difference from our work is that modules are recruited for the entire duration of a task, rather than on the more fine-grained step-by-step basis used in our approach. There are number of possible future directions related to the current work, including potential benefits of our approach that were not explored here. These include the ability to take advantage of conditional computation; in principle, only the subset of the interface needed to carry out the chosen action needs to be executed every time step. If the interface contains many large networks or other computationally intensive modules, large speedups can likely be realized along these lines. A related idea is that of adaptive computation time; in the current work, all episodes ran for a fixed number of time steps, but it should be possible to have the controller decide when it has the correct answer and stop computation at that point, saving valuable computational resources. Furthermore, it may be beneficial to train the perceptual modules and controller simultaneously, allowing the modules to adapt to better perform the uses that the controller finds for them. Finally, the ability of reinforcement learning to make use of discrete and non-differentiable modules opens up a wide array of possible interface components; for instance, a discrete knowledge base may serve as a long term memory. Any generally intelligent system will need many individual competencies at its disposal, both perceptual and algorithmic; in this work we have proposed one path by which a system may learn to coordinate such competencies. We have proposed a novel approach for solving tasks that require both sophisticated perception and symbolic computation. This approach consists in first designing an interface that contains information processing modules such as pre-trained deep neural networks for processing perceptual data and modules for manipulating stored symbolic representations. Reinforcement learning is then used to train a controller to use the interface to solve tasks. Using the Visual Arithmetic task domain as an example, we demonstrated empirically that the interface acts as a source of inductive bias that allows tasks to be solved using a much smaller number of training examples than required by traditional approaches.\",\n          \"Current end-to-end deep learning driving models have two problems: (1) Poor\\ngeneralization ability of unobserved driving environment when diversity of train-\\ning driving dataset is limited (2) Lack of accident explanation ability when driving\\nmodels don\\u2019t work as expected. To tackle these two problems, rooted on the be-\\nlieve that knowledge of associated easy task is benificial for addressing difficult\\ntask, we proposed a new driving model which is composed of perception module\\nfor see and think and driving module for behave, and trained it with multi-task\\nperception-related basic knowledge and driving knowledge stepwisely.   Specifi-\\ncally segmentation map and depth map (pixel level understanding of images) were\\nconsidered as what & where and how far knowledge for tackling easier driving-\\nrelated perception problems before generating final control commands for difficult\\ndriving task. The results of experiments demonstrated the effectiveness of multi-\\ntask perception knowledge for better generalization and accident explanation abil-\\nity. With our method the average sucess rate of finishing most difficult navigation\\ntasks in untrained city of CoRL test surpassed current benchmark method for 15\\npercent in trained weather and 20 percent in untrained weathers. Observing progressive improvement in various fields of pattern recognition with end-to-end deep learning based methods BID13 BID8 , self-driving researchers try to revolutionize autonomous car field with the help of end-to-end deep learning techniques BID3 BID4 . Impressive results have been acquired by mapping camera images directly to driving control commands BID3 with simple structure similar to ones for image classfication task BID19 . Further researches were conducted to improve the performance of deep learning based autonomous driving system, for example, Conditional Imitation Learning approach has been proposed to solve the ambigious action problem. However, two crutial problems failed to be spotted: (1) Poor generalization ability of unobserved driving environment given limited diversity of training scenerios. For example, though addressed the driving direction selection problem, it showed poor generalization ability in unseen test town which has different map and building structure than training town's. This generalization problem is extremely important since collected driving dataset always has limitation of diversity (2) Current end-to-end autonomous approaches lack of accident explanation ability when these models behave unexpectedly. Although saliency map based visualization methods BID20 BID23 BID21 BID2 have been proposed to dig into the 'black box', the only information these methods could bring is the possible attention of the model instead of the perception process of the model. We proposed a new driving approach to solve the two aforementioned problems by using multi-task basic perception knowledge. We argue that when end-to-end model is trained to address a specific difficult task, it's better to train the model with some basic knowledge to solve relevant easier tasks before BID17 ). An analogy for this can be observed when human beings learn a difficult knowledge. For example, to solve a complex integration problem, compared with students without basic math knowledge, students who know about basic knowledge of math are able to learn the core of intergration more quickly and solve other similar integration problems instead of memorizing the solution of the specific problem. Our proposed model consists of two modules: perception module and driving module as in FIG0 . The perception module is used for learning easier driving-related perception knowledge, which we refer as ability of pixel level understanding of input including what & where and how far knowledge. We trained perception module with segmentation map and depth map first, while the former serves as what & where knowledge and the latter serves as how far knowledge. By visualizing inferenced segmentation and depth results whether perception process works well or not could be inferred. After the perception module was trained to have ability of pixel level understanding of its image input, we freezed the perception module weights and trained driving module with driving dataset. This decomposition of end-to-end driving network strucuture is considered to be mediated perception approach BID25 . With our proposed driving structure and stepwise training strategy, the generalization and accident explanation problems were addressed to a certain extent. Depending on whether mediated perception knowledge are generated, self-driving models are categorized into mediated perception approach BID25 and behavior reflex approach. For mediated perception approaches, there are several well-behaved deep learning methods, for example, Deep-Driving method BID4 fisrtly converts input RGB images to some key perception indicators related to final driving controls. They designed a very simple driving controller based on predicted perception indicators. Problem of this approach is that key perception indicators have limitation of describing unseen scenerios and are difficult to collect in reality. Except for inferencing for final driving controls, there are approaches which focus on inferencing intermediate description of driving situation only. For separate scene understanding task, car detection BID15 and lane detection BID0 are two main topics in this area. Instead of inferencing one perception task at most, multi-task learning method aims at tackling several relevant tasks simultaneously. BID24 uses input image to solve both object detection and road segmentation tasks. Branched E-Net BID16 ) not only infers for segmentation map, but also depth map of current driving scenarios. These multi-task learning methods shows better result when sharing the encoder of different perception tasks together, but they haven't really tried to make the car drive either in simulator or reality. As for behavior reflex approach which is also called 'end-to-end learning', NVIDIA firstly proposed a model for mapping input image pixels directly to final driving control output(steer only) BID3 . Some other approaches further atempted to create more robust models, for example, long short-term memory (LSTM) was utilized to make driving models store a memory of past BID5 .One problem is that aforementioned methods were tested in dissimlar driving scenerios using different driving dataset, thus it's hard to determine if model itself is the source of the better driving behavior instead of effectiveness of data BID22 . was tested in a public urban driving simulator and sucessed to tackle the ambigous action problem which refers as optimal driving action can't be inferred from perceptual input alone. Benefit from CoRL test in , fair comparision could be conducted using same driving dataset. showed limitation of generalization ability problem in test town different from train town as in CoRL test training dataset could be only collected from single train town. When the end-to-end driving method behaves badly and causes accidents, accident explanation ability is required. Though saliency-map based visualization methods BID2 BID20 help understand the influence of input on final driving control, it's extremely hard to derive which module of the model fails when driving problems happen -If the model percepts incorrectly or the driving inference processes wrongly based on good perception information. Driving system was enabled to give quantitative explanation by visualizing inferenced multi-task basic knowledge to solve this problem. Basic strucure of the proposed model is shown in FIG0 . The proposed model has two parts: (1) Multi-task basic knowledge perception module (2) Driving decision branch module. The perception module is used to percept the world by inferencing dpeth map and segmentation map, which is composed of one shared encoder and two decoders for two different basic perception knowledge: (1) Segmentation decoder for generating 'what & where' information by predicting segmentation maps; (2) Depth decoder for predicting 'how far' the objects in vision are by inferencing depth maps. The perception module is aimed at extracting encoded feature map containing pixel level understanding information for driving module and qualitative explanation when proposed model doesn't work as expected by visualizing the predicted segmentation and depth maps to determine if the driving problem is caused by percept process or driving process. The driving module enbales the model to generate driving decisions for different direction following guidances. We categorized the real world driving guidance into four types: (1) Following lane (2) Turning left (3) Going straight (4) Turning right as done in . For each driving guidance direction, there is a driving branch(which predicts the value of driving controls) corresponding to it, therefore there are four driving guidance branches totally. The output of second last layer in perception module is inputted to the driving module, therefore the training of which could benefit from the multi-knowledge extracted by the perception module. Instead of linear layers, convolution layers are utilized for inferencing final driving controls for each direction, which helps keeping the spatial relation of information and reducing number of parameters as non-negligible quantity of direction branches. The perception module is built with residual block proposed in BID12 which solves gradient vanishing and 'degradation problem', and it has a structure similar to Segnet BID1 prosposed for efficient image segmentation task. Huge difference is that in our proposed method there are two different decoders for inferencing both segmentation and depth maps simultaneously instead of segmentation map only. Besides, we constraint the total strides in encoder to 8 for keeping resolution of feature map, as large total stride has negative influence on feature map size reconstuction. Hybrid Dilated Convolution BID26 is adapted as last part of the encoder as it enlarges the receptive field and avoids theoretical issue of gridding problem. Groupout(Park) is also adapted to avoid overfitting problem in the convolution network. The driving module is built with residual block and has a general form as in last output layer for several direction outputs. It is all based on convolutional layers in order to keep the spatial information and reduce parameters motivated by BID21 . Four different high level driving guidance such as \\\"turning right\\\" are utilized for selecting which direction branch's output is supposed to be considered as final driving outputs. Driving outputs contain steering and acceleration/brake, both of them range from -1 to 1. Since there are 4 output branches corresponding to 4 high level driving guidances, 8 feature map size convolution kernels are set in the last layer for output scalar value, in which each two are regarded as driving controls for one driving guidance. To determine the limitation of RGB image, no other information such as current speed or steering angle were used as input. Instead we atempted to predict the current speed based on current RGB image to keep the driving smoothly as done in . The input of the driving module is not from the last layer's output of the encoder part in the perception module, but the second last layer's output of the encoder part due to empirically selection for best generalization. The training dataset is collected in CARLA simulator . CARLA simulator is a self-driving simulator developed by Intel Co. for collecting self-driving related information and evaluating driving model with a standard testing environment named CoRL test. CoRL test is composed of 4 tasks of increasing difficulty: (1) Straight: the goal is straight ahead of the starting position. (2) One turn: getting to the goal takes one turn, left or right. (3) Navigation: navigation with an arbitrary number of turns. (4) Navigation with dynamic obstacles: same as previous task, but with other vehicles and pedestrians .The main metric for quantitatively evaluating is the average success rate of finishing seperate tasks in CoRL test. CoRL test contains tests both in trained town and untrained town under both trained and untrained weathers. Test trained town and untrained town are constructed with different maps and different building texture. Dataset for training our model could be categorized into 2 items: (1) Perception module training dataset (2) Driving module training dataset. For perception module, we trained it with 35,000 pairs of RGB images, segmentation and depth maps and evaluated with 5,000 pairs. As for driving module, we trained with 455,000 dataset, and evaluated on 62,000 evaluation dataset. Before training our proposed model, two vital data processing methods were used: balancing dataset and data augmentation. For fair comparison, we use same driving dataset published by Conditional Imitation Learning ) except that we collected extra segmentation and depth maps in train town for training our proposed perception module. Dataset balancing contributed to better generalization of both perception module and driving module in our experiments as it enables each mini-batch to be a microcosm of the whole dataset. For perception module, dataset were balanced to ensure that each mini-batch contains all different training weathers and an equal amount of going straight and turning situations. For driving module, we balance each training mini-batch to ensure equally distribution of different driving direction guidance, and reorganized large steer(absolute value larger than 0.4) data accounts for 1/3 in each mini-batch, brake situation data acounts for 1/3, noise steer situation for 1/10. We add guassian noise, coarse dropout, contrast normalization, Guassian blur to both training dataset for perception and driving module for enlarging training dataset distribution. We trained the whole system using a step-wise training method which is firstly we trained the perception module with multi-task basic perception knowledge, then we freezed the weights of perception module and train driving module with driving dataset. For training the perception module, we used mini-batch size 24 and set ratio of segmentation loss and depth loss to be 1.5:1. Softmax categorical crossentropy is used for segmentation loss, binary crossentropy is used for depth loss. Adam of 0.001, which is multiplied by a factor of 0.2 of previous learning rate if validation loss does't drop for 1 epoch is used as optmizer. L2 weight decay and early stopping are also used for avoid overfitting. As for training the driving module, we consider MSE loss and use Adam with starting learning rate of 0.002 which exponentially decay of 0.9 every epoch. Early stopping and L2 decay are used for regularization. We compare the results of driving performance between our proposal and other methods tested in CoRL test via success rate of finishing each task. The details of results are shown in Table. 1From Table. 1, though our proposal finished slightly less in training conditions comparing with other methods, our proposal achieved much higher success rate in untrained town environments, which demonstrates our model has much better generalization ability of adapting to untrain town than other methods tested in the CoRL test when trained with limited diversity of training conditions. One important notice is that we use the almost the same driving dataset for training as the method showed in the Table. ?? .We could also visualize the perception process when the model works. One example of test in untrained town and untrained weather is shown in Fig. 2 .Figure 2: Screenshots of captured single RGB image and our proposal's inference results which consists of predicted segmentation map and depth map during test in untrained town under untrained weather. Obviously from the inferenced segmentation and depth maps we get information that the driving model knows a car is passing by the left side. Since we observed our training model has better generalization ability in unseen town comparing with other methods when almost the same driving dataset were used to train (except that we collected extra depth maps and segmentation maps in same training environments), we want to investigate the origin of the better generalization ability. There are two possible reasons why our training model has better generalization ability in unseen town: (1) Basic knowledge (segmentation map and depth map) (2) Network structure. Therefore we conduct experiments by comparing performance of two methods:\\u2022 Our original proposal: Firstly train perception module with basic knowledge, after training perception module, freeze its weights and train driving module with driving dataset \\u2022 Compared method: Train the encoder of perception module and driving module together with driving dataset. No basic perception knowledge is used for training model. Since tests in CoRL cost much time, we limited our evaluation to the most difficult untrained town under untrained weathers test. Results are shown in Table. 2. From the results it's obvious that multibasic knowledge we use in the training phase is the origin of our proposal's good generalization ability of untrained town instead of the network structure. Moreover, the network structure could be improved to achieve better performance in the furture. Besides basic knowledge leads to better generalization ability, it could also be used to give a qualitative explanation of driving problems. Basic knowledge of segmentation map and depth map are output from the perception module during test phase, therefore how the driving module percepts the current scenario could be known by simply visualizing the outputs of segmentation and depth from perception module. Depending on the predicted pixel understanding of the situation, cause of driving problem could be inferred. One example is shown in Fig. 3 . For a failed straight task in untrained town under untrained weather soft rain sunset as the driving model failed to move forward, we visualized outputs of segmentation and depth maps predicted by perception module. It's obvious that this failure case is caused by the perception module since the model falsely percepted that there is a car in front of it and in order to avoid collision it did't start. There is no car actually thus the perception module made false judgement. However, what's interesting is that sometimes it fools readers to think that there is a car in Fig. 3 because of sun ray reflection on the wet road and the perception module has the similar understanding as these readers. Therefore in some aspects the perception module makes the right judgement instead of wrong's. For traditional end-to-end learning driving methods BID3 it's impossible to reason as they don't focus on the cause explanation ability which is of great importance for practice use of deep learning driving models. Figure 3: Failed straight task in untrained town under untrained soft rain sunset weather. From predicted segmentation and depth maps, we know that the driving model thinks that there is a car in front of it but actually there isn't any car in front of it. Fine-tuneYosinski et al. (2014) , which refers to use other well-trained models weights on different target-related dataset as initial weights for training with target dataset instead of using weights initializing methods BID9 BID11 BID14 , is a common trick used in deep learning since empirically it could leads to better generalization on new target dataset. Here in our specific case we refer fine-tune method to be after training perception module we train the weights of the encoder of the perception module as well instead of freezing these weights. In Table. 3 we compare the performance of fine-tune method and our original proposed method. In this comparison we achieved counter-intuition results: after fine-tune the weights of the perception module the driving model achieved worse results than original method which freeze the weights of perception module when training the driving module. One possible reason is that the generalization ability lies in the perception module instead of the driving module, therefore when we train the perception module again with driving dataset, the ability of generating compressed multi-knowdege information is destoryed. As the fine-tune model couldn't benefit from the multi-task knowledge anymore, it failed to produce the same generalization ability as the original proposal did. Furthermore we conduct experiment on visualizing one direction of loss surface by projecting the loss surface to 2 dimension BID10 to investigate some qualitative explanation for this comparison result. x axis corresponds to linear interpolation of the weights of original proposed method and weights of compared fine-tuned method after training. Formulation of calculating the weights in this projection direction is Equation. 1. DISPLAYFORM0 \\u03b1 is linear interpolation ratio, x f intune and x rgb0 are trained weights of fine-tune method and original proposal method. f (x) is loss function of the whole model while input is considered as different model weights. We draw out the projected loss surface as FIG1 by sampling from the interpolation weigthts. FIG1 we can get one possible qualitative reason for worse behavior of fine-tune method from a loss surface perspective: Model weight got by using fine-tune method is stuck in a super flat surface, while model weights of original proposed method successfully finds a local minimum. In this paper we propose a new driving system for better generalization and accident explanation ability by enabling it to do simpler driving-related perception task before generating commands for diffult driving task. Through multiple experiments we empirically proved the effectiveness of the multi basic perception knowledge for better generalization ability of unobserved town when diversity of training dataset is limited. Besides our proposed model has self-explanation ability by visualizing the predicted segmentation and depth maps from the perception module to determine the cause of driving problems when they happen. One interesting result we acquired by comparing different train strategies is that the generalization ability of driving origins from basic knowledge and lies in weights of the perception module which should not be modified during training with driving dataset. We hope our work could movitivate other researches to use multi-task target related perception knowledge for better performance in robot learning. In future we will investigate more effective network structures.\",\n          \"In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies \\u201cimage\\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples. We made an accompanying demo video here: https://www.cs.toronto.edu/~huang/TimbreTron/index.html which we strongly encourage you to watch before reading the paper. Timbre is a perceptual characteristic that distinguishes one musical instrument from another playing the same note with the same intensity and duration. Modeling timbre is very hard, and it has been referred to as \\\"the psychoacoustician's multidimensional waste-basket category for everything that cannot be labeled pitch or loudness\\\" 2 . The timbre of a single note at a single pitch has a nonlinear dependence on the volume, time and even the particular way the instrument is played by the performer. While there is a substantial body of research in timbre modelling and synthesis BID6 ; BID32 ; BID35 BID36 ), state-of-the-art musical sound libraries used by orchestral composers for analog instruments (e.g. the Vienna Symphonic Library (GmbH, 2018) ) are still obtained by extremely careful audio sampling of real instrument recordings. Being able to model and manipulate timbre electronically carries importance for musicians who wish to experiment with different sounds, or compose for multiple instruments. (Appendix A discusses the components of music in more detail.)In this paper, we consider the problem of high quality timbre transfer between audio clips obtained with different instruments. More specifically, the goal is to transform the timbre of a musical recording to match a set of reference recordings while preserving other musical content, such as pitch and loudness. We take inspiration from recent successes in style transfer for images using neural networks BID13 BID22 BID7 ). An appealing strategy would be to directly apply image-based style transfer techniques to time-frequency representations of images, such as short-time Fourier transform (STFT) spectrograms. However, needing to convert the generated spectrogram into a waveform presents a fundamental obstacle, since accurate reconstruction requires phase information, which is difficult to predict BID10 , and existing techniques for inferring phase (e.g., BID16 ) can produce characteristic artifacts which are undesirable for high quality audio generation BID34 .Recent years have seen rapid progress on audio generation methods that directly generate high-quality waveforms, such as WaveNet BID41 , SampleRNN BID28 , and Tacotron2 BID34 ). WaveNet's ability to condition on abstract audio representations is particularly relevant, since it enables one to perform manipulations in high-level auditory representations from which reconstruction would have previously been impractical. Tacotron2 performs high-level processing on time-frequency representations of speech, and then uses WaveNet to output high-quality audio conditioned on the generated mel spectrogram. We adapt this general strategy to the music domain. We propose TimbreTron, a pipeline that performs CQT-based timbre transfer with high-quality waveform output. It is trained only on unrelated samples of two instruments. For our time-frequency representation, we choose the constant Q transform (CQT), a perceptually motivated representation of music BID4 . We show that this representation is particularly well-suited to musical timbre transfer and other manipulations due to its pitch equivariance and the way it simultaneously achieves high frequency resolution at low frequencies and high temporal resolution at high frequencies, a property that STFT lacks. TimbreTron performs timbre transfer by three steps, shown in Figure 1 . First, it computes the CQT spectrogram and treats its log-magnitude values as an image (discarding phase information). Second, it performs timbre transfer in the log-CQT domain using a CycleGAN (Zhu et al., 2017) . Finally, it converts the generated log-CQT to a waveform using a conditional WaveNet synthesizer (which implicitly must infer the missing phase information). Empirically, our TimbreTron can successfully perform musical timbre transfer on some instrument pairs. The generated audio samples have realistic timbre that matches the target timbre while otherwise expressing the same musical content (e.g., rhythm, loudness, pitch). We empirically verified that the use of a CQT representation is a crucial component in TimbreTron as it consistently yields qualitatively better timbre transfer than its STFT counterpart. Raw Violin Waveform Violin CQT Generated Flute CQT Figure 1: The TimbreTron pipeline that performs timbre transfer from Violin to Flute. Time-frequency analysis refers to techniques that aim to measure how the signal's frequency domain representation changes over time. The STFT is one of the most commonly applied techniques for this purpose. The discrete STFT operation can be compactly expressed as follows: DISPLAYFORM0 The above formula computes the STFT of an input time-domain signal x[n] at time step m and frequency \\u03c9 k . w refers to a zero-centered window function (such as Hann Window), which acts as a means of masking out the values that are away from m. Hence, the equation above can be interpreted as the discrete Fourier transform of the masked signal x[n]w[n \\u2212 m]. An example spectrogram is shown in FIG0 .Constant Q Transform (CQT). The CQT (Brown, 1991) is another time-frequency analysis technique in which the frequency values are geometrically spaced, with the following particular pat- DISPLAYFORM1 Here, k \\u2208 {1, 2, 3, ...k max } and b is a constant that determines the geometric separation between the different frequency bands. To make the filter for different frequencies adjacent to each other, the bandwidth of the k th filter is chosen as: DISPLAYFORM2 . This results in a constant frequency to resolution ratio (as known as the \\\"quality (Q) factor\\\"): DISPLAYFORM3 Huzaifah FORMULA4 showed that CQT consistently outperformed traditional representations such as Mel-frequency cepstral coefficients (MFCCs) in environmental sound classification tasks using CNNs. Rainbowgram. BID10 introduced the rainbowgram, a visualization of the CQT which uses color to encode time derivatives of phase; this highlights subtle timbral features which are invisible in a magnitude CQT. Examples of CQTs and rainbowgrams are shown in FIG0 . Synthesis (waveform reconstruction) from the aforementioned time-frequency analysis techniques can be performed in the presence of both magnitude and phase information (Allen and Rabiner, 1977) BID20 . In the absence of phase information, one of the common methods of synthetically generating phase from STFT magnitude is the Griffin-Lim algorithm BID16 . This algorithm works by randomly guessing the phase values, and iteratively refining them by performing STFT and inverse STFT operations until convergence, while keeping the magnitude values constant throughout the process. Developed to minimize the mean squared error between the target spectrogram and predicted spectrogram, this algorithm is shown to reduce the objective function at each iteration, while having no optimality guarantees due to the non-convexity of the optimization problem BID16 BID37 Although recent developments in the field have enabled performing the inverse operation of CQT BID42 BID12 , these techniques still require both phase and magnitude information. WaveNet, proposed by van den BID41 , is an auto-regressive generative model for generating raw audio waveform with high quality. The model consists of stacks of dilated causal convolution layers with residual and skip connections. WaveNet can be easily modified to perform conditional waveform generation; for example, it can be trained as a vocoder for synthesizing natural, high-quality human speech in TTS systems from low-level acoustic features (e.g., phoneme, fundamental frequency, and spectrogram) BID1 BID34 . One limitation of WaveNet is that the generation of waveforms can be expensive, which is undesirable for training procedures that require auto-regressive generation (e.g., GAN training, scheduled sampling). Generative Adversarial Networks (GANs) are a class of implicit generative models introduced by BID15 . A GAN consists of a discriminator and a generator, which are trained adversarially via a two-player min-max game, where the discriminator attempts to distinguish real data from samples, and the generator attempts to fool the discriminator. The objective is: DISPLAYFORM0 where D is the discriminator, G is the generator, z is the latent code vector sampled from Gaussian distribution Z, and x is sampled from data distribution X . GANs constituted a significant advance over previous generative models in terms of the quality of the generated samples. CycleGAN (Zhu et al., 2017) is an architecture for unsupervised domain transfer: learning a mapping between two domains without any paired data. (Similar architectures were proposed independently by Yi et al. (2017) ; BID26 ; BID24 .) The CycleGAN learns two generator mappings: F : X \\u2192 Y and G : Y \\u2192 X ; and two discriminators: DISPLAYFORM1 The loss function of CycleGAN consists of both adversarial losses (Eqn. 1), combined with a cycle consistency constraint which forces it to preserve the structure of the input: DISPLAYFORM2 3 MUSIC PROCESSING WITH CONSTANT-Q-TRANSFORM REPRESENTATION This section focuses on the first and last steps of the TimbreTron pipeline: the steps related to the transforming raw waveforms to and from time frequency representations. We explain our reasoning for choosing the CQT representation and introduce our conditional WaveNet synthesizer which converts a (possibly generated) CQT to a high-quality audio waveform. The CQT representation BID4 has desirable characteristics that make it especially suitable for processing musical audio signals. It uses a logarithmic representation of frequency, where the frequencies are generally chosen to exactly cover all the pitches present in the twelve tone, welltempered scale. Unlike the STFT, the CQT has higher frequency resolution towards lower frequencies, which leads to better pitch resolution for lower register instruments (such as cello or trombone), and higher time resolution towards higher frequencies, which is advantageous for recovering the fine timing of rhythms. Since individual notes contain information across many frequencies (due to their pattern of overtones), this combination of resolutions ought to allow simultaneous recovery of pitch and timing information for any particular note. (While this information is preserved in the signal, waveform recovery is a difficult problem in practice; this is discussed in Section 3.2).Another key feature of the CQT representation in the context of TimbreTron is (approximate) pitch equivariance. Thanks to the geometric spacing of frequencies, a pitch shift corresponds (approximately) to a vertical translation of the \\\"spectral signature\\\" (unique pattern of harmonics) of musical instruments. This means that the convolution operation is approximately equivariant under pitch translation, which allows convolutional architectures to share structure between different pitches. A demonstration of this can be seen in FIG1 . Since the harmonics of a musical instrument are approximately integer multiples of the fundamental frequency, scaling the fundamental frequency (hence the pitch) corresponds to a constant shift in all of the harmonics in log scale. We also want to emphasize on some of the reasons why the equivariance is only approximate:\\u2022 Imperfect multiples: In real audio samples from instruments, the harmonics are only approximately integer multiples of the fundamental frequency, due to the material properties of the instruments producing the sound.\\u2022 Dependence of spectral signature on pitch and beyond: For each pitch, each instrument has a slightly different spectral signature, meaning that a simple translation in the frequency axis cannot completely account for the changes in the frequency spectrum. Furthermore, even at a given pitch it can still change depending on how it's played. We used 16ms frame hop (256 time steps under 16kHz). More details can be found in Appendix B. Since empirical studies have shown it is difficult to directly predict phase in time-frequency representations BID10 , we discard the phase information and perform the image-based processing directly on a log-amplitude CQT representation. Therefore, in order to recover a waveform consistent with the generated CQT, we need to infer the missing phase information, which is a difficult problem BID42 .To convert log magnitude CQT spectrograms back to waveforms, we use a 40-layer conditional WaveNet with the dilation rate of 2 k (mod 10) for the k th layer. The model is trained using pairs of a CQT and a waveform; this requires only a collection of unlabeled waveforms, since the CQT can be computed from the waveform.3 See Appendix C.4 for the details of the WaveNet architecture. WaveNet reconstructed audio samples can be found here 4Beam Search Because the conditional WaveNet generates stochastically from its predictive distribution, it sometimes produces low-probability outputs, such as hallucinated notes. Also, because it has difficulty modeling the local loudness, the loudness often drifts significantly over the timescale of seconds. While these issues could potentially be addressed by improving the WaveNet architecture or training method, we instead take the perspective that the WaveNet's role is to produce a waveform which matches the target CQT. Since the above artifacts are macro-scale errors which happen only stochastically, the WaveNet has a significant probability of producing high-quality outputs over a short segment (e.g. hundreds of milliseconds). Therefore, we perform a beam search using the WaveNet's generations in order to better match the target CQT. See Appendix C.5 for more details about our beam search procedure. Reverse Generation In early experiments, we observed that percussive attacks (onset characteristics of an instrument in which it reaches a large amplitude quickly) are sometimes hard to model during forward generation, resulting in multiple attacks or missing attacks. We believe this problem occurs because it is difficult to determine the onset of a note from a CQT spectrogram (in which information is blurred in frequency), and it is difficult to predict precise pitch at the note onset due to the broad frequency spectrum at that moment. We found that the problems of missing and doubled attacks could be mostly solved by having the WaveNet generate the waveform samples in reverse order, from end to beginning. In this section, we describe the middle step of our TimbreTron pipeline, which performs timbre transfer on log-amplitude CQT representations of the waveforms. As training data, we have collections of unrelated recordings of different musical instruments. Hence, our timbre transfer problem on log-amplitude CQT \\\"images\\\" is an instance of unsupervised \\\"image-to-image\\\" translation. To achieve this, we applied the CycleGAN architecture, but adapted it in several ways to make it more effective for time-frequency representations of audio. Removing Checkerboard Artifacts The convnet-resnet-deconvnet based generators from the original CycleGAN led to significant checkerboard artifacts in the generated CQT, which corresponds to severe noise in the generated waveform. To alleviate this problem, we replaced the deconvolution operation with nearest neighbor interpolation followed with regular convolution, as recommended by BID31 .Full-Spectrogram Discriminator Due to the local nature of the original CycleGAN's transformations, Zhu et al. FORMULA4 found it advantageous for the discriminator only to process a local patch of the image. However, when generating spectrograms, it's crucial that different partials of the same pitch be consistent with each other; a discriminator which is local in frequency cannot enforce this. Therefore, we gave the discriminator the full spectrogram as input. Gradient Penalty(GP) Replacing the patch discriminator with the full-spectrogram one led to unstable training dynamics because the discriminator was too powerful. To compensate for this, we added the Gradient Penalty(GP) BID18 to enforce a soft Lipschitz constraint: DISPLAYFORM0 HereX are samples taken along a line between the true data distribution X and the generator's data distribution X g = {F (z)|z \\u223c Z} via convex combination of a real data point and a generated data point. BID11 showed empirically that the GP can stabilize GAN training. Furthermore, BID14 showed that GP can also stabilize and improve CycleGAN training with word embeddings. We observed the same benefits in our experiments. Identity loss In addition to the adversarial loss and the reconstruction loss that we applied to the generators, we also added identity loss, which was proposed by Zhu et al. (2017) to preserve color composition in the original CycleGAN. Empirically, we found out that the identity loss component helps generators to preserve music content, which yields better audio quality empirically. DISPLAYFORM1 Our weighting of the identity loss followed a linear decay schedule (details in Appendix C.3). In this way, at the start of training, the generator is encouraged to learn a mapping that preserves pitch; as training progresses, the enforcement is reduced, allowing the generator to learn more expressive mappings. See Appendix C.2, C.3, and C.6 for more details of our CycleGAN architecture, and training and generation methods. There is a long history of using clever representations of images or audio signals in order to perform manipulations which are not straightforward on the raw signals. In a seminal work, Tenenbaum and Freeman (1999) used a multilinear representation to separate style and content of images. and BID43 then applied the optimization technique proposed by BID13 to the audio domain by applying the image-based architectures to spectrogram representations of the signals. BID17 took a similar approach, but used hand-crafted features to extract statistics from the spectrograms. However, a recent review by pointed out that the disentanglement of timbre and performance control information remains unsolved. Zhu et al. FORMULA4 introduced Cycle GAN approach to learn an \\\"unsupervised image-to-image mapping\\\" between two unpaired datasets using two generator networks and two discriminator networks with generative adversarial training. Given the success of the CycleGAN on image domain style transfer, BID23 applied the same architecture to translate between human voices in the Mel-cepstral coefficient (MCEP) domain and BID5 applied it to musical style transfer with MIDI representations. What the aforementioned audio style transfer approaches have in common is that the reconstruction quality is limited by the existing non-parametric algorithms for audio reconstruction (e.g., the GriffinLim algorithm for STFT domain reconstruction BID16 , or the WORLD vocoder for MCEP domain reconstruction of speech signals BID30 ), or existing MIDI synthesizer. Another strategy is to operate directly on waveforms. van den BID41 demonstrated high-quality audio generation using WaveNet. Following on this, BID10 proposed a WaveNet-style autoencoder model operating on raw waveforms that was capable of creating new, realistic timbres by interpolating between already existing ones. BID9 proposed a method to synthesize waveforms directly using GANs with improved quality over naive generative models such as SampleRNN BID28 and WaveNet. BID29 used an encoderdecoder approach for the Timbre Transfer problem, where they trained a universal encoder to learn a shared representation of raw waveforms of various instruments, as well as instrument-specific decoders to reconstruct waveforms from the shared representation. In a parallel work, BID2 approached the many-to-many timbre transfer problem with their MoVE model which is based on UNIT BID26 but with Maximum Mean Discrepancy (MMD) as their objective. While their approach has the advantage of training a single model for many transfer directions, our TimbreTron model has the advantage that it uses a GAN-based training objective, which (in the image domain) typically results in outputs with higher perceptual quality compared to VAEs. We conducted two sets of experiments to 1) experiment with pitch-shifting and tempo-changing to further validate our choice of CQT representation; 2) test our full TimbreTron pipeline (along with ablation experiments to validate our architectural choices). See Appendix C for the details of our experimental setup. For this section, please listen to audio samples we provided in our website 5 as you read along. Training TimbreTron requires collections of unrelated recordings of the source and target instruments. We built our own MIDI and real world datasets of classical music for training TimbreTron. Within each type of dataset, we gathered unrelated recordings of Piano, Flute, Violin and Harpsichord and then divided the entire dataset into training set and test set. We ensured that the training and test sets were entirely disjoint in terms of musical content by splitting the datasets by musical piece. The training dataset was divided into 4-second chunks, which were the basic units processed by our CycleGAN and WaveNet. Links to source audio and more details about our dataset are given in Appendix C.1 6.2 DISENTANGLING PITCH AND TEMPO USING CQT REPRESENTATION Before presenting our timbre transfer results, we first consider the simpler task of disentangling pitch and tempo. Recall that the two properties are entangled in the time domain representation, e.g. subsampling the waveform simultaneously increases the tempo and raises the pitch. Changing the two independently requires more sophisticated analysis of the signal. In the context of our TimbreTron pipeline, due to the CQT's pitch equivariance property, pitch shifting can be (approximately) performed simply by translating the CQT representation on the log-frequency axis. (Since the STFT uses linearly sampled frequencies, it does not lend itself easily to this type of simple transformation.) Audio time stretching can be done using either the CQT or STFT representations, combined with the WaveNet synthesizer, by changing the number of waveform samples generated per CQT window. Regardless of the number of samples generated, the WaveNet synthesizer is able to produce the correct pitch based on the local frequency content. (See section 6.2 of the OneDrive folder) In conclusion, our method was able to vary the pitch and tempo independently while otherwise preserving the timbre and musical structure. While most of our experiments on timbre transfer are conducted on real world music recordings, we also use synthetic MIDI audio data in our ablation studies because it is possible to produce paired dataset for evaluation purpose. In this section, we show our experimental findings on the full TimbreTron pipeline using real world data, verify the correctness of our reasoning about CQT, and show the generalization capability of TimbreTron. Comparing CQT and STFT Representations One of the key design choices in TimbreTron was whether to use an STFT or CQT representation. If the STFT representation is used, there is an additional choice of whether to reconstruct using the Griffin-Lim algorithm or the conditional WaveNet synthesizer. We found that the STFT-based pipeline had two problems: 1) it sometimes failed to correctly transfer low pitches, likely due to the STFT's poor frequency resolution at low frequencies, and 2) it sometimes produced a random permutation of pitches. For example, we ran TimbreTron on a Bach piano sample played by a professional musician. The STFT TimbreTron transposed parts of the longer excerpt by different amounts, and for a few notes in particular, seemed to fail to transpose them by the same amount as it did the others. As is shown by audio samples here 6 , those problems were completely solved using CQT TimbreTron (likely due to the CQT's pitch equivariance and higher frequency resolution at low frequencies). Both of these artifacts occurred in both WaveNet and Griffin-Lim reconstruction methods (See Table 4 ), which suggests that the source of the artifacts are likely to be from the CycleGAN stage of the pipeline. (Please listen to corresponding samples in section 6.3 of the OneDrive folder) This empirically demonstrates the effectiveness of the CQT representation compared with STFT.Generalizing from MIDI to Real-World Audio To further explore the generalization capability of TimbreTron, we also tried one domain adaptation experiment where we took a CycleGAN trained on MIDI data, tested it on the real world test dataset, and synthesized audio with Wavenet trained on training real world data. As is shown from the corresponding audio examples in this section 7 , the quality of generated audio is very good, with pitch preserved and timbre transfered. The ability to generalize from MIDI to real-world is interesting, in that it opens up the possibility of training on paired examples. We conducted a human study to investigate whether TimbreTron could transfer the timbre of a reference collection of signals while otherwise preserving the musical content. We also evaluated the effectiveness of the CQT representation by comparing with a variant of TimbreTron with the CQT replaced by the STFT. All results are showns in TAB2 and 4, with detailed discussion in this section. A list of questions asked in AMT can be found in TAB0 . Does TimbreTron transfer timbre while preserving the musical piece? To be effective, the system must transform a given audio input so that the output is (1) recognizable as the same (or appropriately similar) basic musical piece, and (2) recognizable as the target instrument. We address both of these criteria by two types of comparison-based experiments: instrument similarity and musical piece similarity. The questions we asked are listed in TAB0 shows results for the instrument similarity comparison and TAB2 shows results for the music piece similarity comparison. The respondents were also asked to provide their subjective judgment about the instrument used for the provided samples. The original questionnaire can be found here 8 .(1) Preserving the musical piece. A different instrument playing the same notes may not always sound subjectively like the same \\\"piece\\\". When this is done in musical contexts, the notes themselves are often changed in order to adapt pieces between instruments, and this is generally referred to as a Table 2 : AMT results on pair-wise instrument comparisons between our proposed TimbreTron without beam search, ground truth original instrument and ground truth target instrument. This corresponds to question type (i) in TAB0 .new \\\"arrangement\\\" of an existing piece. Thus, even in the cases where we had a recording available in the target domain, the exact notes or timings were not always identical to those in the original recording from which we transferred. Overall, when we did have such a target domain recording of a real instrument, we found that for the pair of (Real Target Instrument, TimbreTron Generated Target Instrument), 67.5% of responses considered the musical pieces to be nearly identical or very similar, while roughly 22.5% considered them related and 10% considered them different. TAB2 .) Thus, it appears that generally the musical piece was indeed preserved.(2) Transferring the timbre. Evaluating this is challenging because, if the transfer is not perfect (which it is not), then judging similarity of not-quite-identical instruments is fraught with perceptual challenges. With this in mind, we included a range of pairwise comparisons and gave a likert scale with various anchors. Overall, we found that for the pair (Ground Truth Target audio, TimbreTron Generated audio), roughly 71.7% of responses considered the instrument generating the audio to be very similar (e.g. still piano, but a different piano) or similar (e.g. another string instrument). (More details in Table 2 .) We also asked participants to identify the instrument that they heard in some of the audio excerpts, with an open-ended question. Generally we found that participants were indeed able to either identify the correct instrument, or confused with a very similar-sounding instrument. For example, one participant described a generated harpsichord as a banjo, which is in fact very close to harpsichord in terms of timbre. As a reference, participants had similar reasonable confusions about identifying ground truth instruments as well (e.g., one participant described a real harpsichord as being a sitar). Based on perceptual evaluations above, we claim that TimbreTron is able to transfer timbre recognizably while preserving the musical content. Table 4 : AMT results on timbre quality comparisons between our proposed TimbreTron, TimbreTron but with STFT Wavenet and TimbreTron with STFT Griffin-Lim. Participants are asked: which one of the following two samples sounds more like the instrument provided in the target instrument sample?Comparing CQT vs. STFT To empirically test if our proposed TimbreTron with CQT representation is better than its STFT-Wavenet counterpart, or its STFT-GriffinLim counterpart, we conducted a human study using AMT. The original questionnaire can be found here 9 In the questionnaire, we asked Turkers to listen to three audio clips: the original audio from instrument A (the \\\"instrument example\\\"), the TimbreTron generated audio of instrument A, and its STFT conterparts, then asked them: \\\"In your opinion, which one of A and B sounds more like the instrument provided in 'instrument example\\\"'? , where A and B in the questions are the generated samples (presented in random order). Naturally, sounding closer to the \\\"instrument sample\\\" means the timbre quality is better. We conducted two groups of experiment. In the first group, the STFT counterpart is the Wavenet and CycleGAN trained on STFT representation and the result is in first row of the Table 4 : most people think the CQT TimbreTron is better. In the second group, we took the same CycleGAN trained on STFT, but instead simply generated the waveform using Griffin-Lim algorithm. The results are in the second row: Even more people think CQT TimbreTron is better. In conclusion, compared to Griffin-Lim as the baseline, training a Wavenet on STFT improved Timbre quality marginally. Furthermore, samples generated by TimbreTron trained on CQT was proven to have significantly better timbre quality. To better understand and justify each modification we made to the original CycleGAN, we conducted an ablation study where we removed one modification at a time for MIDI CQT experiment. (We used MIDI data for ablation because the dataset has paired samples, which provides a convenient ground truth for transfer quality evaluation.) FIG3 demonstrates the necessity of each modification for the success of TimbreTron. We presented the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer. We perform the timbre transfer in the time-frequency domain, and then reconstruct the inputs using a WaveNet (circumventing the difficulty of phase recovery from an amplitude CQT). The CQT is particularly well suited to convolutional architectures due to its approximate pitch equivariance. The entire pipeline can be trained on unrelated real-world music segments, and intriguingly, the MIDI-trained CycleGAN demonstrated generalization capability to real-world musical signals. Based on an AMT study, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and poly- In this section, we will briefly describe the main components of a musical tone: pitch, loudness and timbre BID33 .Pitch is described subjectively as the \\\"height\\\" of a musical tone, and is closely tied to the fundamental mode of oscillation of the instrument that is producing the tone. This oscillation mode is often called the fundamental frequency, and can often be observed as the lowest band in spectrogram visualizations ( FIG1 ).Loudness is linked to the perception of sound pressure, and is often subjectively described as the \\\"intensity\\\" of the tone. It roughly correlates with the amplitude of the waveform of the perceived tone, and has a weak dependence to pitch BID19 .Timbre is the perceptual quality of a musical tone that enables us to distinguish between different instruments and sound sources with the same pitch and loudness BID33 . The physical characteristics that define the timbre of a tone are its energy spectrum (the magnitude of the corresponding spectrogram) and its envelope. Since sounds generated by physical instruments mostly rely on oscillations of physical material, the energy spectra of instruments consist of bands, which correspond to (approximately) the integer multiples of the fundamental frequency. These multiples are called harmonics, or overtones, and can be observed in FIG1 . The timbre of an instrument is tightly related to the relative strengths of the harmonics. The spectral signature of an instrument not only depends on the pitch of the tone played, but also changes over time. To see this clearly, consider that a single piano note of duration 500 milliseconds is played in reverse -the resultant sound will not be recognizable as a piano, although it will have the same spectral energy. The envelope of a tone corresponds to how the instantaneous amplitude changes over time, and is mainly affected by the instrument's attack time (the transient \\\"noise\\\" created by the instrument when it is first played), decay/sustain (how the amplitude decreases over time, or can be sustained by the player of the instrument) and release (the very end of the tone, following the time the player \\\"releases\\\" the note). All these factors add to the complexity and richness of an instrument's sound, while also making it difficult to model it explicitly. Waveform to CQT Spectrogram Using constant-Q transform as described in Section 2.1, CQT spectrogram can be easily computed from time-domain waveforms. In this work, we use a 16 ms frame hop (256 time steps under 16kHz), \\u03c9 0 = 32.70 Hz (the frequency of C1 10 ), b = 48, k max = 336 for the CQT transform. Standard implementations of CQT (e.g., librosa (librosa)) also allow scaling the Q values by a constant \\u03b3 > 0 to have finer control over time resolution -choosing \\u03b3 \\u2208 (0, 1) results in increased time resolution. In our experiments, we choose \\u03b3 = 0.8. After the transformation, we take the log magnitude of the CQT spectrogram as the spectrogram representation. Waveform to STFT Spectrogram All the STFT spectrograms are generated using STFT with k max = 337. The window function is picked to be Hann Window with a window length of 672. A 16 ms frame hop is also used (256 time steps under 16kHz). Similar to CQT spectrogram, we also take the log magnitude of the STFT spectrogram as the spectrogram representation after the STFT. C DETAILED EXPERIMENTAL SETTINGS C.1 DATASETS MIDI Dataset Our MIDI dataset consists of two parts: MIDI-BACH 11 and MIDI-Chopin 12 . MIDI-BACH dataset is synthesized from a collection of bach MIDI files which have a total duration of around 10 hours 13 . Each dataset contains 6 instruments: acoustic grand, violin, electric guitar, flute, and harpsichord. We generated the audio with the same melody but different timbre, which makes it possible to obtain paired data during evaluation. Real World Dataset Our Real World Dataset comprises of data collected from YouTube videos of people performing solo on different instruments including piano, harpsichord, violin and flute. Each instrument contains around 3 to 10 hours of recording. Here is a complete list of YouTube links from which we collected our Real World Dataset. Note that we've also randomly taken out some segments for the validation set. For the conditional wavenet , we used kernel size of 3 for all the dilated convolution layers and the initial causal convolution. The residual connections and the skip connections all have width of 256 for all the residual blocks. The initial causal convolution maps from a channel size of 1 to 256. The dilated convolutions map from a channel size of 256 to 512 before going through the gated activation unit. The conditional wavenet is trained with a learning rate of 0.0001 using Adam optimizer BID25 , batch size of 4, sample length of 8196 (\\u2248 0.5s for audio with 16000Hz sampling rate). To improve the generation quality we maintain an exponential moving average of the weights of the network with a decaying factor of 0.999. The averaged weights are then used to perform the autoregressive generation. To make the model more robust, we augmented the training dataset by randomly rescaling the original waveform based on its peak value based on a uniform distribution unif orm(0.1, 1.0). In addition, we also added a constant shift to the spectrogram before feeding it into the WaveNet as the local conditioning signal; this shift of +2 was chosen to achieve a mean of approximately zero. During autoregressive generation, we perform a modified beam search where the global objective is to minimize the discrepancy between the target CQT spectrogram and the CQT spectrogram of the synthesized audio waveform. Our beam search alternates between two steps: 1) run the autoregressive WaveNet on each existing candidate waveforms for n steps (n = 2048) to extend the candidate waveforms, 2) prune the waveforms that have large squared error between the waveforms' CQT spectrogram and the target CQT spectrogram (beam search heuristic). We maintain a constant number of candidates (beam width = 8) by replicating the remaining candidate waveforms after each pruning process. To make sure the local beam search heuristic is approximately aligned with the global objective, we take n extra prediction steps forward and use the extra n samples along with the candidate waveforms to obtain a better prediction of the spectrogram for the candidate waveforms. The algorithm is provided in details as follows given the target spectrogram C target :1. k \\u2190 0 2. Perform 2n autoregressive synthesis step on WaveNet on {x 1 , \\u00b7 \\u00b7 \\u00b7 , x k } with m parallel probes (m is the beam width) to produce m subsequent waveforms: {x In our earlier attempts, we tried generating 4 seconds segments and then merge them back. However, this resulted in volume inconsistencies between the 4 second generations. We suspect the CycleGAN learned a random volume permutation, because essentially there's no explicit gradient signal against it from the discriminator, after we enabled volume augmentation during train time. To resolve this issue, we removed the size constraint in our generator during test time so that it can generate based on input of arbitrary length. At test time, the dataset is no longer 4 second chunks, instead, we preserved the original length of the musical piece(except when the piece is too long we cut it down to 2 minutes due to GPU memory constraint). During test time generation, the entire piece is fed into the CycleGAN generator in one shot. Figure 5 shows that the rough distribution of spectrograms are centered at -2. As is discussed in Section 3.1, we globally normalized our input data based oh the distribution of spectrograms for each domain of instruments.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"H1kMMmb0-\",\n          \"B14rPj0qY7\",\n          \"S1lvm305YQ\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"We use reinforcement learning to train an agent to solve a set of visual arithmetic tasks using provided pre-trained perceptual modules and transformations of internal representations created by those modules.\",\n          \"we proposed a new self-driving model which is composed of perception module for see and think and driving module for behave to acquire better generalization  and accident explanation ability. Presents a multitask learning architecture for depth and segmentation map estimation and the driving prediction using a perception module and a driving decision module. A method for a modified end-to-end architecture that has better generalization and explanation ability, is more robust to a different testing setting, and has decoder output that can help with debugging the model. The authors present a multi-task convolutional neural network for end-to-end driving and provide evaluations with the CARLA open source simulator showing better generalization performance in new driving conditions than baselines\",\n          \"We present the TimbreTron, a pipeline for perfoming high-quality timbre transfer on musical waveforms using CQT-domain style transfer. A method for converting recordings of a specific musical instrument to another by applying CycleGAN, developed for image style transfer, to transfer spectrograms. The authors use multiple techniques/tools to enable neural timbre transfer (converting music from one instrument to another) without paired training examples.  Describes a model for musical timbre transfer with the results indicating that the proposed system is effective for pitch and tempo transfer, as well as timbre adaptation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"TimbreTron: A WaveNet(CycleGAN(CQT(Audio))) Pipeline for Musical Timbre Transfer\",\n          \"RETHINKING SELF-DRIVING : MULTI -TASK KNOWLEDGE FOR BETTER GENERALIZATION AND ACCIDENT EXPLANATION ABILITY\",\n          \"StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 32,\n        \"min\": 30,\n        \"max\": 116,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          30,\n          116,\n          89\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"In this work, we address the question of creating machine learning systems that can be trained to solve such perceptuo-symbolic problems from a small number of examples. While the universal approximation theorem BID9 suggests that an architecturally homogeneous network such as a CNN should be able to solve any task when it is made large enough and given sufficient data, imposing model structure becomes important when one is aiming to capture human-like abilities of strong generalization and learning from small datasets BID16 .In particular, in this instance we would like to provide the learner with access to modules implementing information processing functions that are relevant for the task at hand -for example, modules that classify individual symbols in the image, or modules that perform symbolic computations on stored representations. However, it is not immediately clear how to include such modules in standard deep networks; the classifiers need to somehow be applied to the correct portion of the image, while the symbolic transformations need to be applied to the correct representations at the appropriate time and, moreover, will typically be non-differentiable, precluding the possibility of training via backpropogation. We evaluate our approach on a family of visual arithmetic tasks wherein the agent is required to perform arithmetical reduction operations on handwritten digits in an image. In Section 4 we present empirical results demonstrating the advantages of our approach as it applies to visual arithmetic, before reviewing related work in Section 5 and concluding with a discussion in Section 6. The interface is assumed to be coupled to the external world in a particular way; each time step E sends an observation to I, which potentially alters its state, after which I emits its own observation to the agent. When the agent responds with an action, it is first processed by I, which once again has the opportunity to change its state, after which I sends an action to E. The agent may thus be regarded as interacting with a POMDP C comprised of the combination of E and I. C's observation and action spaces are the same as those of I, its state is the concatenation of the states of I and E, and its dynamics are determined by the nature of the coupling between I and E. BID30 learn to control interfaces in order to solve purely algorithmic tasks, such as copying lists of abstractly (rather than perceptually) represented digits. We begin by describing the external environment E, before describing the interface I, and conclude the section with a specification of the manner in which E and I are coupled in order to produce the POMDP C with which the controller ultimately interacts. The salience network is pre-trained to output a salience map when given as input scenes consisting of randomly scattered EMNIST characters (both letters and digits). We believe this to be a reasonable assumption given that the interface is designed by the machine learning practitioner and consists of a collection of information processing modules which will, in most cases, take the form of computer programs that can be executed as needed. Our approach is most similar to the latter category, the main difference being that we have elected not to require the external computational medium to be differentiable, which provides it with greater flexibility in terms of the components that can be included in the interface. In fact, our work is most similar to BID30 , which also uses reinforcement learning to learn algorithms, and from which we borrowed the idea of an interface, the main difference being that we have included deep networks in our interfaces in order to tackle tasks with non-trivial perceptual components. Gaunt et al. FORMULA0 addresses a task domain that is similar to Visual Arithmetic, and makes use of a differentiable code-generation method built on top of TerpreT. Their work has the advantage that their perceptual modules are learned rather than being pre-trained, but is perhaps less general since it requires all components to be differentiable. At each node of the DAG, an outgoing edge is stochastically selected for traversal by a learned controller which takes the current features as input. A related idea is that of adaptive computation time; in the current work, all episodes ran for a fixed number of time steps, but it should be possible to have the controller decide when it has the correct answer and stop computation at that point, saving valuable computational resources. Using the Visual Arithmetic task domain as an example, we demonstrated empirically that the interface acts as a source of inductive bias that allows tasks to be solved using a much smaller number of training examples than required by traditional approaches.\",\n          \"For example, though addressed the driving direction selection problem, it showed poor generalization ability in unseen test town which has different map and building structure than training town's. For example, to solve a complex integration problem, compared with students without basic math knowledge, students who know about basic knowledge of math are able to learn the core of intergration more quickly and solve other similar integration problems instead of memorizing the solution of the specific problem. After the perception module was trained to have ability of pixel level understanding of its image input, we freezed the perception module weights and trained driving module with driving dataset. Some other approaches further atempted to create more robust models, for example, long short-term memory (LSTM) was utilized to make driving models store a memory of past BID5 .One problem is that aforementioned methods were tested in dissimlar driving scenerios using different driving dataset, thus it's hard to determine if model itself is the source of the better driving behavior instead of effectiveness of data BID22 . was tested in a public urban driving simulator and sucessed to tackle the ambigous action problem which refers as optimal driving action can't be inferred from perceptual input alone. Though saliency-map based visualization methods BID2 BID20 help understand the influence of input on final driving control, it's extremely hard to derive which module of the model fails when driving problems happen -If the model percepts incorrectly or the driving inference processes wrongly based on good perception information. The perception module is built with residual block proposed in BID12 which solves gradient vanishing and 'degradation problem', and it has a structure similar to Segnet BID1 prosposed for efficient image segmentation task. Besides, we constraint the total strides in encoder to 8 for keeping resolution of feature map, as large total stride has negative influence on feature map size reconstuction. To determine the limitation of RGB image, no other information such as current speed or steering angle were used as input. Before training our proposed model, two vital data processing methods were used: balancing dataset and data augmentation. For fair comparison, we use same driving dataset published by Conditional Imitation Learning ) except that we collected extra segmentation and depth maps in train town for training our proposed perception module. For a failed straight task in untrained town under untrained weather soft rain sunset as the driving model failed to move forward, we visualized outputs of segmentation and depth maps predicted by perception module. For traditional end-to-end learning driving methods BID3 it's impossible to reason as they don't focus on the cause explanation ability which is of great importance for practice use of deep learning driving models. Fine-tuneYosinski et al. (2014) , which refers to use other well-trained models weights on different target-related dataset as initial weights for training with target dataset instead of using weights initializing methods BID9 BID11 BID14 , is a common trick used in deep learning since empirically it could leads to better generalization on new target dataset. One possible reason is that the generalization ability lies in the perception module instead of the driving module, therefore when we train the perception module again with driving dataset, the ability of generating compressed multi-knowdege information is destoryed. Furthermore we conduct experiment on visualizing one direction of loss surface by projecting the loss surface to 2 dimension BID10 to investigate some qualitative explanation for this comparison result. One interesting result we acquired by comparing different train strategies is that the generalization ability of driving origins from basic knowledge and lies in weights of the perception module which should not be modified during training with driving dataset.\",\n          \"We introduce TimbreTron, a method for musical timbre transfer which applies \\u201cimage\\u201d domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. Modeling timbre is very hard, and it has been referred to as \\\"the psychoacoustician's multidimensional waste-basket category for everything that cannot be labeled pitch or loudness\\\" 2 . Being able to model and manipulate timbre electronically carries importance for musicians who wish to experiment with different sounds, or compose for multiple instruments. (Appendix A discusses the components of music in more detail.)In this paper, we consider the problem of high quality timbre transfer between audio clips obtained with different instruments. However, needing to convert the generated spectrogram into a waveform presents a fundamental obstacle, since accurate reconstruction requires phase information, which is difficult to predict BID10 , and existing techniques for inferring phase (e.g., BID16 ) can produce characteristic artifacts which are undesirable for high quality audio generation BID34 .Recent years have seen rapid progress on audio generation methods that directly generate high-quality waveforms, such as WaveNet BID41 , SampleRNN BID28 , and Tacotron2 BID34 ). The CQT (Brown, 1991) is another time-frequency analysis technique in which the frequency values are geometrically spaced, with the following particular pat- DISPLAYFORM1 Here, k \\u2208 {1, 2, 3, ...k max } and b is a constant that determines the geometric separation between the different frequency bands. Developed to minimize the mean squared error between the target spectrogram and predicted spectrogram, this algorithm is shown to reduce the objective function at each iteration, while having no optimality guarantees due to the non-convexity of the optimization problem BID16 BID37 Although recent developments in the field have enabled performing the inverse operation of CQT BID42 BID12 , these techniques still require both phase and magnitude information. WaveNet can be easily modified to perform conditional waveform generation; for example, it can be trained as a vocoder for synthesizing natural, high-quality human speech in TTS systems from low-level acoustic features (e.g., phoneme, fundamental frequency, and spectrogram) BID1 BID34 . Since individual notes contain information across many frequencies (due to their pattern of overtones), this combination of resolutions ought to allow simultaneous recovery of pitch and timing information for any particular note. (While this information is preserved in the signal, waveform recovery is a difficult problem in practice; this is discussed in Section 3.2).Another key feature of the CQT representation in the context of TimbreTron is (approximate) pitch equivariance. We conducted two sets of experiments to 1) experiment with pitch-shifting and tempo-changing to further validate our choice of CQT representation; 2) test our full TimbreTron pipeline (along with ablation experiments to validate our architectural choices). Regardless of the number of samples generated, the WaveNet synthesizer is able to produce the correct pitch based on the local frequency content. (See section 6.2 of the OneDrive folder) In conclusion, our method was able to vary the pitch and tempo independently while otherwise preserving the timbre and musical structure. In this section, we show our experimental findings on the full TimbreTron pipeline using real world data, verify the correctness of our reasoning about CQT, and show the generalization capability of TimbreTron. Both of these artifacts occurred in both WaveNet and Griffin-Lim reconstruction methods (See Table 4 ), which suggests that the source of the artifacts are likely to be from the CycleGAN stage of the pipeline. (Please listen to corresponding samples in section 6.3 of the OneDrive folder) This empirically demonstrates the effectiveness of the CQT representation compared with STFT.Generalizing from MIDI to Real-World Audio To further explore the generalization capability of TimbreTron, we also tried one domain adaptation experiment where we took a CycleGAN trained on MIDI data, tested it on the real world test dataset, and synthesized audio with Wavenet trained on training real world data. As is shown from the corresponding audio examples in this section 7 , the quality of generated audio is very good, with pitch preserved and timbre transfered. Overall, when we did have such a target domain recording of a real instrument, we found that for the pair of (Real Target Instrument, TimbreTron Generated Target Instrument), 67.5% of responses considered the musical pieces to be nearly identical or very similar, while roughly 22.5% considered them related and 10% considered them different. Participants are asked: which one of the following two samples sounds more like the instrument provided in the target instrument sample?Comparing CQT vs. STFT To empirically test if our proposed TimbreTron with CQT representation is better than its STFT-Wavenet counterpart, or its STFT-GriffinLim counterpart, we conducted a human study using AMT. In the first group, the STFT counterpart is the Wavenet and CycleGAN trained on STFT representation and the result is in first row of the Table 4 : most people think the CQT TimbreTron is better.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "def shuffle_list(text):\n",
        "    lst = text.split('. ')\n",
        "    lst[:-1] = [sentence + '.' for sentence in lst[:-1]]\n",
        "    random.shuffle(lst)\n",
        "    return ' '.join(lst)\n",
        "\n",
        "data['target'] = data['target'].apply(shuffle_list)"
      ],
      "metadata": {
        "id": "03zjLD42tXLy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "15ca60e9-1de9-492f-9a71-52fed4b1361d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "bae626e2-c6c9-482d-fdc9-d14086397ad3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "18477fc2-3952-479b-c440-ce2d582a344c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      shuffle=True,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        shuffle=True,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "90d909a7-0de3-447a-e98a-950bbda03908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(647, 8) (162, 8) (809, 8) (203, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenize data\n",
        "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# # Function in order to tokenize source and target\n",
        "# max_input_length = 1024\n",
        "\n",
        "# def tokenize_function(data):\n",
        "#   model_inputs = tokenizer(text=data['extractive_summary'], text_target=data['target'], max_length=max_input_length, truncation=True)\n",
        "#   return model_inputs\n",
        "\n",
        "# tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "VyuuPBgIsJim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "cc6bfb25-4858-49de-9b6c-a51d2d7feb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407,
          "referenced_widgets": [
            "b4d28b89ed4648cdbfcfa1dc2d08b7cb",
            "d2b2a5fe93f9415980f831986473d455",
            "3bd92aabd33a453890f6d77c0c9ce58e",
            "b38d08cbb29643628b5a57f18d8c07e0",
            "dbf959cfc5194f3f8631910922da9274",
            "87b05b5bf33f443c9f7f52b9e39ab4f1",
            "1124f43d738941eea0f1b2ee1ec54b40",
            "5e65538d1d0c49b9af091c304166612a",
            "33f64d780a6d45cc97f7b50f2255a6ba",
            "d2f3b7212371421fb43818c2680f8c2b",
            "813bd197893b4b819661b8775e72b314",
            "5f372eecdb7547d1a24daec8cd505b8f",
            "c87abf7642b84ae790b0e746646b28b4",
            "f7fcc25b1d4a415abd27212aa94d09e6",
            "44c2c811c62a4e94bcae4f14f27170a2",
            "3e2b8be15fcc49a2a4afc96956a1de1c",
            "320cab01ec50434b8a812793a37e06bd",
            "faeb3aa6f7664fe998b0b8ed0f51e6cd",
            "6ae179f184f6495d9c768305caeb2178",
            "ab0422df0fe74f169ea0ea6c2bef1a1a",
            "90760b25130b47d9b78007582b9a043d",
            "f2fe25adc28649b6823a1eea840b9b39",
            "f492ec3a35dd438081e6a57fc3d06eb8",
            "9513f84be238471088846023d5e6ad84",
            "d4340e9b05b6494796d94251ba269628",
            "09573227d7ba4a608af773bcb763a571",
            "ea19de9f580c4f4a904cb2f4bf744eab",
            "76ac0c0873234d93afcb930ccb93e3be",
            "0b9caf83693548c7a27a7b69d178b074",
            "c0c97540d5984a29ad5e14d930698000",
            "f77d62be823a47d3acce116e46e86318",
            "b6b266ce44d54509a0201847761d00cc",
            "2f97b2a1032942c9a15e4d716d7ec9d6",
            "69941b69a1e244989afcf0e53aa2d4a2",
            "18bc9f8a06d140cb8d50e2ca81c8b731",
            "86819f5875854efcbbb5c151c06692fe",
            "270b8e1b56da456fa45b67aab3ea5aab",
            "7183b140556d430e8b785391603a643e",
            "468b186d3ce8438abd033520d23e63e3",
            "2fece72c7e06476fab3cb917c66deaf3",
            "51c8162823bd494aa02f3ff0d7ef7404",
            "afdb3d34f3264d0f8ad1b6244edec1af",
            "a8f5258623e948c28535e88a49cd5ccb",
            "ac67f89ed3cc4a05a6a4b1d48abc2a27",
            "7a7371cf82074d2db890323fc76aacb2",
            "743aa9e8a1e941e7b4e3fe657ed0acff",
            "6caf976daaa44b46bf2a49511c835d73",
            "eb24ecc8196a4537b8cbd46491d14af2",
            "85dad04b66c642e099fdf97b57035633",
            "2498e547c967491e809adbb3f07ae203",
            "e2f2e58c939345aca2fac6d087a8c6ba",
            "d9364cc4e3a849b888d661b23d8ce77b",
            "8d60b054454543e987b45cd1721ba735",
            "20213a2ac43b4708ab4c67630f9afe8e",
            "c436cda1061245ad8dbf9e0cf191a0ee",
            "4cfc504e4ded48e98e12e825fbca1041",
            "282d947990f644509171a568a31498dc",
            "2f1ed1e53c4d44a3a23cfd1a29fbb4f8",
            "73cc9fb106554137b1d6315cce7c5a7e",
            "b99a4e7dc8234a4c9bf14ecd11328c7c",
            "f48d93091a4942b6b6148ead197fadf9",
            "ffac937b9a634ef49dd57df0dca106aa",
            "a080cede88b8402d88f59e1b0b31fee6",
            "f5c646dcd6ca445ab40d1286fb3b8f8f",
            "1ce2243e57984549bf2eb04829eb2e5b",
            "25f58819cdf7432ebf55e15b3207eb6e",
            "30200b78063e4e1c9ec387156fa1a14b",
            "9b73afe522ac451088745530d61f89fb",
            "752f6f030fc549bb9e8151cafbd56712",
            "9cbfe0e9a50d4058a3369c586ebc472d",
            "5200594676784c08b8cf0f23e10d30bc",
            "8e4ae7851f9d42859febfc2a4c7b99ee",
            "35bc7f44805a449f850879134d8bdfc6",
            "e9bb9601314849559d013ebf13c77862",
            "6eabb24ff5a34451b68a4c9a5b18ca51",
            "f91d15c638f54ae2970720ef37b565e4",
            "031ddb2105334b04bef6d4bcc5d4371f"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4d28b89ed4648cdbfcfa1dc2d08b7cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f372eecdb7547d1a24daec8cd505b8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f492ec3a35dd438081e6a57fc3d06eb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69941b69a1e244989afcf0e53aa2d4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a7371cf82074d2db890323fc76aacb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cfc504e4ded48e98e12e825fbca1041"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30200b78063e4e1c9ec387156fa1a14b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = True\n",
        "model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "# model.generation_config.renormalize_logits = True\n",
        "\n",
        "model.config.attention_dropout = 0.1\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "name_model = 'sampling-norep-v3/'\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "2160793f-5100-4075-8d55-b3b4c91ed2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXfD8z7vdqC",
        "outputId": "90204fd9-098e-49f2-905d-a0c38187493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartConfig {\n",
              "  \"_name_or_path\": \"facebook/bart-base\",\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"gelu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"BartModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.1,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 768,\n",
              "  \"decoder_attention_heads\": 12,\n",
              "  \"decoder_ffn_dim\": 3072,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"dropout\": 0.1,\n",
              "  \"early_stopping\": true,\n",
              "  \"encoder_attention_heads\": 12,\n",
              "  \"encoder_ffn_dim\": 3072,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_position_embeddings\": 1024,\n",
              "  \"model_type\": \"bart\",\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": true,\n",
              "  \"num_beams\": 4,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"scale_embedding\": false,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 128,\n",
              "      \"min_length\": 12,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_cnn\": {\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 142,\n",
              "      \"min_length\": 56,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_xsum\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 62,\n",
              "      \"min_length\": 11,\n",
              "      \"num_beams\": 6\n",
              "    }\n",
              "  },\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset to inspect the batches\n",
        "for batch in train_dataset.take(100):  # Take the first batch for inspection\n",
        "    print(batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CsKTRNhvqCQ",
        "outputId": "38adf07c-d213-4243-e990-5af2b6feae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3376,  5969, ...,     1,     1,     1],\n",
            "       [    0, 44891,     7, ...,     1,     1,     1],\n",
            "       [    0,     6,   992, ...,    81, 14307,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 39936, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4195, ..., 28695,     5,     2],\n",
            "       [    0, 13863,    89, ...,     1,     1,     1],\n",
            "       [    0, 46797,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16991,     9, ...,     1,     1,     1],\n",
            "       [    0,  9690, 16894, ...,  5342,  2222,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 26039, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 15243,   484, ...,     1,     1,     1],\n",
            "       [    0, 21119,  4945, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  6680, ...,     1,     1,     1],\n",
            "       [    0,   170,  3608, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 29235, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 16215, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 47380, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1106,   215, ...,     8,  1850,     2],\n",
            "       [    0,  3972, 22016, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,   170,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 47302, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 43043, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 33731,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,  4340, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,  6448, ...,     1,     1,     1],\n",
            "       [    0,   387, 35948, ...,     1,     1,     1],\n",
            "       [    0,  1121,   937, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 39231, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   717, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   846,    12, ...,     1,     1,     1],\n",
            "       [    0, 10105,     9, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    28, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 17629, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 47032, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6373, ...,     1,     1,     1],\n",
            "       [    0, 46874,  2088, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 43123, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,  3854,     9,     2],\n",
            "       [    0,   170,    67, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13360, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 13033, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  5709, ...,   230,     6,     2],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,  8269, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709, 25342, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0,  2522,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0, 3684, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  4528,   426, ...,     1,     1,     1],\n",
            "       [    0,   250,   864, ...,     1,     1,     1],\n",
            "       [    0,   170,  2807, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  9437, ...,     1,     1,     1],\n",
            "       [    0, 40450,  9097, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3084, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46498, ...,     1,     1,     1],\n",
            "       [    2,     0, 17105, ...,     1,     1,     1],\n",
            "       [    2,     0, 46444, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,    41, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     5, 14612,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 5320, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9355, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42158, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  6243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 3762,    9, ...,    1,    1,    1],\n",
            "       [   0, 3762,  169, ...,    1,    1,    1],\n",
            "       [   0,  713,   34, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  170, 1455, ...,    1,    1,    1],\n",
            "       [   0,  170,  109, ...,    1,    1,    1],\n",
            "       [   0, 5975,  272, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 42578, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   534, ..., 37357,     5, 23341],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 18377,     5, ...,     1,     1,     1],\n",
            "       [    0, 39936,  1364, ...,     1,     1,     1],\n",
            "       [    0,   133,  4472, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   713,   892, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1966, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 14563, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 30597, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133, 30673, ...,     1,     1,     1],\n",
            "       [    0,   170, 33461, ...,     1,     1,     1],\n",
            "       [    0, 49111,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   243,    16, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42274, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 46692, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   216, ...,     1,     1,     1],\n",
            "       [    0,  9058,  1537, ...,  3854,  6533,     2],\n",
            "       [    0,  2522, 15491, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   510,  8631, ...,     1,     1,     1],\n",
            "       [    0, 45461,  6448, ...,     1,     1,     1],\n",
            "       [    0, 27728,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 46011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 41084, ...,     1,     1,     1],\n",
            "       [    2,     0,   113, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   386, ...,     1,     1,     1],\n",
            "       [    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,   713,  1639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    64, ...,     1,     1,     1],\n",
            "       [    0,   565, 26582, ...,     1,     1,     1],\n",
            "       [    0,  4528,  6448, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1],\n",
            "       [    2,     0,  8532, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,   133,  2731, ...,   141,  1365,     2],\n",
            "       [    0,  5771,   258, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 14246, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       [    0,   170,   492, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 34447, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 48293,  1836, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,  5428, 22098,     2],\n",
            "       [    0,   133,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       [    2,     0, 47744, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12592, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,    12,   170, ...,     1,     1,     1],\n",
            "       [    0,   713,   173, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,  1779,    89, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 40089, 25373, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   448,  7629, ...,     1,     1,     1],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1106,    52, ...,    33,  4163,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 25077, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  2765, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1106,    52, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 45288,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133,   434, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     7,  1807,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0, 9167, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  1197, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   717,  6486, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 33837, 10518, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 18522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,     5, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  7939,  1423, ...,  3278,    63,     2],\n",
            "       [    0,   133,   335, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42489, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 28062, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,   173, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,  1296,   114,     2],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2847,     6, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11321, 20237, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 34647, ...,     1,     1,     1],\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,    52, ...,     1,     1,     1],\n",
            "       [    0, 21461,    11, ...,     1,     1,     1],\n",
            "       [    0,  2765,  4655, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  4442, ...,     1,     1,     1],\n",
            "       [    0, 45408, 19047, ...,     1,     1,     1],\n",
            "       [    0,   713,  1548, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 6179, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0, 2709, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 38386,    10, ...,     1,     1,     1],\n",
            "       [    0,  4528, 15716, ...,     1,     1,     1],\n",
            "       [    0,   448,    36, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1850, ...,     1,     1,     1],\n",
            "       [    0, 35416,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 14484, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  9344, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3813,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,     5, ...,     1,     1,     1],\n",
            "       [    0, 44863,  1319, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,    84, ...,     1,     1,     1],\n",
            "       [    0,   133,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36984, ...,     1,     1,     1],\n",
            "       [    2,     0, 20930, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,   819, 21154,     2],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1106,   854, ...,     1,     1,     1],\n",
            "       [    0,  1213,    67, ...,  4091, 48981,     2],\n",
            "       [    0,   170,   694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   102, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 34788, ...,     1,     1,     1],\n",
            "       [    2,     0, 35660, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0, 1121,  171, ...,  347,   12,    2],\n",
            "       [   0, 1121, 1285, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  713,   16, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170, 9637, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 33020, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42713,     6, ...,     1,     1,     1],\n",
            "       [    0,  4771,  3109, ...,     1,     1,     1],\n",
            "       [    0, 44908,  4843, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35716,    87, ...,    11, 37365,     2],\n",
            "       [    0,   133, 39135, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13755, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  5393, ...,     1,     1,     1],\n",
            "       [    0,  1121,  6477, ...,     1,     1,     1],\n",
            "       [    0,   243,    64, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 30872,   724, ...,     1,     1,     1],\n",
            "       [    0, 12444,   857, ...,     1,     1,     1],\n",
            "       [    0,  9690,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  5448, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1],\n",
            "       [    0, 18377,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,  9157, 16771, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,  6647, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   102, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43253, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 30770,     6, ...,     1,     1,     1],\n",
            "       [    0,   170, 17013, ...,     1,     1,     1],\n",
            "       [    0,   133,  1850, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1],\n",
            "       [    0,   133, 13477, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 23996, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4286, ...,     1,     1,     1],\n",
            "       [    0, 44311,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,   817, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5383, 38416, ...,     1,     1,     1],\n",
            "       [    0,  1779,  3563, ...,     9,   230,     2],\n",
            "       [    0, 13863,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  1109, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2765, 24097, ...,     1,     1,     1],\n",
            "       [    0, 23055,  8738, ...,     1,     1,     1],\n",
            "       [    0,  2522,  5694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 18776, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40103, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 14721,  9179, ...,     1,     1,     1],\n",
            "       [    0,   170,  6053, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0, 13863,  3326, ...,    16,   888,     2],\n",
            "       [    0, 43872,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42124, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0,  250, 4819, ...,    1,    1,    1],\n",
            "       [   0, 2709, 4327, ...,    1,    1,    1],\n",
            "       [   0,  713,  173, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  133, 5849, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170,  311, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 24514, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42489, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,  2655, 20992,     2],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 43195,  7651, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,  1236,    15,     2],\n",
            "       [    0,  1121,  1524, ..., 45371,    15,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44188, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38416, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0, 13863,    51, ...,     1,     1,     1],\n",
            "       [    0,  3762,  1860, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 40846, ...,     1,     1,     1],\n",
            "       [    0,  1620,    52, ...,     1,     1,     1],\n",
            "       [    0,  5771,   171, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 13360,    12, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 27477, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43780, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0, 39936,   775, ...,    52,    33,     2],\n",
            "       ...,\n",
            "       [    0,  1342,  4458, ...,     1,     1,     1],\n",
            "       [    0,  3908,     5, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ..., 19282,     6,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 36949,    41, ...,     1,     1,     1],\n",
            "       [    0,  4688,   419, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,    42, ...,   775,    36,     2],\n",
            "       [    0,  9344,  1938, ...,     1,     1,     1],\n",
            "       [    0,   170,  7015, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 1694, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  5203, ...,     1,     1,     1],\n",
            "       [    0, 39531,  4400, ...,     1,     1,     1],\n",
            "       [    0, 45297,    15, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 15491, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 36542, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0,  3908,  2284, ...,   922,  4791,     2],\n",
            "       ...,\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 30597, 10244, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 48313, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   173, ...,     1,     1,     1],\n",
            "       [    0, 40566,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  1548, ...,     1,     1,     1],\n",
            "       [    0, 23271,     9, ..., 42472, 26070,     2],\n",
            "       [    0, 48454,    12, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41933, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   170,   240, ...,     1,     1,     1],\n",
            "       [    0, 43714,    40, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0,   133,   485, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  9685, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45336, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   486, ...,     1,     1,     1],\n",
            "       [    0,   133, 28894, ...,     1,     1,     1],\n",
            "       [    0, 38386,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   970,    16, ...,  3364,     5,     2],\n",
            "       [    0,   713, 12360, ...,     1,     1,     1],\n",
            "       [    0,  1121,   485, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  1034, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0, 29182,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0, 18377,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,   170,  2883, ...,     1,     1,     1],\n",
            "       [    0,   170,  2639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  5848, ...,    36, 13424,     2],\n",
            "       [    0, 44863,    31, ...,     1,     1,     1],\n",
            "       [    0, 48684,   680, ..., 20145,  4007,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 49360,    11, ...,  6068,   600,     2],\n",
            "       [    0, 45942,  6448, ...,     1,     1,     1],\n",
            "       [    0, 30383, 26713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 41542,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1285, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45356, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40884, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1213,  1157, ..., 20910,    73,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0,  1779,  1058, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 39972,    52, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 28588, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     1,     1,     1],\n",
            "       [    0, 20319,  2408, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   800, ...,     1,     1,     1],\n",
            "       [    0,  2709,    55, ...,     1,     1,     1],\n",
            "       [    0, 10653,   428, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 243, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    97, ...,     1,     1,     1],\n",
            "       [    0,  4528, 41885, ...,     1,     1,     1],\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,   170,  1455, ...,     1,     1,     1],\n",
            "       [    0,  1620,    41, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10127, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   243, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250,  1353, ...,     1,     1,     1],\n",
            "       [    0,   713,  3315, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  1989, ..., 14612, 26070,     2],\n",
            "       [    0,  3972,  1100, ...,     1,     1,     1],\n",
            "       [    0,  5320, 10074, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44466, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44863, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   133, 32809, ...,     1,     1,     1],\n",
            "       [    0,     6,  3023, ...,  1558, 15421,     2],\n",
            "       ...,\n",
            "       [    0, 10653,   428, ...,     1,     1,     1],\n",
            "       [    0, 20861, 44871, ...,     1,     1,     1],\n",
            "       [    0,   713,   839, ...,     8,    63,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   347, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 48455, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,     1,     1,     1],\n",
            "       [    0, 21438,   520, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522, 11909, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,   133, 16681, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,   936, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  4528,  8369, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 27331,   937, ...,     1,     1,     1],\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 21680, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  6209,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 15393, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0, 20086, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3972,     5, ...,     1,     1,     1],\n",
            "       [    0,   170, 24934, ...,     1,     1,     1],\n",
            "       [    0,  2522, 39030, ...,  3907,     4,     2],\n",
            "       ...,\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       [    0,   170,   892, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     6,   549,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   176, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 47515, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45566, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   574,  3439, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   163, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,    43,   396,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133, 15306, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ..., 12612,   534,     2],\n",
            "       [    0,  3972,  1306, ...,     1,     1,     1],\n",
            "       [    0, 19847,  1239, ...,  6315, 36173,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4993, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 46159, 43141, ...,     1,     1,     1],\n",
            "       [    0, 10777,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121, 14117, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 13863,  1337, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42200,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 565, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 28084,     7, ...,     1,     1,     1],\n",
            "       [    0,  1121,   144, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 38416,    29, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,  1121,  5709, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 25382, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,   511, 22772,     2],\n",
            "       [    0,  3762,     9, ...,     1,     1,     1],\n",
            "       [    0,   133,   986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   172, ...,     1,     1,     1],\n",
            "       [    0,  3972, 33942, ...,   892,  2939,     2],\n",
            "       [    0, 45875,     6, ...,    31,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 1121, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  717, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 15393, ...,    11,   130,     2],\n",
            "       [    0, 20867,  7316, ...,     1,     1,     1],\n",
            "       [    0,   713,  5665, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  9058, 24454, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44426, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771, 10364, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,   209, ...,     1,     1,     1],\n",
            "       [    0,   133,  8611, ...,     1,     1,     1],\n",
            "       [    0,  3972, 19893, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44908, ...,     1,     1,     1],\n",
            "       [    2,     0, 34002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 19186, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23803,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133,   538, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0,  5771,   144, ...,     1,     1,     1],\n",
            "       [    0,  2765,  2623, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 19163, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  3034, ...,     1,     1,     1],\n",
            "       [    0,  1121,  2171, ...,     1,     1,     1],\n",
            "       [    0,  3972,     5, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0, 17425, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   448, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   387,   293, ...,    16,   505,     2],\n",
            "       [    0, 21518,  1537, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 45628, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2409,   114, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288,  8150, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,    45,   946,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,   211, ...,   683,    36,     2],\n",
            "       [    0,   250,   194, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1620,   251, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,   740,     6,     2],\n",
            "       [    0,  1121,   103, ...,   255,     8,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 28062, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17312, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38741, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 35166, 37700, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,     1,     1,     1],\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 24989,  7373, ...,     1,     1,     1],\n",
            "       [    0,  9157, 37794, ...,     1,     1,     1],\n",
            "       [    0,   717,  4182, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10928, ...,     1,     1,     1],\n",
            "       [    2,     0, 15622, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4897, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   387,   293, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771,   419, ...,   468,   321,     2],\n",
            "       ...,\n",
            "       [    0,   530,   495, ...,     1,     1,     1],\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,  2522, 40150, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 9685, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4554, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11913, 26739, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35490,     5, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       [    0,   713,  5044, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   338, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,  1365, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,  1121,   171, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 48812,  2577, ...,    14, 20070,     2],\n",
            "       [    0,  5771,   608, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,    14,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 10516, ...,     1,     1,     1],\n",
            "       [    0,   713,  1421, ...,   163,  2688,     2],\n",
            "       [    0,  3762,    16, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,   133,  2270, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12444, ...,     1,     1,     1],\n",
            "       [    2,     0, 22011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250, 17309, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9322, ...,     1,     1,     1],\n",
            "       [    0,  3762,   169, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288, 15380, ...,     1,     1,     1],\n",
            "       [    0,   133,  7626, ...,     1,     1,     1],\n",
            "       [    0,   170,   304, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 26412, ...,     1,     1,     1],\n",
            "       [    2,     0, 46101, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,    10, ...,   204,     6,     2],\n",
            "       [    0,  9690,  1202, ...,     1,     1,     1],\n",
            "       [    0,  7605,   209, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2571,  6018, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   250, 31809, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23295, 37465, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3762, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1620,    10, ...,     1,     1,     1],\n",
            "       [    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0,   170,    40, ..., 13956,  1916,     2],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 19192,    52, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(7, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  1000, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "96a37571-a85b-46d0-822c-15337ad2a4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_model"
      ],
      "metadata": {
        "id": "dyGROt7TwXn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "fec0f7cd-d17d-4c56-fb64-c523f8dc25b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n",
            "81/81 [==============================] - 3557s 44s/step - loss: 3.8539 - val_loss: 3.3430 - rouge1: 38.3698 - rouge2: 10.1688 - rougeL: 22.2593 - rougeLsum: 31.8182 - gen_len: 88.4506\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3040s 38s/step - loss: 3.4917 - val_loss: 3.2807 - rouge1: 40.0500 - rouge2: 10.8711 - rougeL: 22.9560 - rougeLsum: 33.0063 - gen_len: 82.0000\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3153s 39s/step - loss: 3.3288 - val_loss: 3.2417 - rouge1: 39.2427 - rouge2: 10.4373 - rougeL: 22.8725 - rougeLsum: 32.7607 - gen_len: 82.4815\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3104s 39s/step - loss: 3.1818 - val_loss: 3.2276 - rouge1: 40.0325 - rouge2: 11.1820 - rougeL: 23.2123 - rougeLsum: 33.2422 - gen_len: 84.0309\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3291s 41s/step - loss: 3.0478 - val_loss: 3.2148 - rouge1: 40.4019 - rouge2: 10.9217 - rougeL: 23.2547 - rougeLsum: 33.4422 - gen_len: 85.5432\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3194s 40s/step - loss: 2.9362 - val_loss: 3.2262 - rouge1: 39.8779 - rouge2: 10.3569 - rougeL: 22.9050 - rougeLsum: 32.9253 - gen_len: 81.9753\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3124s 39s/step - loss: 2.8306 - val_loss: 3.2327 - rouge1: 40.3397 - rouge2: 10.9453 - rougeL: 23.1704 - rougeLsum: 33.6539 - gen_len: 83.5309\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3146s 39s/step - loss: 2.7277 - val_loss: 3.2288 - rouge1: 39.9447 - rouge2: 10.5412 - rougeL: 22.8769 - rougeLsum: 33.1718 - gen_len: 85.0062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "f09db19d-85fd-44b8-d76e-3ee5168e9405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c25434ec-c808-43da-b3df-579bfce352b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "699df823-a6bf-4496-f7a2-78909e82cba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {'sampling-norep-v0' : 'sampling-norep-v0/',\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              #  'greedy-norep-v3' : 'greedy-norep-v3/',\n",
        "               'sampling-norep-v3' : 'sampling-norep-v3'}\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Tjj2cjL2mE",
        "outputId": "184d644e-2892-4f36-e503-667824846e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v0/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2,\n",
            "  \"max_length\": 150,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.2,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'\n",
        "save_path = save_paths[name]\n",
        "\n",
        "with open(save_path + '/training_history.json', 'r') as file:\n",
        "    loaded_history = json.load(file)\n",
        "\n",
        "H = History()\n",
        "H.history = loaded_history\n",
        "\n",
        "\n",
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/figure.png')"
      ],
      "metadata": {
        "id": "CEUKdl4cdUPx",
        "outputId": "dd6c46b3-440c-4144-9a18-40cbf4278f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'"
      ],
      "metadata": {
        "id": "AQseeydBMTlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:5],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][6:8],\n",
        "                                        generation_config=model.generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "DXuVco4oyC_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j6DcwmJShR",
        "outputId": "f366e627-2a4f-4cae-9161-273836d3ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet framework for in-and out-of-distribution classification. This paper proposes a variational approach to solve the uncertainty estimation problem in deep neural networks by considering the label-level distribution of image input and output labels. The authors propose a new uncertainty metric for deep neural network classification that is more robust than existing uncertainty measures. This article proposes a novel variational method for solving uncertainty estimation on deep neural nets.',\n",
              " 'An unsupervised method for analyzing the contribution of individual neurons to NMT models. This paper proposes a novel approach to interpret language pairs using neural networks, and proposes a new method for measuring the contributions of each neuron to the model. The authors propose a novel method for learning languages from neural networks that can be used to control the translation performance of language pairs.',\n",
              " 'A deep diagonal-circulant ReLU network that can be decomposed into products of diagonal and circulant matrices This paper proposes to replace the weight matrix of a fully connected layer with a new type of matrix. The authors propose a method for building deep ReLU networks based on a combination of diagonal/circular matrices with low rank approximators in order to improve performance.',\n",
              " 'Explicit cognitive theory or analogy-like computation in neural networks This paper proposes a novel approach to solving complex examples of visual and symbolic representations. The authors propose an approach to the problem of visual analogy by proposing a new model that learns to contrast abstract relational structures with visual representations. This work proposes a method for learning to compare different representations of objects, which can be used to solve complex analogical problems such as visual analogy.',\n",
              " 'A novel concept annotation task for medical time series data. This paper proposes a novel method of predicting and localizing medical concepts by modeling the medical context data as input. The authors propose a novel approach to the problem of identifying medical concepts in medical time-series data, which can be used to predict and localize medical concepts. This article introduces a novel framework for understanding medical concepts using medical context information.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMr_neO6pJNK",
        "outputId": "25d85c44-b7e6-4820-927c-54279cb4c854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The study proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This study proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The article investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TReOxk1fk1cx",
        "outputId": "eea11b79-2cc4-44e3-9121-a817e0d65fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The article proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This article proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = False\n",
        "model.generation_config.temperature = 1\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "                                          tokenizer.convert_tokens_to_ids('ƒ†propose'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†proposes'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "\n",
        "model.generation_config.num_beam_groups = 4\n",
        "model.generation_config.diversity_penalty = 0.7\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.3\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r4mAa8_-ql",
        "outputId": "a4abf904-55d4-42b6-9cbe-7b5095a3ea61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"diversity_penalty\": 0.7,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.3,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    15393,\n",
            "    21037,\n",
            "    32687\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('they proposes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EysouFNL26dN",
        "outputId": "decc56fd-e82f-48df-89e2-9b621383fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['they', 'ƒ†proposes']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisXx8nuVDQ4",
        "outputId": "f70b8744-cbe0-4319-fc58-4bc80c0ff137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: 0/202\n",
            "[INFO]: 10/202\n",
            "[INFO]: 20/202\n",
            "[INFO]: 30/202\n",
            "[INFO]: 40/202\n",
            "[INFO]: 50/202\n",
            "[INFO]: 60/202\n",
            "[INFO]: 70/202\n",
            "[INFO]: 80/202\n",
            "[INFO]: 90/202\n",
            "[INFO]: 100/202\n",
            "[INFO]: 110/202\n",
            "[INFO]: 120/202\n",
            "[INFO]: 130/202\n",
            "[INFO]: 140/202\n",
            "[INFO]: 150/202\n",
            "[INFO]: 160/202\n",
            "[INFO]: 170/202\n",
            "[INFO]: 180/202\n",
            "[INFO]: 190/202\n",
            "[INFO]: 200/202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFgiii-sbTK4",
        "outputId": "41684db4-b656-4232-d3b5-51f1c72b8369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    170\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"early_stopping\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"repetition_penalty\": 1.8,\n",
              "  \"suppress_tokens\": [\n",
              "    1698,\n",
              "    32687\n",
              "  ]\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bNZCUqSvN7NV",
        "outputId": "bc3472f2-1429-4206-dcf8-b0d504c53db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/greedy-norep-v5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "beb7244c-9a96-46aa-e5a8-66046cfd17bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       1.420792\n",
              "std       18.729447\n",
              "min      -39.000000\n",
              "25%      -12.000000\n",
              "50%        0.500000\n",
              "75%       14.000000\n",
              "max       53.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 6\n",
        "print('ORIGINAL:' + tokenized_data['test']['target'][i])\n",
        "print('FINE TUNED MODEL:' + tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[i])\n",
        "print('PRETRAINED MODEL:' + tokenizer.batch_decode(pretrained_generated_ids, skip_special_tokens=True)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzU40R-ALuzq",
        "outputId": "7e6ad700-beac-4a0d-f100-174353d2d63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL:OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work\n",
            "FINE TUNED MODEL:Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. We show that Outlier Exposure can improve calibration performance in this realistic setting.\n",
            "PRETRAINED MODEL:However, when there is a distribution mismatch, deep neural network classifiers tend to give\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 28\n",
        "print(summaries.iloc[m, 0])\n",
        "print(summaries.iloc[m, 1])"
      ],
      "metadata": {
        "id": "lcWyZXvGLKcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1428815a-8c86-41c0-b4c1-7c39c492acdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space.  This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.\n",
            "We extend the reinforcement learning paradigm to a d-dimensional hypercube and show that quantile regression is capable of training orders of magnitudes faster in high dimensional metric spaces. This paper proposes a method to train a deep neural network to approximate the quantile function of the optimal action distribution. The authors propose a new reinforcement learning algorithm to train convolutional neural networks with quantile functions, showing that it can be used to train orders of magnitude faster on vector rewards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "-_h7Z8KBp09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small', errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 512\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "MwmAPTxGp28R",
        "outputId": "b0adadd8-2ef7-4f4d-fa30-3a37b3295996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "366ed1d70d544e9c97248005697acad0",
            "b9e07ac6d7204fbbb1fcf9fb606b8456",
            "334c8f7e157c4bc29f4ae54b987bb6d5",
            "51b0ab51368142b0ad36c64500c15d94",
            "bbbcd5af65134593a1c299578d1a006f",
            "6925772cebe94ce1a9af6a219ebf5963",
            "b705632920d94c3aa0d57693e9557b71",
            "1f36d8adf0d24c9e986dab2f91b79ec1",
            "200e3ebc520e4569a71a465600efffe4",
            "d4f6aea3b3af41e794f954dc06fc72b9",
            "54e610be50994c32a65444a4ac1329ab",
            "91a33594f8744bd5a167b0372fb466ff",
            "2518909058fe4c5ba1667fba2550520d",
            "27d7bd9d1e0444a9b6d2991561e4c532",
            "cdcf83dda1c34a58bc9e74f5d4546527",
            "71a3ce69a4814716a6e8cf15f8485117",
            "6c1cbd0557b44d2eb775007ef9b3f99e",
            "87a69decb29c4ba4b5aaef30bd4254ba",
            "54e46a897c5c4c368a0889f9b44cda7d",
            "99dcefb79f8540fabe1705701731fd1a",
            "a5538e38f4794548a8105d9182f7a369",
            "560e644845a94532a714972fe6618781",
            "8d6970f032224c209be6654fa611ef88",
            "43e88319206440a289a51a00d4088603",
            "6bd3860856ce42008255450d10bc4ffd",
            "c1088e3863ab4a9f9bccd1c44abae1ea",
            "cab0ac902d9543fda41c351485b96090",
            "c1a35a0519954256a29ba63f1a8cd1ed",
            "9609bbe856704d4fb5e80f37b21aca8b",
            "ee1ad541e1864693803e974de2fb19d3",
            "272ed0d510664ba8bc3c4f1895d959e2",
            "6066e23bd59845e997efe24a09227157",
            "7aa4e27aa26245bcb6548d64a8e5b304"
          ]
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "366ed1d70d544e9c97248005697acad0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91a33594f8744bd5a167b0372fb466ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d6970f032224c209be6654fa611ef88"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "\n",
        "model.generation_config.max_length = 100\n",
        "model.generation_config.min_length = 80\n",
        "model.generation_config.num_beams = 4\n",
        "model.generation_config.length_penalty = 2.0\n",
        "\n",
        "\n",
        "# model.generation_config.do_sample = True\n",
        "# model.generation_config.temperature = 0.5\n",
        "bad_words = ['We', 'we', \"propose\", 'authors']\n",
        "model.generation_config.bad_words_ids=tokenizer(bad_words, add_special_tokens=False).input_ids\n",
        "# model.generation_config.suppress_tokens = [\n",
        "#     # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "#                                           # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "name_model = 'greedy-norep-v1/'\n",
        "\n",
        "print(model.config)\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "_mad6Hc1ttRX",
        "outputId": "747bb75c-2cc5-4b3a-d6eb-25656f324f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"google-t5/t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    1326\n",
            "  ],\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"diversity_penalty\": 0.5,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 100,\n",
            "  \"min_length\": 80,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 1.8\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "6XdKgWLNwgRD"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "6PGwmRZ2yS78"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yvPZ6vubyYl1",
        "outputId": "804c7763-ec26-4a51-e596-b19fef0ab8d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  16449536  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  35330816  \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  41625344  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60506624 (230.81 MB)\n",
            "Trainable params: 60506624 (230.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + name_model"
      ],
      "metadata": {
        "id": "HceVCymny0eR"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "p_wTT5KMyi86"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 12\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "VeSspMbVyp_q",
        "outputId": "2b5ea941-476c-4345-c9db-012df4dc305e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7d710758dfc0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7d710758dfc0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - 1631s 19s/step - loss: 4.3709 - val_loss: 2.4068 - rouge1: 29.8819 - rouge2: 5.2172 - rougeL: 16.7945 - rougeLsum: 24.2270 - gen_len: 92.5802\n",
            "Epoch 2/12\n",
            "81/81 [==============================] - 1552s 19s/step - loss: 2.5377 - val_loss: 2.2036 - rouge1: 30.2248 - rouge2: 5.4157 - rougeL: 16.9600 - rougeLsum: 24.1914 - gen_len: 90.7284\n",
            "Epoch 3/12\n",
            "81/81 [==============================] - 1686s 21s/step - loss: 2.2209 - val_loss: 2.1401 - rouge1: 30.6119 - rouge2: 5.5416 - rougeL: 17.2819 - rougeLsum: 24.5264 - gen_len: 90.7346\n",
            "Epoch 4/12\n",
            "81/81 [==============================] - 1697s 21s/step - loss: 2.0419 - val_loss: 2.0977 - rouge1: 31.8359 - rouge2: 6.2041 - rougeL: 18.1048 - rougeLsum: 25.5877 - gen_len: 91.2593\n",
            "Epoch 5/12\n",
            "81/81 [==============================] - 1594s 20s/step - loss: 1.9561 - val_loss: 2.0665 - rouge1: 32.9223 - rouge2: 6.5595 - rougeL: 19.0516 - rougeLsum: 26.6204 - gen_len: 91.4259\n",
            "Epoch 6/12\n",
            "81/81 [==============================] - 1597s 20s/step - loss: 1.9162 - val_loss: 2.0414 - rouge1: 33.7783 - rouge2: 7.1199 - rougeL: 19.1982 - rougeLsum: 27.8575 - gen_len: 91.1914\n",
            "Epoch 7/12\n",
            "81/81 [==============================] - 1599s 20s/step - loss: 1.8779 - val_loss: 2.0206 - rouge1: 34.0933 - rouge2: 7.5508 - rougeL: 19.6190 - rougeLsum: 28.0882 - gen_len: 91.2469\n",
            "Epoch 8/12\n",
            "81/81 [==============================] - 1580s 20s/step - loss: 1.8490 - val_loss: 2.0017 - rouge1: 34.7404 - rouge2: 7.6813 - rougeL: 19.7821 - rougeLsum: 28.3774 - gen_len: 90.1173\n",
            "Epoch 9/12\n",
            "81/81 [==============================] - 1648s 21s/step - loss: 1.8279 - val_loss: 1.9852 - rouge1: 35.9081 - rouge2: 8.1724 - rougeL: 20.2634 - rougeLsum: 29.2445 - gen_len: 90.6667\n",
            "Epoch 10/12\n",
            "81/81 [==============================] - 1598s 20s/step - loss: 1.8032 - val_loss: 1.9696 - rouge1: 35.9699 - rouge2: 8.3326 - rougeL: 20.5566 - rougeLsum: 29.2474 - gen_len: 90.6852\n",
            "Epoch 11/12\n",
            "81/81 [==============================] - 1562s 19s/step - loss: 1.7901 - val_loss: 1.9563 - rouge1: 36.1760 - rouge2: 8.5379 - rougeL: 20.6175 - rougeLsum: 29.3251 - gen_len: 91.6173\n",
            "Epoch 12/12\n",
            "81/81 [==============================] - 1494s 19s/step - loss: 1.7742 - val_loss: 1.9446 - rouge1: 36.6943 - rouge2: 8.7839 - rougeL: 20.5929 - rougeLsum: 30.2126 - gen_len: 91.9136\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/spiece.model',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "VMgzhMiysJ1o",
        "outputId": "22815f72-faaf-44cc-86e6-8bd9c1e6f8d2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 8\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "CxJbe2zOCOUp",
        "outputId": "a6ce0afb-5c17-4aa6-decc-42e4fcacdb28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/8\n",
            "81/81 [==============================] - 1567s 20s/step - loss: 1.7588 - val_loss: 1.9339 - rouge1: 36.5276 - rouge2: 8.4833 - rougeL: 20.5481 - rougeLsum: 29.9601 - gen_len: 91.7222\n",
            "Epoch 2/8\n",
            "81/81 [==============================] - 1573s 20s/step - loss: 1.7486 - val_loss: 1.9256 - rouge1: 36.7505 - rouge2: 8.4455 - rougeL: 20.7753 - rougeLsum: 29.9915 - gen_len: 92.0123\n",
            "Epoch 3/8\n",
            "81/81 [==============================] - 1593s 20s/step - loss: 1.7329 - val_loss: 1.9178 - rouge1: 36.3956 - rouge2: 8.3596 - rougeL: 20.4156 - rougeLsum: 29.7746 - gen_len: 92.5679\n",
            "Epoch 4/8\n",
            "81/81 [==============================] - 1562s 19s/step - loss: 1.7235 - val_loss: 1.9111 - rouge1: 36.8426 - rouge2: 8.6895 - rougeL: 20.5990 - rougeLsum: 30.4010 - gen_len: 92.9012\n",
            "Epoch 5/8\n",
            "81/81 [==============================] - 1502s 19s/step - loss: 1.7131 - val_loss: 1.9054 - rouge1: 37.1103 - rouge2: 8.6180 - rougeL: 20.7252 - rougeLsum: 30.3790 - gen_len: 92.0494\n",
            "Epoch 6/8\n",
            "81/81 [==============================] - 1552s 19s/step - loss: 1.7021 - val_loss: 1.8995 - rouge1: 36.7019 - rouge2: 8.5444 - rougeL: 20.6683 - rougeLsum: 30.1170 - gen_len: 92.4259\n",
            "Epoch 7/8\n",
            "81/81 [==============================] - 1575s 20s/step - loss: 1.6934 - val_loss: 1.8954 - rouge1: 36.9292 - rouge2: 8.6131 - rougeL: 20.5078 - rougeLsum: 30.2810 - gen_len: 92.9012\n",
            "Epoch 8/8\n",
            "81/81 [==============================] - 1572s 20s/step - loss: 1.6890 - val_loss: 1.8900 - rouge1: 37.1864 - rouge2: 8.8561 - rougeL: 20.7912 - rougeLsum: 30.6667 - gen_len: 93.0494\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/spiece.model',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v0/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "id": "rDucTbuVCVXx",
        "outputId": "19181155-9138-4ae4-865a-5cec6e46d181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAHACAYAAABOPpIiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACA60lEQVR4nO3deXwTdf7H8VeStumdUkpbjnIIyCmHCAioiIIIgrDgsequsAKrLgiI6yqeeOJ6609l11VRXFhdUXC9UFABRZRLBJHDYoEC5aZN07vJ/P6YNjS0QAtp07Tv5+Mxj2Qmk8lnEsjk3e93vmMxDMNAREREREREzog10AWIiIiIiIjUBQpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifhAS6AJqI4/Hw969e4mJicFisQS6HBGResMwDLKzs2nSpAlWq/7+V5aOTSIigVGVY5PCVQX27t1LSkpKoMsQEam30tPTadasWaDLqFV0bBIRCazKHJsUrioQExMDmG9gbGxsgKsREak/nE4nKSkp3u9hOUbHJhGRwKjKsUnhqgKl3S1iY2N1ABMRCQB1eytPxyYRkcCqzLFJHdpFRERERET8QOFKRERERETEDxSuRERERERE/EDnXIlInWUYBsXFxbjd7kCXImWEhoZis9kCXYaIiIjfKVyJSJ1UWFhIRkYGubm5gS5FjmOxWGjWrBnR0dGBLkVERMSvFK5EpM7xeDykpaVhs9lo0qQJYWFhGn2uljAMg4MHD7J7927atm2rFiwREalTFK5EpM4pLCzE4/GQkpJCZGRkoMuR4zRq1IgdO3ZQVFSkcCUiInWKBrQQkTrLatVXXG2kVkQREamr9MtDRERERETEDxSuRERERERE/EDhSkSkFrn44ouZOnVqoMsQERGR06BwJSIiIiIi4gcKV9XAYxiBLkFERERERGqYhmKvBsM3biSzuJhLGzTg0gYNOD82FrtGLRMJLMOAQFxQODISTnN0vKNHjzJlyhQ++ugjCgoK6N+/Py+++CJt27YFYOfOnUyaNIlvv/2WwsJCWrZsyVNPPcXQoUM5evQokyZN4osvvsDlctGsWTPuuece/vSnP/lz70RERAIiv8iNM6+IzLwisvKKyMote7+QrDKPZeYW4cwr4q2bepESX72XaFG48rNCj4evMzPJ83j4zunkkZ07ibBaucDhMMNWXBzdY2KwaShikZqVmwvR0TX/ui4XREWd1lPHjh3Lr7/+yv/+9z9iY2O56667GDp0KL/88guhoaFMnDiRwsJCli9fTlRUFL/88gvRJft4//3388svv/DZZ5+RkJBAamoqeXl5/twzEZFqUVjsMX8g5xWSmWv+MD5a+mM5t4jMkuVZeebyzNwi7CFWGjsiSHaEkxwbTrIjnMaOcO98fJQuJl8buT2GT0DKLPmcnSWfddZx4cmcN9fJL/JU+fWO5hYqXAWbMKuVTT178lVmJl8ePcpXR4+yv6iIxUePsvjoUQDiQkK4OC6OS+PiuLRBA9pHRuo/vIj4KA1VK1asoG/fvgDMnTuXlJQUFi5cyNVXX82uXbsYPXo055xzDgBnnXWW9/m7du2ie/funHfeeQC0bNmyxvdBROq3gmL3sUCUa/5wziz5kXy0zP3MMiEqM7eQnEL3ab3e9oM5J3wsLMTqDV3JsceCV2NHOEmx4TR2RNAoxo7Nqt9jVWUYBjmFbm8wKg1BFbUclQaj0uCUnV98Rq9ttUBsRChxEaE4IkJxRIbhKDMfFxnq8/hZjar/j6wKV9WgVUQE4yIiGNe4MYZh8EtuLl8ePcqXR4+yNDOTzOJiFh46xMJDhwBoHBbGJSVB69IGDWgeHh7gPRCpgyIjzVakQLzuadi8eTMhISH07t3bu6xhw4a0a9eOzZs3AzB58mRuvfVWvvjiCwYOHMjo0aPp0qULALfeeiujR49m3bp1XHbZZYwcOdIb0kREqiK/qGxIKuRobplWJe8P5UKO5pT8mC4JTrmnGZLA7E3t/ZEcGUZcRCgNIkOJK/3xHFkyRYThiAwlv9BNRlY++5z57MvKL7mfx76sfA65Ciks9rDrSC67jpy4e7jNaiExxl4StsqHr8aOcBJj7dhDbKe9X7WRYRjkFrrJzi/GmV9Edn4RzvxinCXhx1zmO398iCr2nNl4A1FhNuIiw3yCUFxkaWAquY0o+bzLhKYYewjWWhaIFa6qmcVioVNUFJ2iopjcrBnFHg/rXC5v2FrhdJJRWMjcAweYe+AAAK3Dw71Ba0BcHI3CwgK8FyJ1gMVy2t3zaqvx48czePBgPvnkE7744gtmzpzJM888w2233caQIUPYuXMnn376KYsXL+bSSy9l4sSJPP3004EuW0RqAY/H4EB2AXsy89ibmee9PeAsKNft7nS6X5WyloakyLCSMHRcQIoIpUFU2LF1Sn84h4f67UdzQbGbA84C9jnN0LW/TPgqnd+fXYDbY5BR8tj69BNvr2FUWLluh8kl4at0Pspecz+xC4s9ZOf7BqHs/CKceea8s8z88euV3rrPMBwBhNmsZjiKDPVpPXKUnS8JxGXXc0SEEmqrO2MTWAxDQ9sdz+l04nA4yMrKIjY2tlpfK9/tZqXTaYatzExWO50c/3eeLlFR3rB1kcNBTIgyscjJ5Ofnk5aWRqtWrQgPspbgiy++mG7dujFx4kTOPvtsn26Bhw8fJiUlhTlz5nDVVVeVe+706dP55JNP2LBhQ7nH/vnPf3LnnXfidDqrfR9O5WSfT01+/wabuvjelHYnKv2LeEU//HIL3MRGhNAoxk6j6HDzNsZOXIT/fnzXRXmF7nLBaU9mHnuO5rE3y2zRKXJX/iegzWop8+P4WBByRIbSoCQ4lQ1IDSLNFqXa2LJQEbfH4JCrwAxdWfnsy8ojw1l6P98bzAqLKxc0Y8JDSsJWBMmx9nLhq7EjHEdEKIYBOYXF3gB0fAtR6byzXHA6ts6ZhN+ybFYLMeEhxISHEBseWuY2lNiIEPO2ZJlPiCq5jQi11dnTXKry/atf6QEWbrMxoEEDBjRowKOAs7iY5ZmZfFlyztbGnBw2lEzP7d6NDegVG+sdHKOPw6GRCEXqoLZt2zJixAgmTJjAP//5T2JiYrj77rtp2rQpI0aMAGDq1KkMGTKEs88+m6NHj/L111/ToUMHAB544AF69OhBp06dKCgo4OOPP/Y+JuIPZbsSZZf963iZH4nHbsv/YMzOL8JVUMzp/sE81GYhIdpeErrs3tBVdj4xxgxjEWF1rxvX4ZxCMyiVhiafIJXPkZzCU27HZrWQHBtO07gImjaIoEmc2QUuLjLM7IIXURKaIkOJDguOkHS6bFYLSbHm/pNS8TqGYXA0t6gkbOWVCWLHwte+rHxcBcUl/8ZdbNt/4u7oYSFWitwe/NXMERVm8wlCPiEpIrRcaDoWmMxlkWF1NxzVJIWrWiY2JIRhCQkMS0gA4EBhIV+XBK0vjx7lt/x8VjqdrHQ6eXTnTsJLRyIsOWfrXI1EKFJnzJ49mylTpjBs2DAKCwu56KKL+PTTTwkNDQXA7XYzceJEdu/eTWxsLJdffjnPPfccAGFhYUyfPp0dO3YQERHBhRdeyDvvvBPI3ZFaxDAM8os8JWGoNBCVD0XHAlGZxwrM7kWuAv90JQIIKfmLeekPwBh7yW14KFF2G868Ig66CjiYbU5Hc4soch/rwnUq0faQikNYmTCWGGMnPiqMkFrQPamg2M2+rHz2HPUNTnsz8733CyrRghJtD6FpnBmazPAUYQapOPN+Yoy9VuxvsLBYLMRHhREfFUbHJiduvcjOL2J/SdjydkMscy7YfqcZfsu2goXaLCcMQmUD0IlCUrQ9RJ9lLaFugRWozV0vduTlHRuJMDOTfYW+f5ly2GzmSIQl3Qg7aCRCqYeCuVtgfaBugafnTN6btEM5TJq3zic4nekJ6KXKdiWKsVf8V/PSH4jlfzCat+Gh1iodqwqLPRzOORa2DmQfu38wu8AbxA5k51epy5TFYp5Pk3Bcy9fxLWKNYuzEhoec1vHVMAyy8oqOddEr09pUGqQOZhdUqtbEGLs3KDVtUBKaHBHeIHW6NUr1yy9yczC7AHuoldjwUOwhVfs/IDVL3QLrsJYREdwUEcFNJSMRbj5uJMIst5sPDx/mw8OHAUg+biTCFvqhKSJS71gtsGlv+fPtrBa8oedY+PENPjE+t+Z9R5kAFYjzLMJKrmnU2BFx0vVKz+k6mF3AAWe+T+tX2RB2MLuAQ64CPAYcchVyyFXIln3ZJ922PcRaYegqnY8OD2G/M5+9mfnsPlra8mROlRlqPDzUWq6lqfS2WYMIkmLDCQtRS0WwCg+1Vfv1liQwFK6CmMVioWNUFB2joritZCTCH0tHIszM5NusLPYVFjLvwAHmlYxEeNZxIxEmaiRCEZE6Lyk2nNl/6ukTmGLDQ+v8ORYWi4Vou9llqlXCyUcLdXsMjuQUlgtdx+bzva1k2fnFFBR72H00j91HT+/i3AnRYT6BqWyQatogggaRoXX6sxGpqxSu6pAQq5WesbH0jI3l7hYtyHe7+d7p9A6Oscrp5Lf8fH7LyOBfGRkAnFM6EmFcHBfFxRGrkQhFROqc8FAbA9olBrqMWs1mtXhbnk6ltEvXyVrCnPlFJMWUPdcpnKZxkTSJC6dJXAThoXVrkA0RMemXdB0WbrNxcYMGXNygAY+0akV2cTHLs7K83Qg35OSwsWR6vmQkwvNiYrgoLo6LHA4ucDiIKzlxXkREREylXbrUrUtEjqdwVY/EhIRwRcOGXNGwIQAHjxuJcHt+Pj9kZ/NDdjZPpadjAbpGR3ORw8FFcXFc6HCoG6GIiIiIyAkoXNVjjcLCuCYxkWsSza4iO/PzWZ6ZyfKsLJZnZrItL4/1LhfrXS5e3LMHgPaRkd6wdZHDQYoGyBARERERARSupIwW4eH8MTmZPyYnA7CvoIBvsrJYVhK4NubksCU3ly25ubxacs5Wq/Bwn7DVOiJCJ+CKiIiISL2kcCUnlGy3c3ViIleXtGwdKSri25JWreVZWazLziYtP5+0/Hze2r8fgMZhYVzkcNC/ZICMDpGRWBW2RERERKQeULiSSosPDeXKhASuTEgAILu4mJVOp7dla5XTSUZhIe8ePMi7Bw8C0DAkhAtLWrUuiouja1QUIVZdl0NERERE6h6FKzltMSEhXBYfz2Xx8QDkud2sys72tmx9l5XF4eJiFh46xMJDh8zn2Gz0czjoXxK2zouJIUxhS8RvWrZsydSpU5k6deop17VYLCxYsICRI0dWe10iIiL1gcKV+E2EzUb/uDj6x8UBUOTxsDY72ztAxrdZWWS53Sw6coRFR44AEG610ic21tuydX5sLJE2XftDRERERIKPwpVUm1CrlfMdDs53OPhb8+a4DYONLpc3bC3PyuJgURFfZ2bydWYm7NxJqMXCeTEx5jlbDgd9HQ4curCxiIiIiAQB9ceSGmOzWOgWE8PkZs2Y37kz+/v25ZeePfnH2WdzfWIiTcPCKDIMVjqdPLFrF0M3biT+22/psWYNt6emsuDgQQ4VFgZ6NyRIGYZBjttd45NhGJWu8dVXX6VJkyZ4PB6f5SNGjOCmm25i+/btjBgxgqSkJKKjo+nZsydLlizx23u0ceNGLrnkEiIiImjYsCF//vOfcblc3seXLl1Kr169iIqKIi4ujn79+rFz504AfvrpJwYMGEBMTAyxsbH06NGDNWvW+K02ERGRYKAmAQkYi8VCh6goOkRFcXOTJhiGwY78fJ+WrdS8PNa5XKxzuXh+924AOkVGeod+vygujiZ2e4D3RIJBrsdD9Dff1Pjrui68kKhKdnW9+uqrue222/j666+59NJLAThy5AiLFi3i008/xeVyMXToUB577DHsdjtz5sxh+PDhbN26lebNm59RnTk5OQwePJg+ffqwevVqDhw4wPjx45k0aRJvvvkmxcXFjBw5kgkTJvCf//yHwsJCVq1a5b30wg033ED37t2ZNWsWNpuN9evXExoaekY1iYiIBBuFK6k1LBYLrSIiaBURwZiSa23tKSjgmzIXNt6Um+udZu3dC0Dr8HD6OBz0iomhV2wsXaOiCNd5WxKEGjRowJAhQ5g3b543XM2fP5+EhAQGDBiA1Wqla9eu3vUfeeQRFixYwP/+9z8mTZp0Rq89b9488vPzmTNnDlFRUQC89NJLDB8+nL///e+EhoaSlZXFsGHDaN26NQAdOnTwPn/Xrl3ceeedtG/fHoC2bdueUT0iIiLBSOFKarWmdju/T0ri90lJABwqLDSvtVVyceP1Lhfb8/PZnp/Pv0uutRVqsdAlKopesbHewNUuMhKbrrdVr0VarbguvDAgr1sVN9xwAxMmTOCVV17Bbrczd+5cfv/732O1WnG5XMyYMYNPPvmEjIwMiouLycvLY9euXWdc5+bNm+natas3WAH069cPj8fD1q1bueiiixg7diyDBw9m0KBBDBw4kGuuuYbGjRsDMG3aNMaPH8/bb7/NwIEDufrqq70hrD6YNWsWs2bNYseOHQB06tSJBx54gCFDhgBw8cUXs2zZMp/n3HzzzfzjH/+o6VJFRKQaKVxJUEkIC2Nko0aMbNQIgKziYlZmZbEqO5tVTiersrM5WFTEWpeLtS4Xs0qeF22zcV5MjDds9YyJIcVu93ZpkrrPYrFUunteIA0fPhzDMPjkk0/o2bMn33zzDc899xwAf/3rX1m8eDFPP/00bdq0ISIigquuuorCGjoXcfbs2UyePJlFixbx7rvvct9997F48WLOP/98ZsyYwfXXX88nn3zCZ599xoMPPsg777zD7373uxqpLdCaNWvGE088Qdu2bTEMg7feeosRI0bw448/0qlTJwAmTJjAww8/7H1OZGRkoMoVEZFqonAlQc0REsLlDRtyecOGgDlowc78fFZnZ3sD15rsbFxuN0szM1mamel9blJoqE/r1nkxMcTrHBEJsPDwcEaNGsXcuXNJTU2lXbt2nHvuuQCsWLGCsWPHegOLy+XytpScqQ4dOvDmm2+Sk5Pjbb1asWIFVquVdu3aedfr3r073bt3Z/r06fTp04d58+Zx/vnnA3D22Wdz9tlnc/vtt3Pdddcxe/bsehOuhg8f7jP/2GOPMWvWLL7//ntvuIqMjCS5pMuziIjUTQpXUqdYLBZaRkTQMiKCqxMTASj2eNicm2sGrpLWrQ0uF/uLivjo8GE+OnzY+/w2ERE+rVvdo6OJCILWDqlbbrjhBoYNG8amTZv4wx/+4F3etm1bPvjgA4YPH47FYuH+++8vN7Lgmbzmgw8+yJgxY5gxYwYHDx7ktttu449//CNJSUmkpaXx6quvcuWVV9KkSRO2bt3Kr7/+yo033kheXh533nknV111Fa1atWL37t2sXr2a0aNH+6W2YON2u3nvvffIycmhT58+3uVz587l3//+N8nJyQwfPpz777//pK1XBQUFFBQUeOedTme11i0iImdO4UrqvBCrlXOiozknOpqbSs4PyXO7We9y+XQnTM3L807zDhwwn2uxcE5UlE/g6hgVpfO3pFpdcsklxMfHs3XrVq6//nrv8meffZabbrqJvn37kpCQwF133eW3H9yRkZF8/vnnTJkyhZ49exIZGcno0aN59tlnvY9v2bKFt956i8OHD9O4cWMmTpzIzTffTHFxMYcPH+bGG29k//79JCQkMGrUKB566CG/1BYsNm7cSJ8+fcjPzyc6OpoFCxbQsWNHAK6//npatGhBkyZN2LBhA3fddRdbt27lgw8+OOH2Zs6cWe/eQxGRYGcxqnIRlnrC6XTicDjIysoiNjY20OVIDTlSVMSaMmFrldPJ/qKicutFWa30KAlbpaGruc7fqlXy8/NJS0ujVatWhIeHB7ocOc7JPp9g/v4tLCxk165dZGVlMX/+fF577TWWLVvmDVhlffXVV1x66aWkpqaecOCPilquUlJSgvK9EREJZlU5NqnlSqREfGgol8XHc1l8PGCev5VeUODTnbD0/K3lJSMWlmoUGuoNWr1iYugZG0tDnb8lUq+EhYXRpk0bAHr06MHq1at54YUX+Oc//1lu3d69ewOcNFzZ7Xbsuo6fiEhQUbgSOQGLxULz8HCah4czumR0QrdhsCU3l9VlWrd+ysnhYFERnxw5widHjniff1Z4uE/rVvfoaCJ1/pbUkLlz53LzzTdX+FiLFi3YtGlTDVdU/3g8Hp+Wp7LWr18P4B3KXkRE6gaFK5EqsFksdIqKolNUFGNLfhTllzl/q7SVa1teHr/l5/Nbfj7vlJy/ZQM6l7n+Vs/YWDpERhJWxesgiVTGlVde6W0dOV6oWlX9bvr06QwZMoTmzZuTnZ3NvHnzWLp0KZ9//jnbt29n3rx5DB06lIYNG7JhwwZuv/12LrroIrp06RLo0kVExI8UrkTOULjNxvkOB+c7HN5lR0vO3yoNWz9kZ7OvsJCfcnL4KSeHf2VkAOaAGe0iIugSHc05UVHmFB2tc7jkjMXExBATExPoMuqNAwcOcOONN5KRkYHD4aBLly58/vnnDBo0iPT0dJYsWcLzzz9PTk4OKSkpjB49mvvuuy/QZYuIiJ8pXIlUgwahoQyKj2dQmfO39hQU+LRurcnOxul2syk3l025ufynzPNjbTZv0PKGrqgo4tTiUCUar6d2qoufy+uvv37Cx1JSUli2bFkNViMiIoGicCVSAywWC83Cw2kWHs6okvO3SgfM2JiTw0aXiw05OWzMyWFLbi5Ot5sVTicrjhtmO8Vu92nhOicqivbqWlhOabe33NxcIiIiAlyNHK+wsBAAm85BFBGROkbhSiRAyg6YcUXDht7lhR4PW3Nz2ZiTwwaXywxfOTmkFxR4p0/LDJwRYrHQPjLSp4WrS3Q0KfW4a6HNZiMuLo4DJee7RUZG1tv3orbxeDwcPHiQyMhIQkJ0CBIRkbpFRzaRWiaszEWPr09K8i7PLCri55KgtaGktWtjTg5Ot5ufc3L4OSfHp2uhw2ajc5kWri5RUXSuR10Lk5OTAbwBS2oPq9VK8+bNFXhFRKTOUbgSCRJxoaFcEBfHBXFx3mVluxaWbeXakptLViW6FpYOpNGuDnYttFgsNG7cmMTERIoquBi0BE5YWBjWOvbvTUREBBSuRILaqboWlm3hqmzXwi5lWrvqQtdCm82mc3tERESkRihcidRBZbsWUkHXwtLBM6rStbBLmYE0HDpXRkRERKQc/UISqUdO1rWwbLfCU3UtbBkeTteoKLpGR9MtOpqu0dG0DA/HGuStXCIiIiJnQuFKpJ4r27VwWEKCd3mhx8OWklELj+9auCM/nx35+Xx4+LB3/RibjS5RUd6w1TU6ms5RUUSqS56IiIjUEwpXIlKhMKuVLtHRdDmua+GRoiI2uFz8lJPDTy4X610uNuXkkF1BK5cVODsyslwrV+OwsKA/l0tERETkeApXIlIl8aGhXNygARc3aOBdVlQygMZPOTmsd7m8oetgURFbcnPZkpvLuwcPetdPCA01g1ZJ6OoaHU2HyEhCNYKciIiIBLGA/pJZvnw5w4cPp0mTJlgsFhYuXHjK57z88st06NCBiIgI2rVrx5w5c8qt895779G+fXvCw8M555xz+PTTT6uhehEpFWq10jk6mhuSkniqdWu+6NqV/X37srdPHz475xyeOOssrktMpENkJFbgUFERS44e5Zndu7lxyxa6rllD9Dff0H3NGsZu3sxz6el8ffQoRzSEuoiIiASRgLZc5eTk0LVrV2666SZGjRp1yvVnzZrF9OnT+de//kXPnj1ZtWoVEyZMoEGDBgwfPhyA7777juuuu46ZM2cybNgw5s2bx8iRI1m3bh2dO3eu7l0SkRIWi4XGdjuN7XYuLzNMfJ7bzaacHJ9Wrp9cLpxuN+tLWrzYv9+7fjO7vVwrV5uICA2eISIiIrWOxTAMI9BFgPlDbMGCBYwcOfKE6/Tt25d+/frx1FNPeZfdcccd/PDDD3z77bcAXHvtteTk5PDxxx971zn//PPp1q0b//jHPypVi9PpxOFwkJWVRWxs7OntkIhUmmEY7MjPN4NWmXO50vLzK1w/qmSo+a5lBtA4JyqKaA0RH/T0/Xtiem9ERAKjKt+/QfVLpKCggPDwcJ9lERERrFq1iqKiIkJDQ1m5ciXTpk3zWWfw4MGV6nIoIoFhsVhoFRFBq4gIRjZq5F3uLC5mQ0nQKg1dG3NyyPF4+N7p5Psyg2dYgNYRET6tXN2io2lWBy6ELCIiIsEhqMLV4MGDee211xg5ciTnnnsua9eu5bXXXqOoqIhDhw7RuHFj9u3bR1KZkc0AkpKS2Ldv3wm3W1BQQEFBgXfeedw1fUQkMGJDQspdl6vY4+HXvDxvK1dp18KMwkJS8/JIzctjfpnBMxqEhHgHzGgVHk7L8HDvbcPQUAUvERER8ZugClf3338/+/bt4/zzz8cwDJKSkhgzZgxPPvkk1jMYZWzmzJk89NBDfqxURKpLiNVKh6goOkRF8fsyyw8WFnq7E5a2cm3OzeVocTFLMzNZmplZblvRNptP2Dr+Ni40tMb2S0RERIJfUIWriIgI3njjDf75z3+yf/9+GjduzKuvvkpMTAyNSroSJScns7/MyfAA+/fvJzk5+YTbnT59uk9XQqfTSUpKSvXshIhUi0ZhYQyMj2dgfLx3WYHHwy8lQSs1L4+0kosfp+Xnk1FYiMvt5uecHH7Oyalwm3EhIbQ8QfBqFR6uc7xERETER1D+MggNDaVZs2YAvPPOOwwbNszbctWnTx++/PJLpk6d6l1/8eLF9OnT54Tbs9vt2O32aq1ZRGqe3Wqle0wM3WNiyj2W73azs6DADFt5ed7QVXp7sKiIzOLiYyMYVqBhSAitIiIqDF8tw8OJsNmqexdFRESkFglouHK5XKSmpnrn09LSWL9+PfHx8TRv3pzp06ezZ88e77Wstm3bxqpVq+jduzdHjx7l2Wef5eeff+att97ybmPKlCn079+fZ555hiuuuIJ33nmHNWvW8Oqrr9b4/olI7RVus9EuMpJ2kZEVPp7jdrOjNGyVhK+yAexIcTGHi4s5nJ3NmuzsCreRFBp6wvDVPDwcuy6aLCIiUqcENFytWbOGAQMGeOdLu+aNGTOGN998k4yMDHbt2uV93O1288wzz7B161ZCQ0MZMGAA3333HS1btvSu07dvX+bNm8d9993HPffcQ9u2bVm4cKGucSUiVRJls9EpKopOUVEVPp5VXMzOkrBVUctXttvN/qIi9hcV+YxqWMoCNAkLO2H4SrHbCVH4EhERCSq15jpXtYmuJSIiZ8IwDI4WF3vDVkVdD3M9npNuw4Z5AeX2kZF0ioqic0nQ6xgZWafP9dL374npvRERCYw6e50rEZFgYLFYiA8NJT40lB4VnO9lGAYHi4p8W7uOC1+FhsHOggJ2FhTw+dGjPs9vGR5uhq0ywat9ZKTO8RIREQkwhSsRkRpmsVhIDAsjMSyMXhX8BcxjGOwrLOS3vDx+yc1lU8mIhptycthfEsp25Ofz8eHD3udYMS+i3KkkdJW2dLWLjCRM3QtFRERqhMKViEgtY7VYaGK308Ru97mAMsChwkI2HRe4fs7J4UhxMb/m5fFrXh4Ly6wfYrHQNiLCG7ZKg1ebiAid0yUiIuJnClciIkEkISyM/mFh9C8TugzDYH9J6CobuDbl5OB0u9mcm8vm3FzeO3jQ+5wwi6Xc+VydIiNpFRGBzWIJwJ6JiIgEP4UrEZEgZ7FYSLbbSbbbubRBA+9ywzDYU1BgBq0ywWtTTg65Hg8bcnLYcNwFlCOsVjqU6VZYGr6a2+1YFLpEREROSuFKRKSOslgsNAsPp1l4OJc3bOhd7jEMdubnH2vhKglem3NyyPN4WOdyse64CydH22w+A2iUBq8mYWEKXSIiIiUUrkRE6hmrxUKriAhaRUQwLCHBu9xtGPyWl+fbtTA3l625ubjcbn7IzuaH4y6YHBcS4jOARultYlhYTe+WiIhIwClciYgIADaLhbaRkbSNjOR3jRp5lxd5PPyal1duEI3UvDwyi4tZ4XSy4rgLJd/bvDmPnnVWTe+CiIhIQClciYjISYVarXSMiqJjVBRXl1me73aztSR0lQ1ev+Xn0yYiImD1ioiIBIrClYiInJZwm42u0dF0jY72WZ7rdgeoIhERkcBSuBIREb+KtNkCXYKIiEhA6AqSIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiByGBLkBERERERORMGR4P7sOHKTpwgOIDByg+cLDk9gBFB/bTZOZMQuLjq7UGhSsREREREam1DMPA43RStH+/T2AqPrC/JEiVLDt0CIqLT7idoowMhSsREREREambPLm5FYQms6Wp7DKjoKByG7RYsCU0JLRRIiGJiYQkJRGS2IiQxERCk5Kqd2dQuBIRERGp1zwFBRTu2EHBr6m4jxwGLGC1ggUsVitYLGCpaN6CxWo56fwZbcNqBcrPY7VgKZ23mI9ZIyOxxcZijY42ty8B5yksLBeYig8eKBekPC5XpbdpczjKBKbEY6Gp7LKGDbGEBC7iKFyJiIiI1AOeggIK09IoSN1OQeqvFG7fTsGvqRTu2gUeT6DL8w+rFVtsLDaHA2ucA1usA5ujzBTnwFryuM0Rhy2uZHlsLJbQ0EBXHxQMt5viQ4e9Ycnb0nRcaHIfPVrpbVoiI30DUmIjc77sskaNsNrt1bhn/qFwJSIicoZmzZrFrFmz2LFjBwCdOnXigQceYMiQIQDk5+dzxx138M4771BQUMDgwYN55ZVXSKqBLipS/3hD1K+pFKSmUrA9lcJfUylMTz9hiLLGxmJv04bQZPPfpGEY4DHAMMDw+M57PBj4zoOBUXbeMMqtc6r5CrdhVGLe48GTl4eRlwceD+7MTNyZmbCzau+bNTLSDGSOOG/gKg1kNocDq6NMWIs7FtgsERFmS1otYhgGRmEhntxcjLw8PHl5eHLz8OTlHluWW7I8r2Q+J7dkvmSZ9/E8PLm5PssqG8YtYWElYSnRNzQlJRHS6NhyW3RUNb8jNUfhSkRE5Aw1a9aMJ554grZt22IYBm+99RYjRozgxx9/pFOnTtx+++188sknvPfeezgcDiZNmsSoUaNYsWJFoEuXIObJzy9piUotaY1KpTC1ciHKnFpjb9OGsNZtCElsVOsCQlV5CgpwZ2XhycrC7XTizsrCnZll3mZl4nE6y8yXTE4nHqcTDMMMELm5FO/NqNoLh4b6to6VBrO4kkBW2krmiPVZxxoTg+F248nJKReAvPPewFM+EHlyjwtAZQNRbm71tkbabIQkJJQPTYlJPstscXFB/++qqiyGYRiBLqK2cTqdOBwOsrKyiI2NDXQ5IiL1Rl36/o2Pj+epp57iqquuolGjRsybN4+rrroKgC1bttChQwdWrlzJ+eefX6nt1aX3RqrGk59P4W+/UVDSja+0NaooffeJQ5TDYQao1maAsrdtQ1jr1oQ0Cv4Q5W+G240nO7tM6HJ6A5k3rHmX+U4UFQW6/FOyhIZiiYw0W+YiIryTJTICa0SZ5ZERWCJKl0VgjYz0nS99blQUtvh4LDZboHetxlTl+1ctVyIiIn7kdrt57733yMnJoU+fPqxdu5aioiIGDhzoXad9+/Y0b978pOGqoKCAgjKjYzmdztOuySguxu10ml2xSrpT4fFguD1geMovr+w6npIuY1Vep7Rb17F1DI/bXNe7/Nh9S0QEtpgYrNEx2GKisUZHY42JwRZt3g/kyev+5MnLo+C337znQhVsN1ujitLTzfe0AjaHg7C2bbC39m2NsiUkKERVksVmwxYXhy0urkrPMwwDIy/vWNgqbRVzlgayssuOBTZPZpbZslRWaGjVwk9EBNYo87ZcAIqMxFJmvq78/wgWerdFRET8YOPGjfTp04f8/Hyio6NZsGABHTt2ZP369YSFhRF33A+3pKQk9u3bd8LtzZw5k4ceesgvtRXuSue3oUP9sq3ayBIRYQatmBis0dFl7kdhi44xg1hpKIuu4H5MDBa7vcbCiDdElenOV5CaStHu3acOUW1KglRbs1VKISpwLBaLt0UotHHjKj3XKCzE7XJhCQkxA5AG06gzFK5ERET8oF27dqxfv56srCzmz5/PmDFjWLZs2Wlvb/r06UybNs0773Q6SUlJOb2Nlf3tbbWC1XpsKGub7dj9ssutVnNIa6u1ZOjr4+7bbCVDZJdf59jzjt+eBay247Zn9dlOudfEDCMeVzZuV47ZfcuVjSfbhZGfD4CRl0dxXh4cPHi6bzeEhFQY0Gwx0VijoisOaDExWKOiy9yP8hkG3JOXR8H233xG5ivYvv3kISouzjwPqk1r7G3aelujbA0bKkTVIZawsGq/mK0EhsKViIiIH4SFhdGmTRsAevTowerVq3nhhRe49tprKSwsJDMz06f1av/+/SQnJ59we3a7Hbufhh0Oa9mS9pt/qXM/zo2iItwuFx6XqyR0lbmfXXLfVXI/Oxt3jgtP9nHrulxm0CkuPjbK3BmwRkVhjYkBq4XijH0nDlENGpQJUcdao2zx8XXucxKpTxSuREREqoHH46GgoIAePXoQGhrKl19+yejRowHYunUru3btok+fPjVSS139sW4JDSWkQQNo0OC0t2F4POYocT6hLBuPy+W9787OxuNtNTPX87hc3vtul8s7sIEnJwdPTo53+7b4eHNQiZIBJczWqNaENGx4xvsvIrWPwpWIiMgZmj59OkOGDKF58+ZkZ2czb948li5dyueff47D4WDcuHFMmzaN+Ph4YmNjue222+jTp0+lRwqU6mOxWrGVdAPkJC2JJ+O9plB2aRBzYRQVE9ayhbp+idQzClciIiJn6MCBA9x4441kZGTgcDjo0qULn3/+OYMGDQLgueeew2q1Mnr0aJ+LCEvdYLFYsNjtWO12QhISAl2OiASQrnNVAV1LREQkMPT9e2J6b0REAqMq37/Wkz4qIiIiIiIilaJwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHAQ1Xy5cvZ/jw4TRp0gSLxcLChQtP+Zy5c+fStWtXIiMjady4MTfddBOHDx/2Pv7mm29isVh8pvDw8GrcCxERERERkQCHq5ycHLp27crLL79cqfVXrFjBjTfeyLhx49i0aRPvvfceq1atYsKECT7rxcbGkpGR4Z127txZHeWLiIiIiIh4hQTyxYcMGcKQIUMqvf7KlStp2bIlkydPBqBVq1bcfPPN/P3vf/dZz2KxkJyc7NdaRURERERETiaozrnq06cP6enpfPrppxiGwf79+5k/fz5Dhw71Wc/lctGiRQtSUlIYMWIEmzZtOul2CwoKcDqdPpOIiIiIiEhVBFW46tevH3PnzuXaa68lLCyM5ORkHA6HT7fCdu3a8cYbb/Dhhx/y73//G4/HQ9++fdm9e/cJtztz5kwcDod3SklJqYndERERERGROiSowtUvv/zClClTeOCBB1i7di2LFi1ix44d3HLLLd51+vTpw4033ki3bt3o378/H3zwAY0aNeKf//znCbc7ffp0srKyvFN6enpN7I6IiIiIiNQhAT3nqqpmzpxJv379uPPOOwHo0qULUVFRXHjhhTz66KM0bty43HNCQ0Pp3r07qampJ9yu3W7HbrdXW90iIiIiIlL3BVXLVW5uLlarb8k2mw0AwzAqfI7b7Wbjxo0VBi8RERERERF/CWjLlcvl8mlRSktLY/369cTHx9O8eXOmT5/Onj17mDNnDgDDhw9nwoQJzJo1i8GDB5ORkcHUqVPp1asXTZo0AeDhhx/m/PPPp02bNmRmZvLUU0+xc+dOxo8fH5B9FBERERGR+iGg4WrNmjUMGDDAOz9t2jQAxowZw5tvvklGRga7du3yPj527Fiys7N56aWXuOOOO4iLi+OSSy7xGYr96NGjTJgwgX379tGgQQN69OjBd999R8eOHWtux0REREREpN6xGCfqT1ePOZ1OHA4HWVlZxMbGBrocEZF6Q9+/J6b3RkQkMKry/RtU51yJiIiIiIjUVgpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiJnaObMmfTs2ZOYmBgSExMZOXIkW7du9Vnn4osvxmKx+Ey33HJLgCoWEZHqoHAlIiJyhpYtW8bEiRP5/vvvWbx4MUVFRVx22WXk5OT4rDdhwgQyMjK805NPPhmgikVEpDqEBLoAERGRYLdo0SKf+TfffJPExETWrl3LRRdd5F0eGRlJcnJyTZcnIiI1RC1XIiIifpaVlQVAfHy8z/K5c+eSkJBA586dmT59Orm5uSfcRkFBAU6n02cSEZHaTS1XIiIifuTxeJg6dSr9+vWjc+fO3uXXX389LVq0oEmTJmzYsIG77rqLrVu38sEHH1S4nZkzZ/LQQw/VVNkiIuIHFsMwjEAXUds4nU4cDgdZWVnExsYGuhwRkXqjLnz/3nrrrXz22Wd8++23NGvW7ITrffXVV1x66aWkpqbSunXrco8XFBRQUFDgnXc6naSkpAT1eyMiEoyqcmxSy5WIiIifTJo0iY8//pjly5efNFgB9O7dG+CE4cput2O326ulThERqR4KVyIiImfIMAxuu+02FixYwNKlS2nVqtUpn7N+/XoAGjduXM3ViYhITVG4EhEROUMTJ05k3rx5fPjhh8TExLBv3z4AHA4HERERbN++nXnz5jF06FAaNmzIhg0buP3227nooovo0qVLgKsXERF/UbgSERE5Q7NmzQLMCwWXNXv2bMaOHUtYWBhLlizh+eefJycnh5SUFEaPHs19990XgGpFRKS6KFyJiIicoVONDZWSksKyZctqqBoREQkUXedKRERERETEDxSuRERERERE/EDhSkRERERExA8UrkRERERERPxA4UpEROq1zMxMXnvtNaZPn86RI0cAWLduHXv27AlwZSIiEmw0WqCIiNRbGzZsYODAgTgcDnbs2MGECROIj4/ngw8+YNeuXcyZMyfQJYqISBBRy5WIiNRb06ZNY+zYsfz666+Eh4d7lw8dOpTly5cHsDIREQlGClciIlJvrV69mptvvrnc8qZNm7Jv374AVCQiIsFM4UpEROotu92O0+kst3zbtm00atQoABWJiEgwU7gSEZF668orr+Thhx+mqKgIAIvFwq5du7jrrrsYPXp0gKsTEZFgo3AlIiL11jPPPIPL5SIxMZG8vDz69+9PmzZtiImJ4bHHHgt0eSIiEmROa7TAt956i4SEBK644goA/va3v/Hqq6/SsWNH/vOf/9CiRQu/FikiIlIdHA4HixcvZsWKFfz000+4XC7OPfdcBg4cGOjSREQkCFkMwzCq+qR27doxa9YsLrnkElauXMnAgQN57rnn+PjjjwkJCeGDDz6ojlprjNPpxOFwkJWVRWxsbKDLERGpN2ry+7eoqIiIiAjWr19P586dq/W1/EHHJhGRwKjK9+9ptVylp6fTpk0bABYuXMjo0aP585//TL9+/bj44otPZ5MiIiI1KjQ0lObNm+N2uwNdioiI1BGndc5VdHQ0hw8fBuCLL75g0KBBAISHh5OXl+e/6kRERKrRvffeyz333MORI0cCXYqIiNQBp9VyNWjQIMaPH0/37t3Ztm0bQ4cOBWDTpk20bNnSn/WJiIhUm5deeonU1FSaNGlCixYtiIqK8nl83bp1AapMRESC0WmFq5dffpn77ruP9PR03n//fRo2bAjA2rVrue666/xaoIiISHUZOXJkoEsQEZE65LQGtKjrdNKwiEhg6Pv3xPTeiIgERlW+f0/rnKtFixbx7bffeudffvllunXrxvXXX8/Ro0dPZ5MiIiIBs3btWv7973/z73//mx9//DHQ5YiISJA6rXB155134nQ6Adi4cSN33HEHQ4cOJS0tjWnTpvm1QBERkepy4MABLrnkEnr27MnkyZOZPHkyPXr04NJLL+XgwYOBLk9ERILMaYWrtLQ0OnbsCMD777/PsGHDePzxx3n55Zf57LPP/FqgiIhIdbntttvIzs5m06ZNHDlyhCNHjvDzzz/jdDqZPHlyoMsTEZEgc1oDWoSFhZGbmwvAkiVLuPHGGwGIj4/3tmiJiIjUdosWLWLJkiV06NDBu6xjx468/PLLXHbZZQGsTEREgtFphasLLriAadOm0a9fP1atWsW7774LwLZt22jWrJlfCxQREakuHo+H0NDQcstDQ0PxeDwBqEhERILZaXULfOmllwgJCWH+/PnMmjWLpk2bAvDZZ59x+eWX+7VAERGR6nLJJZcwZcoU9u7d6122Z88ebr/9di699NIAViYiIsFIQ7FXQMPdiogERk1//6anp3PllVeyadMmUlJSvMs6d+7M//73v1rVG0PHJhGRwKjK9+9pdQsEcLvdLFy4kM2bNwPQqVMnrrzySmw22+luUkREpEalpKSwbt06lixZwpYtWwDo0KEDAwcODHBlIiISjE4rXKWmpjJ06FD27NlDu3btAJg5cyYpKSl88skntG7d2q9FioiIVBeLxcKgQYMYNGhQoEsREZEgd1rnXE2ePJnWrVuTnp7OunXrWLduHbt27aJVq1YaulZERILG5MmTefHFF8stf+mll5g6dWrNFyQiIkHttMLVsmXLePLJJ4mPj/cua9iwIU888QTLli3zW3EiIiLV6f3336dfv37llvft25f58+cHoCIREQlmpxWu7HY72dnZ5Za7XC7CwsLOuCgREZGacPjwYRwOR7nlsbGxHDp0KAAViYhIMDutcDVs2DD+/Oc/88MPP2AYBoZh8P3333PLLbdw5ZVX+rtGERGRatGmTRsWLVpUbvlnn33GWWedFYCKREQkmJ3WgBYvvvgiY8aMoU+fPt6LLxYVFTFixAief/55f9YnIiJSbaZNm8akSZM4ePAgl1xyCQBffvklTz/9NC+88EKAqxMRkWBzWi1XcXFxfPjhh2zbto358+czf/58tm3bxoIFC4iLi6v0dpYvX87w4cNp0qQJFouFhQsXnvI5c+fOpWvXrkRGRtK4cWNuuukmDh8+7LPOe++9R/v27QkPD+ecc87h008/reIeiohIfXDTTTfxzDPP8PrrrzNgwAAGDBjA3Llz+cc//sGECRMCXZ6IiASZSrdcTZs27aSPf/311977zz77bKW2mZOTQ9euXbnpppsYNWrUKddfsWIFN954I8899xzDhw9nz5493HLLLUyYMIEPPvgAgO+++47rrruOmTNnMmzYMObNm8fIkSNZt24dnTt3rlRdIiJSP+Tl5TFmzBhuvfVWDh48yP79+1m8eDFJSUmBLk1ERIJQpcPVjz/+WKn1LBZLpV98yJAhDBkypNLrr1y5kpYtW3qHe2/VqhU333wzf//7373rvPDCC1x++eXceeedADzyyCMsXryYl156iX/84x+Vfi0REan7RowYwahRo7jlllsIDQ1l4MCBhIaGcujQIZ599lluvfXWQJcoIiJBpNLhqmzLVKD06dOHe+65h08//ZQhQ4Zw4MAB5s+fz9ChQ73rrFy5slwr2+DBgyvV5VBEROqXdevW8dxzzwEwf/58kpKS+PHHH3n//fd54IEHFK5ERKRKTuucq0Dp168fc+fO5dprryUsLIzk5GQcDgcvv/yyd519+/aV686RlJTEvn37TrjdgoICnE6nzyQiInVfbm4uMTExAHzxxReMGjUKq9XK+eefz86dOwNcnYiIBJugCle//PILU6ZM4YEHHmDt2rUsWrSIHTt2cMstt5zRdmfOnInD4fBOKSkpfqpYRERqszZt2rBw4ULS09P5/PPPueyyywA4cOAAsbGxAa5ORESCTVCFq5kzZ9KvXz/uvPNOunTpwuDBg3nllVd44403yMjIACA5OZn9+/f7PG///v0kJyefcLvTp08nKyvLO6Wnp1frfoiISO3wwAMP8Ne//pWWLVvSu3dv+vTpA5itWN27dw9wdSIiEmyCKlzl5uZitfqWbLPZADAMAzDPy/ryyy991lm8eLH3gFkRu91ObGyszyQiInXfVVddxa5du1izZo3PxYQvvfRS77lYIiIilXVaFxH2F5fLRWpqqnc+LS2N9evXEx8fT/PmzZk+fTp79uxhzpw5AAwfPpwJEyYwa9YsBg8eTEZGBlOnTqVXr140adIEgClTptC/f3+eeeYZrrjiCt555x3WrFnDq6++GpB9FBGR2i05Oblc74ZevXoFqBoREQlmAQ1Xa9asYcCAAd750lH+xowZw5tvvklGRga7du3yPj527Fiys7N56aWXuOOOO4iLi+OSSy7xGYq9b9++zJs3j/vuu4977rmHtm3bsnDhQl3jSkREREREqpXFKO1PJ15OpxOHw0FWVpa6CIqI1CB9/56Y3hsRkcCoyvdvQFuuREREpP4q8hThKnSRXZhNdlH2sfslk6vIRU5RDjFhMSREJNAoohEJEQkkRCQQHxFPqDU00LsgIgGWV5xHVkEWmQWZHM0/6r1//JSVn8VzA54jOerEg9z5g8KViIiIVJnH8JBblIuryIWz0FlxSCoqCUkVPOYqcpFXnHdGNTSwNyAhMoGE8AQaRTaiYURDnwBWOkWHRmOxWPy053IqHsNDfnE+he5C8t35FLgLzKm44Nj9kqnset71K1jvZNsocBdgGAY2q40QSwhWi9V732a1YbPYCLGGYLPYsFqs3vvHr+O9reCxEGvJdstsq9zzy2yj7Don2qbNaiPcFk5ESMSxKdS8rY9/ODAMA1eRi8z88sEosyDzWGjK912W786v9GsczjuscCUiIiL+V+AuKBd8ThSEKnospygHj+HxSy0RIRHEhMUQExpDTFgM0WHR3vuRoZFkF2ZzMO8gh/IOcSjvEIfzDuM23BwtOMrRgqP8yq8n3X64LfyEwatsKIsPjyfEWnd/Grk9bvKK88gpyiG3ONecisypdFlecZ5PkCkbegrdhacMTfnufIo9xYHe1aAXYg0xw5btWOA6o6nMNsJt4dX+xwa3x01WYZY3AJVtUTpaUHL/uBDlLHBSbJzev50QSwgOu4MG4Q1w2B3E2eN8ptLHmsU08/OeVlBLtb+CiIiIBNRO507uXn63TytToafQL9sOsYb4hqKSkFT2/vGPlQ1Q0WHRVQ40HsNDZkEmB3MPcjjvsE/wOn5yFbnId+ezx7WHPa49J92uBQsNwhuUD2AVhLKo0Khq/YFqGAYF7gJvAMopyvEJRjlFOWYwKvYNR7lFueQU55BX5LtuXnHeGbcUno4QSwj2EDt223FTmWXhtnDCbGGEh4QTZi25tYURbguvcH3v80rWs9vsWLFSbBTj9rhxGyVTyf1iT7F3vtgoxmN4vPcrWqfs8ytax2N4yq1/Os8vNorJL84nrzjPe+s23AAUe4rNP2SQDX7+2CxYCA8JP62AFm4LJ9QWirPA6dN6dHx3PGeh87TriwiJKBeQSudPFJ6q+/9jVShciYiI1HEWLPx8+OcKH4sOjT5h8DlhKCoToGrir+DHs1qsxIfHEx8ef8p1c4tyOZx/2CdwHcw9yOH8wxzMLdMaln8Yj+HhSP4RjuQfYdvRbSfdbkRIBA3DG9IosnzwSohIwGF3UFBcUC7glA1CZQNRaSgqvZ9bnOv9oe1vNouNyJBIIkPNKSokynu/tGWjNNyUhp6ThaOyYafsfJgtrE63BPqbYRgUeYq8Qbi0JTGvyJzPd+f7zJdbr0xIq2gqcBeYr4NRY2E7JizGt/XIflw4Ci8foMJDwqu9ruqkf/EiIiJ1XGJkIi8OeNEMRWHHQlJUSBQ2qy3Q5VWr0tCQEpNy0vXcHrOb4eE8M4gd3xpWGsgO5R3yBqXdrt3sdu2u9n0obTmICo0iMiSSqNAoIkIjjoWikmWl9yNDI33XLX1uyfIwa1it+Su/HGOxWAizhRFmC8Nhd/h9+26P+1hAO346LrDlu/PJLcqtcN0iTxGxYbEVhqMG9gbe+w67o16G6/q3xyIiIvVMeEg4A5oPOPWK9ZjNavO2OrWj3UnXzS3K9emOeDDvoE8oO5x3mKyCLMJDwr0hp6JWIm8oqiAclQ1GdT0AS82wWW1EWaOICo0KdCl1msKViIjIGZo5cyYffPABW7ZsISIigr59+/L3v/+ddu2O/UjPz8/njjvu4J133qGgoIDBgwfzyiuvkJSUFMDK5XR4W8NiT94aJiL1jzXQBYiIiAS7ZcuWMXHiRL7//nsWL15MUVERl112GTk5Od51br/9dj766CPee+89li1bxt69exk1alQAqxYREX+zGIZhBLqI2qYqV2EWERH/qSvfvwcPHiQxMZFly5Zx0UUXkZWVRaNGjZg3bx5XXXUVAFu2bKFDhw6sXLmS888//5TbrCvvjYhIsKnK969arkRERPwsKysLgPh4czS7tWvXUlRUxMCBA73rtG/fnubNm7Ny5coKt1FQUIDT6fSZRESkdlO4EhER8SOPx8PUqVPp168fnTt3BmDfvn2EhYURFxfns25SUhL79u2rcDszZ87E4XB4p5QUnd8jIlLbKVyJiIj40cSJE/n555955513zmg706dPJysryzulp6f7qUIREakuGi1QRETETyZNmsTHH3/M8uXLadasmXd5cnIyhYWFZGZm+rRe7d+/n+Tk5Aq3Zbfbsdvt1V2yiIj4kVquREREzpBhGEyaNIkFCxbw1Vdf0apVK5/He/ToQWhoKF9++aV32datW9m1axd9+vSp6XJFRKSaqOVKRETkDE2cOJF58+bx4YcfEhMT4z2PyuFwEBERgcPhYNy4cUybNo34+HhiY2O57bbb6NOnT6VGChQRkeCgcCUiInKGZs2aBcDFF1/ss3z27NmMHTsWgOeeew6r1cro0aN9LiIsIiJ1h8KViIjIGarMJSPDw8N5+eWXefnll2ugIhERCQSdcyUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiBwpXIiIiIiIifqBwJSIiIiIi4gcKVyIiIiIiIn6gcCUiIiIiIuIHClciIiIiIiJ+oHAlIiIiIiLiByGBLqBOevJJMAz43e/g7LMDXY2IiIiIiNQAtVz5m9sNTz8Nd98N7dpBp05w//2wbp0ZuEREREREpE5SuPK34mJ4+GEYNAhCQuCXX+DRR6FHD2jVCm6/HZYvN0OYiIiIiIjUGQpX/ma3wy23wBdfwIED8PbbZvfAiAjYuROefx7694fGjWHCBPj0UygoCHTVIiIiIiJyhhSuqlODBvCHP8AHH8ChQ7BgAfzxjxAXBwcPwmuvwRVXQKNGcN118N//QnZ2oKsWEREREZHToHBVUyIjYeRImDPHbNFavBhuvdVswcrOhnfegWuvNYPW8OHwxhtmIBMRERERkaCgcBUIoaEwcCC88grs3g0rV8Kdd0KbNmYXwY8/hnHjICkJBgyAF1+E9PRAVy0iIiIiIiehcBVoViucf745fPu2bbBxIzz0EHTrBh4PLF0KU6ZA8+bQsyc8/jhs3hzoqkVERERE5DgWw9D44MdzOp04HA6ysrKIjY0NXCFpaeZ5WgsWwIoVvkO5t29vDpTxu9/BeeeBxRK4OkVE/KTWfP/WQnpvREQCoyrfv2q5qs1atYJp0+CbbyAjA159FS6/3OxWuGULzJwJvXpBixYwebLZylVcHOiqRURERETqpYCGq+XLlzN8+HCaNGmCxWJh4cKFJ11/7NixWCyWclOnTp2868yYMaPc4+3bt6/mPakBSUnm0O2ffWaONDhvHlx1FURFmedj/d//mednJSfDTTeZ523l5we6ahERERGReiOg4SonJ4euXbvy8ssvV2r9F154gYyMDO+Unp5OfHw8V199tc96nTp18lnv22+/rY7yA8fhMIduf+89M2j9738wdizEx8PhwzB7tjniYKNGcM018J//gNMZ6KpFREREROq0kEC++JAhQxgyZEil13c4HDgcDu/8woULOXr0KH/605981gsJCSE5OdlvddZqERFmkBo+3OwS+M035nW1Fi40RyJ87z1zCguDSy81z9EaMQISEwNduYiIiIhInRLU51y9/vrrDBw4kBYtWvgs//XXX2nSpAlnnXUWN9xwA7t27TrpdgoKCnA6nT5TUAoJMbsG/t//wa5dsGoV3H03tGsHhYVml8I//9m8ttZFF8Hzz8POnYGuWkRERESkTgjacLV3714+++wzxo8f77O8d+/evPnmmyxatIhZs2aRlpbGhRdeSHZ29gm3NXPmTG+rmMPhICUlpbrLr34Wizl0+8yZ5uAXv/wCjz0GPXqYQ7x/8w3cfju0bAnnnguPPgqbNvmOSCgiIiIiIpVWa4Zit1gsLFiwgJEjR1Zq/ZkzZ/LMM8+wd+9ewsLCTrheZmYmLVq04Nlnn2XcuHEVrlNQUEBBQYF33ul0kpKSUneHu921y+w2+MEHZsjyeI491q0b3Hef2X3QGrTZW0SClIYbPzG9NyIigVHnh2I3DIM33niDP/7xjycNVgBxcXGcffbZpKamnnAdu91ObGysz1SnNW9+bOj2ffvg9dfhiivM87LWrzdHIezaFf77X3C7A12tiEitd6rRbysa7fbyyy8PTLEiIlJtgjJcLVu2jNTU1BO2RJXlcrnYvn07jRs3roHKglCjRseGbs/IgPvvh9hY+PlnuPZaOOccc9h3hSwRkROqzOi3l19+uc9Itv/5z39qsEIREakJAQ1XLpeL9evXs379egDS0tJYv369dwCK6dOnc+ONN5Z73uuvv07v3r3p3Llzucf++te/smzZMnbs2MF3333H7373O2w2G9ddd1217kudEB8PDz9sDnLx0EMQFwebN8MNN0DHjjBnji5SLCJSgSFDhvDoo4/yu9/97oTr2O12kpOTvVODBg1qsEIREakJAQ1Xa9asoXv37nTv3h2AadOm0b17dx544AEAMjIyyo30l5WVxfvvv3/CVqvdu3dz3XXX0a5dO6655hoaNmzI999/T6NGjap3Z+qSuDh44AEzZD32mBm6tm2DMWPMkQffeAOKigJdpYhIUFm6dCmJiYm0a9eOW2+9lcOHDwe6JBER8bNaM6BFbaKTho+TnQ2zZsHTT5sXLQZzlMHp082LF5/ivDcRkcqqC9+/FQ3Q9M477xAZGUmrVq3Yvn0799xzD9HR0axcuRKbzVbhdurdYEsiIrVUnR/QQmpYTAz87W+QlmYGrKQk2LEDbr4Z2rSBV16B/PxAVykiUmv9/ve/58orr+Scc85h5MiRfPzxx6xevZqlS5ee8Dl18jIhIiJ1nMKVVF5UFNxxB/z2m3kB4saNIT0dJk6E1q3hxRchLy/QVYqI1HpnnXUWCQkJJx3Jdvr06WRlZXmn9PT0GqxQREROR0igC5AgFBkJU6aYLVevvw5PPAG7d5vLZs40W7luvtlcT0REytm9ezeHDx8+6Ui2drsdu91eg1VJTXG73RTp3OV6IzQ09ITdf6XuUbiS0xcebrZajR8Pb74Jjz9uXqB42jQzcP31r3DrrRAdHehKRUSqlcvl8mmFKh39Nj4+nvj4eB566CFGjx5NcnIy27dv529/+xtt2rRh8ODBAaxaapphGOzbt4/MzMxAlyI1LC4ujuTkZCwWS6BLkWqmAS0qUBdOqA6IwkJ4+21zhMG0NHNZw4ZmV8KJE83rZ4mInESwfv8uXbqUAQMGlFs+ZswYZs2axciRI/nxxx/JzMykSZMmXHbZZTzyyCMkJSVV+jWC9b2RYzIyMsjMzCQxMZHIyEj90K4HDMMgNzeXAwcOEBcXp+uuBqmqfP8qXFVAB7AzVFRkXnj40Ueh9C+5DRrA7bfDbbeZQ72LiFRA378npvcmuLndbrZt20ZiYiINGzYMdDlSww4fPsyBAwc4++yz1UUwCGm0QAms0FDzmlibN8O//21eG+voUfPaWS1bwoMPwpEjga5SRESkxpSeYxWp85HrpdLPXefa1X0KV1J9QkLghhtg0yb4z3+gY0fIyoKHHzZD1r33wqFDga5SRESkxqgrYP2kz73+ULiS6mezwe9/Dxs3wnvvQZcu5oWJH3/cDFl33QUHDgS6ShERERGRM6JwJTXHaoWrroIff4QFC6B7d8jJgSefNEPWHXfAvn2BrlJERERE5LQoXEnNs1ph5EhYuxY++gh69jQvPvzss9CqFUydCnv3BrpKERERqeU2bdrE6NGjadmyJRaLheeffz7QJUk9p3AlgWOxwLBh8MMP8NlncP75kJ8PL7wAZ50FkyZBenqgqxQREZEyCgsLA12CV25uLmeddRZPPPEEycnJgS5HROFKagGLBS6/HL77DhYvhgsugIICePllaN0abrkFduwIdJUiIiL10sUXX8ykSZOYOnUqCQkJDB48mGXLltGrVy/sdjuNGzfm7rvvpri42Pucli1blmtF6tatGzNmzPDOb9myhQsuuIDw8HA6duzIkiVLsFgsLFy40LtOeno611xzDXFxccTHxzNixAh2lPlN0LNnT5566il+//vfY7fbq+kdEKk8hSupPSwWGDgQli+Hr76Ciy82r5n1z39C27Ywfjxs3x7oKkVERPzCMAxyC4sDMlX1MqdvvfUWYWFhrFixghkzZjB06FB69uzJTz/9xKxZs3j99dd59NFHK709t9vNyJEjiYyM5IcffuDVV1/l3nvv9VmnqKiIwYMHExMTwzfffMOKFSuIjo7m8ssvr1WtZyJlhQS6AJFyLBYYMMCcli+HRx6BJUvg9dfhzTfhD3+Ae+6Bs88OdKUiIiKnLa/ITccHPg/Ia//y8GAiwyr/M7Bt27Y8+eSTAMyZM4eUlBReeuklLBYL7du3Z+/evdx111088MADWK2n/tv94sWL2b59O0uXLvV253vssccYNGiQd513330Xj8fDa6+95h3KfPbs2cTFxbF06VIuu+yyquyySI1Qy5XUbhddZHYVXLHC7DrodsNbb0GHDmbI2rw50BWKiIjUeT169PDe37x5M3369PG5dlO/fv1wuVzs3r27UtvbunUrKSkpPudJ9erVy2edn376idTUVGJiYoiOjiY6Opr4+Hjy8/PZrp4sUkup5UqCQ9++5qAXq1aZLVkffwxz58K8eXDNNfDnP5vnaoWFBbpSERGRSokItfHLw4MD9tpVERUVVaX1rVZrua6HRUVFVdqGy+WiR48ezJ07t9xjjRo1qtK2RGqKwpUEl169zOHb160zQ9bChfDuu+YUEwOXXQZXXAFDhoBGDRIRkVrMYrFUqWtebdGhQwfef/99DMPwtl6tWLGCmJgYmjVrBpjhJyMjw/scp9NJWlqad75du3akp6ezf/9+kpKSAFi9erXP65x77rm8++67JCYmEhsbW927JeIX6hYowencc80LEa9fD2PHQqNGkJ0N778PN90EjRub18968EGztcvjCXTFIiIidcJf/vIX0tPTue2229iyZQsffvghDz74INOmTfOeb3XJJZfw9ttv880337Bx40bGjBmDzXastWzQoEG0bt2aMWPGsGHDBlasWMF9990H4A1sN9xwAwkJCYwYMYJvvvmGtLQ0li5dyuTJk73dDwsLC1m/fj3r16+nsLCQPXv2sH79elJTU2v4XRExKVxJcOvaFWbPhn37zOtlPfAAlPYLX7MGHn4Yevc2w9bYsfDf/0JmZiArFhERCWpNmzbl008/ZdWqVXTt2pVbbrmFcePGecMRwPTp0+nfvz/Dhg3jiiuuYOTIkbRu3dr7uM1mY+HChbhcLnr27Mn48eO9owWGh4cDEBkZyfLly2nevDmjRo2iQ4cOjBs3jvz8fG9L1t69e+nevTvdu3cnIyODp59+mu7duzN+/PgafEdEjrEYVR2Lsx5wOp04HA6ysrLUDB2sMjJg0SL45BP44guzVauUzWaen3XFFebUoYM5QqGIBJy+f09M701wy8/PJy0tjVatWnnDg/hasWIFF1xwAampqT5BrC7Q5x/cqvL9G3wdfUUqo3Fj+NOfzKmwEL79Fj791AxbW7bAsmXm9Le/QcuWMHSoGbQGDICIiEBXLyIiUuctWLCA6Oho2rZtS2pqKlOmTKFfv351LlhJ/aJugVL3hYXBJZfA00+bQ7dv3w4vvmgO7W63w44d8MorZrhq2BCGDTPnd+4MdOUiIiJ1VnZ2NhMnTqR9+/aMHTuWnj178uGHHwa6LJEzom6BFVDXi3okJwe++sps0frkEzj++hydOh3rPti3L4SosVekOun798T03gQ3dQur3/T5Bzd1CxSprKgoGD7cnAwDfv75WND67jvYtMmcnnwSHA4YPPjYUO+6xoaIiIiIlKFwJVLKYoFzzjGnu++GI0fg88/Nc7U++wwOHzZHG/zvf811e/Uyg9bQodC9O1jVy1ZERESkPtOvQZETiY+H666Dt9+G/fvNlqx774Vu3cxWrtKh3887D5o2hXHj4IMPfEcmFBEREZF6Q+FKpDJsNujTBx59FH780Tw361//gpEjza6F+/bBG2/A6NHmoBiXXgrPPgtbt5pBTERERETqPIUrkdPRtCmMHw8LFpjdBRcvhqlToW1bKCoyB8m44w5o395cNnmy2cUwPz/QlYuIiIhINVG4EjlTdjsMHAjPPQfbtpnTc8/BoEHmMPDbt8P//Z859HvDhjBiBLz6avmRCUVEREQkqClcifhb27ZmK9YXX5itWgsWmK1cTZpAbi78739w882QkmIOivHkk2YAExEREZGgpnAlUp2io83zsv71L7Ol6scfzfO2+vQxRxxcvRruugvatDFHHHzsMdiyJdBVi4iIBIV//etfXHjhhTRo0IAGDRowcOBAVq1aFeiypB5TuBKpKRaLOdLgvfeaIw9mZMA//mF2KbTZYP16uO8+6NABOneGBx+EjRs1IIaIiNQqhYWFgS7Ba+nSpVx33XV8/fXXrFy5kpSUFC677DL27NkT6NKknlK4EgmUpCSze+DixeZQ76+/bl6cODTUvHDxww9Dly7Qrh3ccw+sXaugJSJSlxgGFOYEZqrC8eTiiy9m0qRJTJ06lYSEBAYPHsyyZcvo1asXdrudxo0bc/fdd1NcXOx9TsuWLXn++ed9ttOtWzdmzJjhnd+yZQsXXHAB4eHhdOzYkSVLlmCxWFi4cKF3nfT0dK655hri4uKIj49nxIgR7Nixw/v43Llz+ctf/kK3bt1o3749r732Gh6Phy+//LKqn4aIX+giwiK1QcOGcNNN5pSZCR99BO+/D4sWwa+/wsyZ5tSyJVx1lTnke69eunCxiEgwK8qFx5sE5rXv2QthUZVe/a233uLWW29lxYoV7Nu3j6FDhzJ27FjmzJnDli1bmDBhAuHh4T7h6WTcbjcjR46kefPm/PDDD2RnZ3PHHXf4rFNUVMTgwYPp06cP33zzDSEhITz66KNcfvnlbNiwgbCwsHLbzc3NpaioiPj4+Ervm4g/KVyJ1DZxcfDHP5pTdjZ8+inMn2/e7tgBTz9tTs2awahRZtjq29fsWigiIlIN2rZty5NPPgnAnDlzSElJ4aWXXsJisdC+fXv27t3LXXfdxQMPPIC1En/4W7x4Mdu3b2fp0qUkJycD8NhjjzFo0CDvOu+++y4ej4fXXnsNi8UCwOzZs4mLi2Pp0qVcdtll5bZ711130aRJEwYOHOiP3RapMoUrkdosJgauvdaccnPNlqz58+Hjj80BMl580ZySksygNXo09O8PIfqvLSJS64VGmi1IgXrtKujRo4f3/ubNm+nTp4838AD069cPl8vF7t27ad68+Sm3t3XrVlJSUrzBCqBXr14+6/z000+kpqYSExPjszw/P5/tFYyy+8QTT/DOO++wdOlSwsPDK71vIv6kX2AiwSIy0gxQo0aZFyNevNjsOvjhh+Y5W7NmmVPDhvC735lB65JLzGttiYhI7WOxVKlrXiBFRVWtTqvVinHceV1FRUVV2obL5aJHjx7MnTu33GONGjXymX/66ad54oknWLJkCV26dKnS64j4k07YEAlG4eEwfDi8+aYZrD77zLyWVkKCeW2t114zB8dISoIxY8xzuPLzA121iIjUAR06dGDlypU+4WnFihXExMTQrFkzwAw/GRkZ3sedTidpaWne+Xbt2pGens7+/fu9y1avXu3zOueeey6//voriYmJtGnTxmdyOBze9Z588kkeeeQRFi1axHnnnef3/RWpCoUrkWAXFgaXX25eSysjA778Em69FZKTzcEx5syBK6+ERo3g+uvN1q7c3EBXLSIiQeovf/kL6enp3HbbbWzZsoUPP/yQBx98kGnTpnnPt7rkkkt4++23+eabb9i4cSNjxozBVubc4EGDBtG6dWvGjBnDhg0bWLFiBffddx+At7vhDTfcQEJCAiNGjOCbb74hLS2NpUuXMnnyZHbv3g3A3//+d+6//37eeOMNWrZsyb59+9i3bx8ul6uG3xURk8KVSF0SEmJ2BXzlFfOcrOXLYcoUc/ALlwv+8x9zAIxGjczbd94xB80QERGppKZNm/Lpp5+yatUqunbtyi233MK4ceO84Qhg+vTp9O/fn2HDhnHFFVcwcuRIWrdu7X3cZrOxcOFCXC4XPXv2ZPz48dx7770A3vOlIiMjWb58Oc2bN2fUqFF06NCBcePGkZ+fT2xsLACzZs2isLCQq666isaNG3unp59+ugbfEZFjLMbxHWIFp9OJw+EgKyvL+59XJKh5PLBqldlq9f77UKZrBnY7DB5shq3hw83RCkUCRN+/J6b3Jrjl5+eTlpZGq1atNNjCCaxYsYILLriA1NRUnyBWF+jzD25V+f7VgBYi9YHVCuefb05PPgk//miOOjh/vnkdrf/9z5xCQ2HgQHMwjBEjzHO4REREqsGCBQuIjo6mbdu2pKamMmXKFPr161fngpXUL+oWKFLfWCxw7rnw+OOwdSts2AAPPgidOkFR0bHBMZKTzaD1j3+Yg2aIiIj4UXZ2NhMnTqR9+/aMHTuWnj178uGHHwa6LJEzom6BFVDXC6m3tmw51nXwxx+PLbdY4MILzRat0aOhadPA1Sh1mr5/T0zvTXBTt7D6TZ9/cKvK969arkTkmPbt4d57Yd06SE2Fv/8devUCw/AdHOPCC+Hll9WiJSIiIlKGwpWIVKx1a/jb3+CHH2DnTnjuOejXz3zs229h0iRo0gQuvdQcBv7w4cDWKyIiIhJgClcicmrNm8PUqWaoSk+HZ581W7Q8HvjqK/jzn81ztIYOhbfegqysQFcsIiIiUuMUrkSkapo1g9tvN1u0tm+HmTOhWzcoLjYHwxg7FhITYeRI87paupCjiIiI1BMKVyJy+s46C+6+2xz8YutWePhh6NgRCgvhww/h+uvNoHXNNeYgGXl5ga5YREREpNooXImIf5x9Ntx/P2zaBBs3mgNjtGljBqr33jMvUpyYCH/4A3z0ERQUBLpiEREREb9SuBIR/+vcGR59FLZtg7Vr4c47zfO2XC6YOxeuvNI8R+umm+Dzz83ra4mIiIgEOYUrEak+pRcsfvJJ2LEDVq40h3Nv3BgyM2H2bLj8cnPUwVtuga+/Brc70FWLiEiQmDFjBt26dQt0GSJeClciUjMsFjj/fHj+eXPEwaVL4dZboVEjOHQI/vlPuOQSc8CMyZPhu+/M0QhFRKRWKSwsDHQJIrWWwpWI1DybDfr3h1degb174YsvYNw4iIuDffvg//7PvKZWy5bw17/CmjXmhYxFROoQwzDILcoNyGRU4Tv14osvZtKkSUydOpWEhAQGDx7MsmXL6NWrF3a7ncaNG3P33XdTXFzsfU7Lli15/vnnfbbTrVs3ZsyY4Z3fsmULF1xwAeHh4XTs2JElS5ZgsVhYuHChd5309HSuueYa4uLiiI+PZ8SIEezYseM033GR6hcS6AJEpJ4LCYFBg8zplVdg8WJ4911YuNBs4XrmGXM66yy49lr4/e/hnHPMljARkSCWV5xH73m9A/LaP1z/A5GhkZVe/6233uLWW29lxYoV7Nu3j6FDhzJ27FjmzJnDli1bmDBhAuHh4T7h6WTcbjcjR46kefPm/PDDD2RnZ3PHHXf4rFNUVMTgwYPp06cP33zzDSEhITz66KNcfvnlbNiwgbCwsKrsskiNUMuViNQeYWFwxRUwZw4cOAAffGAGqshI+O0385paXbuaw70/9BBs2RLoikUAWL58OcOHD6dJkybl/vIOZgvFAw88QOPGjYmIiGDgwIH8+uuvgSlW5DS0bduWJ598knbt2vHFF1+QkpLCSy+9RPv27Rk5ciQPPfQQzzzzDJ5KdudevHgx27dvZ86cOXTt2pULLriAxx57zGedd999F4/Hw2uvvcY555xDhw4dmD17Nrt27WLp0qXVsJciZ04tVyJSO4WHw+9+Z045OfDxx2aL1qefmqFqxgxz6tLFbM269lqzdUskAHJycujatSs33XQTo0aNKvf4k08+yYsvvshbb71Fq1atuP/++xk8eDC//PIL4eHhAahYaoOIkAh+uP6HgL12VfTo0cN7f/PmzfTp0wdLmR4E/fr1w+VysXv3bpo3b37K7W3dupWUlBSSk5O9y3r16uWzzk8//URqaioxMTE+y/Pz89m+fXuV6hepKQpXIlL7RUWZ4enaa8HpNC9Q/O675jDuGzaY0z33wHnnmUHrmmsgJSXQVUs9MmTIEIYMGVLhY4Zh8Pzzz3PfffcxYsQIAObMmUNSUhILFy7k97//fU2WKrWIxWKpUte8QIqKiqrS+lartdx5XUVVvOyGy+WiR48ezJ07t9xjjRo1qtK2RGqKugWKSHCJjYU//tFsydq/H157DQYOBKvVHPjir381r6nVr585MEZGRqArlnouLS2Nffv2MXDgQO8yh8NB7969Wbly5QmfV1BQgNPp9JlEaoMOHTqwcuVKn/C0YsUKYmJiaNasGWCGn4wy379Op5O0tDTvfLt27UhPT2f//v3eZatXr/Z5nXPPPZdff/2VxMRE2rRp4zM5HI7q2j2RMxLQcHWqPurHGzt2LBaLpdzUqVMnn/VefvllWrZsSXh4OL1792bVqlXVuBciEjDx8eYog4sXmyHqlVfgoovMwS6++84c0r1pUxgwwAxa27Zp1EGpcfv27QMgKSnJZ3lSUpL3sYrMnDkTh8PhnVLUGiu1xF/+8hfS09O57bbb2LJlCx9++CEPPvgg06ZNw2o1f1pecsklvP3223zzzTds3LiRMWPGYLPZvNsYNGgQrVu3ZsyYMWzYsIEVK1Zw3333AXi7G95www0kJCQwYsQIvvnmG9LS0li6dCmTJ09m9+7d3m3l5eWxfv16n0ndBiVQAhquSvuov/zyy5Va/4UXXiAjI8M7paenEx8fz9VXX+1d591332XatGk8+OCDrFu3jq5duzJ48GAOHDhQXbshIrVBYqJ53axly8xRBp9/3ryulmGY19SaPBnatTOHd58wAf77Xzh8OMBFi5zY9OnTycrK8k7p6emBLkkEgKZNm/Lpp5+yatUqunbtyi233MK4ceO84QjMf7/9+/dn2LBhXHHFFYwcOZLWrVt7H7fZbCxcuBCXy0XPnj0ZP3489957L4D3PMTIyEiWL19O8+bNGTVqFB06dGDcuHHk5+cTGxvr3da2bdvo3r27z3TzzTfX0Lsh4stiVOVCB9XIYrGwYMECRo4cWennLFy4kFGjRpGWlkaLFi0A6N27Nz179uSll14CwOPxkJKSwm233cbdd99dqe06nU4cDgdZWVk+/3lFJAjt3AnvvQeffQbffgtlL35psZjnaQ0aBJddBn36mCMWSsDUhe/f449nv/32G61bt+bHH3+kW7du3vX69+9Pt27deOGFFyq13brw3tRn+fn5pKWl0apVKw1icgIrVqzgggsuIDU11SeI1QX6/INbVb5/g/qcq9dff52BAwd6g1VhYSFr16716ddutVoZOHDgSfu1i0gd1qKFeR7Wl1/C0aNmyLr9dujc2WzVWr0aHn8cLr7Y7GY4bBi8+CJs3qwuhOIXrVq1Ijk5mS+//NK7zOl08sMPP9CnT58AViYSWAsWLGDx4sXs2LGDJUuW8Oc//5l+/frVuWAl9UvQjha4d+9ePvvsM+bNm+dddujQIdxud4X92rec5Ho4BQUFFBQUeOd10rBIHRUZCZdfbk4Ae/ea52uVTgcOwCefmBNAs2Zmi9agQeagGQkJgatdajWXy0Vqaqp3Pi0tjfXr1xMfH0/z5s2ZOnUqjz76KG3btvUOxd6kSZMq9dYQqWuys7O566672LVrFwkJCQwcOJBnnnkm0GWJnJGgDVdvvfUWcXFxfjkwzZw5k4ceeujMixKR4NKkCYwZY04eD2zcCF98YU7ffAO7d8Mbb5iTxQLdu5th67LLoG9fsNsDvQdSS6xZs4YBAwZ456dNmwbAmDFjePPNN/nb3/5GTk4Of/7zn8nMzOSCCy5g0aJF6h4k9dqNN97IjTfeGOgyRPwqKMOVYRi88cYb/PGPfySszPkRCQkJ2Gw2n2E9Afbv3+9zkbrjTZ8+3XsgBLPlSqMyidQzVit07WpOd94JeXlmwFq82AxbGzbAunXm9MQTZitY//7HWrY6djQDmNRLF198cblr+pRlsVh4+OGHefjhh2uwKhERqWlBec7VsmXLSE1NZdy4cT7Lw8LC6NGjh0+/do/Hw5dffnnSfu12u53Y2FifSUTquYgIMzg99RT89JM51Pvbb5vX2EpOhtxc3/O3mjWDsWNh3jyze6GIiIjUOwFtuTpVH/Xp06ezZ88e5syZ4/O8119/nd69e9O5c+dy25w2bRpjxozhvPPOo1evXjz//PPk5OTwpz/9qdr3R0TqsORk+MMfzMkw4OefzRatxYvN4d/37oW33jIngG7djrVqXXABqPuXiIhInRfQcHWqPuoZGRns2rXL5zlZWVm8//77Jxy69tprr+XgwYM88MAD7Nu3j27durFo0aJyg1yIiJw2iwXOOcec7rgD8vPNYd5LuxCuX39sevJJM1j1739syPfOndWFUEREpA6qNde5qk10LREROSMHDsCSJcdatvbu9X08OflY0Bo40JwXQN+/J6P3JrjpOkf1mz7/4FaV79+gHNBCRKRWS0yE6683J8OAX3451qq1dCns22eev/X22+b6Xboc60J44YXm+V4iIiISdBSuRESqk8UCnTqZ09SpUFAA3313bMj3devMkQg3bICnnzaHd7/wwmNhq3NnCNFXtYiISDDQEVtEpCbZ7TBggDnNnAkHD8KXXx5r2dq92+xSuGSJub7NBi1aQJs20Lq173TWWRAVFdj9EREJoBkzZrBw4ULWr18f6FKkphgGFOVBfhbkZ5q3eZkVzFfw2NiPoUHLai1P4UpEJJAaNYLf/96cDAO2bj3WqrVsGbhc8Ntv5lSR5OSKg1fr1tCwoQbOEBG/Kyws9LnOqEiVedwlgecEAanCsFRm3l14eq+be0ThSkSk3rBYoH17c5o8GTwe8/paqamwfXv56ehR8/ytffvM0QqP53BUHLpatzavy2UNyksditQZhmFg5OXV1IuZP2g9bvAUYbGHYCm5j6cY3MXmreEu+aNMyWSxcPGIP9C5w9mEhITw7/c+5JyO7Zhx11TufHAmP/28mfgGDsZcdzWP3v83QkJCAQstz+nF1Fv/zNRJN5dsB7r1GcDI4UOZce/dYLGwZeuvjP/LFNas+5GzWrXkxWeeYtAVV7Lgv+8wcuSVgIX03enc8bfpfLF4CVarlQsvuIAXnn+Olq1aldR4cq+88grPPfcc6enpOBwOLrzwQubPnw9Ay5YtmTp1KlOnTvWu361bN0aOHMmMGTMA8wLg//jHP/joo4/46quvaNGiBW+88QaNGjVi/PjxrF69mq5du/L222/TunVr/35mtZlhQHF+5VuMjp8vcJ55DRYrhDsgPM68jYg7yXycOd+o3Zm/7ikoXImI1FZWKzRtak79+5d//MiRikPX9u2wZw9kZZnndK1bV/65dju0alVx8GrVynxcRKqVkZfH1nN7BOS1233yBpaISo5aZ7h5650PuPXGq1ix4HX2HTzM0GvGMvaa4cx57gG2pO5gwp2PEG4tZsYdt5jP8bihIAuce45tx11k/rjO3Inb7WbkVdfSvGkyP3z0Ftk5Odxx3z3metl74OAWioqKGDz49/Tp0YVv3n+VkBAbj77wOpdfNpANS94lLCwUsjPMLmIZP1EaBktv16zfxOTJk3n7pb/Tt1d3jmQ6+eb7tXDoV3M9TzHkHoYjaceeV1pj1m5Kw9sjDz/Es4/P4NmH7+GuBx/l+ut+z1ktWzD99ttontKMm26dzKRbb+azD98/1lugTB1ggaJCc9tZe6AgHGyhYA0Bw1MSbotKgq67zP3S0FvR/ZJbvzy3ittxF5xZ61FZoZGVDEcVzNtjamXvDIUrEZFgFR9vTj17ln8sLw/S0ipu9dqxwxxYY8sWczqexQIpKSdu9XI4qn3XxM8KsiH1S/MvvVYbWGwl963mrXf+VI+VzJ/uYxZLrfwxVC+FRUN4tPkD3xYC1pIf+5bSFm3DbJ3AgJBw2rZpzZNPPQMYzJnxGCnNmvHSiy+YDe7dzmfv0XzumvE4D9x3L1aLxfy8QyMhosGx7VhtYLNDWDSLv1zG9p27WbpwDsmJjQCDx6YbDLpmfMm/oxDe/egzPB6D155+0PvPZvazM4jr0J+lK9dwWf8+x/bH8HjLLrUrfRdRkREMu6Q3MdFRtEhuQPf2LaDQdew5RXlmS0opTzEU5ULOQe+iP119BdcMMr9n7/rztfS5ciz3Tx7L4J5tAJgydjR/mjYDjp6g+zZAsQHZB2HRHeBKr9JHVatVqvXo+Pul6zggpO51L1W4EhGpiyIioGNHczqe2w3p6Sfubuhywa5d5vT11+Wfn5BQPnCVnveVlKQfz7VR9j54b0ygqzCdLHj5hLvSx6zmX/ltYWVuS+5bT7C83Polt9bQCtYtvR9S8XLrCZbbQiv3b91dZLaOHN0PRR6za1Sx2WpiKS6i3ZL5ZqtAabe8sumgcm+oWaNPSLKV3C+Zt4SYg+OUCU+WiIjK/1+1htCjZ2+ISgBg8/Zd9Ol3AZbYxt5V+g28Atdf72V3NjRvnmLWEBnve36LLcxcltCWrfs+JSUlheTOF3of7jW4KTAeGrSA5HP4aeccUnekE9Pu2DoA+QUFbD/igaRzICoRQsIhseOxEFdyO2hkMi1eeJOz+o3k8kGXcvmgS/ndlVcQGRFhrmcNMX/wxzY99jxbqBk8o5NKtgNdup0LkQ3BMEhKOQuAc7p0B3ssYJDUuCn5+QU48z3ExkQdVwfmrcVjvvchEeZn4ikq8xGWfDalrVne+6WfZUX3jwvFPveP387J7of6/luxlbxOhfdLnxt2LBzV0tajQFK4EhGpb2w2aNnSnAYO9H3MMMwRDE8UvA4cgEOHzOmHH8pvOyrKHMVwyhQYN64m9kYqI8QOzfuaf6k33CVdkUpufe5X5rGK1nMfazk4ldLtArirb5drhDd4VRD6wAxVeUfM+9Ep0O8ZyDYgxPwxaqEk61iBEBtgM9e1WI/70VzBj+DSx0qDaTWLquLIpFarFcPwDYpFRUUnWLtiLpeLHj16MHfu3HKPNWrUqOT9KNn/kPJdmWMSolj343qWLl3KF198wQOP/p0Zjz/F6tWriYuLw2oLwQiNgujEYzW6DTMwxDbxLguNawxxzQGwZJn7FJrQEhqa51hZ4nYD4IlvDXFxFe9Mfj64QuGWbyA8/Ng5cKWtxFJnKFyJiMgxFot5EeTEROjbt/zj2dknPs9r1y7IyYGNGyE3t+ZrlxOLaw43fVa9r2GU/KXeJ3iVvV+Jx04U5ErP/3AXmed5uAvL3K9guafouHWOX6/Mck/xKbZZZt3jW5VKz1U5VWawWCEi3gxfoZHmj2trRaGpNDjV7h/bHTp04P3338cwDCwlwW7FihXExMTQrFkzwAw/GRkZ3uc4nU7S0tK88+3atSM9PZ39+/eTlJQEwOrVq31e59xzz+Xdd98lMTGR2NjY06o1JCSEgQMHMnDgQB588EHi4uL46quvGDVq1ClrrFYWi/m5S52jT1VERCovJga6dTOn4xUWmudzbd8OHTrUcGEScN7zqWp3MDgjHnflwp270AyHkQ3NVpGIBlBYZJ4H2aClGa6C2F/+8heef/55brvtNiZNmsTWrVt58MEHmTZtGtaSYHjJJZfw5ptvMnz4cOLi4njggQew2WzebQwaNIjWrVszZswYnnzySbKzs7nvvvsAvIHthhtu4KmnnmLEiBE8/PDDNGvWjJ07d/LBBx/wt7/9zRvk8vLyyl3nKiYmhs2bN/Pbb79x0UUX0aBBAz799FM8Hg/t2rWrVI0ip0PhSkRE/CMsDM4+25xE6iKrDawREBpxGk+uWpe42qxp06Z8+umn3HnnnXTt2pX4+HjGjRvnDUcA06dPJy0tjWHDhuFwOHjkkUd8WoVsNhsLFy5k/Pjx9OzZk7POOounnnqK4cOHE14SPiMjI1m+fDl33XUXo0aNIjs7m6ZNm3LppZf6tGRt27aN7t27+9R46aWXMmPGDD744ANmzJhBfn4+bdu25T//+Q+dOnWqVI0ip8NiHN8hVnA6nTgcDrKysk67GVpERKpO378npvcmuOXn55OWlkarVq284UF8rVixggsuuIDU1NQ6d80off7BrSrfv2q5EhEREZEat2DBAqKjo2nbti2pqalMmTKFfv361blgJfWLwpWIiIiI1Ljs7Gzuuusudu3aRUJCAgMHDuSZZ54JdFkiZ0ThSkRERERq3I033siNN94Y6DJE/KoOD+kjIiIiIiJScxSuRERERGqIxhGrn/S51x8KVyIiIiLVLDQ0FIBcXWC7Xir93Ev/HUjdpXOuRERERKqZzWYjLi6OAwcOAOY1nEovlit1l2EY5ObmcuDAAeLi4nSR4npA4UpERESkBiQnJwN4A5bUH3Fxcd7PX+o2hSsRERGRGmCxWGjcuDGJiYkUFRUFuhypIaGhoWqxqkcUrkRERERqkM1m049tkTpKA1qIiIiIiIj4gcKViIiIiIiIHyhciYiIiIiI+IHOuapA6YXenE5ngCsREalfSr93dcHN8nRsEhEJjKocmxSuKpCdnQ1ASkpKgCsREamfsrOzcTgcgS6jVtGxSUQksCpzbLIY+vNgOR6Ph7179xITE3NaF/hzOp2kpKSQnp5ObGxsNVRYPVR3zQnGmkF117RgrPtMazYMg+zsbJo0aYLVqp7rZenYFDx1B2PNoLprWjDWHYw1Q80em9RyVQGr1UqzZs3OeDuxsbFB9Q+vlOquOcFYM6jumhaMdZ9JzWqxqpiOTcFXdzDWDKq7pgVj3cFYM9TMsUl/FhQREREREfEDhSsRERERERE/ULiqBna7nQcffBC73R7oUqpEddecYKwZVHdNC8a6g7Hm+iJYP5tgrDsYawbVXdOCse5grBlqtm4NaCEiIiIiIuIHarkSERERERHxA4UrERERERERP1C4EhERERER8QOFKxERERERET9QuKoGL7/8Mi1btiQ8PJzevXuzatWqQJd0UsuXL2f48OE0adIEi8XCwoULA13SKc2cOZOePXsSExNDYmIiI0eOZOvWrYEu65RmzZpFly5dvBex69OnD5999lmgy6qSJ554AovFwtSpUwNdyinNmDEDi8XiM7Vv3z7QZZ3Snj17+MMf/kDDhg2JiIjgnHPOYc2aNYEu66RatmxZ7r22WCxMnDgx0KUJwXdcAh2bapKOTTUnWI9LoGNTZSlc+dm7777LtGnTePDBB1m3bh1du3Zl8ODBHDhwINClnVBOTg5du3bl5ZdfDnQplbZs2TImTpzI999/z+LFiykqKuKyyy4jJycn0KWdVLNmzXjiiSdYu3Yta9as4ZJLLmHEiBFs2rQp0KVVyurVq/nnP/9Jly5dAl1KpXXq1ImMjAzv9O233wa6pJM6evQo/fr1IzQ0lM8++4xffvmFZ555hgYNGgS6tJNavXq1z/u8ePFiAK6++uoAVybBeFwCHZtqko5NNSvYjkugY1OVGOJXvXr1MiZOnOidd7vdRpMmTYyZM2cGsKrKA4wFCxYEuowqO3DggAEYy5YtC3QpVdagQQPjtddeC3QZp5SdnW20bdvWWLx4sdG/f39jypQpgS7plB588EGja9eugS6jSu666y7jggsuCHQZZ2zKlClG69atDY/HE+hS6r1gPy4Zho5NgaBjU/UIxuOSYejYVBVqufKjwsJC1q5dy8CBA73LrFYrAwcOZOXKlQGsrO7LysoCID4+PsCVVJ7b7eadd94hJyeHPn36BLqcU5o4cSJXXHGFz7/vYPDrr7/SpEkTzjrrLG644QZ27doV6JJO6n//+x/nnXceV199NYmJiXTv3p1//etfgS6rSgoLC/n3v//NTTfdhMViCXQ59ZqOS4GlY1P1C8ZjU7Adl0DHpqpQuPKjQ4cO4Xa7SUpK8lmelJTEvn37AlRV3efxeJg6dSr9+vWjc+fOgS7nlDZu3Eh0dDR2u51bbrmFBQsW0LFjx0CXdVLvvPMO69atY+bMmYEupUp69+7Nm2++yaJFi5g1axZpaWlceOGFZGdnB7q0E/rtt9+YNWsWbdu25fPPP+fWW29l8uTJvPXWW4EurdIWLlxIZmYmY8eODXQp9Z6OS4GjY1P1C8ZjUzAel0DHpqoIqdati9SAiRMn8vPPPwdFn2WAdu3asX79erKyspg/fz5jxoxh2bJltfYglp6ezpQpU1i8eDHh4eGBLqdKhgwZ4r3fpUsXevfuTYsWLfjvf//LuHHjAljZiXk8Hs477zwef/xxALp3787PP//MP/7xD8aMGRPg6irn9ddfZ8iQITRp0iTQpYgEjI5N1StYj03BeFwCHZuqQi1XfpSQkIDNZmP//v0+y/fv309ycnKAqqrbJk2axMcff8zXX39Ns2bNAl1OpYSFhdGmTRt69OjBzJkz6dq1Ky+88EKgyzqhtWvXcuDAAc4991xCQkIICQlh2bJlvPjii4SEhOB2uwNdYqXFxcVx9tlnk5qaGuhSTqhx48blfsx06NAhKLqNAOzcuZMlS5Ywfvz4QJci6LgUKDo2Vb+6cmwKhuMS6NhUFQpXfhQWFkaPHj348ssvvcs8Hg9ffvllUPRbDiaGYTBp0iQWLFjAV199RatWrQJd0mnzeDwUFBQEuowTuvTSS9m4cSPr16/3Tueddx433HAD69evx2azBbrESnO5XGzfvp3GjRsHupQT6tevX7mhm7dt20aLFi0CVFHVzJ49m8TERK644opAlyLouFTTdGyqOXXl2BQMxyXQsakq1C3Qz6ZNm8aYMWM477zz6NWrF88//zw5OTn86U9/CnRpJ+RyuXz+YpKWlsb69euJj4+nefPmAazsxCZOnMi8efP48MMPiYmJ8Z474HA4iIiICHB1JzZ9+nSGDBlC8+bNyc7OZt68eSxdupTPP/880KWdUExMTLnzBaKiomjYsGGtP4/gr3/9K8OHD6dFixbs3buXBx98EJvNxnXXXRfo0k7o9ttvp2/fvjz++ONcc801rFq1ildffZVXX3010KWdksfjYfbs2YwZM4aQEB1eaotgPC6Bjk01ScemmhOMxyXQsalKqm0cwnrs//7v/4zmzZsbYWFhRq9evYzvv/8+0CWd1Ndff20A5aYxY8YEurQTqqhewJg9e3agSzupm266yWjRooURFhZmNGrUyLj00kuNL774ItBlVVkwDHdrGIZx7bXXGo0bNzbCwsKMpk2bGtdee62Rmpoa6LJO6aOPPjI6d+5s2O12o3379sarr74a6JIq5fPPPzcAY+vWrYEuRY4TbMclw9CxqSbp2FRzgvW4ZBg6NlWWxTAMo/ojnIiIiIiISN2mc65ERERERET8QOFKRERERETEDxSuRERERERE/EDhSkRERERExA8UrkRERERERPxA4UpERERERMQPFK5ERERERET8QOFKRABYunQpFouFzMzMQJciIiIC6NgkwUfhSkRERERExA8UrkRERERERPxA4UqklvB4PMycOZNWrVoRERFB165dmT9/PnCsW8Qnn3xCly5dCA8P5/zzz+fnn3/22cb7779Pp06dsNvttGzZkmeeecbn8YKCAu666y5SUlKw2+20adOG119/3WedtWvXct555xEZGUnfvn3ZunVr9e64iIjUWjo2iVSRISK1wqOPPmq0b9/eWLRokbF9+3Zj9uzZht1uN5YuXWp8/fXXBmB06NDB+OKLL4wNGzYYw4YNM1q2bGkUFhYahmEYa9asMaxWq/Hwww8bW7duNWbPnm1EREQYs2fP9r7GNddcY6SkpBgffPCBsX37dmPJkiXGO++8YxiG4X2N3r17G0uXLjU2bdpkXHjhhUbfvn0D8XaIiEgtoGOTSNUoXInUAvn5+UZkZKTx3Xff+SwfN26ccd1113kPLqUHG8MwjMOHDxsRERHGu+++axiGYVx//fXGoEGDfJ5/5513Gh07djQMwzC2bt1qAMbixYsrrKH0NZYsWeJd9sknnxiAkZeX55f9FBGR4KFjk0jVqVugSC2QmppKbm4ugwYNIjo62jvNmTOH7du3e9fr06eP9358fDzt2rVj8+bNAGzevJl+/fr5bLdfv378+uuvuN1u1q9fj81mo3///ietpUuXLt77jRs3BuDAgQNnvI8iIhJcdGwSqbqQQBcgIuByuQD45JNPaNq0qc9jdrvd5yB2uiIiIiq1XmhoqPe+xWIBzD73IiJSv+jYJFJ1arkSqQU6duyI3W5n165dtGnTxmdKSUnxrvf999977x89epRt27bRoUMHADp06MCKFSt8trtixQrOPvtsbDYb55xzDh6Ph2XLltXMTomISFDTsUmk6tRyJVILxMTE8Ne//pXbb78dj8fDBRdcQFZWFitWrCA2NpYWLVoA8PDDD9OwYUOSkpK49957SUhIYOTIkQDccccd9OzZk0ceeYRrr72WlStX8tJLL/HKK68A0LJlS8aMGcNNN93Eiy++SNeuXdm5cycHDhzgmmuuCdSui4hILaVjk8hpCPRJXyJi8ng8xvPPP2+0a9fOCA0NNRo1amQMHjzYWLZsmfeE3o8++sjo1KmTERYWZvTq1cv46aeffLYxf/58o2PHjkZoaKjRvHlz46mnnvJ5PC8vz7j99tuNxo0bG2FhYUabNm2MN954wzCMYycNHz161Lv+jz/+aABGWlpade++iIjUQjo2iVSNxTAMI5DhTkRObenSpQwYMICjR48SFxcX6HJERER0bBKpgM65EhERERER8QOFKxERERERET9Qt0ARERERERE/UMuViIiIiIiIHyhciYiIiIiI+IHClYiIiIiIiB8oXImIiIiIiPiBwpWIiIiIiIgfKFyJiIiIiIj4gcKViIiIiIiIHyhciYiIiIiI+IHClYiIiIiIiB/8Pw2+aKPWm7hQAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config"
      ],
      "metadata": {
        "id": "-dD2Lsh95H9a",
        "outputId": "bbc87417-43d5-402e-86b0-e85953026ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    1326\n",
              "  ],\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"repetition_penalty\": 1.8\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = ['We',\"we\", \"propose\", 'authors']\n",
        "bad_words_ids=tokenizer(bad_words, add_special_tokens=False).input_ids"
      ],
      "metadata": {
        "id": "T5Tvn0_691lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = ['We',\"we\", \"propose\", 'authors']\n",
        "ids = model.generate(tokenized_data['test']['input_ids'][12:22],\n",
        "                    bad_words_ids=tokenizer(bad_words, add_special_tokens=False).input_ids,\n",
        "\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "KqOLYlN5AsTr",
        "outputId": "71a41a3b-c53c-477d-ab3c-ead4b8f97be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The aim of the WGAN is to minimize the Wasserstein-1 distance, also known as Earth-Mover (EM) distance, which is defined for any Polish space (M, c) and probability distributions on M. This study introduces a new method to reduce the waterstein-2 distance in an alternating manner by minimizing the Wasserenstein-1 distance. This paper presents a solution to the problem given by GAN training.',\n",
              " 'The study introduces a weight-tying mechanism that enables learning input and output representations jointly while significantly reducing the number of network parameters. This paper presents a new adaptive input method for learning input/output representations from different subsets of the input layer using sparse and dense connections, allowing HGT to learn deep representations efficiently from each layer through multiple paths, thereby enabling it to learn effective representations.',\n",
              " 'This article introduces a two-point noise matrix for the lossfunction landscape, which can be used to determine stationarity of the probability distribution that governs the model parameters at sufficiently long time. This article presents an analogous fluctuation-dissipation relations that quantitatively link the noise in mini-batched data to the observable evolution of the model performance and facilitates the learning process.',\n",
              " 'This paper introduces a new quantum circuit analogue of the neural network U QN N that takes in a given set of weights (encoded within qubits as above) and an empty register, and outputs on the register the corresponding accuracy according to the chosen neural network. This paper presents a method for using a gate-based form of quantum computation rather than a binary annealing device.',\n",
              " 'To evaluate transfer learning for CATE estimation on real data, reanalyze a set of large field experiments with more than 1.96 million potential voters. This paper introduces a new model that uses memory to adapt to existing DGPs and provides a way to improve the performance of these experiments. The aim of this study is to use the results from some old experiments to obtain faster training with less data on other new experiments.',\n",
              " 'This paper examines the extent to which a related forgetting process occurs as a model learns examples traditionally considered to belong to the same task. This paper explores the possibility of identifying important, or most informative examples with noisy labels and images with \"uncommon\" features that are difficult to classify. This article presents a method for learning forgotten examples in a neural network using an implicit regularization performed by stochastic gradient descent (SGD).',\n",
              " 'Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support. This paper introduces a class of probabilistic models to be used in a social context (e.g., in healthcare) and presents a new class of probability models for images and text data.',\n",
              " 'This paper introduces a method for learning data distribution from its samples and provides a way to train DSGAN with the flexibility to learn different target distributions in two key applications: semi-supervised learning and adversarial training. This article presents a new method for teaching GANs, which is more stable to train than conventional GAN. The study examines the generator distribution as p g and the discriminator in GAN classes.',\n",
              " 'This paper focuses on the problem of disease normalization, a biomedical knowledge base for treating patients with the same condition. The paper presents a novel approach to addressing the issue of disease-normalization by combining two sub-models that leverage both semantic features and topical coherence to perform disease normization. This paper introduces a new method for identifying diseases in the context where a mention appears.',\n",
              " 'This paper identifies an optimal structure in terms of both width and depth, without any iterative searches for thresholds. The paper presents a regression problem to predict house rates with N samples of houses and M features in each sample. This study examines the number of significant filters that are trained on a dataset rather than dependent on some metric. This article explores the importance of a sub-space of the entire filter space.']"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = ['We',\"we\", \"propose\"]\n",
        "tokenizer(bad_words, add_special_tokens=False)"
      ],
      "metadata": {
        "id": "74DE1w0W4abC",
        "outputId": "2ae18c5c-0195-4d43-8b40-915e2190b583",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1], [62, 1], [4230, 1]], 'attention_mask': [[1, 1], [1, 1], [1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('authors')"
      ],
      "metadata": {
        "id": "97pM4w9azMcD",
        "outputId": "0a61c3c2-6d92-4aae-8552-23b73dc76202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‚ñÅauthors']"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_ids(['_We'])"
      ],
      "metadata": {
        "id": "Ztw1MN_aup37",
        "outputId": "bcd9dbb6-0143-4b12-f9c8-c847cfc50129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"We is an example sentence.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "# Print the tokenized input\n",
        "print(\"Tokenized input:\", tokenized_input)\n",
        "\n",
        "# Generate output based on the tokenized input\n",
        "output = model.generate(**tokenized_input)\n"
      ],
      "metadata": {
        "id": "xl577YvIwQAs",
        "outputId": "fbf5fb02-a2e8-498a-8efd-b26e58366ee9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized input: {'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[ 101,   19,   46,  677, 7142,    5,    1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output"
      ],
      "metadata": {
        "id": "8eNz9_UkxorM",
        "outputId": "a66c086c-4698-422a-ae3c-8159bfa709c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a example example sentence for example. The example sentence is an example sentence, and it is based on a simple example sentence in this example sentence. It is one example sentence of example sentence to a sentence that is used as a examples sentence for Example Example Example For Example Example In An Example Example To Example Example A Example Example Examples Example Example Uses Example Example sentences.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_ids(['‚ñÅWe',\n",
        " '‚ñÅuse',\n",
        " '‚ñÅit',\n",
        " 'er',\n",
        " 'ative',\n",
        " '‚ñÅpruning',\n",
        " '‚ñÅas',\n",
        " '‚ñÅ',\n",
        " 'a',\n",
        " '‚ñÅproxy',\n",
        " '‚ñÅfor',\n",
        " '‚ñÅthe',\n",
        " '‚ñÅspeed',\n",
        " '‚ñÅat',\n",
        " '‚ñÅwhich',\n",
        " '‚ñÅ',\n",
        " 'a',\n",
        " '‚ñÅnetwork',\n",
        " '‚ñÅlearn',\n",
        " 's',\n",
        " '.'])"
      ],
      "metadata": {
        "id": "r8TYcDO5wHw8",
        "outputId": "204b397a-93ac-4197-8be4-700555a0f738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 169,\n",
              " 34,\n",
              " 49,\n",
              " 1528,\n",
              " 31858,\n",
              " 38,\n",
              " 3,\n",
              " 9,\n",
              " 19784,\n",
              " 21,\n",
              " 8,\n",
              " 1634,\n",
              " 44,\n",
              " 84,\n",
              " 3,\n",
              " 9,\n",
              " 1229,\n",
              " 669,\n",
              " 7,\n",
              " 5]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2BXUHuhyWyyh",
        "outputId": "ec7115b9-52d5-49bb-aa6a-e433dce70cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. the watermark must be secure against adversarial attacks and leave no tangible footprints in the target DNN. a retraining procedure resembles 'adversarial training' BID16. a retraining procedure resembles 'adversarial training' BID16.\",\n",
              " '',\n",
              " '',\n",
              " 'we hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the BatchNorm layer. we consider a standard classification task for data, having underlying distribution denoted as D, over the pair of examples x  R n and corresponding true labels y  1, 2,..., k where k represents different labels.',\n",
              " 'a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive.',\n",
              " '',\n",
              " 'DPQ-VQ is a more evenly distributed code utilization, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:8]"
      ],
      "metadata": {
        "id": "xNqaRwbOzUVD",
        "outputId": "8f0c5a88-4ccc-461f-f339-335d52d8e792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This paper presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The paper proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This paper proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The article addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.',\n",
              " 'We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This paper proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines',\n",
              " 'We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts This article focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up',\n",
              " \"Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations\"]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.do_lower_case"
      ],
      "metadata": {
        "id": "6QMQKmjqFkHL",
        "outputId": "4ed990f5-4663-46a2-cebf-d851ddb2d147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'T5Config' object has no attribute 'do_lower_case'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1d79c395fff9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'T5Config' object has no attribute 'do_lower_case'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('hello')\n"
      ],
      "metadata": {
        "id": "-MTiOKDZGJRk",
        "outputId": "b0bfdba3-3d3b-4840-ae0d-6c9550086d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21820, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('Hello')"
      ],
      "metadata": {
        "id": "Glz2s6NaHWTa",
        "outputId": "9847b24e-40cd-4d82-8889-69b9d5219511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8774, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmf68b93HqIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyN6IkNXEx7Hss/lMk/MnZ8e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9100de41ec6946c495c032b05d90913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40d035dada3347e2963c183d035d445c",
              "IPY_MODEL_ecd31a14240f408b82b5224d97694ab7",
              "IPY_MODEL_7afeab237ced4f01b70361e2a3ce7552",
              "IPY_MODEL_5b7408314641483980402784564e62e6",
              "IPY_MODEL_c4a0eef84a5e4061884e43d2a09da02a"
            ],
            "layout": "IPY_MODEL_b114a3fe857949408305dde0c9436974"
          }
        },
        "40d035dada3347e2963c183d035d445c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_52801cc2a7a646109c4a7141ef768900",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ecd31a14240f408b82b5224d97694ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_db74bb5ffc4f4900b17aaf842a618019",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6deef8837ace4d94b5b803666ff4aa5a",
            "value": ""
          }
        },
        "7afeab237ced4f01b70361e2a3ce7552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2e00c125b4d942368e702682002f94ff",
            "style": "IPY_MODEL_571b4b08342141c798092b6b23ab410a",
            "value": true
          }
        },
        "5b7408314641483980402784564e62e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ca46452e63a34d99865718ab86726748",
            "style": "IPY_MODEL_493ec2745ddf4bf2bdce528219a1b309",
            "tooltip": ""
          }
        },
        "c4a0eef84a5e4061884e43d2a09da02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c8673d4bac474d9c5799e3ee02bf6b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f44df68731c14e24914d1c8816c8edce",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b114a3fe857949408305dde0c9436974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f2ddbf8c4aaf42fba6f967bf32d45ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52801cc2a7a646109c4a7141ef768900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db74bb5ffc4f4900b17aaf842a618019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6deef8837ace4d94b5b803666ff4aa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e00c125b4d942368e702682002f94ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571b4b08342141c798092b6b23ab410a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca46452e63a34d99865718ab86726748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "493ec2745ddf4bf2bdce528219a1b309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d3c8673d4bac474d9c5799e3ee02bf6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44df68731c14e24914d1c8816c8edce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4d28b89ed4648cdbfcfa1dc2d08b7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2b2a5fe93f9415980f831986473d455",
              "IPY_MODEL_3bd92aabd33a453890f6d77c0c9ce58e",
              "IPY_MODEL_b38d08cbb29643628b5a57f18d8c07e0"
            ],
            "layout": "IPY_MODEL_dbf959cfc5194f3f8631910922da9274"
          }
        },
        "d2b2a5fe93f9415980f831986473d455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b05b5bf33f443c9f7f52b9e39ab4f1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1124f43d738941eea0f1b2ee1ec54b40",
            "value": "vocab.json:‚Äá100%"
          }
        },
        "3bd92aabd33a453890f6d77c0c9ce58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e65538d1d0c49b9af091c304166612a",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33f64d780a6d45cc97f7b50f2255a6ba",
            "value": 898823
          }
        },
        "b38d08cbb29643628b5a57f18d8c07e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f3b7212371421fb43818c2680f8c2b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_813bd197893b4b819661b8775e72b314",
            "value": "‚Äá899k/899k‚Äá[00:00&lt;00:00,‚Äá8.37MB/s]"
          }
        },
        "dbf959cfc5194f3f8631910922da9274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b05b5bf33f443c9f7f52b9e39ab4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1124f43d738941eea0f1b2ee1ec54b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e65538d1d0c49b9af091c304166612a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33f64d780a6d45cc97f7b50f2255a6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2f3b7212371421fb43818c2680f8c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813bd197893b4b819661b8775e72b314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f372eecdb7547d1a24daec8cd505b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c87abf7642b84ae790b0e746646b28b4",
              "IPY_MODEL_f7fcc25b1d4a415abd27212aa94d09e6",
              "IPY_MODEL_44c2c811c62a4e94bcae4f14f27170a2"
            ],
            "layout": "IPY_MODEL_3e2b8be15fcc49a2a4afc96956a1de1c"
          }
        },
        "c87abf7642b84ae790b0e746646b28b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_320cab01ec50434b8a812793a37e06bd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_faeb3aa6f7664fe998b0b8ed0f51e6cd",
            "value": "merges.txt:‚Äá100%"
          }
        },
        "f7fcc25b1d4a415abd27212aa94d09e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ae179f184f6495d9c768305caeb2178",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab0422df0fe74f169ea0ea6c2bef1a1a",
            "value": 456318
          }
        },
        "44c2c811c62a4e94bcae4f14f27170a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90760b25130b47d9b78007582b9a043d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f2fe25adc28649b6823a1eea840b9b39",
            "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá16.8MB/s]"
          }
        },
        "3e2b8be15fcc49a2a4afc96956a1de1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320cab01ec50434b8a812793a37e06bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faeb3aa6f7664fe998b0b8ed0f51e6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ae179f184f6495d9c768305caeb2178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab0422df0fe74f169ea0ea6c2bef1a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90760b25130b47d9b78007582b9a043d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fe25adc28649b6823a1eea840b9b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f492ec3a35dd438081e6a57fc3d06eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9513f84be238471088846023d5e6ad84",
              "IPY_MODEL_d4340e9b05b6494796d94251ba269628",
              "IPY_MODEL_09573227d7ba4a608af773bcb763a571"
            ],
            "layout": "IPY_MODEL_ea19de9f580c4f4a904cb2f4bf744eab"
          }
        },
        "9513f84be238471088846023d5e6ad84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ac0c0873234d93afcb930ccb93e3be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0b9caf83693548c7a27a7b69d178b074",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "d4340e9b05b6494796d94251ba269628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c97540d5984a29ad5e14d930698000",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f77d62be823a47d3acce116e46e86318",
            "value": 1355863
          }
        },
        "09573227d7ba4a608af773bcb763a571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6b266ce44d54509a0201847761d00cc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2f97b2a1032942c9a15e4d716d7ec9d6",
            "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá12.4MB/s]"
          }
        },
        "ea19de9f580c4f4a904cb2f4bf744eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ac0c0873234d93afcb930ccb93e3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9caf83693548c7a27a7b69d178b074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0c97540d5984a29ad5e14d930698000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f77d62be823a47d3acce116e46e86318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6b266ce44d54509a0201847761d00cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f97b2a1032942c9a15e4d716d7ec9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69941b69a1e244989afcf0e53aa2d4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18bc9f8a06d140cb8d50e2ca81c8b731",
              "IPY_MODEL_86819f5875854efcbbb5c151c06692fe",
              "IPY_MODEL_270b8e1b56da456fa45b67aab3ea5aab"
            ],
            "layout": "IPY_MODEL_7183b140556d430e8b785391603a643e"
          }
        },
        "18bc9f8a06d140cb8d50e2ca81c8b731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468b186d3ce8438abd033520d23e63e3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2fece72c7e06476fab3cb917c66deaf3",
            "value": "config.json:‚Äá100%"
          }
        },
        "86819f5875854efcbbb5c151c06692fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c8162823bd494aa02f3ff0d7ef7404",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afdb3d34f3264d0f8ad1b6244edec1af",
            "value": 1716
          }
        },
        "270b8e1b56da456fa45b67aab3ea5aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f5258623e948c28535e88a49cd5ccb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ac67f89ed3cc4a05a6a4b1d48abc2a27",
            "value": "‚Äá1.72k/1.72k‚Äá[00:00&lt;00:00,‚Äá147kB/s]"
          }
        },
        "7183b140556d430e8b785391603a643e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468b186d3ce8438abd033520d23e63e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fece72c7e06476fab3cb917c66deaf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c8162823bd494aa02f3ff0d7ef7404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdb3d34f3264d0f8ad1b6244edec1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8f5258623e948c28535e88a49cd5ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac67f89ed3cc4a05a6a4b1d48abc2a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a7371cf82074d2db890323fc76aacb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_743aa9e8a1e941e7b4e3fe657ed0acff",
              "IPY_MODEL_6caf976daaa44b46bf2a49511c835d73",
              "IPY_MODEL_eb24ecc8196a4537b8cbd46491d14af2"
            ],
            "layout": "IPY_MODEL_85dad04b66c642e099fdf97b57035633"
          }
        },
        "743aa9e8a1e941e7b4e3fe657ed0acff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2498e547c967491e809adbb3f07ae203",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e2f2e58c939345aca2fac6d087a8c6ba",
            "value": "Map:‚Äá100%"
          }
        },
        "6caf976daaa44b46bf2a49511c835d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9364cc4e3a849b888d661b23d8ce77b",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d60b054454543e987b45cd1721ba735",
            "value": 647
          }
        },
        "eb24ecc8196a4537b8cbd46491d14af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20213a2ac43b4708ab4c67630f9afe8e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c436cda1061245ad8dbf9e0cf191a0ee",
            "value": "‚Äá647/647‚Äá[00:04&lt;00:00,‚Äá145.46‚Äáexamples/s]"
          }
        },
        "85dad04b66c642e099fdf97b57035633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2498e547c967491e809adbb3f07ae203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2f2e58c939345aca2fac6d087a8c6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9364cc4e3a849b888d661b23d8ce77b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d60b054454543e987b45cd1721ba735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20213a2ac43b4708ab4c67630f9afe8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c436cda1061245ad8dbf9e0cf191a0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cfc504e4ded48e98e12e825fbca1041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_282d947990f644509171a568a31498dc",
              "IPY_MODEL_2f1ed1e53c4d44a3a23cfd1a29fbb4f8",
              "IPY_MODEL_73cc9fb106554137b1d6315cce7c5a7e"
            ],
            "layout": "IPY_MODEL_b99a4e7dc8234a4c9bf14ecd11328c7c"
          }
        },
        "282d947990f644509171a568a31498dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48d93091a4942b6b6148ead197fadf9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ffac937b9a634ef49dd57df0dca106aa",
            "value": "Map:‚Äá100%"
          }
        },
        "2f1ed1e53c4d44a3a23cfd1a29fbb4f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a080cede88b8402d88f59e1b0b31fee6",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5c646dcd6ca445ab40d1286fb3b8f8f",
            "value": 162
          }
        },
        "73cc9fb106554137b1d6315cce7c5a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce2243e57984549bf2eb04829eb2e5b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_25f58819cdf7432ebf55e15b3207eb6e",
            "value": "‚Äá162/162‚Äá[00:01&lt;00:00,‚Äá130.84‚Äáexamples/s]"
          }
        },
        "b99a4e7dc8234a4c9bf14ecd11328c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48d93091a4942b6b6148ead197fadf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffac937b9a634ef49dd57df0dca106aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a080cede88b8402d88f59e1b0b31fee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c646dcd6ca445ab40d1286fb3b8f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ce2243e57984549bf2eb04829eb2e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f58819cdf7432ebf55e15b3207eb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30200b78063e4e1c9ec387156fa1a14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b73afe522ac451088745530d61f89fb",
              "IPY_MODEL_752f6f030fc549bb9e8151cafbd56712",
              "IPY_MODEL_9cbfe0e9a50d4058a3369c586ebc472d"
            ],
            "layout": "IPY_MODEL_5200594676784c08b8cf0f23e10d30bc"
          }
        },
        "9b73afe522ac451088745530d61f89fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4ae7851f9d42859febfc2a4c7b99ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_35bc7f44805a449f850879134d8bdfc6",
            "value": "Map:‚Äá100%"
          }
        },
        "752f6f030fc549bb9e8151cafbd56712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9bb9601314849559d013ebf13c77862",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eabb24ff5a34451b68a4c9a5b18ca51",
            "value": 203
          }
        },
        "9cbfe0e9a50d4058a3369c586ebc472d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91d15c638f54ae2970720ef37b565e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_031ddb2105334b04bef6d4bcc5d4371f",
            "value": "‚Äá203/203‚Äá[00:01&lt;00:00,‚Äá133.77‚Äáexamples/s]"
          }
        },
        "5200594676784c08b8cf0f23e10d30bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4ae7851f9d42859febfc2a4c7b99ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35bc7f44805a449f850879134d8bdfc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9bb9601314849559d013ebf13c77862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eabb24ff5a34451b68a4c9a5b18ca51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f91d15c638f54ae2970720ef37b565e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031ddb2105334b04bef6d4bcc5d4371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "366ed1d70d544e9c97248005697acad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9e07ac6d7204fbbb1fcf9fb606b8456",
              "IPY_MODEL_334c8f7e157c4bc29f4ae54b987bb6d5",
              "IPY_MODEL_51b0ab51368142b0ad36c64500c15d94"
            ],
            "layout": "IPY_MODEL_bbbcd5af65134593a1c299578d1a006f"
          }
        },
        "b9e07ac6d7204fbbb1fcf9fb606b8456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6925772cebe94ce1a9af6a219ebf5963",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b705632920d94c3aa0d57693e9557b71",
            "value": "Map:‚Äá100%"
          }
        },
        "334c8f7e157c4bc29f4ae54b987bb6d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f36d8adf0d24c9e986dab2f91b79ec1",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_200e3ebc520e4569a71a465600efffe4",
            "value": 647
          }
        },
        "51b0ab51368142b0ad36c64500c15d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f6aea3b3af41e794f954dc06fc72b9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_54e610be50994c32a65444a4ac1329ab",
            "value": "‚Äá647/647‚Äá[00:02&lt;00:00,‚Äá222.35‚Äáexamples/s]"
          }
        },
        "bbbcd5af65134593a1c299578d1a006f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6925772cebe94ce1a9af6a219ebf5963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b705632920d94c3aa0d57693e9557b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f36d8adf0d24c9e986dab2f91b79ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200e3ebc520e4569a71a465600efffe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4f6aea3b3af41e794f954dc06fc72b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e610be50994c32a65444a4ac1329ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a33594f8744bd5a167b0372fb466ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2518909058fe4c5ba1667fba2550520d",
              "IPY_MODEL_27d7bd9d1e0444a9b6d2991561e4c532",
              "IPY_MODEL_cdcf83dda1c34a58bc9e74f5d4546527"
            ],
            "layout": "IPY_MODEL_71a3ce69a4814716a6e8cf15f8485117"
          }
        },
        "2518909058fe4c5ba1667fba2550520d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c1cbd0557b44d2eb775007ef9b3f99e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_87a69decb29c4ba4b5aaef30bd4254ba",
            "value": "Map:‚Äá100%"
          }
        },
        "27d7bd9d1e0444a9b6d2991561e4c532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e46a897c5c4c368a0889f9b44cda7d",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99dcefb79f8540fabe1705701731fd1a",
            "value": 162
          }
        },
        "cdcf83dda1c34a58bc9e74f5d4546527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5538e38f4794548a8105d9182f7a369",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_560e644845a94532a714972fe6618781",
            "value": "‚Äá162/162‚Äá[00:00&lt;00:00,‚Äá222.21‚Äáexamples/s]"
          }
        },
        "71a3ce69a4814716a6e8cf15f8485117": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1cbd0557b44d2eb775007ef9b3f99e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a69decb29c4ba4b5aaef30bd4254ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54e46a897c5c4c368a0889f9b44cda7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99dcefb79f8540fabe1705701731fd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5538e38f4794548a8105d9182f7a369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "560e644845a94532a714972fe6618781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d6970f032224c209be6654fa611ef88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43e88319206440a289a51a00d4088603",
              "IPY_MODEL_6bd3860856ce42008255450d10bc4ffd",
              "IPY_MODEL_c1088e3863ab4a9f9bccd1c44abae1ea"
            ],
            "layout": "IPY_MODEL_cab0ac902d9543fda41c351485b96090"
          }
        },
        "43e88319206440a289a51a00d4088603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1a35a0519954256a29ba63f1a8cd1ed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9609bbe856704d4fb5e80f37b21aca8b",
            "value": "Map:‚Äá100%"
          }
        },
        "6bd3860856ce42008255450d10bc4ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee1ad541e1864693803e974de2fb19d3",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_272ed0d510664ba8bc3c4f1895d959e2",
            "value": 203
          }
        },
        "c1088e3863ab4a9f9bccd1c44abae1ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6066e23bd59845e997efe24a09227157",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7aa4e27aa26245bcb6548d64a8e5b304",
            "value": "‚Äá203/203‚Äá[00:00&lt;00:00,‚Äá217.74‚Äáexamples/s]"
          }
        },
        "cab0ac902d9543fda41c351485b96090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a35a0519954256a29ba63f1a8cd1ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9609bbe856704d4fb5e80f37b21aca8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee1ad541e1864693803e974de2fb19d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272ed0d510664ba8bc3c4f1895d959e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6066e23bd59845e997efe24a09227157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aa4e27aa26245bcb6548d64a8e5b304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}