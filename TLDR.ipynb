{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/main/TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "70c8ba82-60a0-42c8-ea95-9a05b2532732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=0a37d6509e1a19cd90a96ec9fa52cdd5fe024c624330e8a524626a449037f94c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "a977bf92-3b3a-4c34-ed22-07fa74a31834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-1-c5cb458fe86c>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TFBartForConditionalGeneration, BartTokenizer, T5Tokenizer, pipeline, TFT5ForConditionalGeneration\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect w/ HuggingFace HUB\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "9100de41ec6946c495c032b05d90913d",
            "40d035dada3347e2963c183d035d445c",
            "ecd31a14240f408b82b5224d97694ab7",
            "7afeab237ced4f01b70361e2a3ce7552",
            "5b7408314641483980402784564e62e6",
            "c4a0eef84a5e4061884e43d2a09da02a",
            "b114a3fe857949408305dde0c9436974",
            "f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "52801cc2a7a646109c4a7141ef768900",
            "db74bb5ffc4f4900b17aaf842a618019",
            "6deef8837ace4d94b5b803666ff4aa5a",
            "2e00c125b4d942368e702682002f94ff",
            "571b4b08342141c798092b6b23ab410a",
            "ca46452e63a34d99865718ab86726748",
            "493ec2745ddf4bf2bdce528219a1b309",
            "d3c8673d4bac474d9c5799e3ee02bf6b",
            "f44df68731c14e24914d1c8816c8edce"
          ]
        },
        "id": "wpR8O7wbdMdI",
        "outputId": "d0ef909e-6736-48c6-bf15-0c9b0cbe318a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9100de41ec6946c495c032b05d90913d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "8c8c4f62-b4fe-4a3d-e353-6f2cc66c898f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "1I5H39lnQW9o",
        "outputId": "75e99565-d37c-41fd-9e29-7311cbf5442d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source    paper_id  \\\n",
              "265   Deep approaches to anomaly detection have rece...  HkgH0TEYwH   \n",
              "998   We present a new unsupervised method for learn...   H1a37GWCZ   \n",
              "538   This paper proposes a Pruning in Training (PiT...  r1GgDj0cKX   \n",
              "1192  Spatiotemporal forecasting has various applica...   SJiHXGWAZ   \n",
              "423   The transformer has become a central model for...  BygdR0VKDr   \n",
              "\n",
              "                                                 target  \\\n",
              "265   We introduce Deep SAD, a deep method for gener...   \n",
              "998   To train a sentence embedding using technical ...   \n",
              "538   we propose an algorithm of learning to prune n...   \n",
              "1192  A neural sequence model that learns to forecas...   \n",
              "423   Discrete transformer which uses hard attention...   \n",
              "\n",
              "                                                  title  number_words_target  \\\n",
              "265              Deep Semi-Supervised Anomaly Detection                   79   \n",
              "998   UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT...                   68   \n",
              "538   PRUNING IN TRAINING: LEARNING AND RANKING SPAR...                   31   \n",
              "1192  Diffusion Convolutional Recurrent Neural Netwo...                   48   \n",
              "423                                Discrete Transformer                   61   \n",
              "\n",
              "                                     extractive_summary  \n",
              "265   Using an information-theoretic perspective on ...  \n",
              "998   ‚Ä¶ Ransomware is computer malware that installs...  \n",
              "538   The expressive power of Deep Convolutional Neu...  \n",
              "1192  To address these challenges, we propose to mod...  \n",
              "423   To facilitate parallel training, as well as to...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-058a4829-f68a-4f46-9832-3655be4a4257\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>Deep approaches to anomaly detection have rece...</td>\n",
              "      <td>HkgH0TEYwH</td>\n",
              "      <td>We introduce Deep SAD, a deep method for gener...</td>\n",
              "      <td>Deep Semi-Supervised Anomaly Detection</td>\n",
              "      <td>79</td>\n",
              "      <td>Using an information-theoretic perspective on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>We present a new unsupervised method for learn...</td>\n",
              "      <td>H1a37GWCZ</td>\n",
              "      <td>To train a sentence embedding using technical ...</td>\n",
              "      <td>UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT...</td>\n",
              "      <td>68</td>\n",
              "      <td>‚Ä¶ Ransomware is computer malware that installs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>This paper proposes a Pruning in Training (PiT...</td>\n",
              "      <td>r1GgDj0cKX</td>\n",
              "      <td>we propose an algorithm of learning to prune n...</td>\n",
              "      <td>PRUNING IN TRAINING: LEARNING AND RANKING SPAR...</td>\n",
              "      <td>31</td>\n",
              "      <td>The expressive power of Deep Convolutional Neu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>Spatiotemporal forecasting has various applica...</td>\n",
              "      <td>SJiHXGWAZ</td>\n",
              "      <td>A neural sequence model that learns to forecas...</td>\n",
              "      <td>Diffusion Convolutional Recurrent Neural Netwo...</td>\n",
              "      <td>48</td>\n",
              "      <td>To address these challenges, we propose to mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>The transformer has become a central model for...</td>\n",
              "      <td>BygdR0VKDr</td>\n",
              "      <td>Discrete transformer which uses hard attention...</td>\n",
              "      <td>Discrete Transformer</td>\n",
              "      <td>61</td>\n",
              "      <td>To facilitate parallel training, as well as to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-058a4829-f68a-4f46-9832-3655be4a4257')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-058a4829-f68a-4f46-9832-3655be4a4257 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-058a4829-f68a-4f46-9832-3655be4a4257');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51b21f14-c76a-4384-8153-66551ba49d2d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51b21f14-c76a-4384-8153-66551ba49d2d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51b21f14-c76a-4384-8153-66551ba49d2d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"We present a new unsupervised method for learning general-purpose sentence embeddings.\\n Unlike existing methods which rely on local contexts, such as words\\ninside the sentence or immediately neighboring sentences, our method selects, for\\neach target sentence, influential sentences in the entire document based on a document\\nstructure. We identify a dependency structure of sentences using metadata\\nor text styles. Furthermore, we propose a novel out-of-vocabulary word handling\\ntechnique to model many domain-specific terms, which were mostly discarded by\\nexisting sentence embedding methods. We validate our model on several tasks\\nshowing 30% precision improvement in coreference resolution in a technical domain,\\nand 7.5% accuracy increase in paraphrase detection compared to baselines. Distributed representations are ever more leveraged to understand text BID20 b; BID16 BID23 . Recently, BID12 proposed a neural network model, SKIP-THOUGHT, that embeds a sentence without supervision by training the network to predict the next sentence for a given sentence. However, unlike human reading with broader context and structure in mind, the existing approaches focus on a small continuous context of neighboring sentences. These approaches work well on less structured text like movie transcripts, but do not work well on structured documents like encylopedic articles and technical reports. To better support semantic understanding of such technical documents, we propose a new unsupervised sentence embedding framework to learn general-purpose sentence representations by leveraging long-distance dependencies between sentences in a document. We observe that understanding a sentence often requires understanding of not only the immediate context but more comprehensive context, including the document title, previous paragraphs or even related articles as shown in Figure 1. For instance, all the sentences in the document can be related to the title of the document (1(a)). The first sentence of each item in a list structure can be influenced by the sentence introducing the list (1(b)). Moreover, html documents can contain hyperlinks to provide more information about a certain term (1(c)). With the contexts obtained from document structure, we can connect ransomware with payment (1(a)) and the four hashes with Locky (1(b)). Millions of spam emails spread new ransomware variant on the day it first appeared. A new variant of ransomware known as Locky (detected by Symantec as Trojan. Cryptolocker. AF) has been spreading quickly since it first appeared on Tuesday (February 16). The attackers behind Locky have pushed the malware aggressively, using massive spam campaigns and compromised websites. \\u2026 \\u2026 Ransomware is computer malware that installs covertly on a victim's computer, executes a cryptovirology attack that adversely affects it, and demands a ransom payment to decrypt it or not publish it. Locky is a new ransomware that has been released (most probably) by the Dridex gang. Not surprisingly, it is well prepared, which means that the threat actor behind it has invested sufficient resources for it, including its mature infrastructure. Let's take a look. o payload: 74dde1905eff75cf3328832988a785de <-main focus of this analysis \\u2022 d9df60c24ceca5c4d623ff48ccd4e9b9 \\u2022 e7aad826559c8448cd8ba9f53f401182 These spam campaigns have many similarities to campaigns used to spread the Dridex financial Trojan. The sheer size of the campaigns, their disguise as financial documents such as invoices, and the use of malicious macros in attached Word documents are all hallmarks of the Dridex group. Built to harvest the banking credentials of victims, the virulent Dridex is now one of the most dangerous pieces of financial malware in circulation. Our approach leveraging such structural elements has several advantages. First, it can learn from technical documents containing several subtopics that may cause sudden context changes. Some sentences have dependences to distant ones if a different perspective of the topic is introduced. Using We validate our model on several NLP tasks using a Wikipedia corpus. When trained with the Wikipedia corpus, our model produces much lower loss than SKIP-THOUGHT in the target sentence prediction task, confirming that training with only local context does not work well for such documents. We also compare the performance of the learned embedding on several NLP tasks including coreference resolution and paraphrase identification. For coreference resolution, our model shows roughly 30% improvement in precision over a state-of-the-art deep learning-based approach on cybersecurity domain, and produces 7.5% increase in accuracy compared with SKIP-THOUGHT for paraphrase identification. The main contributions of the paper include:\\u2022 We propose a general-purpose sentence embedding method which leverages long distance sentence dependencies extracted from the document structure.\\u2022 We developed a rule-baed dependency annotator to automatically determine the document structure and extract all governing sentences for each sentence.\\u2022 We also present a new OOV handling technique based on the document structure.\\u2022 We have applied our methods to several NLP applications using cybersecurity datasets. The experiments show that our model consistently outperform existing methods. Distributed representation of sentences, which is often called sentence embedding, has gained much attention recently, as word-level representations BID20 b; BID16 BID23 are not sufficient for many sentence-level or document-level tasks, such as machine translation, sentiment analysis and coreference resolution. Recent approaches using neural networks consider some form of dependencies to train the network. Dependencies can be continuous (relating two adjacent words or sentences) or discontinuous (relating two distant words or sentences), and intra-sentence (dependency of words within a sentence) or inter-sentence (dependency between sentences). Many sentence embedding approaches leverage these dependencies of words to combine word embeddings, and can be categorized as shown in 1.One direct extension of word embedding to sentences is combining words vectors in a continuous context window. BID13 use a weighted average of the constituent word vectors. BID27 , BID3 , and BID22 use supervised approaches to train a long short-term memory (LSTM) network that merges word vectors. BID10 and BID11 use convolutional neural networks (CNN) over continuous context window to generate sentence representations. BID14 include a paragraph vector in the bag of word vectors, and apply a word embedding approaches BID20 b) .Recently, several researchers have proposed dependency-based embedding methods using a dependency parser to consider discontinuous intra-sentence relationships BID25 BID18 BID26 . BID25 uses recursive neural network to consider discontinuous dependencies. BID18 proposes a dependency-based convolutional neural network which concatenate a word with its ancestors and siblings based on the dependency tree structure. BID26 proposes tree structured long short-term memory networks. These studies show that dependency-based (discontinuous) networks outperform their sequential (continuous) counterparts. Unlike these approaches, considering only intra-sentence dependencies, SKIP-THOUGHT BID12 joins two recurrent neural networks, encoder and decoder. The encoder combines the words in a sentence into a sentence vector, and the decoder generates the next sentence. Our approach is similar to SKIP-THOUGHT since both approaches are unsupervised and use inter-sentential dependencies. However, SKIP-THOUGHT considers only continuous dependency. Furthermore, we propose a new method to handle OOV words in sentence embedding based on the position of an OOVword in a sentence and the dependency type of the sentence. To our knowledge, there has been no sentence embedding work incorporating OOV words in formulating the training goal. Most existing systems map all OOV words to a generic unknown word token (i.e., < unk >).Santos & Zadrozny FORMULA2 and BID9 build an embedding of an OOV word on the fly that can be used as input to our system, but not to set the training goal. BID17 propose a word position-based approach to address the OOV problem for neural machine translation (NMT) systems. Their methods allow a neural machine translation (NMT) system to emit, for each unknown word in the target sentence, the position of the corresponding word in the source sentence. However, their methods are not applicable to sentence embedding, as they rely on an aligned corpus. Also, our approach considers not only word positions but also the dependency types to represent OOV words in a finer-grained OOV level. Previous methods use intra-sentence dependencies such as dependency tree, or immediately neighboring sentences for sentence embedding. However, we identify more semantically related content to a target sentence based on the document structure as shown in FIG1 . In this section, we describe a range of such inter-sentence dependencies that can be utilized for sentence embedding and the techniques to automatically identify them. We use the following notations to describe the extraction of document structure-based context for a given sentence. Suppose we have a document D = {S 1 , . . . , S |D| }, which is a sequence of sentences. Each sentence S i is a sequence of words: s i,1 , . . . , s i,|Si| . For each target sentence S t \\u2208 D, there can be a subset G \\u2282 D that S t depends on (For simplicity, we use G to denote a S t specific set). We call such a sentence in G a governing sentence of S t , and say G i governs S t , or S t depends on G i . Each G i is associated with S t through one of the dependency types in D described below. The title of a document, especially a technical document, contains the gist of the document, and all other sentences support the title in a certain way. For instance, the title of the document can clarify the meaning of a definite noun in the sentence. Section titles play a similar role, but, mostly to the sentences within the section. We detect different levels of titles, starting from the document title to chapter, section and subsection titles. Then, we identify the region in the document which each title governs and incorporate the title in the embedding of all the sentences in the region. To identify titles in a document, we use various information from the metadata and the document content. DISPLAYFORM0 We extract a document title from the <title> tag in a HTML document or from the title field in Word or PDF document metadata. Since the document title influences all sentences in a document, we consider a title obtained from D T M governs every sentence in D.Heading Tag (D T Hn ): The heading tags <h1> to <h6> in HTML documents are often used to show document or section titles. We consider all the sentences between a heading tag and the next occurrence of the same level tag are considered under the influence of the title. Header and Footer (D T R ): Technical documents often contain the document or section titles in the headers or footers. Thus, if the same text is repeated in the header or in the footer in many pages, we take the text as a title and consider all the sentences appearing in these pages belong to the title. Text Styles (D T S ): Titles often have a distinctive text style. They tend to have no period at the end and contain a larger font size, a higher number of italic or bold text, and a higher ratio of capitalized words compared to non-title sentences. We first build a text style model for sentences appearing in the document body, capturing the three style attributes. If a sentence ends without a period and any dimension of its style model has higher value than that of the text style model, we consider the sentence as a title. Then, we split the document based on the detected titles and treat each slice as a section. Authors often employ a list structure to describe several elements of a subject. These list structures typically state the main concept first, and, then, the supporting points are described in a bulleted, numbered or in-text list as illustrated in FIG2 . In these lists, an item is conceptually more related to the introductory sentence than the other items in the list, but the distance can be long because of other items. Once list items are identified, we consider the sentence appearing prior to the list items as the introductory sentence and assume that it governs all the items in the list. The categories of the products State Farm offers are as follows:\\u2022 We have property and casualty insurance.\\u2022 We offer comprehensive types of life and health insurances.\\u2022 We have bank products. To extract numbered or bulleted lists, we use the list tags (e.g., <ul>, <ol>, <li>) for HTML documents. For non-HTML documents, we detect a number sequence (i.e., 1, 2, ...) or bullet symbols (e.g., -, \\u00b7) repeating in multiple lines. In-text List (D LT ): We also identify in-text lists such as \\\"First(ly), . . .. Second(ly), . . .. Last(ly), . . .\\\" by identifying these cue words. We consider the sentence appearing prior to the list items as the introductory sentence and assume that it governs the list items. Hyperlinks (D H ): Some sentences contain hyperlinks or references to provide additional information or clarify the meaning of the sentence. We can enrich the representation of the sentence using the linked document. In this work, we use the title of the linked document in the embedding of the sentence. Alternatively, we can use the embedding of the linked document. Footnotes and In-document Links (D F ): Footnotes also provide additional information for the target sentence. In an HTML document, such information is usually expressed with in-document hyperlinks, which ends with \\\"#dest\\\". In this case, we identify a sentence marked with \\\"#dest\\\" and add a dependency between the two sentences. We also consider the traditional sequential dependency used in previous methods BID12 BID7 . Given a document D = {S 1 , . . . , S |D| }, the target sentence S t is considered to be governed by n sentences prior to (n < 0) or following (n > 0) S t . In our implementation, we use only one left sentence. Similarly to SKIP-THOUGHT BID12 , we train our model to generate a target sentence S t using a set of governing sentences G. However, SKIP-THOUGHT takes into account only the window-based context (D W n ), while our model considers diverse long distance context. Furthermore, we handle out-of-vocabulary (OOV) words based on their occurrences in the context. Our model has several encoders (one encoder for each G i \\u2208 G), a decoder and an OOV handler as shown in FIG3 . The input to each cell is a word, represented as a dense vector. In this work, we use the pre-trained vectors from the CBOW model BID21 , and the word vectors can be optionally updated during the training step. Unlike existing sentence embedding methods, which include only a small fraction of words (typically high frequency words) in the vocabulary and map all other words to one OOV word by averaging all word vectors, we introduce a new OOV handler in our model. The OOV handler maps all OOV words appearing in governing sentences to variables and extend the vocabulary with the OOV variables. More details about OOV handler is described in Section 5.We now formally describe the model given a target sentence S t and a set G of its governing sentences. We first describe the encoders that digest each G i \\u2208 G. Given the i-th governing sentence G i = FIG1 , . . . , g i,|Gi| ) let w(g i,t ) be the word representation (pre-trained or randomly initialized) of word g i,t . Then, the following equations define the encoder for S i . DISPLAYFORM0 where RC is a recurrent neural network cell (e.g., LSTM or GRU) that updates the memory h i,t ; \\u03b8 E is the parameters for the encoder RC; \\u03bb i is an OOV weight vector that decides how much we rely on out-of-vocabulary words; d i denotes the OOV features for G i ; U and g are linear regression parameters; \\u03c3(\\u00b7) is the sigmoid function; u dep and a dep are dependency-specific weight parameters; W and b are a matrix and a bias for a fully connected layer; andh 0 is the aggregated information of G and is passed to the decoder for target sentence generation. Now, we define the decoder as follows: DISPLAYFORM1 where RC is a recurrent neural network cell that updates the memoryh t and generates the output o t ; \\u03b8 D is a set of parameters for the decoder RC; softmax(\\u00b7) is the softmax function; and V o t + c transforms the output into the vocabulary space. That is, V o t + c generates logits for words in the vocabulary set and is used to predict the words in the target sentence. To strike a balance between the model accuracy and the training time, we use K randomly chosen governing sentences from G for all target sentence. We use the cross entropy between y t and o t as the optimization function and update \\u03b8 E , W dep(i) , b, V, c, \\u03b8 D and optionally w(\\u00b7). DISPLAYFORM2 Incorporating all the words from a large text collection in deep learning models is infeasible, since the amounts of memory use and training time will be too costly. Existing sentence embedding techniques reduce the vocabulary size mainly by using only high frequency words and by collapsing all other words to one unknown word. The unknown word is typically represented by the average vector of all the word vectors in the vocabulary or as a single dimension in a bag-of-word representation. However, this frequency-based filtering can lose many important words including domain-specific words and proper nouns resulting in unsatisfactory results for technical documents. Specifically, OOV word handling is desired in the following three places: (1) input embeddings to encode the governing sentences (G); (2) input embeddings to decode the target sentence (S t ); and (3) output logits to compute the loss with respect to S t . In this work, we apply the most commonly used approach, i.e., using the average vector of all the words in the vocabulary to represent all OOV words, to generate the input embeddings of G or S t for the encoder and the decoder. To handle the OOV words in the output logits, we propose a new method using two vocabulary sets. We first select N most frequent words in the training corpus as an initial vocabulary V 0 . Note that N (typically, tens of thousands) is much smaller than the vocabulary size in the training corpus (typically, millions or billions). The OOV mapper reduces the OOV words into a smaller vocabulary V OOV of OOV variables that can represent certain OOV words given a context (e.g., an OOV variable may indicate the actor in the previous sentence).We note that only the OOV words appearing in governing sentences influence in model training, and many semantically important words tend to appear in the beginning or at the end of the governing sentences. Thus, we use OOV variables to represent the first and the last \\u03b7 OOV words in a governing sentences. Specifically, we denote a j-th OOV word in the i dependency governing sentence by an OOV variable O i (j) \\u2208 V OOV . This idea of encoding OOV words based on their positions in a sentence is similar to BID17 . However, we encode OOV words using the dependency type of the sentence as well as their position in the sentence. Our OOV handler performs the following steps. First, we build an OOV map to convert OOV words to OOV variables and vice versa. Algorithm 1 summarizes the steps to build a map which converts the first \\u03b7 OOV words into OOV variables. To model the last \\u03b7 OOV words, we reverse the words in each G i , and index them as w \\u22121 , w \\u22122 , . . ., then pass them to BuildOOVMap to construct DISPLAYFORM0 Note that the mapping between OOV words and OOV variables is many-to-many. For example, suppose \\\"We discuss Keras first' is a target sentence S t , and, \\\"Slim and Keras are two tools you must know\\\" is extracted as the document title by the dependency type D T S , \\\"PLA's weekly review: Slim and Keras are two tools you must know\\\" is extracted as the document title by D T M for S t , and, words 'Slim', 'Keras' and 'PLA' are OOV words. Then, we map the 'Slim' and 'Keras' from the first title to OOV variable O T S (1) and O T S (2) and 'PLA', 'Slim' and 'Keras' from the second title to O T M (1), O T M (2), and O T M (3) respectively. As a result, 'Keras' in S t is mapped to O T S (1) and O T M (3).Once we have the OOV mapping and the augmented vocabulary, we can formulate an optimization goal taking into account the OOV words with a vocabulary with a manageable size. The optimization goal of each RNN cell without OOV words is to predict the next word with one correct answer. In contrast, our model allows multiple correct answers, since an OOV word can be mapped to multiple OOV variables. We use the cross entropy with soft labels as the optimization loss function. The weight of each label is determined by the inverse-square law, i.e., the weight is inversely proportional to the square of the number of words associated with the label. This weighting scheme gives a higher weight to less ambiguous dependency. One additional component we add related to OOV words is a weight function for the governing sentences based on occurrences of proper nouns (\\u03bb i in Equation 1). Instead of equally weighing all governing sentences, we can give a higher weight to sentences with proper nouns, which are more likely to be OOV words. Thus, we introduce a feature vector representing the number of OOV proper nouns in the i-th governing sentence (d i in FIG1 ). Currently, the features include # of OOV words whose initials are uppercased, # of OOV words that are uppercased, and # of OOV words with any of the letters are uppercased. Together with the linear regression parameters, U and g, the model learns the weights for different dependency types. In this section, we empirically evaluate our approach on various NLP tasks and compare the results with other existing methods. We trained the proposed model (OURS) and the baseline systems on 807,647 randomly selected documents from the 2009 Wikipedia dump, which is the latest Wikipedia dump in HTML format, after removing the discussion and resource (e.g., images) articles among. Since our approach leverages HTML tags to identify document structures, our model use the raw HTML files. For the baseline systems, we provide plain text version of the same articles. All models were train for 300K steps with 64-sized batches and the Adagrad optimizer BID5 . For the evaluation, we use up-to 8 governing sentences as the context for a target sentence. When a sentence has more than 8 governing sentences, we randomly choose 8 sentences. We set the maximum number of words in a sentence to be 30 and pad each sentence with special start and end of sentence symbols. We set \\u03b7 to 4, resulting in |V OOV | = 80. Unlike most other approaches, our model and SKIP-THOUGHT BID12 can learn application-independent sentence representations without task-specific labels. Both models are trained to predict a target sentence given context. The prediction is a sequence of vectors representing probabilities of words in the target sentence. For a quantitative evaluation between the two models, we compare the prediction losses by using the same loss function, namely cross entropy loss. We randomly chose 640,000 target sentences for evaluation and computed the average loss over the 640K sentences. We compare SKIP-THOUGHT with two versions of our model. OURS denotes our model using the document structure-based dependencies and the OOV handler. OURS\\u2212DEP denotes our model with the OOV handler but using only local context like SKIP-THOUGHT to show the impact of the OOV handler. TAB3 shows the comparison of the three models. The values in the table are the average loss per sentence. We measure the average loss value excluding OOV words for SKIP-THOUGHT, as it cannot handle OOV words. However, for our models, we measure the loss values with and without OOV words. As we can see, both OURS\\u2212DEP and OURS significantly outperform SKIP-THOUGHT resulting in 25.8% and 26.9% reduction in the loss values respectively. Further, we compare our model with SKIP-THOUGHT on a paraphrase detection task using the Microsoft Research Paraphrase corpus BID19 . The data consists of 5,801 sentence pairs extracted from news data and their boolean assessments (if the pair of sentences are paraphrases of each other or not), which were determined by three assessors using majority voting. The goal is correctly classifying the boolean assessments and accuracy (# correct pairs / # all pairs) is measured. We used 4,076 pairs for training and 1,725 pairs for testing. Since the data sets contain sentence pairs only and no structural context, we evaluate only the effectiveness of the trained encoder. To compare the qualities of sentence embeddings by the two models, we use the same logistic regression classifier with features based on embedded sentences as in BID12 . Given a pair of sentences S 1 and S 2 , the features are the two embeddings of S 1 and S 2 , their entry-wise absolute difference, and their entry-wise products. Our model shows a 5% points higher accuracy than SKIP-THOUGHT in paraphrase detection (Table 3 ), demonstrating the effectiveness of our encoder trained with the structural dependencies. Note that SKIP-THOUGHT trained on the Wikipedia corpus performs worse than a model trained on books or movie scripts due to more sophisticated and less sequential structure in Wikipedia documents. Traditionally, the coreference resolution problem is considered as a supervised pairwise classification (i.e., mention linking) or clustering problem (coreference cluster identification) relying on an annotated corpus BID8 BID6 BID1 b; BID15 . While, recently, there have been an impressive improvement in coreference resolution, existing coreference models are usually trained for general domain entity types (i.e., 'Person', 'Location', 'Organization') and leverage metadata that are not available in technical documents (e.g., Speaker). D'Souza & Ng FORMULA2 and BID0 have shown that general domain coreference resolution models do not work well for domain specific entity types. While our system is not intended to be a coreference resolution tool, the rich sentence embedding can be used for unsupervised coreference resolution allowing it applicable to any domain. Although building a dedicated coreference resolution method to a given domain can produce better results, we claim that our approach can build a good starting set of features without supervision for a new domain. Specifically, we treat the coreference resolution problem as an inference problem given the context. To apply our model, we assume that entity mentions are detected in advance (any mention detection tool can be used), and, for a pronoun or a generic entity reference (e.g., a definite noun phrase), we select a list of candidate referents that conform to the mention types allowed by the pronoun or the definite noun. We apply the mention type-based filtering to reduce the search space, but, a span-based approach as in BID15 can be used as well. Then, we replace the entity reference with each of the candidate referents and compute the loss of the new sentence. Finally, we choose the referent with the lowest loss value as the result, if the ratio of its loss to the original sentence loss value is less than a threshold value \\u03b8. To show the effectiveness of the unsupervised coreference resolution method, we compare our approach with the Stanford Deep Coreference Resolution tool BID2 ) using a set of cybersecurity-related documents. The evaluation data consists of 563 coreferences extracted from 38 Wikipedia articles about malware programs which were not included in the training document set. We conducted experiments for several cybersecurity related entity types such as 'Malware' and 'Operating System' in addition to general entity types including 'Person' and 'Organization'. For the evaluation, we set \\u03b8 to 0.99 and 1.00. TAB4 summarizes the results of the two systems. Our model achieves higher precision and recall than DEEPCOREF. Since DEEPCOREF was trained for a general domain, its overall performance on domain specific documents is very low. FIG4 shows the two systems' performance on different entity types. As we can see, OURS works well for domain specific entities such as 'Malware' and 'Vulnerability', while DEEPCOREF shows higher precision for 'Person' and 'Organization'. The reason OURS performs worse for 'Person' and 'Organization' is because the security documents have only a few mentions about people or organizations, and we did not use carefully crafted features as in DEEPCOREF. In this paper, we presented a novel sentence embedding technique exploiting diverse types of structural contexts and domain-specific OOV words. Our method is unsupervised and applicationindependent, and it can be applied to various NLP applications. We evaluated the method on several NLP tasks including coreference resolution, paraphrase detection and sentence prediction. The results show that our model consistently outperforms the existing approaches confirming that considering the structural context generates better quality sentence representations.\",\n          \"The transformer has become a central model for many NLP tasks from translation to language modeling to representation learning. Its success demonstrates the effectiveness of stacked attention as a replacement for recurrence for many tasks. In theory attention also offers more insights into the model\\u2019s internal decisions; however, in practice when stacked it quickly becomes nearly as fully-connected as recurrent models. In this work, we propose an alternative transformer architecture, discrete transformer, with the goal of better separating out internal model decisions. The model uses hard attention to ensure that each step only depends on a fixed context. Additionally, the model uses a separate \\u201csyntactic\\u201d controller to separate out network structure from decision making. Finally we show that this approach can be further sparsified with direct regularization. Empirically, this approach is able to maintain the same level of performance on several datasets, while discretizing reasoning decisions over the data. The transformer has achieved state-of-the-art performances in a variety of sequence modeling tasks, including language modeling (Radford et al., 2019) , machine translation (Vaswani et al., 2017) , question answering (Radford et al., 2018; Devlin et al., 2018) , among others. To facilitate parallel training, as well as to reduce the path length of the dependencies, transformer dispenses recurrence and builds up hidden states by attending to the source side (inter-attention) and attending to its past predictions (self-attention) with multiple heads in multiple layers (Vaswani et al., 2017) . Compared to recurrent models the attention mechanism adds some \\\"interpretability\\\" to a model's decision (Bahdanau et al., 2014; Xu et al., 2015; Chan et al., 2015) . However, in the commonly used soft attention mechanism (Luong et al., 2015) each input element receives non-zero weight, and so it is unclear whether the magnitude of attention weights reflects the relative importance of the corresponding inputs (Jain & Wallace, 2019) . To make things worse, due to the existence of multiple stacked attention layers in transformer, it becomes even harder to discriminate the contributions of each input to the final decisions made by the model. Can we force the transformer to make sharper, discrete internal decisions? In this work, we consider a variant of the transformer architecture with the goal of maintaining performance while forcing discrete decisions. Specifically, we consider a discrete transformer with three changes to the architecture: (a) we propose to treat attention as a categorical latent variable (Deng et al., 2018; Shankar et al., 2018) and use hard attention mechanism to get discrete attention decisions (Xu et al., 2015) , (b) we propose to separate out the querying mechanism from value computation into intertwine soft \\\"syntactic\\\" and hard \\\"semantic\\\" model streams, and (c) we consider extension to the discrete transformer to allow for further additions such as attention sparsity regularization. Training of the model is very similar to standard transformer training. The key benefits come at inference time. First, we can use a simple decoding procedure where we take argmax attentions such that each intermediate representation is only built up based on the subset of the attended lower layer outputs. In turn, each final prediction uses limited receptive field, and we can even the guarantee that any hidden state does not depend on input elements not being directly attended to. Second, we can split out attention prediction from computation, and even fix the structure of the feed-forward network for a given example. To validate this approach, we perform experiments on several tasks. We first validate that with proper attention and sparsity regularization the model can learn the truly necessary attentions on a synthetic language modeling task. Next on two real world machine translation datasets, we show that with our approach we can learn transformer models using limited context for making predictions while not deteriorating their performance by too much, indirectly validating the selectiveness of the attention mechanism. The rest of the paper is organized as follows: In Section 2 we draw the connections of our work to the literature. We introduce background and discuss our approach in Sections 3, 4 and 5. Experiments, results and analyses are presented in Sections 6 and 7, and we conclude our paper in Section 8. Attention has been used to imply transparency into model's prediction. This is crucially important in domains health care (Caruana et al., 2015; Choi et al., 2016; Rajkomar et al., 2018) but has also been used in other natural language sequence modeling tasks (Rush et al., 2015; Deng et al., 2017; Alvarez-Melis & Jaakkola, 2017) . Since the soft attention mechanism assigns non-zero weights everywhere, to get interpretability, a general assumption is that larger attention weights correspond to higher importance in making a decision (Unanue et al., 2018) . However, a recent work (Jain & Wallace, 2019) shows that attention magnitude does not correlate well with gradient-based measures of input elements importance (Selvaraju et al., 2017) . To get around with the difficulty of credit assignment in soft attention, researchers have proposed to use sparse attentions. Peters et al. (2018) uses sparsemax (Martins & Astudillo, 2016) to induce sparse attention structures. Lei et al. (2016) model attention as Bernoulli random variables and use an encoder to produce the final prediction only from the attended input elements such that the final predictions can be rationalized. To optimize the final objective, Lei et al. (2016) apply policy gradients. Our work follows the spirit of their work, but we consider multi-head multi-layer attentions in transformer, which subjects REINFORCE algorithm to large gradient variance. Instead we use the Gumbel-Softmax trick (Jang et al., 2016; Maddison et al., 2016) to get reparameterizable samples and reduce the gradient variance (Kingma & Welling, 2013) . Broadly speaking, there are two directions of work towards improving interpretability: model interpretability and prediction interpretability (Alvarez-Melis & Jaakkola, 2017). Prediction interpretability relies on an external interpreter that is both interpretable and locally consistent with the black box model being explained, through which we can analyze the causal relationships between inputs and outputs (Ribeiro et al., 2016) . In model interpretability, researchers try to build models that are commonly regarded as interpretable (Louppe, 2014; Calders et al., 2013; Doshi-Velez & Kim, 2017; Peters et al., 2018) . Our approach aims to improve model interpretability by modifying the attention mechanism and objective function where we implicitly assume that more sparsity in the attention structure implies more interpretability, rather than relying on another model to analyze an existing one. While our approach does not directly lead to prediction interpretability, we can draw connections between our approach and the prediction interpretability framework of Alvarez-Melis & Jaakkola (2017) if we consider local permutations of input embeddings: for inputs not being directly or indirectly attended to at a specific prediction step, the local interpreter does not need to use them at all, hence there is no causal relationship between these inputs and the prediction. The separation between query mechanism and value computation resembles the two-stream attention mechanism in XLNet (Yang et al., 2019) , where a separate query stream is introduced in addition to the normal content stream to enable the usage of target position information while avoiding \\\"cheating\\\" to work with arbitrary generation factorization order. Recently, Russin et al. (2019) used word embeddings as content vectors whereas the attention is computed based on the outputs of an LSTM network. This approach shares a similar goal with ours to separate syntax and semantics, but transformer presents its unique challenges due to the existence of multiple layers and multi-headed attentions and its lack of recurrence. We begin by briefly reviewing the transformer architecture that will serve as the basis for this work. Specifically we will consider a transformer for simplified classification task over a sentence (experiments will expand this to autoregressive models). Let x 1:T be a sequence of input tokens and y is a discrete output label. We begin by encoding tokens with a position-specific embedding function e to vectors h 0 i . Each layer of the transformer then produces new vectors under the following recursion 1 : Here l is the layer index, FFN is a large, feed-forward NN, W are learned projection parameters, and d k is a constant for the size of the network. The key intermediary terms are the key K, value V , and query Q matrices which contain a vector for each position 1 . . . T , and the attention matrix A which gives a distribution for each position over its \\\"attention\\\" to all other positions. This attention is computed from the queries and keys and then used to take a convex combination of the values. At the final layer L we can then make a prediction utilizing h (L) , e.g. a softmax over all possible y labels. The main processing work of the transformer happens in the FFN layers which contain the majority of the non-embedding parameters. These can be thought of as large, width-1 convolutional networks that process the full sentence at each layer. However, for these layers to be effective it is crucial that information from other tokens be aggregated together. The attention layer serves as the single source of this aggregation in the model. Attention is the only point where inter-word information routing occurs. Because attention is central for routing and determines the receptive field of the transformer, it has been a main source of study for the transformer and related models. If attention can be understood then in theory the interconnection between words can be mapped and perhaps even manipulated. Unfortunately much of this work has so far produced negative results. The underlying problem is that while any one attention layer may target a small amount of keys, in aggregate repeated applications of multi-headed attention quickly connect every position to every other. Learned attention acts in a soft way and roughly pools together all elements into a vector. While this may be usable for high-level analysis, it does not allow us to truly separate out anything about the model decisions. In several recent works (Deng et al., 2018; Shankar et al., 2018) , researchers have explored alternatives to soft attention for single-layer (pre-transfomer) attention models. The goal of this work has been to learn models that can replace soft-attention with latent control variables that select a single position to use. This form of discrete attention can produce models that make these intermediate decisions explicit, which has been shown to produce models that perform as well or better than soft models. We begin by considering applying this approach directly to transformer. Formally, we can replace the above deterministic equations with an intermediate sampling step: This makes the transformer stochastic and requires computing, p(y|x) = z p(y, z|x) since we do not observe z. In a single layer model such as a sequence-to-sequence RNN, this might be a tractable sum, but for stacked attention, we would need to be able to sum out over all possible choices for z. In transformer this is combinatorial, and computing this term even for single-headed attention is O(T L ). Because of this complexity several alternative methods have been proposed in the literature. Given the success of soft attention for transformer, we opt for using a Gumbel-Softmax approach for training, which modulates between soft training and hard inference (Jang et al., 2016; Maddison et al., 2016) . The Gumbel-Softmax approach gives a continuous approximation to sampling from the categorical distribution. Given a categorical distribution defined by log probabilities l, the Gumbel-Softmax generative process is defined by first sampling U k \\u223c Uniform(0, 1), and then returns where g k = log( log(U k )) is called Gumbel noise (the distribution of g k is Gumbel distribution), and \\u03c4 is a temperature parameter controlling the entropy of the distribution. As \\u03c4 \\u2192 0, samples given by the Gumbel-Softmax function conforms to the same distribution as one-hot categorical samples. At training time, we can use this approach to obtain differentiable samples approximating sampling from the categorical attention distributions. Unlike REINFORCE, we can apply reparameterization to get a low variance gradient estimator and directly back-propagate through it in modern deep learning libraries. We use fixed temperature \\u03c4 throughout training but we tune its value on the validation set. At test time, we replace Gumbel-Softmax with the argmax from the vector. This corresponds to a greedy choice over the random variable. This test time model is almost identical to the original transformer: we simply replace softmax with argmax. The main benefit of this method though is that for every position we have a fixed tree of all the previous words that influenced it. We can effectively guarantee that if a word was not in this tree, then it did not contribute to the hidden state value of that position. 5 TRANSFORMER WITH SEPARATE SYNTACTIC AND SEMANTIC STREAMS Figure 1 : Discrete Transformer architecture. Syntactic transformer stream computes the attention distribution which is used to produce next hidden states while also constructing the semantic architecture through latent hard attention. By utilizing discrete attention, we can ensure that model routing is done through hard choices by the transformer. Since the final computation is done only based on the hidden state at the top layer, we can ensure that this decision was only made based on the hard pathway to the original words. To formalize this concept we consider the continuous receptive field of any hidden state h i , that is the input vectors that directly determined its current value. The structure of hard attention ensures that the receptive field is defined by the recursion r(i, l) = r(i, l \\u2212 1) \\u222a r(z is the hard sample taken at layer l for position i. While this receptive field grows exponentially with layers, its branching factor is much more constrained than with soft attention. However, we note that this same property can be obtained by giving more flexibility to the calculation of keys and values used in hard attention. In fact, these calculations can be kept soft throughout the entirety of the transformer without changing the continuous receptive field. We therefore propose an extension to the model structure of transformer that aims to separate out the \\\"synatactic\\\" routing control of the model from the \\\"semantic\\\" computation part. Motivated by Russin et al. (2019) , we follow the distinction that the semantic part of the model should consist of a fixed, sparse feed forward network, whereas the syntactic part is free to consider the entire sentence at any step. We build these together using a two stream transformer network. To build this model, we make the observation that values should be computed only by the semantic network, and that keys and queries should be computed only by the syntactic network. Let the syntatic representation at timestep i in layer l be g (l) i and the semantic representation be h (l) i . The first layer representations are set to the corresponding word embedding, g For each attention layer l, the two streams are updated as follows: Note that the syntactic stream (right) is completely independent of the semantic stream and could even be computed before hand. Furthermore assuming a fixed syntactic distribution the semantic distribution becomes a vanilla feedforward network. Practically, the two streams distributions can be trained together as a single network. Each attention step can be aligned as well as the FFN computations. A secondary benefit of utilizing an explicit latent variable within the model is the ability to impose structural constraints on the variable directly based on prior knowledge such as structured attention or sparse attention (Niculae & Blondel, 2017) . Here we consider a way to penalize the size of the receptive field at the final layer | i r(i, L)|. We want a differentiable version of | i r(i, L)|. Let's use s l ij to denote whether the representation of token i relies on embedding of token j at layer l, i.e. j \\u2208 r(i, l), then we have the recursion that Where the internal representation of token i depends on embedding of token j if s l\\u22121 ij = 1 (due to residual connections, the dependencies of a layer below are also the current dependencies) or if i attends to k at layer l \\u2212 1 and s l\\u22121 kj = 1 (the dependencies of the token attended to also become the current dependencies). Based on this recursion (and initial conditions that s 0 ij = 1(i = j)), the final layer receptive field size can be calculated as We note that since during training z comes from the Gumbel-Softmax, the z ik values are computed as a soft approximation (as opposed to indices). Therefore it provides useful gradients and can be directly applied as a regularizer. We run experiments on several different benchmark datasets including machine translation and language modeling. We also test if our approach is able to recover the true underlying dependencies with the sparsity regularizer. For each dataset, we start with a strong transformer baseline, with the goal of inducing a similarly performing model with explicit latent structure. Data For translation, we conduct experiments on two machine translation datasets: IWSLT (Cettolo et al., 2014) , a standard small-scale benchmark, and the much larger WMT English to German (Bojar et al., 2017) . To test whether our approach is possible to discover the true dependencies in the data, we also constructed a synthetic language modeling task with known underlying dependencies. Architecture and Hyperparameters While we describe the method using a simplified model, for experiments, we model our system directly off standard transformer models. For translation that means using both an encoder and a decoder transformer stack. Our DISCRETE TRANSFORMER uses encoder and decoder stacks analogous to those in TRANSFORMER. For self attention, we replace the scaled dot-product attention in TRANSFORMER by our two stream attention. The input consists of queries Q and keys K obtained from the syntactic representation g and two values V and V , one each from the syntactic representation g and the semantic representation h respectively. Multi-Head Attention is computed analogously. For encoder-decoder context attention, the syntactic stream of the encoder provides the keys K and the syntactic values V , the encoder's semantic stream provides the semantic values V , and the decoder's syntactic stream provides the queries Q. For WMT, we use the base model with d model = 512, d ff = 2048, L = 6, h = 8 (we refer to Vaswani et al. (2017) for hyperparameter definitions). For IWSLT, we use d model = 512, d ff = 1024, L = 6, h = 4 since it is more prone to overfitting. We implement our models based on Fairseq (Ott et al., 2019) . For WMT a single model was obtained by averaging the last 5 checkpoints saved every 1000 update steps. The Gumbel temperature is set to 1 throughout this paper, and we set the sparisity regularizer strength to 0.1 for the synthetic language modeling task. For language modeling, we we use only the decoder network of the transformer for language modeling. We use d model = 64, d ff = 256, L = 4, h = 2 for the synthetic stack language modeling task. Baselines The first baseline we consider is the normal transformer model with soft attention TRANSFORMER. Then we make the attentions discrete by applying Gumbel-Softmax at training time and argmax at test time, which we term SINGLE STREAM DISCRETE TRANSFORMER. We compare those baselines to our model equipped with discrete attention and two-stream attention DISCRETE TRANSFORMER. We also consider experiments separating out the syntactic and semantic streams. To test whether the sparse attentions learned by our approach correspond to true underlying dependencies, we adapt a synthetic stack language dataset from Strobelt et al. (2017) where we know the true dependencies. The vocabulary consists of {0, 1, 2, 3, 4, (, )}, and the language must match parentheses. Numbers are emitted randomly, but must match the nesting level (the number of open left parentheses). Nesting is limited to depth 4. We follow this grammar and created a training set of 50k sequences, validation/test sets of 5k/5k sequences, with sequence length being 30. We train models to do language modeling on this dataset, where the true dependency for a given target word is the span between the last number and the previous word. On this dataset, we train SINGLE STREAM DISCRETE TRANSFORMER with proper sparsity regularization (coefficient 0.1). At test time, we use argmax to get discrete attentions, and aggregate attentions to get the receptive field of each prediction. We were able to get precision of 0.959 and recall of 0.920 compared to the ground truth dependencies. In Figure 2 , we show an example of the learned receptive field versus the ground truth dependency. On the other hand, if we aggregate attentions of a soft TRANSFORMER model via Eq. 1, the result is much messier, even with the attention sparsity regularizer. A benefit of the two-stream model is the separation of the syntactic routing control from the semantic computation. We perform analysis to see how these two components of the model differ. First, we look at a qualitative experiment and visualize the embeddings of the 10,000 most frequent words in the vocabulary using t-SNE (the two streams use independent embeddings). Figure 3 shows the projections for both aspects of the model. We can observe immediately that the embeddings from the syntatic controller network cluster directly by part-of-speech (POS) tags while those from value network do not seem to have a clear pattern. The qualitative nearest neighbors example in Table 3 further confirms that the syntatic controller embeddings cluster by POS tags whereas those from the value network cluster by semantics. To quantify the difference in learned representations we consider utilizing the learned representations. We experiment with using different source encoders from the WMT model as pretrained representations for performing a syntactic chunking task from the CoNLL-2000 dataset. Syntatic chunking consists of dividing a text in syntactically correlated parts of words. For this task we use the pretrained encoder to obtain some vector representation of the source sentence which is then passed through a linear layer to project it to the space of chunk types. The encoder is frozen and only the linear projection layer is trained. For words broken into multiple tokens using the BPE tokenization, we use the vectors from first token. Table 2 shows the results from different models. The baseline result was obtained by selecting the chunk tag which was most frequently associated with the current part-of-speech tag. We see that both models outperform a standard transformer, and that the syntactic stream of the discrete transformer does the best. Interestingly the semantic stream, which does not have to determine word relations, performs worse then even the baseline model. This work presents the discrete transformer, a modification to the transformer to make discrete attention decisions and to separate out dependencies from semantic state value. Experiments show that despite the more structured decisions the model is able to maintain similar performance on standard machine translation benchmarks. Analysis shows that the model separates out syntactic properties and even learns precise decisions on clean data. This style of model opens up the potential for many possible experiments in NLP. Because the model makes hard intermediary decisions the semantic model can be shown to only depend on a subset of the data. This property could be used to check for or remove bias from a model, for instance to ensure that production gendered pronoun does not depend on spurious context. Similarly because the dependencies are predicted separately additional priors or regularization could be used to enforce specific syntactic structure. Finally, this method could be used to train pretrained models that allow for discrete intermediary structure.\",\n          \"This paper proposes a Pruning in Training (PiT) framework of learning to reduce the parameter size of networks. Different from existing works, our PiT framework employs the sparse penalties to train networks and thus help rank the importance of weights and filters. Our PiT algorithms can directly prune the network without any fine-tuning. The pruned networks can still achieve comparable performance to the original networks. In particular, we introduce the (Group) Lasso-type Penalty (L-P /GL-P), and (Group) Split LBI Penalty (S-P / GS-P) to regularize the networks, and a pruning strategy proposed  is used in help prune the network. We conduct the extensive experiments on MNIST, Cifar-10, and miniImageNet. The results validate the efficacy of our proposed methods. Remarkably, on MNIST dataset, our PiT framework can save 17.5% parameter size of LeNet-5, which achieves the 98.47% recognition accuracy. The expressive power of Deep Convolutional Neural Networks (DNNs) comes from the millions of parameters, which are optimized by various algorithms such as Stochastic Gradient Descent (SGD), and Adam BID18 . However, one has to strike a trade-off between the representation capability and computational cost, caused by the plenty of parameters in the real world applications, e.g., robotics, self-driving cars, and augmented reality. Pruning significant number of parameters would be essential to reduce the computational complexity and thus facilitate a timely and efficient fashion on a resource-limited platform, e.g. devices of Internet of Things (IoT). In addition, it has long been conjectured that the state-of-the-art DNNs may be too complicated for most specific tasks; and we may have the free lunch of \\\"reducing 2\\u00d7 connections without losing accuracy and without retraining\\\" BID7 .To compress DNNs, recent efforts had been made on learning the DNNs of small size. They either reduce the number and size of weights of parameters of original networks, and fine-tune the pruned networks BID0 ; BID32 , or distill the knowledge of large model , or directly learning the compact and lightweight small DNNs, such as ShuffleNet BID24 , MobileNet Howard et al. (2017) , and SqueezeNet BID13 . Note that, (1) to efficiently learn the compressed DNNs, previous works had to introduce additional computational cost in fine-tuning, or training the updated networks; (2) it is not practical nor desirable to learn the tailored, or bespoke networks for any applications, beyond computer vision tasks. To this end, the center idea of this paper is to propose a Pruning in Training (PiT) framework that enables pruning networks in the training process. Particularly, the sparsity regularizers, including lasso-type, and split LBI penalties are applied to train the networks. Such regularizers not only encourage the sparsity of DNNs, i.e., fewer (sparse) connections with non-zero values, but also can accelerate the speed of DNNs convergence. Furthermore, in the learning process, we can iteratively compute the regularization path of layer-wise parameters of DNNs. The parameters can be ranked by the regularization path in a descending order, as BID3 . The parameters in the high rank are in the high priority of not being pruned. More importantly, our PiT can learn the sparse structures of DNNs, and utilize the functionality of filters and connection weights (in fully connected layers). In the optimal cases, the weights (or filters) of each layer should be learned fully orthogonal to each other and thus formulate an orthogonal basis. The orthogonal constraint may be only enforced as the initialization (e.g., SVD Jia (2017) and BID26 ), or via the other regularization tricks, such as dropout preventing co-adaption BID27 , or batch normalization reducing the internal covariate shift of hidden layers BID14 . Therefore, our PiT can help uncover redundant information in a network by compressing less important filters and weights, and facilitate pruning out more interpretable networks. The deeper and wider deep CNN architectures can enable the superior performance on various tasks, and yet cause the prohibitively expensive computation cost. To efficiently train the networks, the regularization is usually applied to the weight parameters (Sec. 2.1). It is also essential to prune networks to reduce the size of networks (Sec. 2.2) Due to large number of parameters, the deep networks require large amount of memory and computational resources, and are inclined to overfit the training data. To alleviate this problem, it is essential to regularize the networks in training stage; such as dropout BID27 preventing the co-adaptation, and adding L 2 or L 1 regularization to weights. In particular, the L 1 regularization enforces the sparsity on the weights and results in a compact, memory-efficient network with slightly sacrificing the prediction performance BID2 . Further, group sparsity regularization BID34 can also been applied to deep networks with desirable properties. Alvarez et al. Alvarez & Salzmann (2016) utilized a group sparsity regularizer to automatically decide the optimal number of neuron groups. The structured sparsity BID30 ; BID33 has also been investigated to exert good data locality and group sparsity. Different from these works, the (Group) Split LBI penalty is for the first time, introduced to regularize the networks. This regularization term can not only enforce the structured sparsity, but also can efficiently compute the solution paths of each variable. Compressing the networks involves the pruning and compressing the weights and filters of DNNs. The common strategies include (1) matrix decomposition methods BID15 ; BID29 by decomposing the weight matrix of DNNs as a low-rank product of two smaller matrices; (2) low-precision weights methods BID11 ; BID40 by learning to store low-precision weights of DNNs; and (3) pruning methods BID7 ; Li et al. (2017) directly removing weights of connections, or neurons. Our framework is one of pruning methods. Previous pruning works, iteratively prune the weights or neurons, and fine-tune the network BID7 ; BID5 . Remarkably, network regularization is of significant important in pruning methods. The sparse properties of features maps and/or weights of DNNs exerted by network regularization, are utilized in Wen et al. (2016b) ; BID19 . BID23 adopt the statistics information from next layer to guide and save the importance of filters of the current layer. BID25 employed Taylor expansion to approximate the change of cost function which can be further utilized as the criterion in pruning network parameters. A LASSO-based channel selection strategy is investigated in BID9 . Abbasi-Asl et al. BID0 defined a filter importance index of greedy pruning the network. Comparing with all the methods, our framework is different in two points: (1) Criterion of importance of weights and filters. We rank the importance of weights and filters by their solution paths computed by sparse regularizers, rather than designing the elaborated metrics as previous works BID0 ; BID32 . Specifically, our algorithm is a process of solving the discrete partial differential equations; and our framework can result in the solution paths of optimizing the weights and filters, whose importance are ranked, according to the selected order in the path, as Fu et al. (2016b) . (2) Pruning in training: once DNNs are trained, we simply prune out less important weights/filter by a threshold. In this section, the Residual Network (ResNet) structureHe et al. FORMULA1 is employed to elaborate our framework. Our algorithms can be used in the other DNNs, e.g. Lenet-5. We adopt the notations of ResNet strucutre, in which the output of the ith block O i can be represented as: DISPLAYFORM0 where x the input of the first layer of the ith block, {W } i and W i respresent the filter weights in the ith block and the shortcut weight matrix, respectively. The function F(\\u00b7) represent the multiple convolutional layers. Denote the weight matrix of the first convolutional layer as W conv1 and that of the fully connected layer as W f c . Suppose there are I blocks, then we denote all the parameters of the network as \\u0398 := {W conv1 , {W } 1 , ..., {W } I , W 1 , ..., W I , W f c } and \\u0398 \\u2212W := \\u0398\\\\W for W \\u2208 \\u0398. Our key objective is to train a sparse DNN of less parameters, and yet comparable performance to the non-sparse DNN. The training function of DNN is defined as, DISPLAYFORM1 where P (\\u00b7) is the penalty function of parameters \\u0398. If we use (X , y) as the sample set of the dataset; then in classification task, the loss function is the cross-entropy function as DISPLAYFORM2 where N and K are the number of samples and classes and p n,k (\\u0398) denotes the probability of the nth sample belongs to class k. Generally, we can use Stochastic Gradient Descent (SGD) algorithm to update \\u0398; and the algorithm is summarized in algorithm 1.Algorithm 1 SGD for ResNet 1: Input: Learning rate \\u03b7, X and y 2: Initialize: DISPLAYFORM3 One direct intuition is to adopt the sparsity regularization on the parameters, or those of the one layer of the network, such as BID22 ; Wen et al. (2016b) . To reduce the number of connection weights, one can consider different types of regularization, including (1) Lasso-type penalty (L 1 ), (2) Group-Lasso-type penalty BID34 ; (3) An iterative regularization path with structural sparsity (e.g., elastic net BID42 , and Split LBI Huang et al. (2016) ): here we employ the Split LBI which learns the structural sparsity via variable splitting and Linearized Bregman Iteration (LBI), due to the computational efficiency of the LBI, and model selection consistency, Lasso-type penalty can be directly implemented on the fully connection layer i as, DISPLAYFORM0 Under review as a conference paper at ICLR 2019Group-Lasso-type penalty BID34 aims at regularizing the groups of parameters \\u0398, and W (g) is a group of partial weights in \\u0398, DISPLAYFORM1 where DISPLAYFORM2 , and W (g) is the number of weights in W (g) ; G is the total number of groups. This Split LBI algorithm BID12 introduces an augmented variable \\u0393 which is enforced sparsity and kept close to W , by variable splitting term DISPLAYFORM0 . Then the objective function turns to: DISPLAYFORM1 To enforce the sparsity of \\u0393, we here implement the LBI algorithm on the W , and the algorithm can be summarized in algorithm 2, where DISPLAYFORM2 The 5th-8th lines are Split LBI algorithm, which returns a regularization path of {\\u0398 DISPLAYFORM3 It starts from the null model with \\u0393 0 = 0, and tends to select more and more variables as the algorithm evolves, until over-fitted. At each step, the sparse estimator W k is the projection of W k onto the subset of the support set of \\u0393 k . The remainder of the projection is affected by weak signals with small magnitude and mostly the ones mainly affected by random noise. Particularly, we highlight several points,\\u2022 The \\u03ba is the damping factor, which enjoys the low bias with larger value, however, at the sacrifice of high computational cost. The \\u03b1 is the step size. In BID12 , it has been proved that the \\u03b1 is the inverse scale with \\u03ba and should be small enough to ensure the statistical property. In our scenario, we set it to 0.01/\\u03ba.\\u2022 The t k = k\\u03b1 is the regularization parameter, which plays the similar role with \\u03bb in Lasso. It's the trade-off between underfiting and overfiting, which can be determined via the loss/accuracy on the validation dataset.\\u2022 The \\u03bd controls the difference between W and W . In BID12 , it has been proved that larger value of \\u03bd can enjoy better model selection consistency, however may suffer from the larger parameter estimation error. In ; BID39 , it has been proved that as long as \\u03bd 0, the dense estimator W can enjoy better prediction error by leveraging weak signals. We will discuss it in the next subsection.\\u2022 Each component of the closed form solution W \\u2208 R p1\\u00d7p2 in equation 5 can be simplified as, DISPLAYFORM4 The pruning algorithm is inspired by the Fu et al. (2016b) . Particularly, it has been pointed out in BID39 that the dense estimator can be orthogonally decomposed into three parts: strong signals which correspond to non-zero elements in W , weak signals and random noise. Due to the ability to leverage additional weak signals as long as \\u03bd is large enough, it has been proved theoretically and experimentally that, the dense estimator outperforms the sparse estimator in prediction. Algorithm 2 SGD for ResNet with Split LBI 1: Input: Learning rate \\u03b7, \\u03bd > 0, step size of LBI \\u03b1, damping factor \\u03ba > 0, X and y 2: Initialize: DISPLAYFORM0 This inspires us to sequentially consider all available solutions for all sparse variables along the Regularization Path (RP) by gradually decreasing the values of regularization coefficients. Specifically, we can order the parameter set \\u0398 according to the magnitude values of weights W . Following this order, we identify the top r% of weights in \\u0398 r . The complementary set \\u0398 1\\u2212r = \\u0398\\\\\\u0398 r can be pruned. Compared to the pruning methods in Han et al. (2015a) , we can prune the weights in the training process and do not need to fine-tune the weights. Furthermore, one can easily extend algorithm 2 to prune at L (L > 1) layers. We take the Split LBI as an example; the other two methods can also be directly applied to multiple layers. The corresponding algorithm is described in algorithm 3. Algorithm 3 SGD for ResNet with Split LBI on multiple layers 1: Input: Learning rate \\u03b7, \\u03bd > 0, step size of LBI \\u03b1, damping factor \\u03ba > 0, X and y 2: Initialize: DISPLAYFORM0 10: DISPLAYFORM1 We conduct the experiments on three datasets, namely, MNIST, CIFAR10, and MiniImageNet. We use the standard supervised training and testing splits on all datasets, except MiniImageNet, whose setting is splitted by ourselves, and will be released. The classification accuracy is reported on each dataset. Competitors. We compare three methods of pruning networks. (1) Plain: we train a plain network and use the L 2 \\u2212 penalty P (W ) = W 2 . For all layers, we set the coefficient \\u03bb as 5e \\u2212 4 in Eq (1). We prune the trained network by ranking the weights and filters, in term of their magnitude values in the descending order. This pruning strategy can be taken as a simplified version of our pruning algorithm in Sec. 3.4. (2) Rand. We randomly remove the weights or filters in the networks. This is a naive baseline. (3) Ridge-Penalty (R-P) BID7 to rank the weights and filters by L 2 regularization path. For that particular layer that we want to do the pruning, the coefficient \\u03bb would be finally set as 1e \\u2212 3.We also compare two variants of our PiT framework. (4) Lasso-type penalty or Group-Lasso-type penalty (L-P / GL-P): the L-P is used to prune the weights of fully connected layers, and we employ the GL-P to directly remove the filters of convolutional layers. (5) Split LBI or Group Split LBI penalty(S-P / GS-P): the split BLI penalty is utilized to prune the weights. Accordingly, we have the Group Split LBI penalty by regularizing the groups of filter parameters as BID34 . Note that all the results are trained for one time; and we do not have fine-tuning step after the pruning. The handwritten digits MNIST dataset is widely used to experimentally evaluate the machine learning methods. We use the standard supervised split and LeNet-5 LeCun et al. (1998) which is composed of 3 convolutional layers and 2 fully connected layers. All the models are trained and get converged in 50 epochs. Note that each experiment is repeated for five times, and the averaged results are reported. In the experiments, we consider saving the portion of 100%, 50%, 25%, 12.5%, 6.25%, 3.13%, and 1.57% of original parameters on each layer. Please refer to the Appendix for more detailed results. Pruning each layer. The results are shown in Tab. 1. We employ our PiT algorithms to prune each individual layers of LeNet-5, while we keep the parameters of the other layers unchanged. We have the following observations:(1) On two fully connected layers (fc.f6 and fc.f7), both the L-P and S-P of our PiT framework work very well. For example, on the fc.f7 layer, our S-P only has 1.57% of the parameters on these layers. Surprisingly, our performance is only 0.03% lower than that of the original network. In contrast, we compare the pruning results with the baseline: Plain, Rand, and R-P. There is significant performance dropping with the more parameters pruned. This shows the efficacy of our PiT framework.(2) On the convolutional layer (conv.c5), our L-P and S-P layers also achieve remarkable results. Note that the conv.c5 layer has 48k out of 60k number of parameters in Lenet-5. We show that our S-P saves 12.5% of total parameters of this layer (i.e., 42k number of parameters have been removed on this layer) and the results get only dropped by 0.3%. This demonstrates that our PiT framework indeed can save the relatively important weights and filters, and effectively do the network pruning.(3) The conv.c3 layer is another convolutional layer in LeNet-5. We found that this layer is very important to maintain a good performance of overall network. Nevertheless, the results of our pruning L-P and S-P are still better than the other baselines. Pruning two layers. Totally, the LeNet-5 has 60k parameters, while the conv.c5 and fc.f6 have 48k and 10k number of parameters respectively. That means these two layers have the most number of Table 2 : Pruning two layers in LeNet-5 on the MNIST dataset. Each column indicates the percentage of parameter saved on these two layers. Com-Rat, is short for the compression ratio of the total network, i.e., the ratio of saved parameters divided the total number of parameters of LeNet-5. Table 3 : Pruning each block in ResNet-18 on Cifar-10 dataset. Note that each block has two CNN layers.parameters. In this case, we utilize our PiT algorithms to prune both fc.f6 + fc.f7, and conv.c5+fc.f6 layers. The results are reported in Tab. 2. We can show that our PiT framework can still efficiently compress the network while preserve significant performance. The best compressed model. When we prune the conv.c5 and fc.f6 layers, our model can achieve the best and efficient performance. With only 17.60% parameter size of original LeNet-5, our model can beat the performance as high as 98.47%. Remarkably, our PiT framework has not done any fine-tuning and re-training the pruned network by any other dataset. This suggests that our PiT can indeed uncover the important weights and filters. Our best models will be downloaded online. Table 4 : Pruning multiple blocks in ResNet-18 on Cifar-10 dataset. (Chance-level = 10%). ComRat, is short for compression ratio of the total network, as in Tab. 2. The CIFAR-10 dataset consists of 60,000 images of size 32 \\u00d7 32 in 10 classes, with 6000 images per class on average. There are 50,000 training images and 10,000 test images. We use the standard supervised split; and ResNet-18 is employed as the classification network. All the models are trained and get converged in 40 epochs. Note that each experiment is repeated for five times, and the averaged results are reported. We still show the results which have 100%, 50%, 25%, 12.5%, 6.25%, 3.13%, and 1.57% parameter size of original networks on each layer. Pruning one Residual Block. The results are shown in Tab. 3. In this table, we apply our PiT algorithm on one residual block while the other layers are unchanged. We draw several conclusions,(1) Our PiT framework (i.e., GS-P and GL-P) can efficiently train and prune the network. From Block #3.0 -Block #4.1, surprisingly the pruned network with 1.57% of original parameter size of ResNet-18, can also achieve almost the same recognition accuracies as the non-pruned ResNet-18. From Block #1.0 -Block #2.1, the smallest pruned ratio of PiT can still hit significant high performance if compared with the other competitors. This reflects the efficacy of our pruning algorithm. In particular, in the training process, our PiT framework is optimized to learn and select the important weights or filters; and our PiT can thus conduct a direct dimension reduction of these parameters.(2) By the increased ratio of pruned parameters, the R-P method can also have better performance than Rand, and Plain methods. This shows that our pruning algorithm also works in the general cases. However, the R-P is not enforcing the sparse constraints in learning the weight parameters of network. Thus it has inferior performance to two PiT methods. Pruning multiple blocks. The ResNet-18 totally has around 10.95M parameters. Block #4.1, #4.0, #3.1, #3.0 have 4.7M , 3.5M , 1.2M , and 0.88M parameters. These blocks have the most number of parameters; and we prune these multiple blocks. The results are shown in Tab. 4. Note that even only 1.57% parameter size of those layers are saved, our PiT algorithms (GL-P, and GS-P) can still remain remarkable high recognition accuracy. Again it shows the efficacy of our PiT framework. TAB6 : Top 5 accuracy on miniImagenet by pruning ResNet-18, the fully connected layer, Block#4.0 and #4.1 layers. The miniImageNet dataset is a subset of ImageNet and is composed of 60,000 images in 100 categories. In each category, we take 500 images as training set and other 100 as testing set. We also use the ResNet-18 structure on miniImageNet. All the models are trained and get converged in 50 epochs. Note that each experiment is repeated for five times, and the averaged results are reported. In term of the analysis in Sec. 4.2, we prune the fully connected layer, Block #4.0, and #4.1. The results are shown in Tab. 5.When no parameters are pruned, the R-P can achieve better results than our PiT algorithms. These results make sense, since the ridge penalty does not enforce the sparsity to the network 1 . However, with the increased ratio of parameters pruned, the performance of R-P gets degraded dramatically. In contrast, the results of our methods in PiT framework get decreased very slow. For example, when only 50% are saved in all the layers, the Top-5 accuracies are reduced by only 0% and 1.1% for L-P / GL-P, and S-P / GS-P respectively. Remarkably, if we only save 1.57% of original parameters on those layers, the S-P / GS-P can still is as high as 65.17, which is only 13.78% performance dropped. Again, note that all the methods have not done any fine-tuning step, and only been trained in one round. That means our S-P / GS-P can indeed select the most expressive weights or filters, and thus reduce the size of networks. As the experiments shown in these three datasets, our PiT indeed can learn to prune networks without fine-tuning. We give some further discussion and highlight the potential future works, 1. In all our experiments, our L-P / GL-P, and S-P / GS-P are applied to, at most, four layers in one network. Theoretically, our PiT algorithms should be able to be directly applied to any layers of DNNs, since PiT only adds some sparse penalties in the loss functions. However, in practice, we found that the network training algorithm, i.e., SGD in Alg. 3, is unstable, if we apply the sparse penalties more than four layers. It will take much more time and training epochs to get the networks converged. 2. Essentially, our PiT presents a feature selection algorithm, which can dynamically learn the importance of weights and filters in the learning process; mostly importantly, we donot need any fine-tuning step, which, we believe, will destroy values and properties of selected weights and filters. Therefore, it would be very interesting to analyze the statistical properties of selected features in each layer. 3. Theoretically, we can not guarantee the orthogonality of weights and filters in the trained model. Empirically, we adapt some strategies. For example, the weights and filters of each layer can be orthogonally initialized; and we apply the common regularization tricks, e.g., dropout, and batch normalization. These can help decorrelate the learned parameters of the same layers. Practically, our PiT framework works well in selecting the important parameters and prune the networks as shown in the experiments. We also visualize the correlation between removed and none removed filters in the Appendix. 4. It is a conjecture that the capacity of DNNs may be too large to learn a small dataset;and it is essential to do network pruning. However, it is also an open question as how to numerically measure the capacity of DNNs and the complexity of one dataset.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"H1a37GWCZ\",\n          \"BygdR0VKDr\",\n          \"r1GgDj0cKX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"To train a sentence embedding using technical documents, our approach considers document structure to find broader context and handle out-of-vocabulary words. Presents ideas for improving sentence embedding by drawing from more context. Learning sentence representations with sentences dependencies information Extends the idea of forming an unsupervised representation of sentences used in the SkipThough approach by using a broader set of evidence for forming the representation of a sentence\",\n          \"Discrete transformer which uses hard attention to ensure that each step only depends on a fixed context. This paper presents modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. This paper proposes three Discrete Transformers: a discrete and stochastic Gumbel-softmax based attention module, a two-stream syntactic and semantic transformer, and sparsity regularization.\",\n          \"we propose an algorithm of learning to prune network by enforcing structure sparsity penalties This paper introduces an approach to pruning while training a network using lasso and split LBI penalties\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT\",\n          \"Discrete Transformer\",\n          \"PRUNING IN TRAINING: LEARNING AND RANKING SPARSE CONNECTIONS IN DEEP CONVOLUTIONAL NETWORKS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 31,\n        \"max\": 79,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          68,\n          61,\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\u2026 Ransomware is computer malware that installs covertly on a victim's computer, executes a cryptovirology attack that adversely affects it, and demands a ransom payment to decrypt it or not publish it. Built to harvest the banking credentials of victims, the virulent Dridex is now one of the most dangerous pieces of financial malware in circulation. However, we identify more semantically related content to a target sentence based on the document structure as shown in FIG1 . They tend to have no period at the end and contain a larger font size, a higher number of italic or bold text, and a higher ratio of capitalized words compared to non-title sentences. We first build a text style model for sentences appearing in the document body, capturing the three style attributes. Once list items are identified, we consider the sentence appearing prior to the list items as the introductory sentence and assume that it governs all the items in the list. We consider the sentence appearing prior to the list items as the introductory sentence and assume that it governs the list items. Similarly to SKIP-THOUGHT BID12 , we train our model to generate a target sentence S t using a set of governing sentences G. However, SKIP-THOUGHT takes into account only the window-based context (D W n ), while our model considers diverse long distance context. In this work, we use the pre-trained vectors from the CBOW model BID21 , and the word vectors can be optionally updated during the training step. In this work, we apply the most commonly used approach, i.e., using the average vector of all the words in the vocabulary to represent all OOV words, to generate the input embeddings of G or S t for the encoder and the decoder. We trained the proposed model (OURS) and the baseline systems on 807,647 randomly selected documents from the 2009 Wikipedia dump, which is the latest Wikipedia dump in HTML format, after removing the discussion and resource (e.g., images) articles among. For a quantitative evaluation between the two models, we compare the prediction losses by using the same loss function, namely cross entropy loss. D'Souza & Ng FORMULA2 and BID0 have shown that general domain coreference resolution models do not work well for domain specific entity types. While our system is not intended to be a coreference resolution tool, the rich sentence embedding can be used for unsupervised coreference resolution allowing it applicable to any domain. To apply our model, we assume that entity mentions are detected in advance (any mention detection tool can be used), and, for a pronoun or a generic entity reference (e.g., a definite noun phrase), we select a list of candidate referents that conform to the mention types allowed by the pronoun or the definite noun. The evaluation data consists of 563 coreferences extracted from 38 Wikipedia articles about malware programs which were not included in the training document set.\",\n          \"To facilitate parallel training, as well as to reduce the path length of the dependencies, transformer dispenses recurrence and builds up hidden states by attending to the source side (inter-attention) and attending to its past predictions (self-attention) with multiple heads in multiple layers (Vaswani et al., 2017) . However, in the commonly used soft attention mechanism (Luong et al., 2015) each input element receives non-zero weight, and so it is unclear whether the magnitude of attention weights reflects the relative importance of the corresponding inputs (Jain & Wallace, 2019) . To make things worse, due to the existence of multiple stacked attention layers in transformer, it becomes even harder to discriminate the contributions of each input to the final decisions made by the model. Specifically, we consider a discrete transformer with three changes to the architecture: (a) we propose to treat attention as a categorical latent variable (Deng et al., 2018; Shankar et al., 2018) and use hard attention mechanism to get discrete attention decisions (Xu et al., 2015) , (b) we propose to separate out the querying mechanism from value computation into intertwine soft \\\"syntactic\\\" and hard \\\"semantic\\\" model streams, and (c) we consider extension to the discrete transformer to allow for further additions such as attention sparsity regularization. Next on two real world machine translation datasets, we show that with our approach we can learn transformer models using limited context for making predictions while not deteriorating their performance by too much, indirectly validating the selectiveness of the attention mechanism. To get around with the difficulty of credit assignment in soft attention, researchers have proposed to use sparse attentions. Our approach aims to improve model interpretability by modifying the attention mechanism and objective function where we implicitly assume that more sparsity in the attention structure implies more interpretability, rather than relying on another model to analyze an existing one. While our approach does not directly lead to prediction interpretability, we can draw connections between our approach and the prediction interpretability framework of Alvarez-Melis & Jaakkola (2017) if we consider local permutations of input embeddings: for inputs not being directly or indirectly attended to at a specific prediction step, the local interpreter does not need to use them at all, hence there is no causal relationship between these inputs and the prediction. The separation between query mechanism and value computation resembles the two-stream attention mechanism in XLNet (Yang et al., 2019) , where a separate query stream is introduced in addition to the normal content stream to enable the usage of target position information while avoiding \\\"cheating\\\" to work with arbitrary generation factorization order. Recently, Russin et al. (2019) used word embeddings as content vectors whereas the attention is computed based on the outputs of an LSTM network. This approach shares a similar goal with ours to separate syntax and semantics, but transformer presents its unique challenges due to the existence of multiple layers and multi-headed attentions and its lack of recurrence. The underlying problem is that while any one attention layer may target a small amount of keys, in aggregate repeated applications of multi-headed attention quickly connect every position to every other. The structure of hard attention ensures that the receptive field is defined by the recursion r(i, l) = r(i, l \\u2212 1) \\u222a r(z is the hard sample taken at layer l for position i. While this receptive field grows exponentially with layers, its branching factor is much more constrained than with soft attention. Motivated by Russin et al. (2019) , we follow the distinction that the semantic part of the model should consist of a fixed, sparse feed forward network, whereas the syntactic part is free to consider the entire sentence at any step. Then we make the attentions discrete by applying Gumbel-Softmax at training time and argmax at test time, which we term SINGLE STREAM DISCRETE TRANSFORMER. We can observe immediately that the embeddings from the syntatic controller network cluster directly by part-of-speech (POS) tags while those from value network do not seem to have a clear pattern.\",\n          \"The expressive power of Deep Convolutional Neural Networks (DNNs) comes from the millions of parameters, which are optimized by various algorithms such as Stochastic Gradient Descent (SGD), and Adam BID18 . However, one has to strike a trade-off between the representation capability and computational cost, caused by the plenty of parameters in the real world applications, e.g., robotics, self-driving cars, and augmented reality. They either reduce the number and size of weights of parameters of original networks, and fine-tune the pruned networks BID0 ; BID32 , or distill the knowledge of large model , or directly learning the compact and lightweight small DNNs, such as ShuffleNet BID24 , MobileNet Howard et al. (2017) , and SqueezeNet BID13 . To efficiently train the networks, the regularization is usually applied to the weight parameters (Sec. 2.1). In particular, the L 1 regularization enforces the sparsity on the weights and results in a compact, memory-efficient network with slightly sacrificing the prediction performance BID2 . BID23 adopt the statistics information from next layer to guide and save the importance of filters of the current layer. Specifically, our algorithm is a process of solving the discrete partial differential equations; and our framework can result in the solution paths of optimizing the weights and filters, whose importance are ranked, according to the selected order in the path, as Fu et al. (2016b) . One direct intuition is to adopt the sparsity regularization on the parameters, or those of the one layer of the network, such as BID22 ; Wen et al. (2016b) . To reduce the number of connection weights, one can consider different types of regularization, including (1) Lasso-type penalty (L 1 ), (2) Group-Lasso-type penalty BID34 ; (3) An iterative regularization path with structural sparsity (e.g., elastic net BID42 , and Split LBI Huang et al. (2016) ): here we employ the Split LBI which learns the structural sparsity via variable splitting and Linearized Bregman Iteration (LBI), due to the computational efficiency of the LBI, and model selection consistency, Lasso-type penalty can be directly implemented on the fully connection layer i as, DISPLAYFORM0 Under review as a conference paper at ICLR 2019Group-Lasso-type penalty BID34 aims at regularizing the groups of parameters \\u0398, and W (g) is a group of partial weights in \\u0398, DISPLAYFORM1 where DISPLAYFORM2 , and W (g) is the number of weights in W (g) ; G is the total number of groups. We show that our S-P saves 12.5% of total parameters of this layer (i.e., 42k number of parameters have been removed on this layer) and the results get only dropped by 0.3%. This demonstrates that our PiT framework indeed can save the relatively important weights and filters, and effectively do the network pruning.(3) The conv.c3 layer is another convolutional layer in LeNet-5. With only 17.60% parameter size of original LeNet-5, our model can beat the performance as high as 98.47%. From Block #3.0 -Block #4.1, surprisingly the pruned network with 1.57% of original parameter size of ResNet-18, can also achieve almost the same recognition accuracies as the non-pruned ResNet-18. In particular, in the training process, our PiT framework is optimized to learn and select the important weights or filters; and our PiT can thus conduct a direct dimension reduction of these parameters.(2) By the increased ratio of pruned parameters, the R-P method can also have better performance than Rand, and Plain methods. 4. Note that even only 1.57% parameter size of those layers are saved, our PiT algorithms (GL-P, and GS-P) can still remain remarkable high recognition accuracy. For example, the weights and filters of each layer can be orthogonally initialized; and we apply the common regularization tricks, e.g., dropout, and batch normalization.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['target'] = data['target'].apply(lambda x : x.split('. '))\n",
        "\n",
        "def shuffle_list(lst):\n",
        "    random.shuffle(lst)\n",
        "    return lst\n",
        "\n",
        "data['target'] = data['target'].apply(shuffle_list)\n",
        "data['target'] = data['target'].apply(lambda x : '. '.join(x))\n",
        "data['target'][451]"
      ],
      "metadata": {
        "id": "03zjLD42tXLy",
        "outputId": "63a32f82-e697-4f73-dacb-f278e41d7838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The paper proposes layer-wise hybrid filter bank which only quantizes a fraction of convolutional filters to ternary values towards the MobileNets architecture.. 2x savings in model size, 28% energy reduction for MobileNets on ImageNet at no loss in accuracy using hybrid layers composed of conventional full-precision filters and ternary filters Focuses on quantizing the MobileNets architecture to ternary values, lowering the required space and computation in order to make neural networks more energy efficient'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = \"welcome to the jungle\"\n",
        "\n",
        "x = txt.split()\n",
        "\n",
        "print(x)"
      ],
      "metadata": {
        "id": "giFnwuRBvEa_",
        "outputId": "b0b445ef-4959-44ba-9745-49e391a6c8f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['welcome', 'to', 'the', 'jungle']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "d82e9210-1918-402f-e8d1-5d1d74368b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "397ea569-7716-43a0-9844-6e85d48cb2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "0a16c414-610c-4c3e-f047-3fdb26828ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      shuffle=True,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        shuffle=True,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "686a38d2-0f41-4406-adb2-501f28f1688d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 8) (16, 8) (80, 8) (20, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenize data\n",
        "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# # Function in order to tokenize source and target\n",
        "# max_input_length = 1024\n",
        "\n",
        "# def tokenize_function(data):\n",
        "#   model_inputs = tokenizer(text=data['extractive_summary'], text_target=data['target'], max_length=max_input_length, truncation=True)\n",
        "#   return model_inputs\n",
        "\n",
        "# tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "VyuuPBgIsJim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "c5099645-f9f4-4e76-bf89-ea14c45d51a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "4436655f7773484fafd1cff33ef78ffb",
            "d0c8fbe4c9cc47ef9de30e6f1d7b8a00",
            "5ddedeae3c37411f809914dd40626747",
            "2b9d07ace2bf4dd6864c4e7f70d8d184",
            "365f866542054b8096f0425b740e3657",
            "88891a0283ed46ec815622f98ec2e88d",
            "f0ae46782ed54cda804a8a992bff0092",
            "9125306d57674cc79e44cbbc6fa73493",
            "7351b89d38c841699116d5aa21d2b8e6",
            "8df3401496304e62b772d6b9dfa54491",
            "34a039133bef4ba297da41e8e252412f",
            "ae83c009d6af44cbb7235c317c8959d7",
            "e4774ef2229d4eb0ab8335fecf443bfa",
            "c2a1348eca4f40ea8f9976036ab98d2c",
            "a2e1287ad5da41138aab3041744c2eb4",
            "1c129f9f527e4511ab67540e3cf838e4",
            "a3ca1257e20946749a7a6b2dc2ea7194",
            "8749f001e7a24926bffc01a5491e1e8a",
            "5e97b25494a744f7bfbf01198bb4eab0",
            "2d628a6203104fa696604ec4609eeffc",
            "ba8eea1a6051482394bcd1176906e8f7",
            "5429d5d2ce5543bfaf691acd3abae273",
            "66beca881e744a0d95d12bbcb071f9e6",
            "e32b30837b684ea1a9369c97610084b5",
            "9174c28f178e4720b081af53ace7648a",
            "148c7002ce6a4dc491ee90bd3fd53e5f",
            "a98ba1580ced44dfaf4573d89e4f20cb",
            "f277c50799e447998d6091f3d671d8ef",
            "77a6a099c602449cab53c4331133baf9",
            "2bdc6e8e939b4f04ace92239a8d59dc2",
            "a20f3e6076ee4028bae0076528f657c4",
            "a809ae1cbf4941e28a5ee6b8990f0d47",
            "582775feaacc4557a9052e37fa54d1f1",
            "79a88598be7e47e2b3df8f13fd34d4b3",
            "cb47d7607757459396206b5c29bd7584",
            "fb62bc7f31ca4f3799a11d035c75a87d",
            "7d7598e022994663a060abfaeebf8f50",
            "8389228f026241feb945dd390466b2ba",
            "33ca2f7b5b2e424aa24ae42a85478c77",
            "0d0586338ea242ac885a4167a824cfa3",
            "387c04fc5fbb421b87f9eaf0cc150e8b",
            "4aa0091c91764f6ca2e21d65c45d18eb",
            "6bcfa13acbad4f1e9fb284a68f61d9a9",
            "77f1d6f94bab409cbfadfd13c7597f3a",
            "972f7cc0237f488ab0a54befbfe85b26",
            "5bac7c1c2a544f19b3bd5b3ec89b5cfa",
            "81aff29d06114ac585119b7c0fe6dfe4",
            "e1794a5fee8b4b3cb693689b7db95cdf",
            "17f3736bee3647979f096f40113e0963",
            "56550977be954f68816b767163347f62",
            "49d7da777634415da2235bf936ee4abe",
            "3d06964dc33345948b552a48f26af805",
            "901d231559a646bf8ef44f11ad6a7d1f",
            "27e9f6cbeacb4e41a518ac76d93be103",
            "ba259db36cbf4c16bfd297cfa6d563a9",
            "55d2683ac2754716b0e0d9e3278cfe89",
            "fc2a494fbc2d4877ae995d6b628a20e9",
            "702b85d5fab64c92ad5aceb22590dad4",
            "f80148a45c1a4719a2be4a2dc2212532",
            "5e092b22d4204ac2a5460189e74181d8",
            "c7488fa5a9a0421ba176d0ff94257498",
            "503836257374431893318438c40c02f3",
            "b477dab0156d418b8628ce7eb0101f2b",
            "be4b80a5029443a687913fab23f85209",
            "4300241ad43a426389660ba557b5041e",
            "97cd20bbd1974fe49435f102ea74242d",
            "c6e6414e83b5478e9bd21b830b0fb7b7",
            "f01fcdd5a3804589b6dd9b0353298e0d",
            "2cb53e5c8fb9432795fa00dbd28a95ae",
            "7f3636d640fe4670b623e492f8a336f7",
            "01871aacaa8649f2ab1bdb072114f1c9",
            "f225c165ce93444a97f893b8ab1b4cc9",
            "152a86e33f864838af4780a780a170b6",
            "7aa29606933f4bed8700a14e1596a938",
            "b9a8843efae24b7a9e987ebc7d47e13c",
            "715ef094f367439bbaf1128eb6d41f6b",
            "cf68125bf1a14540825eab2fa30ce1fa"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4436655f7773484fafd1cff33ef78ffb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae83c009d6af44cbb7235c317c8959d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66beca881e744a0d95d12bbcb071f9e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79a88598be7e47e2b3df8f13fd34d4b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/192 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "972f7cc0237f488ab0a54befbfe85b26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55d2683ac2754716b0e0d9e3278cfe89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6e6414e83b5478e9bd21b830b0fb7b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = True\n",
        "model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "# model.generation_config.renormalize_logits = True\n",
        "\n",
        "model.config.attention_dropout = 0.1\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "name_model = 'sampling-norep-v3/'\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "2160793f-5100-4075-8d55-b3b4c91ed2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXfD8z7vdqC",
        "outputId": "90204fd9-098e-49f2-905d-a0c38187493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartConfig {\n",
              "  \"_name_or_path\": \"facebook/bart-base\",\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"gelu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"BartModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.1,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 768,\n",
              "  \"decoder_attention_heads\": 12,\n",
              "  \"decoder_ffn_dim\": 3072,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"dropout\": 0.1,\n",
              "  \"early_stopping\": true,\n",
              "  \"encoder_attention_heads\": 12,\n",
              "  \"encoder_ffn_dim\": 3072,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_position_embeddings\": 1024,\n",
              "  \"model_type\": \"bart\",\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": true,\n",
              "  \"num_beams\": 4,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"scale_embedding\": false,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 128,\n",
              "      \"min_length\": 12,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_cnn\": {\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 142,\n",
              "      \"min_length\": 56,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_xsum\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 62,\n",
              "      \"min_length\": 11,\n",
              "      \"num_beams\": 6\n",
              "    }\n",
              "  },\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset to inspect the batches\n",
        "for batch in train_dataset.take(100):  # Take the first batch for inspection\n",
        "    print(batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CsKTRNhvqCQ",
        "outputId": "38adf07c-d213-4243-e990-5af2b6feae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3376,  5969, ...,     1,     1,     1],\n",
            "       [    0, 44891,     7, ...,     1,     1,     1],\n",
            "       [    0,     6,   992, ...,    81, 14307,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 39936, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4195, ..., 28695,     5,     2],\n",
            "       [    0, 13863,    89, ...,     1,     1,     1],\n",
            "       [    0, 46797,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16991,     9, ...,     1,     1,     1],\n",
            "       [    0,  9690, 16894, ...,  5342,  2222,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 26039, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 15243,   484, ...,     1,     1,     1],\n",
            "       [    0, 21119,  4945, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  6680, ...,     1,     1,     1],\n",
            "       [    0,   170,  3608, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 29235, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 16215, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 47380, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1106,   215, ...,     8,  1850,     2],\n",
            "       [    0,  3972, 22016, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,   170,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 47302, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 43043, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 33731,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,  4340, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,  6448, ...,     1,     1,     1],\n",
            "       [    0,   387, 35948, ...,     1,     1,     1],\n",
            "       [    0,  1121,   937, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 39231, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   717, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   846,    12, ...,     1,     1,     1],\n",
            "       [    0, 10105,     9, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    28, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 17629, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 47032, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6373, ...,     1,     1,     1],\n",
            "       [    0, 46874,  2088, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 43123, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,  3854,     9,     2],\n",
            "       [    0,   170,    67, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13360, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 13033, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  5709, ...,   230,     6,     2],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,  8269, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709, 25342, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0,  2522,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0, 3684, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  4528,   426, ...,     1,     1,     1],\n",
            "       [    0,   250,   864, ...,     1,     1,     1],\n",
            "       [    0,   170,  2807, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  9437, ...,     1,     1,     1],\n",
            "       [    0, 40450,  9097, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3084, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46498, ...,     1,     1,     1],\n",
            "       [    2,     0, 17105, ...,     1,     1,     1],\n",
            "       [    2,     0, 46444, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,    41, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     5, 14612,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 5320, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9355, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42158, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  6243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 3762,    9, ...,    1,    1,    1],\n",
            "       [   0, 3762,  169, ...,    1,    1,    1],\n",
            "       [   0,  713,   34, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  170, 1455, ...,    1,    1,    1],\n",
            "       [   0,  170,  109, ...,    1,    1,    1],\n",
            "       [   0, 5975,  272, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 42578, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   534, ..., 37357,     5, 23341],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 18377,     5, ...,     1,     1,     1],\n",
            "       [    0, 39936,  1364, ...,     1,     1,     1],\n",
            "       [    0,   133,  4472, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   713,   892, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1966, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 14563, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 30597, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133, 30673, ...,     1,     1,     1],\n",
            "       [    0,   170, 33461, ...,     1,     1,     1],\n",
            "       [    0, 49111,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   243,    16, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42274, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 46692, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   216, ...,     1,     1,     1],\n",
            "       [    0,  9058,  1537, ...,  3854,  6533,     2],\n",
            "       [    0,  2522, 15491, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   510,  8631, ...,     1,     1,     1],\n",
            "       [    0, 45461,  6448, ...,     1,     1,     1],\n",
            "       [    0, 27728,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 46011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 41084, ...,     1,     1,     1],\n",
            "       [    2,     0,   113, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   386, ...,     1,     1,     1],\n",
            "       [    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,   713,  1639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    64, ...,     1,     1,     1],\n",
            "       [    0,   565, 26582, ...,     1,     1,     1],\n",
            "       [    0,  4528,  6448, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1],\n",
            "       [    2,     0,  8532, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,   133,  2731, ...,   141,  1365,     2],\n",
            "       [    0,  5771,   258, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 14246, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       [    0,   170,   492, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 34447, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 48293,  1836, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,  5428, 22098,     2],\n",
            "       [    0,   133,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       [    2,     0, 47744, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12592, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,    12,   170, ...,     1,     1,     1],\n",
            "       [    0,   713,   173, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,  1779,    89, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 40089, 25373, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   448,  7629, ...,     1,     1,     1],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1106,    52, ...,    33,  4163,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 25077, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  2765, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1106,    52, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 45288,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133,   434, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     7,  1807,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0, 9167, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  1197, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   717,  6486, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 33837, 10518, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 18522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,     5, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  7939,  1423, ...,  3278,    63,     2],\n",
            "       [    0,   133,   335, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42489, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 28062, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,   173, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,  1296,   114,     2],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2847,     6, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11321, 20237, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 34647, ...,     1,     1,     1],\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,    52, ...,     1,     1,     1],\n",
            "       [    0, 21461,    11, ...,     1,     1,     1],\n",
            "       [    0,  2765,  4655, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  4442, ...,     1,     1,     1],\n",
            "       [    0, 45408, 19047, ...,     1,     1,     1],\n",
            "       [    0,   713,  1548, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 6179, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0, 2709, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 38386,    10, ...,     1,     1,     1],\n",
            "       [    0,  4528, 15716, ...,     1,     1,     1],\n",
            "       [    0,   448,    36, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1850, ...,     1,     1,     1],\n",
            "       [    0, 35416,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 14484, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  9344, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3813,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,     5, ...,     1,     1,     1],\n",
            "       [    0, 44863,  1319, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,    84, ...,     1,     1,     1],\n",
            "       [    0,   133,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36984, ...,     1,     1,     1],\n",
            "       [    2,     0, 20930, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,   819, 21154,     2],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1106,   854, ...,     1,     1,     1],\n",
            "       [    0,  1213,    67, ...,  4091, 48981,     2],\n",
            "       [    0,   170,   694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   102, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 34788, ...,     1,     1,     1],\n",
            "       [    2,     0, 35660, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0, 1121,  171, ...,  347,   12,    2],\n",
            "       [   0, 1121, 1285, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  713,   16, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170, 9637, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 33020, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42713,     6, ...,     1,     1,     1],\n",
            "       [    0,  4771,  3109, ...,     1,     1,     1],\n",
            "       [    0, 44908,  4843, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35716,    87, ...,    11, 37365,     2],\n",
            "       [    0,   133, 39135, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13755, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  5393, ...,     1,     1,     1],\n",
            "       [    0,  1121,  6477, ...,     1,     1,     1],\n",
            "       [    0,   243,    64, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 30872,   724, ...,     1,     1,     1],\n",
            "       [    0, 12444,   857, ...,     1,     1,     1],\n",
            "       [    0,  9690,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  5448, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1],\n",
            "       [    0, 18377,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,  9157, 16771, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,  6647, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   102, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43253, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 30770,     6, ...,     1,     1,     1],\n",
            "       [    0,   170, 17013, ...,     1,     1,     1],\n",
            "       [    0,   133,  1850, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1],\n",
            "       [    0,   133, 13477, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 23996, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4286, ...,     1,     1,     1],\n",
            "       [    0, 44311,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,   817, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5383, 38416, ...,     1,     1,     1],\n",
            "       [    0,  1779,  3563, ...,     9,   230,     2],\n",
            "       [    0, 13863,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  1109, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2765, 24097, ...,     1,     1,     1],\n",
            "       [    0, 23055,  8738, ...,     1,     1,     1],\n",
            "       [    0,  2522,  5694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 18776, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40103, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 14721,  9179, ...,     1,     1,     1],\n",
            "       [    0,   170,  6053, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0, 13863,  3326, ...,    16,   888,     2],\n",
            "       [    0, 43872,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42124, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0,  250, 4819, ...,    1,    1,    1],\n",
            "       [   0, 2709, 4327, ...,    1,    1,    1],\n",
            "       [   0,  713,  173, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  133, 5849, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170,  311, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 24514, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42489, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,  2655, 20992,     2],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 43195,  7651, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,  1236,    15,     2],\n",
            "       [    0,  1121,  1524, ..., 45371,    15,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44188, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38416, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0, 13863,    51, ...,     1,     1,     1],\n",
            "       [    0,  3762,  1860, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 40846, ...,     1,     1,     1],\n",
            "       [    0,  1620,    52, ...,     1,     1,     1],\n",
            "       [    0,  5771,   171, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 13360,    12, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 27477, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43780, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0, 39936,   775, ...,    52,    33,     2],\n",
            "       ...,\n",
            "       [    0,  1342,  4458, ...,     1,     1,     1],\n",
            "       [    0,  3908,     5, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ..., 19282,     6,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 36949,    41, ...,     1,     1,     1],\n",
            "       [    0,  4688,   419, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,    42, ...,   775,    36,     2],\n",
            "       [    0,  9344,  1938, ...,     1,     1,     1],\n",
            "       [    0,   170,  7015, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 1694, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  5203, ...,     1,     1,     1],\n",
            "       [    0, 39531,  4400, ...,     1,     1,     1],\n",
            "       [    0, 45297,    15, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 15491, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 36542, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0,  3908,  2284, ...,   922,  4791,     2],\n",
            "       ...,\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 30597, 10244, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 48313, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   173, ...,     1,     1,     1],\n",
            "       [    0, 40566,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  1548, ...,     1,     1,     1],\n",
            "       [    0, 23271,     9, ..., 42472, 26070,     2],\n",
            "       [    0, 48454,    12, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41933, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   170,   240, ...,     1,     1,     1],\n",
            "       [    0, 43714,    40, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0,   133,   485, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  9685, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45336, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   486, ...,     1,     1,     1],\n",
            "       [    0,   133, 28894, ...,     1,     1,     1],\n",
            "       [    0, 38386,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   970,    16, ...,  3364,     5,     2],\n",
            "       [    0,   713, 12360, ...,     1,     1,     1],\n",
            "       [    0,  1121,   485, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  1034, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0, 29182,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0, 18377,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,   170,  2883, ...,     1,     1,     1],\n",
            "       [    0,   170,  2639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  5848, ...,    36, 13424,     2],\n",
            "       [    0, 44863,    31, ...,     1,     1,     1],\n",
            "       [    0, 48684,   680, ..., 20145,  4007,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 49360,    11, ...,  6068,   600,     2],\n",
            "       [    0, 45942,  6448, ...,     1,     1,     1],\n",
            "       [    0, 30383, 26713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 41542,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1285, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45356, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40884, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1213,  1157, ..., 20910,    73,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0,  1779,  1058, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 39972,    52, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 28588, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     1,     1,     1],\n",
            "       [    0, 20319,  2408, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   800, ...,     1,     1,     1],\n",
            "       [    0,  2709,    55, ...,     1,     1,     1],\n",
            "       [    0, 10653,   428, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 243, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    97, ...,     1,     1,     1],\n",
            "       [    0,  4528, 41885, ...,     1,     1,     1],\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,   170,  1455, ...,     1,     1,     1],\n",
            "       [    0,  1620,    41, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10127, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   243, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250,  1353, ...,     1,     1,     1],\n",
            "       [    0,   713,  3315, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  1989, ..., 14612, 26070,     2],\n",
            "       [    0,  3972,  1100, ...,     1,     1,     1],\n",
            "       [    0,  5320, 10074, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44466, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44863, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   133, 32809, ...,     1,     1,     1],\n",
            "       [    0,     6,  3023, ...,  1558, 15421,     2],\n",
            "       ...,\n",
            "       [    0, 10653,   428, ...,     1,     1,     1],\n",
            "       [    0, 20861, 44871, ...,     1,     1,     1],\n",
            "       [    0,   713,   839, ...,     8,    63,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   347, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 48455, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,     1,     1,     1],\n",
            "       [    0, 21438,   520, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522, 11909, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,   133, 16681, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,   936, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  4528,  8369, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 27331,   937, ...,     1,     1,     1],\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 21680, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  6209,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 15393, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0, 20086, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3972,     5, ...,     1,     1,     1],\n",
            "       [    0,   170, 24934, ...,     1,     1,     1],\n",
            "       [    0,  2522, 39030, ...,  3907,     4,     2],\n",
            "       ...,\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       [    0,   170,   892, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     6,   549,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   176, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 47515, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45566, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   574,  3439, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   163, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,    43,   396,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133, 15306, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ..., 12612,   534,     2],\n",
            "       [    0,  3972,  1306, ...,     1,     1,     1],\n",
            "       [    0, 19847,  1239, ...,  6315, 36173,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4993, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 46159, 43141, ...,     1,     1,     1],\n",
            "       [    0, 10777,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121, 14117, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 13863,  1337, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42200,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 565, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 28084,     7, ...,     1,     1,     1],\n",
            "       [    0,  1121,   144, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 38416,    29, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,  1121,  5709, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 25382, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,   511, 22772,     2],\n",
            "       [    0,  3762,     9, ...,     1,     1,     1],\n",
            "       [    0,   133,   986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   172, ...,     1,     1,     1],\n",
            "       [    0,  3972, 33942, ...,   892,  2939,     2],\n",
            "       [    0, 45875,     6, ...,    31,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 1121, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  717, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 15393, ...,    11,   130,     2],\n",
            "       [    0, 20867,  7316, ...,     1,     1,     1],\n",
            "       [    0,   713,  5665, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  9058, 24454, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44426, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771, 10364, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,   209, ...,     1,     1,     1],\n",
            "       [    0,   133,  8611, ...,     1,     1,     1],\n",
            "       [    0,  3972, 19893, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44908, ...,     1,     1,     1],\n",
            "       [    2,     0, 34002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 19186, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23803,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133,   538, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0,  5771,   144, ...,     1,     1,     1],\n",
            "       [    0,  2765,  2623, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 19163, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  3034, ...,     1,     1,     1],\n",
            "       [    0,  1121,  2171, ...,     1,     1,     1],\n",
            "       [    0,  3972,     5, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0, 17425, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   448, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   387,   293, ...,    16,   505,     2],\n",
            "       [    0, 21518,  1537, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 45628, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2409,   114, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288,  8150, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,    45,   946,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,   211, ...,   683,    36,     2],\n",
            "       [    0,   250,   194, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1620,   251, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,   740,     6,     2],\n",
            "       [    0,  1121,   103, ...,   255,     8,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 28062, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17312, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38741, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 35166, 37700, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,     1,     1,     1],\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 24989,  7373, ...,     1,     1,     1],\n",
            "       [    0,  9157, 37794, ...,     1,     1,     1],\n",
            "       [    0,   717,  4182, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10928, ...,     1,     1,     1],\n",
            "       [    2,     0, 15622, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4897, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   387,   293, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771,   419, ...,   468,   321,     2],\n",
            "       ...,\n",
            "       [    0,   530,   495, ...,     1,     1,     1],\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,  2522, 40150, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 9685, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4554, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11913, 26739, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35490,     5, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       [    0,   713,  5044, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   338, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,  1365, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,  1121,   171, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 48812,  2577, ...,    14, 20070,     2],\n",
            "       [    0,  5771,   608, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,    14,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 10516, ...,     1,     1,     1],\n",
            "       [    0,   713,  1421, ...,   163,  2688,     2],\n",
            "       [    0,  3762,    16, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,   133,  2270, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12444, ...,     1,     1,     1],\n",
            "       [    2,     0, 22011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250, 17309, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9322, ...,     1,     1,     1],\n",
            "       [    0,  3762,   169, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288, 15380, ...,     1,     1,     1],\n",
            "       [    0,   133,  7626, ...,     1,     1,     1],\n",
            "       [    0,   170,   304, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 26412, ...,     1,     1,     1],\n",
            "       [    2,     0, 46101, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,    10, ...,   204,     6,     2],\n",
            "       [    0,  9690,  1202, ...,     1,     1,     1],\n",
            "       [    0,  7605,   209, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2571,  6018, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   250, 31809, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23295, 37465, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3762, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1620,    10, ...,     1,     1,     1],\n",
            "       [    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0,   170,    40, ..., 13956,  1916,     2],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 19192,    52, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(7, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  1000, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "96a37571-a85b-46d0-822c-15337ad2a4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_model"
      ],
      "metadata": {
        "id": "dyGROt7TwXn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "fec0f7cd-d17d-4c56-fb64-c523f8dc25b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n",
            "81/81 [==============================] - 3557s 44s/step - loss: 3.8539 - val_loss: 3.3430 - rouge1: 38.3698 - rouge2: 10.1688 - rougeL: 22.2593 - rougeLsum: 31.8182 - gen_len: 88.4506\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3040s 38s/step - loss: 3.4917 - val_loss: 3.2807 - rouge1: 40.0500 - rouge2: 10.8711 - rougeL: 22.9560 - rougeLsum: 33.0063 - gen_len: 82.0000\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3153s 39s/step - loss: 3.3288 - val_loss: 3.2417 - rouge1: 39.2427 - rouge2: 10.4373 - rougeL: 22.8725 - rougeLsum: 32.7607 - gen_len: 82.4815\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3104s 39s/step - loss: 3.1818 - val_loss: 3.2276 - rouge1: 40.0325 - rouge2: 11.1820 - rougeL: 23.2123 - rougeLsum: 33.2422 - gen_len: 84.0309\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3291s 41s/step - loss: 3.0478 - val_loss: 3.2148 - rouge1: 40.4019 - rouge2: 10.9217 - rougeL: 23.2547 - rougeLsum: 33.4422 - gen_len: 85.5432\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3194s 40s/step - loss: 2.9362 - val_loss: 3.2262 - rouge1: 39.8779 - rouge2: 10.3569 - rougeL: 22.9050 - rougeLsum: 32.9253 - gen_len: 81.9753\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3124s 39s/step - loss: 2.8306 - val_loss: 3.2327 - rouge1: 40.3397 - rouge2: 10.9453 - rougeL: 23.1704 - rougeLsum: 33.6539 - gen_len: 83.5309\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3146s 39s/step - loss: 2.7277 - val_loss: 3.2288 - rouge1: 39.9447 - rouge2: 10.5412 - rougeL: 22.8769 - rougeLsum: 33.1718 - gen_len: 85.0062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "f09db19d-85fd-44b8-d76e-3ee5168e9405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c25434ec-c808-43da-b3df-579bfce352b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "699df823-a6bf-4496-f7a2-78909e82cba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {'sampling-norep-v0' : 'sampling-norep-v0/',\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              #  'greedy-norep-v3' : 'greedy-norep-v3/',\n",
        "               'sampling-norep-v3' : 'sampling-norep-v3'}\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Tjj2cjL2mE",
        "outputId": "184d644e-2892-4f36-e503-667824846e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v0/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2,\n",
            "  \"max_length\": 150,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.2,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'\n",
        "save_path = save_paths[name]\n",
        "\n",
        "with open(save_path + '/training_history.json', 'r') as file:\n",
        "    loaded_history = json.load(file)\n",
        "\n",
        "H = History()\n",
        "H.history = loaded_history\n",
        "\n",
        "\n",
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/figure.png')"
      ],
      "metadata": {
        "id": "CEUKdl4cdUPx",
        "outputId": "dd6c46b3-440c-4144-9a18-40cbf4278f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'"
      ],
      "metadata": {
        "id": "AQseeydBMTlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:5],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][6:8],\n",
        "                                        generation_config=model.generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "DXuVco4oyC_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j6DcwmJShR",
        "outputId": "f366e627-2a4f-4cae-9161-273836d3ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet framework for in-and out-of-distribution classification. This paper proposes a variational approach to solve the uncertainty estimation problem in deep neural networks by considering the label-level distribution of image input and output labels. The authors propose a new uncertainty metric for deep neural network classification that is more robust than existing uncertainty measures. This article proposes a novel variational method for solving uncertainty estimation on deep neural nets.',\n",
              " 'An unsupervised method for analyzing the contribution of individual neurons to NMT models. This paper proposes a novel approach to interpret language pairs using neural networks, and proposes a new method for measuring the contributions of each neuron to the model. The authors propose a novel method for learning languages from neural networks that can be used to control the translation performance of language pairs.',\n",
              " 'A deep diagonal-circulant ReLU network that can be decomposed into products of diagonal and circulant matrices This paper proposes to replace the weight matrix of a fully connected layer with a new type of matrix. The authors propose a method for building deep ReLU networks based on a combination of diagonal/circular matrices with low rank approximators in order to improve performance.',\n",
              " 'Explicit cognitive theory or analogy-like computation in neural networks This paper proposes a novel approach to solving complex examples of visual and symbolic representations. The authors propose an approach to the problem of visual analogy by proposing a new model that learns to contrast abstract relational structures with visual representations. This work proposes a method for learning to compare different representations of objects, which can be used to solve complex analogical problems such as visual analogy.',\n",
              " 'A novel concept annotation task for medical time series data. This paper proposes a novel method of predicting and localizing medical concepts by modeling the medical context data as input. The authors propose a novel approach to the problem of identifying medical concepts in medical time-series data, which can be used to predict and localize medical concepts. This article introduces a novel framework for understanding medical concepts using medical context information.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMr_neO6pJNK",
        "outputId": "25d85c44-b7e6-4820-927c-54279cb4c854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The study proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This study proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The article investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TReOxk1fk1cx",
        "outputId": "eea11b79-2cc4-44e3-9121-a817e0d65fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The article proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This article proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = False\n",
        "model.generation_config.temperature = 1\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "                                          tokenizer.convert_tokens_to_ids('ƒ†propose'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†proposes'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "\n",
        "model.generation_config.num_beam_groups = 4\n",
        "model.generation_config.diversity_penalty = 0.7\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.3\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r4mAa8_-ql",
        "outputId": "a4abf904-55d4-42b6-9cbe-7b5095a3ea61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"diversity_penalty\": 0.7,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.3,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    15393,\n",
            "    21037,\n",
            "    32687\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('they proposes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EysouFNL26dN",
        "outputId": "decc56fd-e82f-48df-89e2-9b621383fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['they', 'ƒ†proposes']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisXx8nuVDQ4",
        "outputId": "f70b8744-cbe0-4319-fc58-4bc80c0ff137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: 0/202\n",
            "[INFO]: 10/202\n",
            "[INFO]: 20/202\n",
            "[INFO]: 30/202\n",
            "[INFO]: 40/202\n",
            "[INFO]: 50/202\n",
            "[INFO]: 60/202\n",
            "[INFO]: 70/202\n",
            "[INFO]: 80/202\n",
            "[INFO]: 90/202\n",
            "[INFO]: 100/202\n",
            "[INFO]: 110/202\n",
            "[INFO]: 120/202\n",
            "[INFO]: 130/202\n",
            "[INFO]: 140/202\n",
            "[INFO]: 150/202\n",
            "[INFO]: 160/202\n",
            "[INFO]: 170/202\n",
            "[INFO]: 180/202\n",
            "[INFO]: 190/202\n",
            "[INFO]: 200/202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFgiii-sbTK4",
        "outputId": "41684db4-b656-4232-d3b5-51f1c72b8369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    170\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"early_stopping\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"repetition_penalty\": 1.8,\n",
              "  \"suppress_tokens\": [\n",
              "    1698,\n",
              "    32687\n",
              "  ]\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bNZCUqSvN7NV",
        "outputId": "bc3472f2-1429-4206-dcf8-b0d504c53db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/greedy-norep-v5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "beb7244c-9a96-46aa-e5a8-66046cfd17bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       1.420792\n",
              "std       18.729447\n",
              "min      -39.000000\n",
              "25%      -12.000000\n",
              "50%        0.500000\n",
              "75%       14.000000\n",
              "max       53.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 6\n",
        "print('ORIGINAL:' + tokenized_data['test']['target'][i])\n",
        "print('FINE TUNED MODEL:' + tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[i])\n",
        "print('PRETRAINED MODEL:' + tokenizer.batch_decode(pretrained_generated_ids, skip_special_tokens=True)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzU40R-ALuzq",
        "outputId": "7e6ad700-beac-4a0d-f100-174353d2d63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL:OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work\n",
            "FINE TUNED MODEL:Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. We show that Outlier Exposure can improve calibration performance in this realistic setting.\n",
            "PRETRAINED MODEL:However, when there is a distribution mismatch, deep neural network classifiers tend to give\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 28\n",
        "print(summaries.iloc[m, 0])\n",
        "print(summaries.iloc[m, 1])"
      ],
      "metadata": {
        "id": "lcWyZXvGLKcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1428815a-8c86-41c0-b4c1-7c39c492acdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space.  This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.\n",
            "We extend the reinforcement learning paradigm to a d-dimensional hypercube and show that quantile regression is capable of training orders of magnitudes faster in high dimensional metric spaces. This paper proposes a method to train a deep neural network to approximate the quantile function of the optimal action distribution. The authors propose a new reinforcement learning algorithm to train convolutional neural networks with quantile functions, showing that it can be used to train orders of magnitude faster on vector rewards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "-_h7Z8KBp09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-base', errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "MwmAPTxGp28R",
        "outputId": "810333e4-4f20-47db-a57d-a3d634679149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "a947a76d796343fb8010d27f818fbf2f",
            "22ce83fed2d444919ba40dfb5526d90b",
            "179503fdfa3f49e6a4e8f998af5d573d",
            "c5198b7842714377acc4efeb347fcf6e",
            "b952ae1ae73f4892aa7d8eef1904e9fe",
            "6394d8d637444b4490dadb9c0dcaf830",
            "b07b49a5db434eff838e6dcec3cfc747",
            "ee1e4766dcde41d7ba690f8fdca722fc",
            "bbd28d54ee2e43f5a2e710515a782406",
            "44799cbdb0c248f8b14434970ccb551b",
            "8214d98138f041079c83d5a9507178b6",
            "f25b823e67c84984b571042ef8376c7e",
            "24d6aa12d60342a19399cf825b332e1c",
            "8962a4e16c4b4fbb846656f35a70a3a9",
            "7fe00692904040c38e5832f11b8975b9",
            "ee17540ad8714e9b903d9f9a181af961",
            "4d14a957fe724702a82ada900361de55",
            "22880aebf50e4f60b2e18fd8b00e88d3",
            "9a69a0d3006141c8a7a7f53c262294cb",
            "ee8a801d8d03459fbc058b37cb528f21",
            "e2001fdf69d5424cab0e321a1e2760ad",
            "5724c8c3c435472da70b8bf114aa6829",
            "853aa86e41dc47d4ae9e4622941f1af3",
            "83579f36b9c44813b9533c37046b2d93",
            "cd35a1f388034fe9929318cebca0e1aa",
            "3adfde6716054b4eb3b968e10bb94264",
            "ae25b03da35e465badd05b18e33c5a89",
            "c7393147bf8f497fb965e3f356a83b70",
            "2489ce6eece4479ea965ddc4a59e95bc",
            "0f3fd7bc5cb042debc5576e5d33750d6",
            "18dfab3436384c5599b39fb00654deaa",
            "53557f686a0046caa7f97ce2da392007",
            "5c8d845574a349cd9133d93d0c5c354b"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a947a76d796343fb8010d27f818fbf2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f25b823e67c84984b571042ef8376c7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "853aa86e41dc47d4ae9e4622941f1af3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google-t5/t5-base')\n",
        "\n",
        "# model.config.task_specific_params['summarization']['max_length'] = 150\n",
        "# model.config.task_specific_params['summarization']['min_length'] = 80\n",
        "\n",
        "model.generation_config.max_length = 100\n",
        "model.generation_config.min_length = 80\n",
        "model.generation_config.num_beams = 4\n",
        "model.generation_config.length_penalty = 2.0\n",
        "\n",
        "\n",
        "# model.generation_config.do_sample = True\n",
        "# model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "# model.generation_config.suppress_tokens = [\n",
        "#     # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "#                                           # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "name_model = 'greedy-norep-v0/'\n",
        "\n",
        "print(model.config)\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "_mad6Hc1ttRX",
        "outputId": "e90f59c5-d35a-4044-a95f-c471fbf4fb4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"google-t5/t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 150,\n",
            "      \"min_length\": 80,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 80,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "6XdKgWLNwgRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "6PGwmRZ2yS78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yvPZ6vubyYl1",
        "outputId": "85b1b4c0-caa5-4d48-cb92-af56d42ffd73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  24674304  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  109628544 \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  137949312 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222903552 (850.31 MB)\n",
            "Trainable params: 222903552 (850.31 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + name_model"
      ],
      "metadata": {
        "id": "HceVCymny0eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "p_wTT5KMyi86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "VeSspMbVyp_q",
        "outputId": "cbbc047d-46fe-4cfb-a8f3-05e52e245838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7e4adf964a60> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7e4adf964a60> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - 1894s 23s/step - loss: 4.3853 - val_loss: 2.4166 - rouge1: 30.9318 - rouge2: 5.6016 - rougeL: 17.2004 - rougeLsum: 25.4458 - gen_len: 109.1358\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 1833s 23s/step - loss: 2.5470 - val_loss: 2.2051 - rouge1: 31.2962 - rouge2: 5.7309 - rougeL: 17.2384 - rougeLsum: 25.5281 - gen_len: 109.6420\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 1896s 24s/step - loss: 2.2275 - val_loss: 2.1377 - rouge1: 31.6351 - rouge2: 5.8724 - rougeL: 17.5436 - rougeLsum: 25.7174 - gen_len: 109.2099\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 2310s 29s/step - loss: 2.0358 - val_loss: 2.0941 - rouge1: 32.6300 - rouge2: 6.3857 - rougeL: 17.8555 - rougeLsum: 26.5149 - gen_len: 109.4259\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 2307s 29s/step - loss: 1.9544 - val_loss: 2.0609 - rouge1: 33.6372 - rouge2: 7.1342 - rougeL: 18.9671 - rougeLsum: 27.6714 - gen_len: 115.8457\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 2317s 29s/step - loss: 1.9086 - val_loss: 2.0342 - rouge1: 34.7117 - rouge2: 7.4484 - rougeL: 19.5791 - rougeLsum: 28.7122 - gen_len: 119.3395\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 2330s 29s/step - loss: 1.8713 - val_loss: 2.0127 - rouge1: 35.0754 - rouge2: 7.7405 - rougeL: 19.7623 - rougeLsum: 29.0526 - gen_len: 117.2593\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 2333s 29s/step - loss: 1.8406 - val_loss: 1.9935 - rouge1: 35.2992 - rouge2: 7.8483 - rougeL: 19.9249 - rougeLsum: 29.1065 - gen_len: 120.1852\n",
            "Epoch 9/10\n",
            "81/81 [==============================] - 2318s 29s/step - loss: 1.8198 - val_loss: 1.9756 - rouge1: 36.2340 - rouge2: 8.4452 - rougeL: 20.2676 - rougeLsum: 29.6919 - gen_len: 125.4198\n",
            "Epoch 10/10\n",
            "81/81 [==============================] - 2300s 29s/step - loss: 1.7997 - val_loss: 1.9610 - rouge1: 36.4780 - rouge2: 8.4900 - rougeL: 20.4889 - rougeLsum: 30.0488 - gen_len: 123.4815\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/spiece.model',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "id": "VMgzhMiysJ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config"
      ],
      "metadata": {
        "id": "-dD2Lsh95H9a",
        "outputId": "bbc87417-43d5-402e-86b0-e85953026ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    1326\n",
              "  ],\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"repetition_penalty\": 1.8\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:16],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "KqOLYlN5AsTr",
        "outputId": "46311e34-810b-4f77-afb0-c7de639ce95b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We propose a variational Dirichlet algorithm for deep neural network classification problem and define a higher-order uncertainty measure. This paper proposes a new method to solve the over-concentration problem with image input as x and output label as y. The authors propose an algorithm that can be applied to deep neural networks to better deploy them into real-world applications. In this paper, we examine the label-wise distribution P(y) as a K-dimensional variable z generated from a lower-order distribution P (episdemic) in-and-out-distribution entropy based on a robust and accurate data.',\n",
              " 'In this work, we propose unsupervised methods for analyzing the contribution of individual neurons to NMT models. This paper proposes a new method to map neurons between pairs of trained NMT model pairs using multiple vectors and canonical correlation analysis (SVCCA). The authors propose a novel approach to control translation in predictable ways by modifying the activation of the tense neurons from the previous section TAB2. We propose an alternative way to change the translation from past to present by altering its activation by changing their activation as a linguistically interpretable neurons to solve the problem of a single neural networks to a multi-dimensional representations.',\n",
              " 'We propose a new model to replace large weight matrices from fully connected layers with more compact circulant layers. This study proposes a way to replace the weight matrix of a fully connected layer by a circulant layer using a circular layer that can be used to approximate the Johson-Lindenstrauss transform, often used to perform dimensionality reduction. This paper proposes an approach to compressing the network by reducing the memory, at the level of individual weights, or by using sparse representations of bounded width and a dense ReLU (BID14).',\n",
              " 'a model trained with random candidates performs very poorly on the more challenging contrasting test questions, which suggests that models trained in the normal regime overfit to a strategy that bears no resemblance to human-like analogical reasoning. This paper proposes a method of comparing abstract relational structures with a different domain (see FIG6 vs 93% for LABC). The authors propose a way to use an analogy-like computation to simulate a spatial representation of semantically plausible and perceptual neural network-based model is based on the same domain as the correct answer.',\n",
              " 'To address this issue, we introduce the concept annotation task as the problem of predicting and localizing medical concepts by using the time series data as input. This paper proposes a method to predict and localize medical concepts in a time series setting. The authors propose a new approach to generating text summaries such as trends from time series datasets. This work proposes an approach for conceptualizing medical time series models that can be used to identify medical concepts. This article proposes the task of generating medical concepts with the medical time-series prediction, which is a process of identifying medical concepts on the topic of concept annotation for the idea of a concept.',\n",
              " 'We propose a multi-way encoding that obstructs the adversarial gradient search. This paper proposes a higher dimensional multi-directional multiway approach for both source and target models. The authors propose an attack for the recent model watermarking algorithm of BID24, which trains a model to misclassify (a) from the green ground-truth class to the blue class, or vice versa.',\n",
              " 'In this paper, we introduce a network that can detect patterns regardless of how they are rotated over the sphere. This paper proposes a way to define the spherical correlation in the higher layers of a CNN BID4 (S 2 -CNN). The authors propose a method for detecting a series of rotations on a single layer of an equivariant and a three-dimensional manifold called SO(3) 2. This paper explores the importance of reversing a a different, more than a third of the work by using a group of moves.',\n",
              " 'In learning-based systems, the analysis tools currently available to researchers focus on the functioning of each agent, and provide insights into the inner workings of multi-agent systems. This class of methods focuses on the behavior of multiple agents, and provides insight into the relational and social structures present in the data. The study proposes a new approach to understanding the behavioral dynamics of multiagent system models that can be leveraged to predict the forward dynamics of different agents as a whole. This paper proposes an analysis tool that will help researchers understand the problem of how complex and complex interactions between the two are more complex. It is difficult to learn.',\n",
              " 'In this paper, we introduce a transparent middleware layer for neural network acceleration. This paper proposes a system for deep learning frameworks that can be used to optimize and train neural networks. The authors propose an optimization cycle for neural networks that supports all hardware platforms and deep learning libraries. This article proposes the use of specialized hardware and its corresponding proprietary libraries to optimize neural networks by combining device-specific libraries and custom optimizations. We present the current development status and some preliminary but encouraging results: on a standard x86 server, we have developed a new algorithm for neural networking. It is a multi-layered middleware for a wide variety of neural networks for deep inference.',\n",
              " 'In the literature on artificial dialogue agents, a distinction is often made between \"goal-oriented\" dialogue and \"chit-chat,\" where an agent should imitate human small talk. We compare agents that have been trained to imitate humans given a goal (an \"inverse model\") to two different RL approaches: optimizing actions with latent discrete variables (topics), or using rewarding actions sampled from the top-K outputs. This paper proposes a large set of human-written game locations, characters, and objects.',\n",
              " 'We train charCNN with the implementation in BID19 using traditional keyboards for our languages. The results are shown in TAB6. This paper proposes a more robust model for noisy texts to address typos and noise. The authors propose a way to improve the performance of charcNN by training on noisy texts, which can be used as a keyboard Typo (noisenoide). In this paper, we introduce a new language-based algorithm that is more robust to different kinds of noise: \"An unintelligible translation for a German version of a single word or char CNN.',\n",
              " 'We use iterative pruning as a proxy for the speed at which a network learns, but not after re-initializing some layers and retraining them. The strategy 1 maintains higher validation accuracy and faster early-stopping times to smaller network sizes. This article proposes a method for iterating a pruned model from scratch that improves the learning rate of training a trained network with a small capacity. In this article, we explore the ways in which an early stopping criterion is a tool to find a way to train a more than a \"strategic training is better to retain the weights.',\n",
              " 'DISPLAYFORM2 3 proposes a regularization term that penalizes the deviation of the gradient norm of the critic function (as based on its input) from one. This paper proposes an alternative to reducing the optimization problem given by GAN training BID8. The aim is to minimize the Wasserstein-1 distance, also known as Earth-Mover (EM) distance, where the maximum is taken over the set of all 1-Lipschitz functions Lip 1 and Lip 1.',\n",
              " 'Inan et al. proposes a weight-tying mechanism that enables learning input and output representations jointly while significantly reducing the number of network parameters. This paper proposes an adaptive embedding method for learning input representations from different subsets of the input layer using sparse and dense connections. The authors propose a tool to improve gradient flow by promoting feature reuse, and improving gradient flow (Section 3.3), which provides a way to learn deep representations efficiently from each input element of a model is a multi-layered, a group transformation algorithm that can be used to create a new m-dimensional vector√™ i.e.',\n",
              " 'The first relation (FDR1) can be used to check stationarity of the probability distribution that governs the model parameters at sufficiently long time. This paper proposes an analogous fluctuation-dissipation relations that quantitatively link the noise in mini-batched data to the observable evolution of the model performance and facilitate the learning process. This article proposes a two-point noise matrix C through DISPLAYFORM2 and, more generally, higher-point sound tensors.',\n",
              " 'This paper proposes a quantum circuit analogue of the neural network U QN N that takes in a given set of weights (encoded within qubits as above) and an empty register, and outputs on the register the corresponding accuracy according to the chosen neural network. Step 2: We then build a gate-based form of quantum computation rather than a qRAM for a general dataset. The study proposes using a different, more general form of binary classification as opposed to the quantum annealing method is used to construct a non-no qBNN, which can be used to generate a large number of quebits.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2BXUHuhyWyyh",
        "outputId": "ec7115b9-52d5-49bb-aa6a-e433dce70cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. the watermark must be secure against adversarial attacks and leave no tangible footprints in the target DNN. a retraining procedure resembles 'adversarial training' BID16. a retraining procedure resembles 'adversarial training' BID16.\",\n",
              " '',\n",
              " '',\n",
              " 'we hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the BatchNorm layer. we consider a standard classification task for data, having underlying distribution denoted as D, over the pair of examples x  R n and corresponding true labels y  1, 2,..., k where k represents different labels.',\n",
              " 'a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive.',\n",
              " '',\n",
              " 'DPQ-VQ is a more evenly distributed code utilization, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:8]"
      ],
      "metadata": {
        "id": "xNqaRwbOzUVD",
        "outputId": "8f0c5a88-4ccc-461f-f339-335d52d8e792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This paper presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The paper proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This paper proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The article addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.',\n",
              " 'We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This paper proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines',\n",
              " 'We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts This article focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up',\n",
              " \"Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations\"]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.do_lower_case"
      ],
      "metadata": {
        "id": "6QMQKmjqFkHL",
        "outputId": "4ed990f5-4663-46a2-cebf-d851ddb2d147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'T5Config' object has no attribute 'do_lower_case'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1d79c395fff9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'T5Config' object has no attribute 'do_lower_case'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('hello')\n"
      ],
      "metadata": {
        "id": "-MTiOKDZGJRk",
        "outputId": "b0bfdba3-3d3b-4840-ae0d-6c9550086d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21820, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('Hello')"
      ],
      "metadata": {
        "id": "Glz2s6NaHWTa",
        "outputId": "9847b24e-40cd-4d82-8889-69b9d5219511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8774, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmf68b93HqIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIOm2Rg8zdNZGai9OcSa9z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9100de41ec6946c495c032b05d90913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40d035dada3347e2963c183d035d445c",
              "IPY_MODEL_ecd31a14240f408b82b5224d97694ab7",
              "IPY_MODEL_7afeab237ced4f01b70361e2a3ce7552",
              "IPY_MODEL_5b7408314641483980402784564e62e6",
              "IPY_MODEL_c4a0eef84a5e4061884e43d2a09da02a"
            ],
            "layout": "IPY_MODEL_b114a3fe857949408305dde0c9436974"
          }
        },
        "40d035dada3347e2963c183d035d445c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_52801cc2a7a646109c4a7141ef768900",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ecd31a14240f408b82b5224d97694ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_db74bb5ffc4f4900b17aaf842a618019",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6deef8837ace4d94b5b803666ff4aa5a",
            "value": ""
          }
        },
        "7afeab237ced4f01b70361e2a3ce7552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2e00c125b4d942368e702682002f94ff",
            "style": "IPY_MODEL_571b4b08342141c798092b6b23ab410a",
            "value": true
          }
        },
        "5b7408314641483980402784564e62e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ca46452e63a34d99865718ab86726748",
            "style": "IPY_MODEL_493ec2745ddf4bf2bdce528219a1b309",
            "tooltip": ""
          }
        },
        "c4a0eef84a5e4061884e43d2a09da02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c8673d4bac474d9c5799e3ee02bf6b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f44df68731c14e24914d1c8816c8edce",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b114a3fe857949408305dde0c9436974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f2ddbf8c4aaf42fba6f967bf32d45ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52801cc2a7a646109c4a7141ef768900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db74bb5ffc4f4900b17aaf842a618019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6deef8837ace4d94b5b803666ff4aa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e00c125b4d942368e702682002f94ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571b4b08342141c798092b6b23ab410a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca46452e63a34d99865718ab86726748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "493ec2745ddf4bf2bdce528219a1b309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d3c8673d4bac474d9c5799e3ee02bf6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44df68731c14e24914d1c8816c8edce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4436655f7773484fafd1cff33ef78ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0c8fbe4c9cc47ef9de30e6f1d7b8a00",
              "IPY_MODEL_5ddedeae3c37411f809914dd40626747",
              "IPY_MODEL_2b9d07ace2bf4dd6864c4e7f70d8d184"
            ],
            "layout": "IPY_MODEL_365f866542054b8096f0425b740e3657"
          }
        },
        "d0c8fbe4c9cc47ef9de30e6f1d7b8a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88891a0283ed46ec815622f98ec2e88d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f0ae46782ed54cda804a8a992bff0092",
            "value": "vocab.json:‚Äá100%"
          }
        },
        "5ddedeae3c37411f809914dd40626747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9125306d57674cc79e44cbbc6fa73493",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7351b89d38c841699116d5aa21d2b8e6",
            "value": 898823
          }
        },
        "2b9d07ace2bf4dd6864c4e7f70d8d184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df3401496304e62b772d6b9dfa54491",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_34a039133bef4ba297da41e8e252412f",
            "value": "‚Äá899k/899k‚Äá[00:00&lt;00:00,‚Äá9.01MB/s]"
          }
        },
        "365f866542054b8096f0425b740e3657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88891a0283ed46ec815622f98ec2e88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ae46782ed54cda804a8a992bff0092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9125306d57674cc79e44cbbc6fa73493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7351b89d38c841699116d5aa21d2b8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8df3401496304e62b772d6b9dfa54491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a039133bef4ba297da41e8e252412f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae83c009d6af44cbb7235c317c8959d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4774ef2229d4eb0ab8335fecf443bfa",
              "IPY_MODEL_c2a1348eca4f40ea8f9976036ab98d2c",
              "IPY_MODEL_a2e1287ad5da41138aab3041744c2eb4"
            ],
            "layout": "IPY_MODEL_1c129f9f527e4511ab67540e3cf838e4"
          }
        },
        "e4774ef2229d4eb0ab8335fecf443bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ca1257e20946749a7a6b2dc2ea7194",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8749f001e7a24926bffc01a5491e1e8a",
            "value": "merges.txt:‚Äá100%"
          }
        },
        "c2a1348eca4f40ea8f9976036ab98d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e97b25494a744f7bfbf01198bb4eab0",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d628a6203104fa696604ec4609eeffc",
            "value": 456318
          }
        },
        "a2e1287ad5da41138aab3041744c2eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba8eea1a6051482394bcd1176906e8f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5429d5d2ce5543bfaf691acd3abae273",
            "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá6.26MB/s]"
          }
        },
        "1c129f9f527e4511ab67540e3cf838e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ca1257e20946749a7a6b2dc2ea7194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8749f001e7a24926bffc01a5491e1e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e97b25494a744f7bfbf01198bb4eab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d628a6203104fa696604ec4609eeffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba8eea1a6051482394bcd1176906e8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5429d5d2ce5543bfaf691acd3abae273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66beca881e744a0d95d12bbcb071f9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e32b30837b684ea1a9369c97610084b5",
              "IPY_MODEL_9174c28f178e4720b081af53ace7648a",
              "IPY_MODEL_148c7002ce6a4dc491ee90bd3fd53e5f"
            ],
            "layout": "IPY_MODEL_a98ba1580ced44dfaf4573d89e4f20cb"
          }
        },
        "e32b30837b684ea1a9369c97610084b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f277c50799e447998d6091f3d671d8ef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_77a6a099c602449cab53c4331133baf9",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "9174c28f178e4720b081af53ace7648a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bdc6e8e939b4f04ace92239a8d59dc2",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a20f3e6076ee4028bae0076528f657c4",
            "value": 1355863
          }
        },
        "148c7002ce6a4dc491ee90bd3fd53e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a809ae1cbf4941e28a5ee6b8990f0d47",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_582775feaacc4557a9052e37fa54d1f1",
            "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá33.7MB/s]"
          }
        },
        "a98ba1580ced44dfaf4573d89e4f20cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f277c50799e447998d6091f3d671d8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a6a099c602449cab53c4331133baf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bdc6e8e939b4f04ace92239a8d59dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a20f3e6076ee4028bae0076528f657c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a809ae1cbf4941e28a5ee6b8990f0d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582775feaacc4557a9052e37fa54d1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79a88598be7e47e2b3df8f13fd34d4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb47d7607757459396206b5c29bd7584",
              "IPY_MODEL_fb62bc7f31ca4f3799a11d035c75a87d",
              "IPY_MODEL_7d7598e022994663a060abfaeebf8f50"
            ],
            "layout": "IPY_MODEL_8389228f026241feb945dd390466b2ba"
          }
        },
        "cb47d7607757459396206b5c29bd7584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ca2f7b5b2e424aa24ae42a85478c77",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0d0586338ea242ac885a4167a824cfa3",
            "value": "config.json:‚Äá100%"
          }
        },
        "fb62bc7f31ca4f3799a11d035c75a87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_387c04fc5fbb421b87f9eaf0cc150e8b",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aa0091c91764f6ca2e21d65c45d18eb",
            "value": 1716
          }
        },
        "7d7598e022994663a060abfaeebf8f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bcfa13acbad4f1e9fb284a68f61d9a9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_77f1d6f94bab409cbfadfd13c7597f3a",
            "value": "‚Äá1.72k/1.72k‚Äá[00:00&lt;00:00,‚Äá135kB/s]"
          }
        },
        "8389228f026241feb945dd390466b2ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ca2f7b5b2e424aa24ae42a85478c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0586338ea242ac885a4167a824cfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "387c04fc5fbb421b87f9eaf0cc150e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa0091c91764f6ca2e21d65c45d18eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bcfa13acbad4f1e9fb284a68f61d9a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f1d6f94bab409cbfadfd13c7597f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "972f7cc0237f488ab0a54befbfe85b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bac7c1c2a544f19b3bd5b3ec89b5cfa",
              "IPY_MODEL_81aff29d06114ac585119b7c0fe6dfe4",
              "IPY_MODEL_e1794a5fee8b4b3cb693689b7db95cdf"
            ],
            "layout": "IPY_MODEL_17f3736bee3647979f096f40113e0963"
          }
        },
        "5bac7c1c2a544f19b3bd5b3ec89b5cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56550977be954f68816b767163347f62",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49d7da777634415da2235bf936ee4abe",
            "value": "Map:‚Äá100%"
          }
        },
        "81aff29d06114ac585119b7c0fe6dfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d06964dc33345948b552a48f26af805",
            "max": 192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_901d231559a646bf8ef44f11ad6a7d1f",
            "value": 192
          }
        },
        "e1794a5fee8b4b3cb693689b7db95cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e9f6cbeacb4e41a518ac76d93be103",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ba259db36cbf4c16bfd297cfa6d563a9",
            "value": "‚Äá192/192‚Äá[00:01&lt;00:00,‚Äá135.28‚Äáexamples/s]"
          }
        },
        "17f3736bee3647979f096f40113e0963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56550977be954f68816b767163347f62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d7da777634415da2235bf936ee4abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d06964dc33345948b552a48f26af805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901d231559a646bf8ef44f11ad6a7d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27e9f6cbeacb4e41a518ac76d93be103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba259db36cbf4c16bfd297cfa6d563a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d2683ac2754716b0e0d9e3278cfe89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc2a494fbc2d4877ae995d6b628a20e9",
              "IPY_MODEL_702b85d5fab64c92ad5aceb22590dad4",
              "IPY_MODEL_f80148a45c1a4719a2be4a2dc2212532"
            ],
            "layout": "IPY_MODEL_5e092b22d4204ac2a5460189e74181d8"
          }
        },
        "fc2a494fbc2d4877ae995d6b628a20e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7488fa5a9a0421ba176d0ff94257498",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_503836257374431893318438c40c02f3",
            "value": "Map:‚Äá100%"
          }
        },
        "702b85d5fab64c92ad5aceb22590dad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b477dab0156d418b8628ce7eb0101f2b",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be4b80a5029443a687913fab23f85209",
            "value": 48
          }
        },
        "f80148a45c1a4719a2be4a2dc2212532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4300241ad43a426389660ba557b5041e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_97cd20bbd1974fe49435f102ea74242d",
            "value": "‚Äá48/48‚Äá[00:00&lt;00:00,‚Äá107.22‚Äáexamples/s]"
          }
        },
        "5e092b22d4204ac2a5460189e74181d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7488fa5a9a0421ba176d0ff94257498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "503836257374431893318438c40c02f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b477dab0156d418b8628ce7eb0101f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4b80a5029443a687913fab23f85209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4300241ad43a426389660ba557b5041e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cd20bbd1974fe49435f102ea74242d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6e6414e83b5478e9bd21b830b0fb7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f01fcdd5a3804589b6dd9b0353298e0d",
              "IPY_MODEL_2cb53e5c8fb9432795fa00dbd28a95ae",
              "IPY_MODEL_7f3636d640fe4670b623e492f8a336f7"
            ],
            "layout": "IPY_MODEL_01871aacaa8649f2ab1bdb072114f1c9"
          }
        },
        "f01fcdd5a3804589b6dd9b0353298e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f225c165ce93444a97f893b8ab1b4cc9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_152a86e33f864838af4780a780a170b6",
            "value": "Map:‚Äá100%"
          }
        },
        "2cb53e5c8fb9432795fa00dbd28a95ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aa29606933f4bed8700a14e1596a938",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9a8843efae24b7a9e987ebc7d47e13c",
            "value": 60
          }
        },
        "7f3636d640fe4670b623e492f8a336f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715ef094f367439bbaf1128eb6d41f6b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf68125bf1a14540825eab2fa30ce1fa",
            "value": "‚Äá60/60‚Äá[00:00&lt;00:00,‚Äá110.26‚Äáexamples/s]"
          }
        },
        "01871aacaa8649f2ab1bdb072114f1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f225c165ce93444a97f893b8ab1b4cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152a86e33f864838af4780a780a170b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7aa29606933f4bed8700a14e1596a938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9a8843efae24b7a9e987ebc7d47e13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715ef094f367439bbaf1128eb6d41f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf68125bf1a14540825eab2fa30ce1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a947a76d796343fb8010d27f818fbf2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22ce83fed2d444919ba40dfb5526d90b",
              "IPY_MODEL_179503fdfa3f49e6a4e8f998af5d573d",
              "IPY_MODEL_c5198b7842714377acc4efeb347fcf6e"
            ],
            "layout": "IPY_MODEL_b952ae1ae73f4892aa7d8eef1904e9fe"
          }
        },
        "22ce83fed2d444919ba40dfb5526d90b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6394d8d637444b4490dadb9c0dcaf830",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b07b49a5db434eff838e6dcec3cfc747",
            "value": "Map:‚Äá100%"
          }
        },
        "179503fdfa3f49e6a4e8f998af5d573d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee1e4766dcde41d7ba690f8fdca722fc",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbd28d54ee2e43f5a2e710515a782406",
            "value": 64
          }
        },
        "c5198b7842714377acc4efeb347fcf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44799cbdb0c248f8b14434970ccb551b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8214d98138f041079c83d5a9507178b6",
            "value": "‚Äá64/64‚Äá[00:00&lt;00:00,‚Äá188.96‚Äáexamples/s]"
          }
        },
        "b952ae1ae73f4892aa7d8eef1904e9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6394d8d637444b4490dadb9c0dcaf830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07b49a5db434eff838e6dcec3cfc747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee1e4766dcde41d7ba690f8fdca722fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd28d54ee2e43f5a2e710515a782406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44799cbdb0c248f8b14434970ccb551b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8214d98138f041079c83d5a9507178b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f25b823e67c84984b571042ef8376c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24d6aa12d60342a19399cf825b332e1c",
              "IPY_MODEL_8962a4e16c4b4fbb846656f35a70a3a9",
              "IPY_MODEL_7fe00692904040c38e5832f11b8975b9"
            ],
            "layout": "IPY_MODEL_ee17540ad8714e9b903d9f9a181af961"
          }
        },
        "24d6aa12d60342a19399cf825b332e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d14a957fe724702a82ada900361de55",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22880aebf50e4f60b2e18fd8b00e88d3",
            "value": "Map:‚Äá100%"
          }
        },
        "8962a4e16c4b4fbb846656f35a70a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a69a0d3006141c8a7a7f53c262294cb",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee8a801d8d03459fbc058b37cb528f21",
            "value": 16
          }
        },
        "7fe00692904040c38e5832f11b8975b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2001fdf69d5424cab0e321a1e2760ad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5724c8c3c435472da70b8bf114aa6829",
            "value": "‚Äá16/16‚Äá[00:00&lt;00:00,‚Äá159.87‚Äáexamples/s]"
          }
        },
        "ee17540ad8714e9b903d9f9a181af961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d14a957fe724702a82ada900361de55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22880aebf50e4f60b2e18fd8b00e88d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a69a0d3006141c8a7a7f53c262294cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee8a801d8d03459fbc058b37cb528f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2001fdf69d5424cab0e321a1e2760ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5724c8c3c435472da70b8bf114aa6829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "853aa86e41dc47d4ae9e4622941f1af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83579f36b9c44813b9533c37046b2d93",
              "IPY_MODEL_cd35a1f388034fe9929318cebca0e1aa",
              "IPY_MODEL_3adfde6716054b4eb3b968e10bb94264"
            ],
            "layout": "IPY_MODEL_ae25b03da35e465badd05b18e33c5a89"
          }
        },
        "83579f36b9c44813b9533c37046b2d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7393147bf8f497fb965e3f356a83b70",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2489ce6eece4479ea965ddc4a59e95bc",
            "value": "Map:‚Äá100%"
          }
        },
        "cd35a1f388034fe9929318cebca0e1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f3fd7bc5cb042debc5576e5d33750d6",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18dfab3436384c5599b39fb00654deaa",
            "value": 20
          }
        },
        "3adfde6716054b4eb3b968e10bb94264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53557f686a0046caa7f97ce2da392007",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5c8d845574a349cd9133d93d0c5c354b",
            "value": "‚Äá20/20‚Äá[00:00&lt;00:00,‚Äá173.39‚Äáexamples/s]"
          }
        },
        "ae25b03da35e465badd05b18e33c5a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7393147bf8f497fb965e3f356a83b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2489ce6eece4479ea965ddc4a59e95bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f3fd7bc5cb042debc5576e5d33750d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18dfab3436384c5599b39fb00654deaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53557f686a0046caa7f97ce2da392007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8d845574a349cd9133d93d0c5c354b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}