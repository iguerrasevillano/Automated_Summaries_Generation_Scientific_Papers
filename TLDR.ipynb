{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/main/TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "2570df25-e54b-450e-e3cf-c012c2182b18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=bb6d39e0e50613eb98f575721bcea91a486bc12bc8835dfe6bd45bf9496848a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "6bcb7b0562514f8396a5e480261bb42f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "7af630999e4b40f6a160b5f1b2ba5fca",
            "b7fb664ef7274f14bcc1abce5cb6037e",
            "a41b2cb057d74491a2a739bc994c3bbe",
            "1a457d6d7236467aafa5b8238fc14767",
            "8dd47d575244433385fe683b503bcc54",
            "e4d7e908b14b40eda3892d03411119f6",
            "ef02dbb55af04b2db8b33c10b44561dd",
            "ff10561189024511af8a953c42345f13",
            "268acb4ba1974301a643be4d3e36101c",
            "f152197af28746e0be359c10d6207030",
            "b130c8b9e88b4333983ccc93d745f6df"
          ]
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "d302f421-20db-4449-a22d-74e2be6102dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<ipython-input-1-c5cb458fe86c>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7af630999e4b40f6a160b5f1b2ba5fca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TFBartForConditionalGeneration, BartTokenizer, T5Tokenizer, pipeline, TFT5ForConditionalGeneration\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect w/ HuggingFace HUB\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "9100de41ec6946c495c032b05d90913d",
            "40d035dada3347e2963c183d035d445c",
            "ecd31a14240f408b82b5224d97694ab7",
            "7afeab237ced4f01b70361e2a3ce7552",
            "5b7408314641483980402784564e62e6",
            "c4a0eef84a5e4061884e43d2a09da02a",
            "b114a3fe857949408305dde0c9436974",
            "f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "52801cc2a7a646109c4a7141ef768900",
            "db74bb5ffc4f4900b17aaf842a618019",
            "6deef8837ace4d94b5b803666ff4aa5a",
            "2e00c125b4d942368e702682002f94ff",
            "571b4b08342141c798092b6b23ab410a",
            "ca46452e63a34d99865718ab86726748",
            "493ec2745ddf4bf2bdce528219a1b309",
            "d3c8673d4bac474d9c5799e3ee02bf6b",
            "f44df68731c14e24914d1c8816c8edce"
          ]
        },
        "id": "wpR8O7wbdMdI",
        "outputId": "d0ef909e-6736-48c6-bf15-0c9b0cbe318a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9100de41ec6946c495c032b05d90913d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "7afe076f-83ed-4355-ac2d-2a2241ae0eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "1I5H39lnQW9o",
        "outputId": "2e98e426-ebce-4cd1-914e-765adc38a26c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source    paper_id  \\\n",
              "1155  Deep neural networks are vulnerable to adversa...  BJzVUj0qtQ   \n",
              "673   Using variational Bayes neural networks, we de...  S1eEmn05tQ   \n",
              "878   Learning a better representation with neural n...   S17mtzbRb   \n",
              "1144  Deep convolutional neural network (DCNN) based...   Hy-lXyDWG   \n",
              "1005  This paper studies the problem of domain divis...  H1GaLiAcY7   \n",
              "\n",
              "                                                 target  \\\n",
              "1155  We propose an attention-invariant attack metho...   \n",
              "673   A scalable method for learning an expressive p...   \n",
              "878   A novel loss component that forces the network...   \n",
              "1144  The paper is about a new energy-efficient meth...   \n",
              "1005   This paper studies the problem of domain divi...   \n",
              "\n",
              "                                                  title  number_words_target  \\\n",
              "1155  Evading Defenses to Transferable Adversarial E...                   66   \n",
              "673          Uncertainty in Multitask Transfer Learning                   70   \n",
              "878   Forced Apart: Discovering Disentangled Represe...                   74   \n",
              "1144  Incremental Learning in Deep Convolutional Neu...                   66   \n",
              "1005  Learning to Separate Domains in Generalized Ze...                   88   \n",
              "\n",
              "                                     extractive_summary  \n",
              "1155  Attacking deep neural networks has drawn an in...  \n",
              "673   More recent tools such as deep Gaussian proces...  \n",
              "878   Output size: 128 In the past few years, a subs...  \n",
              "1144  One of the major challenges for convolutional ...  \n",
              "1005  This is different from the conventional Zero-S...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8de07eef-88d7-4527-8eea-db9c4f3db322\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1155</th>\n",
              "      <td>Deep neural networks are vulnerable to adversa...</td>\n",
              "      <td>BJzVUj0qtQ</td>\n",
              "      <td>We propose an attention-invariant attack metho...</td>\n",
              "      <td>Evading Defenses to Transferable Adversarial E...</td>\n",
              "      <td>66</td>\n",
              "      <td>Attacking deep neural networks has drawn an in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>Using variational Bayes neural networks, we de...</td>\n",
              "      <td>S1eEmn05tQ</td>\n",
              "      <td>A scalable method for learning an expressive p...</td>\n",
              "      <td>Uncertainty in Multitask Transfer Learning</td>\n",
              "      <td>70</td>\n",
              "      <td>More recent tools such as deep Gaussian proces...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>878</th>\n",
              "      <td>Learning a better representation with neural n...</td>\n",
              "      <td>S17mtzbRb</td>\n",
              "      <td>A novel loss component that forces the network...</td>\n",
              "      <td>Forced Apart: Discovering Disentangled Represe...</td>\n",
              "      <td>74</td>\n",
              "      <td>Output size: 128 In the past few years, a subs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1144</th>\n",
              "      <td>Deep convolutional neural network (DCNN) based...</td>\n",
              "      <td>Hy-lXyDWG</td>\n",
              "      <td>The paper is about a new energy-efficient meth...</td>\n",
              "      <td>Incremental Learning in Deep Convolutional Neu...</td>\n",
              "      <td>66</td>\n",
              "      <td>One of the major challenges for convolutional ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>This paper studies the problem of domain divis...</td>\n",
              "      <td>H1GaLiAcY7</td>\n",
              "      <td>This paper studies the problem of domain divi...</td>\n",
              "      <td>Learning to Separate Domains in Generalized Ze...</td>\n",
              "      <td>88</td>\n",
              "      <td>This is different from the conventional Zero-S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8de07eef-88d7-4527-8eea-db9c4f3db322')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8de07eef-88d7-4527-8eea-db9c4f3db322 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8de07eef-88d7-4527-8eea-db9c4f3db322');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ceae0ae3-8089-4a44-a026-6cabf8448288\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ceae0ae3-8089-4a44-a026-6cabf8448288')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ceae0ae3-8089-4a44-a026-6cabf8448288 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Using variational Bayes neural networks, we develop an algorithm capable of accumulating knowledge into a prior from multiple different tasks. This results in a rich prior capable of few-shot learning on new tasks. The posterior can go beyond the mean field approximation and yields good uncertainty on the performed experiments. Analysis on toy tasks show that it can learn from significantly different tasks while finding similarities among them. Experiments on Mini-Imagenet reach state of the art with 74.5% accuracy on 5 shot learning. Finally, we provide two new benchmarks, each showing a failure mode of existing meta learning algorithms such as MAML and prototypical Networks. Recently, significant progress has been made to scale Bayesian neural networks to large tasks and to provide better approximations of the posterior distribution BID4 . Recent works extend fully factorized posterior distributions to more general families BID22 BID21 . It is also possible to sample from the posterior distribution trough mini-batch updates BID23 BID36 .However, for neural networks, the prior is often chosen for convenience. This may become a problem when the number of observations is insufficient to overcome the choice of the prior. In this regime, the prior must express our current knowledge on the task and, most importantly, our lack of knowledge on it. In addition to that, a good approximation of the posterior under the small sample size regime is required, including the ability to model multiple modes. This is indeed the case for Bayesian optimization BID30 , Bayesian active learning BID12 , continual learning BID20 , safe reinforcement learning BID3 , exploration-exploitation trade-off in reinforcement learning BID16 . Gaussian processes BID27 have historically been used for these applications, but an RBF kernel constitute a prior that is unsuited for many tasks. More recent tools such as deep Gaussian processes BID6 show great potential and yet their scalability whilst learning from multiple tasks needs to be improved. Our contributions are as follow:1. We provide a simple and scalable procedure to learn an expressive prior and posterior over models from multiple tasks.2. We reach state of the art performances on mini-imagenet.3. We propose two new benchmarks, each exposing a failure mode of popular meta learning algorithms. In contrast, our method perform well on these benchmarks.\\u2022 MAML BID11 does not perform well on a collection of sinus tasks when the frequency varies.\\u2022 Prototypical Network BID29 )'s performance decrease considerably when the diversity of tasks increases. Outline: We first describe the proposed approach in Section 2. In Section 3, we extend to three level of hierarchies and obtain a model more suited for classification. Section 4 review related methods and outline the key differences. Finally, In Section 5, we conduct experiments on three different benchmarks to gain insight in the behavior of our algorithm. By leveraging the variational Bayesian approach, we show how we can learn a prior over models with neural networks. We start our analysis with the goal of learning a prior p(w|\\u03b1) over the weights w of neural networks across multiple tasks. We then provide a reduction of the Evidence Lower BOund (ELBO) showing that it is not necessary to explicitly model a distribution in the very high dimension of the weight space of neural networks. Instead the algorithm learns a subspace suitable for expressing model uncertainty within the distributions of tasks considered in the multi-task environment. This simplification results in a scalable algorithm which we refer to as deep prior. To learn a probability distribution p(w|\\u03b1) over the weights w of a network parameterized by \\u03b1, we use a hierarchical Bayes approach across N tasks, with hyper-prior p(\\u03b1). Each task has its own parameters w j , with DISPLAYFORM0 , we have the following posterior: DISPLAYFORM1 The term p(y ij |x ij , w j ) corresponds to the likelihood of sample i of task j given a model parameterized by w j e.g. the probability of class y ij from the softmax of a neural network parameterized by w j with input x ij . For the posterior p(\\u03b1|D), we assume that the large amount of data available across multiple tasks will be enough to overcome a generic prior p(\\u03b1), such as an isotropic Normal distribution. Hence, we consider a point estimate of the posterior p(\\u03b1|D) using maximum a posteriori 2 .We can now focus on the remaining term: p(w j |\\u03b1). Since w j is potentially high dimensional with intricate correlations among the different dimensions, we cannot use a simple Gaussian distribution. Following inspiration from generative models such as GANs BID13 and VAE BID18 , we use an auxiliary variable z \\u223c N (0, I dz ) and a deterministic function projecting the noise z to the space of w i.e. w = h \\u03b1 (z). Marginalizing z, we have: p(w|\\u03b1) = z p(z)p(w|z, \\u03b1)dz = z p(z)\\u03b4 h\\u03b1(z)\\u2212w dz, where \\u03b4 is the Dirac delta function. Unfortunately, directly marginalizing z is intractable for general h \\u03b1 . To overcome this issue, we add z to the joint inference and marginalize it at inference time. Considering the point estimation of \\u03b1, the full posterior is factorized as follows: DISPLAYFORM2 where p(y ij |x ij , w j ) is the conventional likelihood function of a neural network with weight matrices generated from the function h \\u03b1 i.e.: w j = h \\u03b1 (z j ). Similar architecture has been used in BID21 and BID22 , but we will soon show that it can be reduced to a simpler architecture in the context of multi-task learning. The other terms are defined as follows: DISPLAYFORM3 The task will consist of jointly learning a function h \\u03b1 common to all tasks and a posterior distribution p(z j |\\u03b1, S j ) for each task. At inference time, predictions are performed by marginalizing z i.e.: DISPLAYFORM4 In the previous section, we described the different components for expressing the posterior distribution of Equation 4. While all these components are tractable, the normalization factor is still intractable. To address this issue, we follow the Variational Bayes approach BID4 .Conditioning on \\u03b1, we saw in Equation 1 that the posterior factorizes independently for all tasks. This reduces the joint Evidence Lower BOund (ELBO) to a sum of individual ELBO for each task. Given a family of distributions q \\u03b8j (z j |S j , \\u03b1), parameterized by {\\u03b8 j } N j=1 and \\u03b1, the Evidence Lower Bound for task j is: DISPLAYFORM0 where, DISPLAYFORM1 Notice that after simplification 3 , KL j is no longer over the space of w j but only over the space z j . Namely, the posterior distribution is factored into two components, one that is task specific and one that is task agnostic and can be shared with the prior. This amounts to finding a low dimensional manifold in the parameter space where the different tasks can be distinguished. Then, the posterior p(z j |S j , \\u03b1) only has to model which of the possible tasks are likely, given observations S j instead of modeling the high dimensional p(w j |S j , \\u03b1).But, most importantly, any explicit reference to w has now vanished from both Equation 5 and Equation 6. This simplification has an important positive impact on the scalability of the proposed approach. Since we no longer need to explicitly calculate the KL on the space of w, we can simplify the likelihood function to p(y ij |x ij , z j , \\u03b1), which can be a deep network parameterized by \\u03b1, taking both x ij and z j as inputs. This contrasts with the previous formulation, where h \\u03b1 (z j ) produces all the weights of a network, yielding an extremely high dimensional representation and slow training. For modeling q \\u03b8j (z j |S j , \\u03b1), we can use N (\\u00b5 j , \\u03c3 j ), where \\u00b5 j and \\u03c3 j can be learned individually for each task. This, however limits the posterior family to express a single mode. For more flexibility, we also explore the usage of more expressive posterior, such as Inverse Autoregressive Flow (IAF) BID19 or Neural Autoregressive Flow BID17 . This gives a flexible tool for learning a rich variety of multivariate distributions. In principle, we can use a different IAF for each task, but for memory and computational reasons, we use a single IAF for all tasks and we condition 4 on an additional task specific context c j .Note that with IAF, we cannot evaluate q \\u03b8j (z j |S j , \\u03b1) for any values of z efficiently, only for these which we just sampled, but this is sufficient for estimating the KL term with a Monte-Carlo approxi-mation i.e.: DISPLAYFORM0 It is common to approximate KL j with a single sample and let the mini-batch average the noise incurred on the gradient. We experimented with n mc = 10, but this did not significantly improve the rate of convergence. In order to compute the loss proposed in Equation 5, we would need to evaluate every sample of every task. To accelerate the training, we use a Monte-Carlo approximation as is commonly done through the mini-batch procedure. First we replace summations with expectations: DISPLAYFORM0 Now it suffices to approximate the gradient with n mb samples across all tasks. Thus, we simply concatenate all datasets into a meta-dataset and added j as an extra field. Then, we sample uniformly 5 n mb times with replacement from the meta-dataset. Notice the term n j appearing in front of the likelihood in Equation 7, this indicates that individually for each task it finds the appropriate trade-off between the prior and the observations. Refer to Algorithm 1 for more details on the procedure.1: for i in 1 .. n mb :2: sample x, y and j uniformly from the meta dataset 3: DISPLAYFORM1 5: DISPLAYFORM2 Calculating the loss for a mini-batch 3 EXTENDING TO 3 LEVEL OF HIERARCHIESDeep prior gives rise to a very flexible way to transfer knowledge from multiple tasks. However, there is still an important assumption at the heart of deep prior (and other VAE-based approach such as BID10 ): the task information must be encoded in a low dimensional variable z. In Section 5, we show that it is appropriate for regression, but for image classification, it is not the most natural assumption. Hence, we propose to extend to a third level of hierarchy by introducing a latent classifier on the obtained representation. This provides a simple way to enhance existing algorithm such as Prototypical Networks (Proto Net) BID29 . , for a given 6 task j, we decomposed the likelihood p(S|z) into n i=1 p(y i |x i , z) by assuming that the neural network is directly predicting p(y i |x i , z). Here, we introduce a latent variable v to make the prediction p(y i |x i , v). This can be, for example, a Gaussian linear regression on the representation \\u03c6 \\u03b1 (x, z) produced by the neural network. The general form now factorizes as follow: DISPLAYFORM0 , which is commonly called the marginal likelihood. To compute ELBO j in 5 and update the parameters \\u03b1, the only requirement is to be able to compute the marginal likelihood p(S|z). There are closed form solutions for, e.g., linear regression with Gaussian prior, but our aim is to compare with algorithms such as Prototypical Networks on a 5 We also explored a sampling scheme that always make sure to have at least k samples from the same task. The aim was to reduce gradient variance on task specific parameters but, we did not observed any benefits. 6 We removed j from equations to alleviate the notation.classification benchmark. Alternatively, we can factor the marginal likelihood as follow p(S|z) = n i=1 p(y i |x i , S 0..i\\u22121 , z). If a well calibrated task uncertainty is not required, one can also use a leave-one-out procedure n i=1 p(y i |x i , S \\\\ {x i , y i }, z). Both of these factorizations correspond to training n times the latent classifier on a subset of the training set and evaluating on a sample left out. We refer the reader to Rasmussen (2004, Chapter 5) for a discussion on the difference between leave-one-out cross-validation and marginal likelihood. For a practical algorithm, we propose a closed form solution for leave-one-out in prototypical networks. In its standard form, the prototypical network produces a prototype c k by averaging all representations \\u03b3 i = \\u03c6 \\u03b1 (x i , z) of class k i.e. c k = 1 |K| i\\u2208K \\u03b3 i , where K = { i : y i = k}. Then, predictions are made using p(y = k|x, \\u03b1, z) \\u221d exp (\\u2212 c k \\u2212 \\u03b3 i 2 ). k \\u2200k be the prototypes computed without example x i , y i in the training set. Then, DISPLAYFORM0 We defer the proof to supplementary materials. Hence, we only need to compute prototypes once and rescale the Euclidean distance when comparing with a sample that was used for computing the current prototype. This gives an efficient algorithm with the same complexity as the original one and a good proxy for the marginal likelihood. Hierarchical Bayes algorithms for multitask learning has a long history BID7 BID35 BID1 . However most of the literature focuses on simple statistical models and does not consider transferring on new tasks. More recently, BID10 and BID5 explore hierarchical Bayesian inference with neural networks and evaluate on new tasks. Both papers use a two-level Hierarchical VAE for modeling the observations. While similar, our approach differs in a few different ways. We use a discriminative approach and focus on model uncertainty. We show that we can obtain a posterior on z without having to explicitly encode S j . We also explore the usage of more complex posterior family such as IAF. these differences make our algorithm simpler to implement, and easier to scale to larger datasets. Other works consider neural networks with latent variables BID32 BID9 BID33 but does not explore the ability to learn across multiple tasks. Some recent works on meta-learning are also targeting transfer learning from multiple tasks. ModelAgnostic Meta-Learning (MAML) BID11 ) finds a shared parameter \\u03b8 such that for a given task, one gradient step on \\u03b8 using the training set will yield a model with good predictions on the test set. Then, a meta-gradient update is performed from the test error through the one gradient step in the training set, to update \\u03b8. This yields a simple and scalable procedure which learns to generalize. Recently BID14 considers a Bayesian version of MAML. Additionally, BID28 ) also consider a meta-learning approach where an encoding network reads the training set and generates the parameters of a model, which is trained to perform well on the test set. Finally, some recent interest in few-shot learning give rise to various algorithms capable of transferring from multiple tasks. Many of these approaches BID34 BID29 find a representation where a simple algorithm can produce a classifier from a small training set. BID2 use a neural network pre-trained on a standard multi-class dataset to obtain a good representation and use classes statistics to transfer prior knowledge to new classes. Through experiments, we want to answer i) Can deep prior learn a meaningful prior on tasks? ii) Can it compete against state of the art on a strong benchmark? iii) In which situations does deep prior and other approaches fail? To gain a good insight into the behavior of the prior and posterior, we choose a collection of one dimensional regression tasks. We also want to test the ability of the method to learn the task and not just match the observed points. For this, we will use periodic functions and test the ability of the regressor to extrapolate outside of its domain. Specifically, each dataset consists of (x, y) pairs (noisily) sampled from a sum of two sine waves with different phase and amplitude and a frequency ratio of 2: f (x) = a 1 sin(\\u03c9\\u00b7x+b 1 )+a 2 sin(2\\u00b7\\u03c9\\u00b7x+b 2 ), where y \\u223c N (f (x), \\u03c3 2 y ). We construct a meta-training set of 5000 tasks, sampling \\u03c9 \\u223c U(5, 7), (b 1 , b 2 ) \\u223c U(0, 2\\u03c0) 2 and (a 1 , a 2 ) \\u223c N (0, 1) 2 independently for each task. To evaluate the ability to extrapolate outside of the task's domain, we make sure that each task has a different domain. Specifically, x values are sampled according to N (\\u00b5 x , 1), where \\u00b5 x is sample from the meta-domain U (\\u22124, 4) . The number of training samples ranges from 4 to 50 for each task and, evaluation is performed on 100 samples from tasks never seen during training. Model Once z is sampled from IAF, we simply concatenate it with x and use 12 densely connected layers of 128 neurons with residual connections between every other layer. The final layer linearly projects to 2 outputs \\u00b5 y and s, where s is used to produce a heteroskedastic noise, \\u03c3 y = sigmoid(s) \\u00b7 0.1 + 0.001. Finally, we use p(y|x, z) = N (\\u00b5 y (x, z), \\u03c3 y (x, z)2 ) to express the likelihood of the training set. To help gradient flow, we use ReLU activation functions and Layer Normalization 7 BID0 .Results Figure 1a depicts examples of tasks with 1, 2, 8, and 64 samples. The true underlying function is in blue while 10 samples from the posterior distributions are faded in the background. The thickness of the line represent 2 standard deviations. The first plot has only one single data point and mostly represents samples from the prior, passing near this observed point. Interestingly, all samples are close to some parametrization of Equation 5.1. Next with only 2 points, the posterior is starting to predict curves highly correlated with the true function. However, note that the uncertainty is over optimistic and that the posterior failed to fully represent all possible harmonics fitting these two points. We discuss this issue more in depth in supplementary materials. Next, with 8 points, it managed to mostly capture the task, with reasonable uncertainty. Finally, with 64 points the model is certain of the task. To add a strong baseline, we experimented with MAML BID11 . After exploring a variety of values for hyper-parameter and architecture design we couldn't make it work for our two harmonics meta-task. We thus reduced the meta-task to a single harmonic and reduced the base frequency range by a factor of two. With these simplifications, we managed to make it converge, but the results are far behind that of deep prior even in this simplified setup. Figure 1b shows some form of adaptation with 16 samples per task but the result is jittery and the extrapolation capacity is very limited. these results were obtained with a densely connected network of 8 hidden layers of 64 units 8 , with residual connections every other layer. The training is performed with two gradient steps and the evaluation with 5 steps. To make sure our implementation is valid, we first replicated their regression result with a fixed frequency as reported in BID11 .Finally, to provide a stronger baseline, we remove the KL regularizer of deep prior and reduced the posterior q \\u03b8j (z j |S j , \\u03b1) to a deterministic distribution centered on \\u00b5 j . The mean square error is reported in Figure 2 for an increasing dataset size. This highlights how the uncertainty provided by deep prior yields a systematic improvement. BID34 proposed to use a subset of Imagenet to generate a benchmark for few-shot learning. Each task is generated by sampling 5 classes uniformly and 5 training samples per class, the remaining images from the 5 classes are used as query images to compute accuracy. The number of unique classes sums to 100, each having 600 examples of 84 \\u00d7 84 images. To perform meta-validation and meta-test on unseen tasks (and classes), we isolate 16 and 20 classes respectively from the original The baseline corresponds to the same model without the KL regularizer. Each value is averaged over 100 tasks and 10 different restart. right: 4 sample tasks from the Synbols dataset. Each row is a class and each column is a sample from the classes. In the 2 left tasks, the symbol have to be predicted while in the two right tasks, the font has to be predicted. set of 100, leaving 64 classes for the training tasks. This follows the procedure suggested in BID28 . The training procedure proposed in Section 2 requires training on a fixed set of tasks. We found that 1000 tasks yields enough diversity and that over 9000 tasks, the embeddings are not being visited often enough over the course of the training. To increase diversity during training, the 5 \\u00d7 5 training and test sets are re-sampled every time from a fixed train-test split of the given task 9 .We first experimented with the vanilla version of deep prior (2). In this formulation, we use a ResNet BID15 network, where we inserted FILM layers BID26 between each residual block to condition on the task. Then, after flattening the output of the final convolution layer and reducing to 64 hidden units, we apply a 64 \\u00d7 5 matrix generated from a transformation of z. Finally, predictions are made through a softmax layer. We found this architecture to be slow to train as the generated last layer is noisy for a long time and prevent the rest of the Matching Networks BID34 60.0 % Meta-Learner BID28 60.6 % MAML BID11 63.2% Prototypical Networks BID29 68.2 % SNAIL BID24 68.9 % Discriminative k-shot BID2 73.9 % adaResNet BID25 71.9 % Deep Prior (Ours) 62.7 % Deep Prior + Proto Net (Ours) 74.5 % 68.6 \\u00b1 0.5% 69.6 \\u00b1 0.8% + ResNet (12) 72.4 \\u00b1 1.0% 76.8 \\u00b1 0.4% + Conditioning 72.3 \\u00b1 0.6% 80.1 \\u00b1 0.9% + Leave-One-Out 73.9 \\u00b1 0.4% 82.7 \\u00b1 0.2% + KL 74.5 \\u00b1 0.5% 83.5 \\u00b1 0.4% Table 2 : Ablation Study of our model. Accuracy is shown with 90% confidence interval over bootstrap of the validation set.network to learn. Nevertheless, we obtained 62.6% accuracy on Mini-Imagenet, on par with many strong baselines. To enhance the model, we combine task conditioning with prototypical networks as proposed in Section 3. This approach alleviates the need to generate the final layer of the network, thus accelerating training and increasing generalization performances. While we no longer have a well calibrated task uncertainty, the KL term still acts as an effective regularizer and prevents overfitting on small datasets 10 . With this improvement, we are now the new state of the art with 74.5% TAB0 . In Table 2 , we perform an ablation study to highlight the contributions of the different components of the model. In sum, a deeper network with residual connections yields major improvements. Also, task conditioning does not yield improvement if the leave-one-out procedure is not used. Finally, the KL regularizer is the final touch to obtain state of the art. In Section 5.2, we saw that conditioning helps, but only yields a minor improvement. This is due to the fact that Mini-Imagenet is a very homogeneous collection of tasks where a single representation is sufficient to obtain good results. To support this claim, we provide a new benchmark 11 of synthetic symbols which we refer to as Synbols. Images are generated using various font family on different alphabets (Latin, Greek, Cyrillic, Chinese) and background noise (Figure 2, right) . For each task we have to predict either a subset of 4 font families or 4 symbols with only 4 examples. Predicting either fonts or symbols with two separate Prototypical Networks, yields 84.2% and 92.3% accuracy respectively, with an average of 88.3%. However, blending the two collections of tasks in a single benchmark, brings prototypical network down to 76.8%. Now, conditioning on the task with deep prior brings back the accuracy to 83.5%. While there is still room for improvement, this supports the claim that a single representation will only work on homogeneous collection of tasks and that task conditioning helps learning a family of representations suitable for heterogeneous benchmarks. Using a variational Bayes framework, we developed a scalable algorithm for hierarchical Bayesian learning of neural networks, called deep prior. This algorithm is capable of transferring information from tasks that are potentially remarkably different. Results on the Harmonics dataset shows that the learned manifold across tasks exhibits the properties of a meaningful prior. Finally, we found that MAML, while very general, will have a hard time adapting when tasks are too different. Also, we found that algorithms based on a single image representation only works well when all tasks can succeed with a very similar set of features. Together, these findings allowed us to reach the state of the art on Mini-Imagenet. 7.1 PROOF OF LEAVE-ONE-OUT Theorem 1. Let c \\u2212i k \\u2200k be the prototypes computed without example x i , y i in the training set. Then, DISPLAYFORM0 Proof. Let \\u03b3 i = \\u03c6 \\u03b1 (x i ), n = |K| and assume y i = k then, DISPLAYFORM1 When y i = k, the result is trivially \\u03b3 i \\u2212 c When experimenting with the Harmonics toy dataset in Section 5.1, we observed issues with repeatability, most likely due to local minima. We decided to investigate further on the multimodality of posterior distributions with small sample size and the capacity of IAF to model them. For this purpose we simplified the problem to a single sine function and removed the burden of learning the prior. The likelihood of the observations is defined as follows: DISPLAYFORM2 f (x) = sin(5(\\u03c9 \\u00b7 x + b)); y \\u223c N (f (x), \\u03c3 2 y ), where \\u03c3 y = 0.1 is given and p(\\u03c9) = p(b) = N (0, 1). Only the frequency \\u03c9 and the bias b are unknown 12 , yielding a bi-dimensional problem that is easy to visualize and quick to train. We use a dataset of 2 points at x = 1.5 and x = 3 and the corresponding posterior distribution is depicted in FIG1 -middle, with an orange point at the location of the true underlying function. Some samples from the posterior distribution can be observed in FIG1 -top. We observe a high amount of multi-modality on the posterior distribution FIG1 . Some of the modes are just the mirror of another mode and correspond to the same functions e.g. b + 2\\u03c0 or \\u2212f ; b + \\u03c0. But most of the time they correspond to different functions and modeling them is crucial for some application. The number of modes varies a lot with the choice of observed dataset, ranging from a few to several dozens. Now, the question is: \\\"How many of those modes can IAF model?\\\". Unfortunately, FIG1 -bottom reveals poor capability for this particular case. After carefully adjusting the hyperparameters 13 of IAF, exploring different initialization schemes and running multiple restarts, we rarely capture more than two modes (sometimes 4). Moreover, it will not be able to fully separate the two modes. There is systematically a thin path of density connecting each modes as a chain. With longer training, the path becomes thinner but never vanishes and the magnitude stays significant.\",\n          \"This paper studies the problem of domain division which aims to segment instances drawn from different probabilistic distributions. This problem exists in many previous recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL), where the testing instances come from either seen or unseen/novel classes with different probabilistic distributions. Previous works only calibrate the con\\ufb01dent prediction of classi\\ufb01ers of seen classes (WSVM Scheirer et al. (2014)) or taking unseen classes as outliers Socher et al. (2013). In contrast, this paper proposes a probabilistic way of directly estimating and \\ufb01ne-tuning the decision boundary between seen and unseen classes. In particular, we propose a domain division algorithm to split the testing instances into known, unknown and uncertain domains, and then conduct recognition tasks in each domain. Two statistical tools, namely, bootstrapping and KolmogorovSmirnov (K-S) Test, for the \\ufb01rst time, are introduced to uncover and \\ufb01ne-tune the decision boundary of each domain. Critically, the uncertain domain is newly introduced in our framework to adopt those instances whose domain labels cannot be predicted con\\ufb01dently. Extensive experiments demonstrate that our approach achieved the state-of-the-art performance on OSL and G-ZSL benchmarks. This paper discusses the problem of learning to separate two domains which include the instances sampled from different distributions. This is a typical and general research topic that can be potentially used in various recognition tasks, such as Open Set Learning (OSL) and Generalized Zero-Shot Learning (G-ZSL). Particularly, OSL can break the constraints of the closed set in supervised learning, and aim at recognizing the testing instances from one of the seen classes (i.e., known domain), and the novel class (i.e., unknown domain). The novel classes include the testing instances which have different distributions from that of the seen ones. In contrast, G-ZSL targets at distinguishing the labels of instances from the seen and unseen classes. Only the seen classes have the training instances, but unseen classes do not. Note that OSL does not explicitly give the class labels for those instances categorized as the novel class, but G-ZSL requires predicting the class labels of unseen classes. To address G-ZSL, semantic attributes or vectors are introduced as the intermediate representations; each (seen/unseen) class has one semantic prototype that contains class level information. Specifically, a reasonable solution of OSL and G-ZSL is via dividing the known and unknown domains. For training classes, the predictors are constructed to map visual features to the class label space (OSL), (or semantic space (G-ZSL)). Testing is performed on each separated domain to identify seen classes and the novel class (OSL), or both seen and unseen classes (G-ZSL).The key question of OSL and ZSL is how to deal with the newly introduced novel class/unseen classes efficiently in the testing time. This is different from the conventional Zero-Shot Learning (ZSL) task which assumes that, in the testing stage, seen classes would not be misclassified as unseens, and vice versa; ZSL only uses the unseen classes for testing. Unfortunately, the predictors learned on training classes will inevitably make OSL or G-ZSL approaches tend to be biased towards the seen classes, and thus leading to very poor classification results for the novel class (OSL) or unseen classes (G-ZSL) BID39 ; . We show an example in Fig. 1 . On aPY dataset (described in Sec. 6.1) BID10 ), t-SNE van der Maaten & Hinton (2008 is The initial boundary of the known domain is estimated by bootstrapping. We can further divide an uncertain domain by K-S Test. Then we can recognize instances in each domain. (b) The distribution of pairwise intraclass and interclass distances: We compute the empirical density of the pairwise distance in aPY dataset (described in Sec. 6.1). There is a large overlapping of the distribution of the intraclass and interclass distances.employed to visualize the distributions of the testing instances of the ResNet-101 features in BID39 (Fig. 1 (a) ), and semantic features learned by SAE Kodirov et al. (2017) (Fig. 1 (b) ). We categorize the SAE prediction as known or unknown domain labels and compare with the groundtruth in Fig. 1(c) . We show that a large portion of unseen instances being predicted as one of the known classes. A natural recipe for addressing this problem is to learn to separate domains by the distributions of instances; and different classifiers can be directly applied in each domain. However, there are still two key problems. First, visual features alone are not discriminative enough to help to distinguish the seen and unseen/novel classes. As Fig. 2 (a) , bicycle and motorbike, respectively, are one of the seen and unseen classes 1 in aPY dataset (described in Sec. 6.1). We can observe that there is a large overlapping region between their t-SNE visualized feature distributions. That is, the visual features may not be representative enough to differentiate these two classes; the instances of motorbike (circled as the uncertain domain) may be taken as the bicycle, or vice versa; Second, the predictors trained on seen classes may be not trustworthy. A not well-trained predictor may negatively affect the recognition algorithms. Third and even worse, the performance of classifiers in each domain is still very sensitive to the results of domain separation: should the domain of one testing instance be wrongly divided, it would never be correctly categorized by the classifiers. To tackle the aforementioned issues, our key insight (see Fig. 2(a) ) is to introduce a novel domain -uncertain domain that accounts for the overlapping regions of testing instances from seen or novel/unseen classes. Thus, the visual or semantic space can be learned to be divided into known, unknown and uncertain domains. The recognition algorithms will be directly employed in each domain. Nonetheless, how to divide the domains based on known information is also a non-trivial task. Though the supervised classifiers can learn the patterns of known classes, not all classes encountered during testing are known. Formally, we propose exploiting the distribution information of seen and novel/unseen classes to efficiently learn to divide the domains from a probabilistic perspective. Our domain separation algorithm has two steps: the initial division of domains by bootstrapping, and fine-tuning by the Kolmogorov-Smirnov test. Specifically, according to extreme value theory BID30 , the maximum/minimum confidence scores predicted by the classifier of each class can be taken as an extreme value distribution. Since we do not have the prior knowledge of the underlying data distributions of each class; bootstrapping is introduced here as an asymptotically consistent method in estimating an initial boundary of known classes. Nevertheless, the initial boundary estimated by bootstrapping is too relaxed to include novel testing instances as is illustrated in Fig. 2(b) . To finetune the boundary, we exploit the K-S Test to validate whether the learned predictors are trustworthy in a specific region. The uncertain domain introduced thus accounts for those testing instances whose labels are hard to be judged. Recognition models can be conducted in each domain. The main contribution is to present a systematic framework of learning to separate domains by probabilistic distributions of instances, which is capable of addressing various recognition tasks, including OSL and G-ZSL. Towards this goal, two simple, most widely used, and very effective tools -bootstrapping and the Kolmogorov-Smirnov test, are employed to firstly initially estimate and then fine-tune the boundary. In particular, we introduce an uncertain domain, which encloses the instances which can hardly be classified into known or unknown with high confidence. We extensively evaluate the importance of domain division on several zero-shot learning benchmarks and achieved significant improvement over existing ZSL approaches. One-Class Classification (OCC). It is also known as the unary classification or class-modeling. The OCC assumes that the training set contains only the positive samples of one specific class. By learning from such positive instances, OCC aims at identifying the instances belonging to that class. The common algorithms of OCC include One-class Support Vector Machine (OCSVM) BID26 , Local Outlier Factor (LOF) BID5 . OCSVM leverages Support Vector Data Description (SVDD) to get a spherical boundary in feature space. It regularizes the volume of hypersphere so that the effects of outliers can be minimized. The LOF measures the local deviation of the density of a given instance comparing to other instances, namely locality. The locality represents the density of the area. The instances in low-density parts can be taken as outliers. Note that all OCC algorithms just considered and build a boundary for the positive instances of one class. Open Set Learning (OSL). It judges whether the instances belong to known/seen classes BID25 ; BID30 ; BID3 or a novel unknown class. Both the OCC and OSL are able to divide the instances into known and unknown domains and recognize the known classes from the known domain. OSL aims at discriminating the instances into seen classes and instances beyond these classes are categorized into a single novel class. Critically OSL does not have the semantic prototypes of unseen classes to further give the class label of those instances in the novel class. Both the OCC and OSL are able to divide the instances into known and unknown domains and recognize the known classes in the known domain. Intrinsically, their key difference lies in whether leveraging the information of different seen classes in building the classifiers. Specifically, OCC only utilizes the instances of one class to learn its class boundary, whilst OSL can use the instances of different seen classes. . ZSL aims at recognizing the novel instances which have never been trained before. It transfers the knowledge learned from known source classes to recognize the testing instances from unknown target classes. The knowledge can be formulated as semantic attributes However, ZSL usually assumes that the unseen classes cannot be mis-classified as seen classes and vice versa. This has greatly simplified the learning task. Generalized Zero-Shot Learning. Chao et al. realized that it is nontrivial to directly utilize the existing Zero-Shot Learning algorithms in a more general setting, i.e., G-ZSL. In such a setting, the testing instances can come from either the seen or unseen classes. A thorough evaluation of G-ZSL is further conducted in BID39 . Their results show that the existing ZSL algorithms do not perform well if directly applied to G-ZSL. The predicted results are inclined to be biased towards seen classes. 3.1 PROBLEM SETUP In learning tasks, we are given the training dataset, i.e., seen classes, of n s instances, DISPLAYFORM0 n is the feature of i th instance with the class label l i \\u2208 C s , where C s is the source class set; n c s is the number of instances in seen class c. Analogous to standard ZSL setting, we introduce the target label classes C t with C s C t = \\u2205 and the total class label set C = C s \\u222a C t . y i is the semantic attribute vector of instance x i . In general, the y i of instances in one class should be the same BID18 . We simplify y c as the semantic prototype for all the instances in class c. Given one test instance x i , our goal is to predict its class label c i . We discuss two tasks: (1) Open set recognition: c i \\u2208 {C s , novel class }; (2) Generalized zero-shot learning: c i \\u2208 {C s , C t }. The semantic prototype is predefined for each class in C t . The 'novel class' is an umbrella term referring to any class not in C s . We firstly introduce the background of modeling the extreme values (i.e., minimum/maximum scores) computed from one supervised classifier as the extreme value distributions. In particular, by using each source class c, we can train a binary predictor function, e.g. SVM, z c = f c (x) : R n \\u2192 R where z is the confidence score of instance x belonging to the class c. In Extreme Value Theory (EVT) , the extreme values (i.e., maximum / minimum confidence scores) of the score distribution computed by the predictor function f c (\\u00b7) can be modeled by an EVT distribution. Specifically, for instance set {x} that belong to class c; the minimum score z c min = minf c ({x}) follows the Weibull distribution, DISPLAYFORM0 where DISPLAYFORM1 . Critically, Eq (1) models the lower boundary distribution of instance confidence scores belonging to class c. On the other hand, for instance set {x} NOT belonging to class c, the maximum score z c max = maxf c ({x}) should follow the reverse Weibull distribution BID27 , DISPLAYFORM2 where rG (\\u00b7) is the CDF of reverse Weibull distribution: DISPLAYFORM3 Eq (2) models the upper boundary distribution of confidence scores NOT belonging to class c. The scale parameters \\u03bb c , \\u03bb c , shape parameters \\u03ba c , \\u03ba c , and location parameters \\u03bd c , \\u03bd c are estimated by Maximum Likelihood Estimator fitted from the training data. Critically, Eq. (2) models the upper boundary distribution of instance confidence scores NOT belonging to class c. The distributions of extreme values defined in Eq (1) and Eq (2) can actually give us the boundary of each event above happened in a probabilistic perspective. Thus we can have the probability W-SVM introduces a threshold \\u03b4 c to determine whether the instance i belongs to the class c as, DISPLAYFORM0 where \\u03b4 c is a fixed value Scheirer et al. (2014) . The instance x i rejected by all the seen classes by Eq FORMULA7 is labeled as the unknown domain. Generalizing to C s class is straightforward by training multiple prediction functions {f c (x)}, c = 1, \\u00b7 \\u00b7 \\u00b7 , |C s |.However, there are several key limitations in directly utilizing Eq (4) and Eq (5) of learning the division of domains: (1) Eq (4) directly multiplies two terms and this indicates that there is a potential hypothesis that no correlation exists between E 1 and \\u00acE 2 , which is generally not the case. FORMULA3 In multiple seen classes, the instances may derive from many different classes. It is hard to determine a single fixed \\u03b4 in Eq (5) for each class. (3) Furthermore, we give an illustration of the non-negligible overlapping between the intra-class and inter-class distances of each pair of instances on the aPY dataset (described in Sec. 6.1, BID10 ). As shown in the feature space of Fig. 2 (b) , we compute the pairwise L 2 distances over (1) the instances within the same classes (intra-class), and (2) the instance from different classes (inter-class) on aPY dataset BID10 . We use the empirical density to show the results in Fig. 2 (c) . Practically, it is hard to predict the class labels of instances of the overlapped region in the known/unknown domain. Due to the large overlapped region, the instances (e.g., CMT in BID32 ) whose domains are wrongly labeled will never be correctly categorized. The W-SVM in Eq (5) and Eq (4) estimates the confidence scores by a fixed threshold empirically per-defined for any data distributions in the known domain. However, intrinsically, it can be taken as a model selection task in estimating the boundary by Eq (4). In this paper, we tackle the question of constructing the boundary of the known domain via the bootstrap approach BID8 . The bootstrapping 2 is a strategy of estimating the standard errors and the confidence intervals of parameters when the underlying distributions are unknown. Its procedures are closely related to the other methods such as cross-validation, and jackknife. Bootstrapping is the most widely used tool in approximating the sampling distributions of test statistics and estimators. To facilitate the discussion, we denote the training set of class c as {x its confidence score f c (x i ), c \\u2208 C s . To determine whether the class of a instance x i is seen or unseen, we calculate the statistic m c (x i ) in Eq (4) and Eq (5) with the threshold \\u03b4 c estimated by the bootstrapping algorithm in Alg. 1. The instances computed in the known and unknown domain will be categorized by supervised, or zero-shot classifiers respectively. There are still two difficulties in the above framework. (1) The whole framework relies on the classifier f c (\\u00b7), c \\u2208 C s which is supposed to be robust and well-trained. However, empirically, we can not always train good classifiers for all classes. For example, some class has small number of labeled training instances which are insufficient in training the classifier; some outliers may affect the predictor; the hyper-parameters of the classifiers are wrongly tuned. (2) The naive bootstrapping in Alg. 1 generally provides the bad approximations of the distributions of empirical quantiles in practice BID9 . Practically, in our tasks, we observe that the estimated \\u03b4 c may be consistently too relaxed to determine the boundary of the known domain. We illustrate such a phenomenon in Figure 2(a) : the low-density of seen class bicycle instances (blue points) in the northwest part extends the decision boundary. The relaxed boundary could inadvertently classify unseen instances (red points) as the false positives. Unfortunately, in the framework above, once one testing instance in unseen class is wrongly labeled as the known domain, this instance will never be correctly classified. To address these two problems, we suggest a shrinking step in updating the initial boundary of bootstrapping in the next subsection. TEST The key idea of updating initial boundary of bootstrapping is to validate whether the learned classifier f c (\\u00b7), c \\u2208 C s is trustworthy. Generally, assume the instances of class c independent and identically distributed and provided training samples sufficient, a ideal classifier f c (\\u00b7) should produce the similar confidence score distributions of training and testing instances of class c. The Kolmogorov-Smirnov (K-S) test is an efficient, straightforward, and qualified choice method for comparing distributions Massey Jr (1951); Miller (1956); BID37 . Remarkably, K-S test is a distribution free test, and the statistics of K-S test is effortless to compute. We define the null and alternative hypothesis as DISPLAYFORM0 When H 0 is accepted, it indicates that the f c (\\u00b7) is trustworthy, and the confidence scores of training and testing instances in class c come from the same distribution. We are certain that a large portion of testing instances {z H 0 is rejected, we are not sure whether f c (\\u00b7) is well learned; and the class labels of these testing instances are uncertain. To this end, we introduce a new domain -uncertain domain to include these instances. Uncertain Domain. The labels of instances in the uncertain domain should be labeled as the most likely seen class, or one of unseen classes. Specifically, we can compute the {z c = f c (x)} |Cs| c=1 over all C s classes; and we can obtain, {c , z } = argmax c\\u2208Cs {z c } .The mapping function g (\\u00b7) is learned on the known domain from features x i to its corresponding semantic attributes y i . Given one testing instance x i : if z i is very high, we can confidently predict x i belonging to one of seen classes; otherwise, the label of x i is either in the uncertain or unknown domain. We thus have, DISPLAYFORM1 where y c is semantic prototype of class c; c is the most likely known class to which x i belongs to. Note that in OSL, we only know the y c (c \\u2208 C s ) of seen classes; We can dynamically construct a C t set by randomly generating y i by making sure y i \\u2212 y j > (\\u2200y j \\u2208 C s ), and = min yi,yj \\u2208Cs;i =j y i \\u2212 y j . The sample size is usually the same with the number of target classes. We can apply different recognition algorithms in each domain. In known domain, the standard supervised classifiers can be learned and applied. In unknown and uncertain domains, we propose a simple yet effective feature prototype embedding recognition algorithm as our plain implement. Feature prototype embedding. Once the domain is well separated, we can use the ZSL algorithms to set up the mapping from feature space to semantic/attribute space. In order to confirm that our main contribution is the domain division part, we do not use very complicated ZSL algorithms. Only the simplest linear predictor is utilized here to recognize the unseen classes. Particularly, we use feature prototypes to replace all the instances of each class to avoid the unbalance sample size among classes. We learn a linear predictor to predict the attribute/word vector g (x) = w T \\u00b7 x. The feature prototype embedding is computed as, DISPLAYFORM0 where DISPLAYFORM1 is the feature prototype of class c; y c is the semantic prototype of class c. When we tackle an instance in the unknown or uncertain domain, we need to embed features into semantic space with g, which can infer the class labels of instances: DISPLAYFORM2 where c is the most likely seen class for x i which is computed by the supervised classifier and y c is the semantic prototype. Also, the experiment in each domain can be done with ANY other ZSL. For instance, we report the implement with f-CLSWGAN (f-C) BID40 so that G-ZSL can be done with f-CLSWGAN within a single domain. Experimental settings. Our model is validated in OSL and G-ZSL settings. OSL identifies whether an image belongs to the one of seen classes or the novel class. G-ZSL gives the class label of testing instances either from seen or unseen classes. We set the significance level \\u03b1 = 0.05 to tolerate 5% Type-I error. By default, we use SVM with RBF kernel with parameter cross-validated, unless otherwise specified. We compare against the competitors, including Attribute Baseline (Attr-B), W-SVM Scheirer et al.(2014), One-class SVM Sch\\u00c4lkopf et al. FORMULA1 , Binary SVM, OSDN Bendale & Boult (2016) and LOF Breunig et al. (2000) . The attribute baseline is the variant of our task without using domain division algorithm. Particularly, the Attr-B uses the same semantic space and embedding as our model, but does not leverage domain division step, i.e., use negative samples and prototypes to identify projected instances directly ( Fig. 1 (c) ).We use the metric -F1-measure, which is defined as the harmonic mean of seen class accuracy (specific class) and unseen prediction accuracy (unnecessary to predict the specific class). The results are compared in Tab. 1. Significant performance gain over existing approaches has been observed, in particular for AwA, aPY and ImageNet. This validates the effectiveness of our framework. We attribute the improvement to the newly introduced uncertain domain which help better differentiate whether testing instances derive from known or unknown domain. 6.3 RESULTS OF GENERALIZED ZERO-SHOT LEARNING Settings: We first compare the experiments on G-ZSL by using the settings in BID39 . The results are summarized in Tab. 2. In particular, we further compare the separate settings; and top-1 accuracy in (%) is reported here: (1) S \\u2192 T: Test instances from seen classes, the prediction candidates include both seen and unseen classes; (2) U \\u2192 T: Test instances from unseen classes, the prediction candidates include both seen and unseen classes. FORMULA4 We employ the harmonic mean as the main evaluation metric to further combine the results of both S \\u2192 T and U \\u2192 T, as DISPLAYFORM0 Competitors. We compare several competitors.(1) DAP Lampert et al. FORMULA1 , trains a probabilistic attribute classifier and utilizes the joint probability to predict labels; (2) ConSE BID22 , maps features into the semantic space by convex combination of attributes; (3) CMT Socher et al. 2 Implement with f-C) Results. As seen in Tab. 2, our harmonic mean results are significantly better than all the competitors on almost all the datasets. This shows that ours can effectively address the G-ZSL tasks. Particularly, DISPLAYFORM1 (1) Our plain results can beat other competitors by a large margin on AwA and aPY dataset, due to the efficacy of our domain division algorithm. Also, thanks to the power of f-CLSWGAN (f-C), our implement with it on both CUB and AwA dataset are impressive. FORMULA3 The key advantage of the proposed framework is learning to better divide the testing instances into known, uncertain and unknown domains. In the known domain, we use the standard SVM classifier. In unknown/uncertain domains, we directly embed feature prototypes into semantic space and match the most likely class in the candidate pool. This is the most simple and straightforward recognition method. Thus our good harmonic mean performance on G-ZSL largely comes from the good domain division algorithm. Additionally, we also highlight that the other advanced supervised or zero-shot algorithms are orthogonal and potentially be useful in each domain if we want to further improve the performance of G-ZSL. (3) Our framework is also applied to large-scale datasets in Tab. 3. We compare several state-of-the-art methods that address G-ZSL on the large-scale dataset. We use the SVM with the linear kernel on this dataset, due to the large data scale. Our harmonic mean results surpass the other competitors with a very significant margin. We notice that other algorithms have very poor performance on U \\u2192 T. This indicates the intrinsic difficulty of G-ZSL on large-scale dataset. In contrast, our domain division algorithm can better separate the testing instances into different domains; thus achieving better recognition performance. Additionally, we found that the prediction of ConSE BID22 is heavily biased towards known classes which is consistent with the results in small datasets. This is due to the probability of unseen classes are expressed as the convex combination of seen classes. Usually, there is no higher probability would be assigned to unseen classes than the most probable seen class, especially for large datasets. DISPLAYFORM2 In the ablation study, we report the F1-measure and Harmonic mean for OSL and G-ZSL respectively with our plain implement. As is illustrated in Fig. 2 , we notice that although the distance statistic shows the different histogram patterns in feature space, the overlapping part is not negligible. Importance of bootstrapping the initial threshold. We introduce a variant A of our framework by replacing bootstrapping step (Sec. 4.1) by using Eq (4) and Eq (5) to fix the threshold (i.e., W-SVM Scheirer et al. FORMULA1 ), i.e., K-S test ( \\u221a ), and Bootstrap (\\u00d7). As in Tab. 4, the results of variant A are significantly lower than our framework on all three datasets. This actually directly validates the importance of determining the initial threshold by bootstrapping. Improvements of fine-tuning the threshold by K-S test. We define the variant B is to only use step without fine-tuning the boundary by K-S Test (in Sec. 4.2). TAB5 directly shows the improvement with/without fine-tuning the threshold, i.e., K-S test (\\u00d7), and Bootstrap ( \\u221a ). In particular, we note that variant B has significant lower results on OSL and G-ZSL than variant A and our framework. One reason is that our bootstrapping step actually learns to determine a very wide boundary of the known domain, to make sure the good results in labeling testing instances as unknown domain samples. The fine-tuning threshold step will further split the individual known domain into known/uncertain domain by shrinking the threshold. Without such a fine-tuning step, variant B may wrongly categorize many instances from unseen classes as one of the known classes. Thus, we can show that the two steps of our framework are very complementary to each other and they work as a whole to enable the good performance on OSL and G-ZSL. Finally, we introduce the variant C in Tab. 4, by using W-SVM to do OSL, and then use our ZSL model for G-ZSL, i.e., K-S test (\\u00d7), and Bootstrap (\\u00d7). The performance of variant C is again significantly lower than that of ours, and this demonstrates the efficacy of our model. This paper learns to divide the instances into known, unknown and uncertain domains for the recognition tasks from a probabilistic perspective. The domain division procedure consists of bootstrapping and K-S Test steps. The bootstrapping is used to set an initial threshold for each class; we further employ the K-S test to fine-tune the boundary. Such a domain division algorithm can be used for OSL and G-ZSL tasks, and achieves remarkable results.\",\n          \"Learning a better representation with neural networks is a challenging problem, which has been tackled from different perspectives in the past few years. In this work, we focus on learning a representation that would be useful in a clustering task. We introduce two novel loss components that substantially improve the quality of produced clusters, are simple to apply to arbitrary models and cost functions, and do not require a complicated training procedure. We perform an extensive set of experiments, supervised and unsupervised, and evaluate the proposed loss components on two most common types of models, Recurrent Neural Networks and Convolutional Neural Networks, showing that the approach we propose consistently improves the quality of KMeans clustering in terms of mutual information scores and outperforms previously proposed methods. Representation learning is an important part of deep learning research, and the ability of deep neural networks to transform the input data into a space that is more suitable to the target task is one of the key reasons for their success. Consider the case of binary classification with a neural network with sigmoid activation function on the last layer, where a network transforms the input data x \\u2208 R n into a space R where two classes are linearly separable by applying a sequence of non-linear transformations f (x) : DISPLAYFORM0 Note that all representations, learned by the network in the sequence of transformations R i \\u2192 R j , are devoted to one goal: binary classification. The learned intermediate representations can easily be used in tasks similar to the binary classification, but using them in a different task may be problematic. Consider the case of multivariate time series classification with an RNN model, depicted in Figure 1 with a sigmoid activation function in the last FC 2 layer and a ReLU activation function in the layer FC 1 . Note that ReLU activation produces non-negative vectors. During a regular training procedure with binary cross-entropy loss, the model will learn weights that produce two patterns of activation of the layer FC 1 : roughly orthogonal vectors for the samples that belong to different classes, and roughly parallel vectors for the samples that belong to the same class. Indeed, the value of the output scalar is the result of taking the dot product between the weights w of the final layer FC 2 (a single vector in this case) and the output h of the penultimate hidden layer FC 1 . Via the geometric interpretation of the dot product, this value is highest when the cosine between the vectors 1, and minimized when the cosine is \\u22121. However, since the penultimate layer has the ReLU activation, the vectors cannot point in opposite directions, therefore, they must be orthogonal. T h = max w h cos \\u03b8 \\u21d2 \\u03b8 = 0min w T h, h \\u2265 0 = min w h cos \\u03b8 \\u21d2 \\u03b8 = \\u03c0 2 (2) DISPLAYFORM0 where y i is the corresponding binary label for hidden state h i .In this work, we focus on learning a better representation of the input that could be used in downstream tasks such as clustering. Specifically, we are interested in learning the representation that would enable clustering by virtue of revealing its latent structure, while using the limited information provided by the binary classification task. In order to force the network to learn such diverged representations, we propose two novel loss components that can be applied to an arbitrary cost function and work in both weakly-supervised and unsupervised settings. We evaluate the proposed loss components empirically on two most common types of models, Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) and different types of input data (time series, images, texts). Our approach shows consistent improvement of the quality of KMeans clustering in terms of mutual information scores, outperforming previous methods. Output size: 128 In the past few years, a substantial amount of work has been dedicated to learning a better representation of the input data that can be either used in downstream tasks, such as KMeans clustering, or to improve generalizability or performance of the model. In general, these works can be divided into three categories: (1) approaches that introduce a new loss component that can be easily applied to an arbitrary cost function (discriminative models), (2) approaches that require a complicated or cumbersome training procedure (discriminative models), and (3) probabilistic generative and/or adversarial models. Approaches from the first group propose new loss components that can be applied in a straightforward manner to an arbitrary cost function, supervised or unsupervised. BID1 proposed a cross-covariance penalty (XCov) to force the network to produce representations with disentangled factors. The proposed penalty is, essentially, cross-covariance between the predicted labels and the activations of samples in a batch. Their experiments showed that the network can produce a representation, with components that are responsible to different characteristics of the input data. For example, in case of the MNIST dataset, there was a class-invariant factor that was responsible for the style of the digit, and in case of the Toronto Faces Dataset BID15 , there was a factor responsible for the subject's identity. Similarly, but with a different goal in mind, BID2 proposed a new regularizer (DeCov), that minimizes cross-covariance of hidden activations, leading to non-redundant representations and, consequently, less overfitting and better generalization. DeCov loss is trying to minimize the Frobenius norm of the covariance matrix between all pairs of activations in the given layer. The authors' experiments showed that the proposed loss significantly reduced overfitting and led to a better classification performance on a variety of datasets. The second group of methods requires a modification of the standard training procedure with backpropagation and stochastic gradient descent optimizers. BID10 proposed a method to learn parsimonious representations. Essentially, the proposed algorithm iteratively calculates cluster centroids, which are updated every M iterations and used in the cost function. The authors' experiments showed that such algorithm leads to a better generalization and a higher test performance of the model in case of supervised learning, as well as unsupervised and even zero-shot learning. Similarly, BID17 proposed an iterative algorithm that first calculates soft cluster assignments, then updates the weights of the network and cluster centroids. This process is repeated until convergence. In contrast to BID10 , the authors specifically focused on the task of learning better representations for clustering, and showed that the proposed algorithm gives a significant improvement in clustering accuracy. Finally, a new group of recently emerged methods focus on disentangling the factors of variation (e.g., style and class). proposed deep generative models for semi-supervised learning and showed that is possible to generate samples from the target class with variations in style, and vice versa. BID11 proposed a new approach, called adversarial autoencoder (AAE) and performed a variety of experiments, including semi-supervised and unsupervised clustering, achieving impressive results on MNIST BID9 and Street View House Numbers BID13 datasets. However, since this methods includes adversarial networks, the training of such systems is rather cumbersome. For example, in the semi-supervised autoencoders experiments, the training of the system consisted of three different phases: a reconstruction phase, a regularization phase, and a semi-supervised classification phase, where the regularization phase itself consists of two sub-phases of updating discriminator and generator respectively. Finally, BID12 proposed a conditional generative model that is a combination of Variational Autoencoder BID6 and Generative Adversarial Networks BID4 for disentangling factors of variations. Our proposed loss components belong to the first group and, in contrast to the other methods do not require a complicated training procedure, can easily be used with any cost function, and work in both weakly-supervised and unsupervised settings. Inspired by Equation 1 and the work of BID1 and BID2 , we propose two novel loss components, which despite their simplicity, significantly improve the quality of the clustering over the representations produced by the model. The first loss component L single works on a single layer and does not affect the other layers in the network, which may be a desirable behaviour in some cases. The second loss component L multi affects the entire network behind the target layer and forces it to produce disentangled representations in more complex and deep networks in which the first loss may not give the desired improvements. Consider the model in Figure 1 . The layer FC 2 has output size of 1 and produces a binary classification decision. The output of the layer FC 1 is used to perform KMeans clustering. Recall from the example in the introduction that we want to force the model to produce divergent representations for the samples that belong to the same class, but are in fact substantively different from each other. One way to do it would be to force the rows of the weight matrix W FC1 of the FC 1 layer to be different from each other, leading to different patterns of activations in the output of the FC 1 layer. Formally, it can be expressed as follows: DISPLAYFORM0 where d k are normalized weights of the row k of the weights matrix W of the given layer: DISPLAYFORM1 and DISPLAYFORM2 is a component of the loss between the rows i and j: DISPLAYFORM3 where m is a hyperparameter that defines the desired margin of the loss component and Note that the loss component L single affects only the weights of the specific layer, as it operates not on the outputs of the layer but directly on its weights, similar to, for example, 2 regularization. Therefore, this loss component may help to learn a better representation only if the input to the target layer still contains the information about latent characteristics of the input data. This might be the case in simple shallow networks, but in case of very deep complex networks the input data is nonlinearly transformed so many times that only the information that is needed for binary classification left, and all the remaining latent characteristics of the input data were lost as not important for binary classification (see the Figure 3a) . Indeed, as we can see from the experiments in Section 4, the loss component described above substantially improves the quality of clustering in a simple baseline case. However, in the case of a more complex model, this improvement is much less impressive. Therefore, we also propose a loss component that can influence not only one specific layer, but all layers before it, in order to force the network to produce a better representation. DISPLAYFORM4 Recall again that we want to force the model to produce disentangled representations of the input data. Namely, that these representations should be sufficiently different from each other even if two samples have the same label. We propose the following loss component in order to produce such properties: DISPLAYFORM5 where h s k is a normalized output of the target layer h for the sample k: h DISPLAYFORM6 y k is its the ground truth label, N is the number of samples in the batch, N s is number of samples that have the same label, and f l (h i , h j ) is the function defined in Equation 7. Note that this loss component L multi works on the outputs of the target layer, and therefore, it affects the whole network behind the layer on which it is applied, overcoming the local properties of the L single loss. Although our main focus in the presented experiments is on a binary classification task, both of our proposed loss components can be used in unsupervised learning as well. The loss component L single does not require any labels so it can be used without modifications. The loss component L multi can be applied to unlabeled data by just taking the summations without consideration of labels of the samples as follows: DISPLAYFORM0 For example, as autoencoder models are a common choice to learn representations to use in a downstream task, the proposed loss components can be easily applied to its cost function as follows: DISPLAYFORM1 where the first part is a standard reconstruction cost for autoencoder, the second is the proposed loss component, and \\u03b1 is a hyperparameter reflecting how much importance is given to it. One important choice to be made while using the proposed loss components is the value of the margin hyperparameter m. A larger value of m corresponds to a larger margin between the rows of the weights matrix in case of L single and a larger margin between the activations of the target layer in case of L multi . The smaller the value of m, the less influence the proposed loss components have. In our experiments, we found that the proposed loss component L single is relatively stable with respect to the choice of m, and generally performs better with larger values (in the range 5-10). In case of the loss component L multi , we found that even a small value of the margin m (0.1 -1) disentangles the learned representations better and consequently leads to substantial improvements in the AMI score. In all of the reported experiments, we found that the proposed loss component with a reasonably chosen m does not hurt the model's performance in the classification task. We performed an extensive set of experiments that covers the two most commonly used in modern research: Recurrent Neural Networks and Convolutional Neural Networks, as well as entirely different modalities of the input data: time series, images, and texts. In all experiments, we used an RNN or an CNN model without any additional loss components as the baseline and compared our proposed loss components L single and L multi with the DeCov regularizer BID2 and XCov penalty BID1 , as those works are most similar to ours. After the model were trained on the binary classification task, we use the output of the penultimate layer to perform a KMeans clustering. We implemented the models used in all experiments with TensorFlow BID0 and used Adam optimizer BID5 to train the them. We performed experiments on the MNIST strokes sequences dataset de BID3 2 to evaluate the proposed loss components the in case of an RNN model and time series data. This dataset contains pen strokes, automatically generated from the original MNIST dataset BID9 . Although the generated sequences do not always reflect a choice a human would made in order to write a digit, the strokes are consistent across the dataset. For this experiment, we split the examples into two groups: samples belonging to the classes from 0 to 4 were assigned to the first group, and samples belonging to the classes from 5 to 9 were assigned to the second group. The model is trained to predict the group of a given sample and does not have any access to the underlying classes. We used the model depicted in Figure 1 for this experiment. After the models were trained on the binary classification task, we used the output of the penultimate layer FC 2 to perform the KMeans clustering and evaluated the quality of the produced clustering using the original class labels as ground truth assignments. Autoencoder experiments In order to investigate the influence of the proposed loss components in the autoencoder settings, we applied them to an autoencoder model that reconstructs the input sequences from the MNIST strokes sequences dataset. We did not use any label information during this experiments, and used the representation from the intermediate layer of the autoencoder to perform KMeans clustering. In order to evaluate the proposed loss components on a different type of model and data, we preformed experimented with the CIFAR-10 dataset BID8 using an CNN model. As in the MNIST strokes sequences experiments, we split the examples in two groups: samples belonging to the classes \\\"airplan\\\", \\\"automobile\\\", \\\"bird\\\", \\\"cat\\\", and \\\"deer\\\" were assigned to the first group, and samples belonging to the classes \\\"dog\\\", \\\"frog\\\", \\\"horse\\\", \\\"ship\\\", \\\"truck\\\" were assigned to the second group. Note that this assignment is quite arbitrary as it simply reflects the order of the labels of the classes in the dataset (namely, the labels 0-4 for the first group and the labels 4-9 for the second group). All groups contain rather different types of objects, both natural and human-made. For these experiments, we used a CNN model based on the VGG-16 architecture BID14 , depicted on the FIG2 . We discarded the bottom fully connected and convolutional layers as, perhaps, they are too big for this dataset. Instead, we appended three convolutional layers to the output of pool3 layer with number of filters 256, 128 and 8 correspondingly. The first two layers use 3x3 convolutions, and the last layer uses 1x1 convolutions. After that, we pass the output through a fully-connected layer of size 15 (FC 1 ), which produces the representations used in clustering, and a fully connected layer of size 1 (FC 2 ) with the sigmoid activation function to produce a binary classification decision. Finally, to prove a wide generalizability of the proposed loss components, we performed text classification experiments using an RNN model again, but on an entirely different type of data. Namely, we used the DBPedia ontology dataset dataset BID18 , which contains titles and abstract of Wikipedia articles labeled by 14 ontology classes. Again, we split the samples into two groups and trained the model on the binary classification task. Classes \\\"Company\\\", \\\"EducationalInstitution\\\", \\\"Artist\\\", \\\"Athlete\\\", \\\"OfficeHolder\\\", \\\"MeanOfTransportation\\\", \\\"Building\\\" belong to the first group, and the classes \\\"NaturalPlace\\\", \\\"Village\\\", \\\"Animal\\\", \\\"Plant\\\", \\\"Album\\\", \\\"Film\\\", \\\"WrittenWork\\\" belong to the second group. As in subsection 4.1, we used the model depicted on Figure 1. Despite the fact the proposed loss components can be directly implemented using two nested for loops, such implementation will not be computationally efficient, as it will lead to a big computational graph operating on separate vectors without using full advantages of highly optimized parallel matrix computations on GPU. Therefore, it is desirable to have an efficient implementation that can use full advantage of modern GPUs. We have developed such an efficient implementation that significantly accelerates the computation of the loss component in return for a higher memory consumption by creating two matrices that contain all combinations of d i and d j from the summations in the Equation 5 and performing the operations to calculate the loss on them. We have made our implementation for TensorFlow BID0 publicly available on GitHub 3 alongside with aforementioned models from the subsection 4.1 and the subsection 4.2.It is worth noting that since the loss component L single operates directly on the weights of the target layer, its computational complexity does not depend on the size of the batch. Instead, it depends on the size of that layer. In contrast, the L multi operates on the activations of the target layer on all samples in the batch, and its computational complexity depends on the number of samples in the batch. In practice, using the implementation described above, we were able to train models with batch size of 512 and higher without exhausting the GPU's memory. We report the average of the Adjusted Mutual Information (AMI max ) and Normalized Mutual Information (NMI sqrt ) scores BID16 across three runs in TAB2 . On the simplest MNIST strokes sequences dataset L single outperforms all other methods, whereas on more challenging and complex datasets L milti works the best, probably due to its ability to influence the learned repre- sentations on all layers of the network behind the target layer. The proposed loss components also improves the quality of clustering in the autoencoder settings, although the gain is marginal. It is also important to note that in all our experiments accuracy of models was not affected in a harmful way when we applied the proposed loss components, or the effect was negligible (less than 0.5%). To examine the influence of the proposed loss components to the activations of the network, we plot the number of samples, belonging to different underlying classes on the x axis for which the neurons on the y axis were active the most in the binary classification task on Figure 2 and Figure 3 for on MNIST strokes sequences and CIFAR-10 datasets correspondingly. As we can see from these figures, during a regular training with the binary classification objective, without the proposed loss component the models tend to learn representations that is specific to the target binary label, even though the samples within one group come from different classes. The model learns to use mostly just two neurons to discriminate between the target groups and hardly uses the rest of the neurons in the layer. We observe this behaviour across different types of models and datasets: an RNN model applied to a timeseries dataset and an CNN model applied to an image classification dataset behave in the exactly the same way. Both proposed loss components L single and L multi force the model to produce diverged representations, and we can see how it changes the patterns of activations in the target layer. It is easy to observe in Figure 2b that the patterns of activations learned by the networks roughly correspond to underlying classes, despite the fact that the network did not have access to them during the training. This pattern is not as easy to see in case of CIFAR-10 dataset (see the Figure 3b ), but we can observe that the proposed loss component nevertheless forced the network to activate different neurons for different classes, leading to a better AMI score on the clustering task. In order to further investigate the representations learned by the model, we visualized the representations of samples from the MNIST strokes sequences dataset in FIG3 using TensorBoard. FIG3 and FIG3 in the top row depict the representations learned by the baseline model, colored according to the binary label and the underlying classes, respectively. FIG3 and FIG3 in the bottom row depict the representations of the same samples, learned by the model with the L multi loss component, colored in the same way. It is easy to see that the L multi indeed forced the model to learn disentangled representations of the input data. Note how the baseline model learned dense clusters of objects, with samples from the same group (but different classes) compactly packed in the same area. In contrast, the model with the proposed loss component learned considerably better representations which disentangle samples belonging to different classes and placed the them more uniformly in the space. In the real world, the number of clusters is rarely known beforehand. To systematically examine the stability of the proposed loss component, we plotted the Adjusted Mutual Information scores for the baselines methods and L multi loss component with respect to the number of clusters in FIG5 , using the CIFAR-10 dataset. As can be seen from FIG5 , our loss component consistently outperforms the previously proposed methods regardless the number of clusters. In this paper, we propose two novel loss components that substantially improve the quality of KMeans clustering, which uses representations of the input data learned by a given model. We performed a comprehensive set of experiments using two popular neural network architectures (RNNs and CNNs), and different modalities of data (image and text). Our results demonstrate that the proposed loss components consistently increase the Mutual Information scores by a significant margin, and outperform previously proposed methods. In addition, we qualitatively analyzed the representations learned by the network by visualizing the activation patterns and relative positions of the samples in the learned space, showing that the proposed loss components indeed force the network to learn diverged representations.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"S1eEmn05tQ\",\n          \"H1GaLiAcY7\",\n          \"S17mtzbRb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"A scalable method for learning an expressive prior over neural networks across multiple tasks. The paper presents a method for training a probabilistic model for Multitasks Transfer Learning by introducing a latent variable per task to capture the commonality in the task instances. The work proposes a variational approach to meta-learning that employs latent variables corresponding to task-specific datasets. Aims to learn a prior over neural networks for multiple tasks. \",\n          \" This paper studies the problem of domain division by segmenting instances drawn from different probabilistic distributions.   This paper deals with the problem of novelty recognition in open set learning and generalized zero-shot learning and proposes a possible solution An approach to domain separation based on bootstrapping to identify similarity cutoff thresholds for known classes, followed by a Kolmogorov-Smirnoff test to refine the bootstrapped in-distribution zones. Proposes to introduce a new domain, the uncertain domain, to better handle the division between seen/unseen domains in open-set and generalized zero-shot learning\",\n          \"A novel loss component that forces the network to learn a representation that is well-suited for clustering during training for a classification task. This paper proposes two regularization terms based on a compound hinge loss over the KL divergence between two softmax-normalized input arguments to encourage learning disentangled representations Proposal for two regularizers intended to make the representations learned in the penultimate layer of a classifier more conforming to inherent structure in the data.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Uncertainty in Multitask Transfer Learning\",\n          \"Learning to Separate Domains in Generalized Zero-Shot and Open Set Learning: a probabilistic perspective\",\n          \"Forced Apart: Discovering Disentangled Representations Without Exhaustive Labels\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 66,\n        \"max\": 88,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          70,\n          88,\n          66\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"More recent tools such as deep Gaussian processes BID6 show great potential and yet their scalability whilst learning from multiple tasks needs to be improved. In contrast, our method perform well on these benchmarks.\\u2022 MAML BID11 does not perform well on a collection of sinus tasks when the frequency varies.\\u2022 Prototypical Network BID29 )'s performance decrease considerably when the diversity of tasks increases. We then provide a reduction of the Evidence Lower BOund (ELBO) showing that it is not necessary to explicitly model a distribution in the very high dimension of the weight space of neural networks. To address this issue, we follow the Variational Bayes approach BID4 .Conditioning on \\u03b1, we saw in Equation 1 that the posterior factorizes independently for all tasks. In principle, we can use a different IAF for each task, but for memory and computational reasons, we use a single IAF for all tasks and we condition 4 on an additional task specific context c j .Note that with IAF, we cannot evaluate q \\u03b8j (z j |S j , \\u03b1) for any values of z efficiently, only for these which we just sampled, but this is sufficient for estimating the KL term with a Monte-Carlo approxi-mation i.e.: DISPLAYFORM0 It is common to approximate KL j with a single sample and let the mini-batch average the noise incurred on the gradient. Notice the term n j appearing in front of the likelihood in Equation 7, this indicates that individually for each task it finds the appropriate trade-off between the prior and the observations. However, there is still an important assumption at the heart of deep prior (and other VAE-based approach such as BID10 ): the task information must be encoded in a low dimensional variable z. In Section 5, we show that it is appropriate for regression, but for image classification, it is not the most natural assumption. However most of the literature focuses on simple statistical models and does not consider transferring on new tasks. Additionally, BID28 ) also consider a meta-learning approach where an encoding network reads the training set and generates the parameters of a model, which is trained to perform well on the test set. Many of these approaches BID34 BID29 find a representation where a simple algorithm can produce a classifier from a small training set. To gain a good insight into the behavior of the prior and posterior, we choose a collection of one dimensional regression tasks. For this, we will use periodic functions and test the ability of the regressor to extrapolate outside of its domain. However, note that the uncertainty is over optimistic and that the posterior failed to fully represent all possible harmonics fitting these two points. This is due to the fact that Mini-Imagenet is a very homogeneous collection of tasks where a single representation is sufficient to obtain good results. Also, we found that algorithms based on a single image representation only works well when all tasks can succeed with a very similar set of features. For this purpose we simplified the problem to a single sine function and removed the burden of learning the prior.\",\n          \"This is different from the conventional Zero-Shot Learning (ZSL) task which assumes that, in the testing stage, seen classes would not be misclassified as unseens, and vice versa; ZSL only uses the unseen classes for testing. Unfortunately, the predictors learned on training classes will inevitably make OSL or G-ZSL approaches tend to be biased towards the seen classes, and thus leading to very poor classification results for the novel class (OSL) or unseen classes (G-ZSL) BID39 ; . There is a large overlapping of the distribution of the intraclass and interclass distances.employed to visualize the distributions of the testing instances of the ResNet-101 features in BID39 (Fig. 1 (a) ), and semantic features learned by SAE Kodirov et al. (2017) (Fig. 1 (b) ). Third and even worse, the performance of classifiers in each domain is still very sensitive to the results of domain separation: should the domain of one testing instance be wrongly divided, it would never be correctly categorized by the classifiers. To tackle the aforementioned issues, our key insight (see Fig. 2(a) ) is to introduce a novel domain -uncertain domain that accounts for the overlapping regions of testing instances from seen or novel/unseen classes. Since we do not have the prior knowledge of the underlying data distributions of each class; bootstrapping is introduced here as an asymptotically consistent method in estimating an initial boundary of known classes. Critically, Eq. (2) models the upper boundary distribution of instance confidence scores NOT belonging to class c. The distributions of extreme values defined in Eq (1) and Eq (2) can actually give us the boundary of each event above happened in a probabilistic perspective. Generalizing to C s class is straightforward by training multiple prediction functions {f c (x)}, c = 1, \\u00b7 \\u00b7 \\u00b7 , |C s |.However, there are several key limitations in directly utilizing Eq (4) and Eq (5) of learning the division of domains: (1) Eq (4) directly multiplies two terms and this indicates that there is a potential hypothesis that no correlation exists between E 1 and \\u00acE 2 , which is generally not the case. (3) Furthermore, we give an illustration of the non-negligible overlapping between the intra-class and inter-class distances of each pair of instances on the aPY dataset (described in Sec. 6.1, BID10 ). Practically, in our tasks, we observe that the estimated \\u03b4 c may be consistently too relaxed to determine the boundary of the known domain. We illustrate such a phenomenon in Figure 2(a) : the low-density of seen class bicycle instances (blue points) in the northwest part extends the decision boundary. We are certain that a large portion of testing instances {z H 0 is rejected, we are not sure whether f c (\\u00b7) is well learned; and the class labels of these testing instances are uncertain. When we tackle an instance in the unknown or uncertain domain, we need to embed features into semantic space with g, which can infer the class labels of instances: DISPLAYFORM2 where c is the most likely seen class for x i which is computed by the supervised classifier and y c is the semantic prototype. We notice that other algorithms have very poor performance on U \\u2192 T. This indicates the intrinsic difficulty of G-ZSL on large-scale dataset. In the ablation study, we report the F1-measure and Harmonic mean for OSL and G-ZSL respectively with our plain implement. As is illustrated in Fig. 2 , we notice that although the distance statistic shows the different histogram patterns in feature space, the overlapping part is not negligible.\",\n          \"Output size: 128 In the past few years, a substantial amount of work has been dedicated to learning a better representation of the input data that can be either used in downstream tasks, such as KMeans clustering, or to improve generalizability or performance of the model. The authors' experiments showed that the proposed loss significantly reduced overfitting and led to a better classification performance on a variety of datasets. Inspired by Equation 1 and the work of BID1 and BID2 , we propose two novel loss components, which despite their simplicity, significantly improve the quality of the clustering over the representations produced by the model. The second loss component L multi affects the entire network behind the target layer and forces it to produce disentangled representations in more complex and deep networks in which the first loss may not give the desired improvements. Recall from the example in the introduction that we want to force the model to produce divergent representations for the samples that belong to the same class, but are in fact substantively different from each other. Therefore, this loss component may help to learn a better representation only if the input to the target layer still contains the information about latent characteristics of the input data. DISPLAYFORM4 Recall again that we want to force the model to produce disentangled representations of the input data. The loss component L multi can be applied to unlabeled data by just taking the summations without consideration of labels of the samples as follows: DISPLAYFORM0 For example, as autoencoder models are a common choice to learn representations to use in a downstream task, the proposed loss components can be easily applied to its cost function as follows: DISPLAYFORM1 where the first part is a standard reconstruction cost for autoencoder, the second is the proposed loss component, and \\u03b1 is a hyperparameter reflecting how much importance is given to it. In our experiments, we found that the proposed loss component L single is relatively stable with respect to the choice of m, and generally performs better with larger values (in the range 5-10). In all of the reported experiments, we found that the proposed loss component with a reasonably chosen m does not hurt the model's performance in the classification task. After the model were trained on the binary classification task, we use the output of the penultimate layer to perform a KMeans clustering. We discarded the bottom fully connected and convolutional layers as, perhaps, they are too big for this dataset. To examine the influence of the proposed loss components to the activations of the network, we plot the number of samples, belonging to different underlying classes on the x axis for which the neurons on the y axis were active the most in the binary classification task on Figure 2 and Figure 3 for on MNIST strokes sequences and CIFAR-10 datasets correspondingly. As we can see from these figures, during a regular training with the binary classification objective, without the proposed loss component the models tend to learn representations that is specific to the target binary label, even though the samples within one group come from different classes. Both proposed loss components L single and L multi force the model to produce diverged representations, and we can see how it changes the patterns of activations in the target layer. To systematically examine the stability of the proposed loss component, we plotted the Adjusted Mutual Information scores for the baselines methods and L multi loss component with respect to the number of clusters in FIG5 , using the CIFAR-10 dataset.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "def shuffle_list(text):\n",
        "    lst = text.split('. ')\n",
        "    lst[:-1] = [sentence + '.' for sentence in lst[:-1]]\n",
        "    random.shuffle(lst)\n",
        "    return ' '.join(lst)\n",
        "\n",
        "data['target'] = data['target'].apply(shuffle_list)"
      ],
      "metadata": {
        "id": "03zjLD42tXLy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "31b84ae0-c97f-48b5-aefc-6eba440a5e79"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "f8cb8583-4e08-445a-fcd4-23501b86cb0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "89be31dd-af2a-4a9b-c322-29607b0ff94a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      shuffle=True,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        shuffle=True,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "0b4993b4-456c-4ee1-c7bf-a2ffa9a09640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(647, 8) (162, 8) (809, 8) (203, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenize data\n",
        "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# # Function in order to tokenize source and target\n",
        "# max_input_length = 1024\n",
        "\n",
        "# def tokenize_function(data):\n",
        "#   model_inputs = tokenizer(text=data['extractive_summary'], text_target=data['target'], max_length=max_input_length, truncation=True)\n",
        "#   return model_inputs\n",
        "\n",
        "# tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "VyuuPBgIsJim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "cc6bfb25-4858-49de-9b6c-a51d2d7feb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "b4d28b89ed4648cdbfcfa1dc2d08b7cb",
            "d2b2a5fe93f9415980f831986473d455",
            "3bd92aabd33a453890f6d77c0c9ce58e",
            "b38d08cbb29643628b5a57f18d8c07e0",
            "dbf959cfc5194f3f8631910922da9274",
            "87b05b5bf33f443c9f7f52b9e39ab4f1",
            "1124f43d738941eea0f1b2ee1ec54b40",
            "5e65538d1d0c49b9af091c304166612a",
            "33f64d780a6d45cc97f7b50f2255a6ba",
            "d2f3b7212371421fb43818c2680f8c2b",
            "813bd197893b4b819661b8775e72b314",
            "5f372eecdb7547d1a24daec8cd505b8f",
            "c87abf7642b84ae790b0e746646b28b4",
            "f7fcc25b1d4a415abd27212aa94d09e6",
            "44c2c811c62a4e94bcae4f14f27170a2",
            "3e2b8be15fcc49a2a4afc96956a1de1c",
            "320cab01ec50434b8a812793a37e06bd",
            "faeb3aa6f7664fe998b0b8ed0f51e6cd",
            "6ae179f184f6495d9c768305caeb2178",
            "ab0422df0fe74f169ea0ea6c2bef1a1a",
            "90760b25130b47d9b78007582b9a043d",
            "f2fe25adc28649b6823a1eea840b9b39",
            "f492ec3a35dd438081e6a57fc3d06eb8",
            "9513f84be238471088846023d5e6ad84",
            "d4340e9b05b6494796d94251ba269628",
            "09573227d7ba4a608af773bcb763a571",
            "ea19de9f580c4f4a904cb2f4bf744eab",
            "76ac0c0873234d93afcb930ccb93e3be",
            "0b9caf83693548c7a27a7b69d178b074",
            "c0c97540d5984a29ad5e14d930698000",
            "f77d62be823a47d3acce116e46e86318",
            "b6b266ce44d54509a0201847761d00cc",
            "2f97b2a1032942c9a15e4d716d7ec9d6",
            "69941b69a1e244989afcf0e53aa2d4a2",
            "18bc9f8a06d140cb8d50e2ca81c8b731",
            "86819f5875854efcbbb5c151c06692fe",
            "270b8e1b56da456fa45b67aab3ea5aab",
            "7183b140556d430e8b785391603a643e",
            "468b186d3ce8438abd033520d23e63e3",
            "2fece72c7e06476fab3cb917c66deaf3",
            "51c8162823bd494aa02f3ff0d7ef7404",
            "afdb3d34f3264d0f8ad1b6244edec1af",
            "a8f5258623e948c28535e88a49cd5ccb",
            "ac67f89ed3cc4a05a6a4b1d48abc2a27",
            "7a7371cf82074d2db890323fc76aacb2",
            "743aa9e8a1e941e7b4e3fe657ed0acff",
            "6caf976daaa44b46bf2a49511c835d73",
            "eb24ecc8196a4537b8cbd46491d14af2",
            "85dad04b66c642e099fdf97b57035633",
            "2498e547c967491e809adbb3f07ae203",
            "e2f2e58c939345aca2fac6d087a8c6ba",
            "d9364cc4e3a849b888d661b23d8ce77b",
            "8d60b054454543e987b45cd1721ba735",
            "20213a2ac43b4708ab4c67630f9afe8e",
            "c436cda1061245ad8dbf9e0cf191a0ee",
            "4cfc504e4ded48e98e12e825fbca1041",
            "282d947990f644509171a568a31498dc",
            "2f1ed1e53c4d44a3a23cfd1a29fbb4f8",
            "73cc9fb106554137b1d6315cce7c5a7e",
            "b99a4e7dc8234a4c9bf14ecd11328c7c",
            "f48d93091a4942b6b6148ead197fadf9",
            "ffac937b9a634ef49dd57df0dca106aa",
            "a080cede88b8402d88f59e1b0b31fee6",
            "f5c646dcd6ca445ab40d1286fb3b8f8f",
            "1ce2243e57984549bf2eb04829eb2e5b",
            "25f58819cdf7432ebf55e15b3207eb6e",
            "30200b78063e4e1c9ec387156fa1a14b",
            "9b73afe522ac451088745530d61f89fb",
            "752f6f030fc549bb9e8151cafbd56712",
            "9cbfe0e9a50d4058a3369c586ebc472d",
            "5200594676784c08b8cf0f23e10d30bc",
            "8e4ae7851f9d42859febfc2a4c7b99ee",
            "35bc7f44805a449f850879134d8bdfc6",
            "e9bb9601314849559d013ebf13c77862",
            "6eabb24ff5a34451b68a4c9a5b18ca51",
            "f91d15c638f54ae2970720ef37b565e4",
            "031ddb2105334b04bef6d4bcc5d4371f"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4d28b89ed4648cdbfcfa1dc2d08b7cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f372eecdb7547d1a24daec8cd505b8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f492ec3a35dd438081e6a57fc3d06eb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69941b69a1e244989afcf0e53aa2d4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a7371cf82074d2db890323fc76aacb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cfc504e4ded48e98e12e825fbca1041"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30200b78063e4e1c9ec387156fa1a14b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = True\n",
        "model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "# model.generation_config.renormalize_logits = True\n",
        "\n",
        "model.config.attention_dropout = 0.1\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "name_model = 'sampling-norep-v3/'\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "2160793f-5100-4075-8d55-b3b4c91ed2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXfD8z7vdqC",
        "outputId": "90204fd9-098e-49f2-905d-a0c38187493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartConfig {\n",
              "  \"_name_or_path\": \"facebook/bart-base\",\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"gelu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"BartModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.1,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 768,\n",
              "  \"decoder_attention_heads\": 12,\n",
              "  \"decoder_ffn_dim\": 3072,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"dropout\": 0.1,\n",
              "  \"early_stopping\": true,\n",
              "  \"encoder_attention_heads\": 12,\n",
              "  \"encoder_ffn_dim\": 3072,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_position_embeddings\": 1024,\n",
              "  \"model_type\": \"bart\",\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": true,\n",
              "  \"num_beams\": 4,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"scale_embedding\": false,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 128,\n",
              "      \"min_length\": 12,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_cnn\": {\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 142,\n",
              "      \"min_length\": 56,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_xsum\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 62,\n",
              "      \"min_length\": 11,\n",
              "      \"num_beams\": 6\n",
              "    }\n",
              "  },\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset to inspect the batches\n",
        "for batch in train_dataset.take(100):  # Take the first batch for inspection\n",
        "    print(batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CsKTRNhvqCQ",
        "outputId": "38adf07c-d213-4243-e990-5af2b6feae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3376,  5969, ...,     1,     1,     1],\n",
            "       [    0, 44891,     7, ...,     1,     1,     1],\n",
            "       [    0,     6,   992, ...,    81, 14307,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 39936, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4195, ..., 28695,     5,     2],\n",
            "       [    0, 13863,    89, ...,     1,     1,     1],\n",
            "       [    0, 46797,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16991,     9, ...,     1,     1,     1],\n",
            "       [    0,  9690, 16894, ...,  5342,  2222,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 26039, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 15243,   484, ...,     1,     1,     1],\n",
            "       [    0, 21119,  4945, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  6680, ...,     1,     1,     1],\n",
            "       [    0,   170,  3608, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 29235, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 16215, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 47380, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1106,   215, ...,     8,  1850,     2],\n",
            "       [    0,  3972, 22016, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,   170,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 47302, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 43043, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 33731,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,  4340, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,  6448, ...,     1,     1,     1],\n",
            "       [    0,   387, 35948, ...,     1,     1,     1],\n",
            "       [    0,  1121,   937, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 39231, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   717, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   846,    12, ...,     1,     1,     1],\n",
            "       [    0, 10105,     9, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    28, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 17629, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 47032, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6373, ...,     1,     1,     1],\n",
            "       [    0, 46874,  2088, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 43123, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,  3854,     9,     2],\n",
            "       [    0,   170,    67, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13360, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 13033, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  5709, ...,   230,     6,     2],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,  8269, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709, 25342, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0,  2522,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0, 3684, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  4528,   426, ...,     1,     1,     1],\n",
            "       [    0,   250,   864, ...,     1,     1,     1],\n",
            "       [    0,   170,  2807, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  9437, ...,     1,     1,     1],\n",
            "       [    0, 40450,  9097, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3084, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46498, ...,     1,     1,     1],\n",
            "       [    2,     0, 17105, ...,     1,     1,     1],\n",
            "       [    2,     0, 46444, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,    41, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     5, 14612,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 5320, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9355, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42158, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  6243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 3762,    9, ...,    1,    1,    1],\n",
            "       [   0, 3762,  169, ...,    1,    1,    1],\n",
            "       [   0,  713,   34, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  170, 1455, ...,    1,    1,    1],\n",
            "       [   0,  170,  109, ...,    1,    1,    1],\n",
            "       [   0, 5975,  272, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 42578, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   534, ..., 37357,     5, 23341],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 18377,     5, ...,     1,     1,     1],\n",
            "       [    0, 39936,  1364, ...,     1,     1,     1],\n",
            "       [    0,   133,  4472, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   713,   892, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1966, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 14563, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 30597, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133, 30673, ...,     1,     1,     1],\n",
            "       [    0,   170, 33461, ...,     1,     1,     1],\n",
            "       [    0, 49111,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   243,    16, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42274, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 46692, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   216, ...,     1,     1,     1],\n",
            "       [    0,  9058,  1537, ...,  3854,  6533,     2],\n",
            "       [    0,  2522, 15491, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   510,  8631, ...,     1,     1,     1],\n",
            "       [    0, 45461,  6448, ...,     1,     1,     1],\n",
            "       [    0, 27728,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 46011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 41084, ...,     1,     1,     1],\n",
            "       [    2,     0,   113, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   386, ...,     1,     1,     1],\n",
            "       [    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,   713,  1639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    64, ...,     1,     1,     1],\n",
            "       [    0,   565, 26582, ...,     1,     1,     1],\n",
            "       [    0,  4528,  6448, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1],\n",
            "       [    2,     0,  8532, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,   133,  2731, ...,   141,  1365,     2],\n",
            "       [    0,  5771,   258, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 14246, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       [    0,   170,   492, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 34447, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 48293,  1836, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,  5428, 22098,     2],\n",
            "       [    0,   133,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       [    2,     0, 47744, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12592, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,    12,   170, ...,     1,     1,     1],\n",
            "       [    0,   713,   173, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,  1779,    89, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 40089, 25373, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   448,  7629, ...,     1,     1,     1],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1106,    52, ...,    33,  4163,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 25077, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  2765, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1106,    52, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 45288,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133,   434, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     7,  1807,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0, 9167, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  1197, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   717,  6486, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 33837, 10518, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 18522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,     5, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  7939,  1423, ...,  3278,    63,     2],\n",
            "       [    0,   133,   335, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42489, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 28062, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,   173, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,  1296,   114,     2],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2847,     6, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11321, 20237, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 34647, ...,     1,     1,     1],\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,    52, ...,     1,     1,     1],\n",
            "       [    0, 21461,    11, ...,     1,     1,     1],\n",
            "       [    0,  2765,  4655, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  4442, ...,     1,     1,     1],\n",
            "       [    0, 45408, 19047, ...,     1,     1,     1],\n",
            "       [    0,   713,  1548, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 6179, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0, 2709, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 38386,    10, ...,     1,     1,     1],\n",
            "       [    0,  4528, 15716, ...,     1,     1,     1],\n",
            "       [    0,   448,    36, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1850, ...,     1,     1,     1],\n",
            "       [    0, 35416,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 14484, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  9344, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3813,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,     5, ...,     1,     1,     1],\n",
            "       [    0, 44863,  1319, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,    84, ...,     1,     1,     1],\n",
            "       [    0,   133,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36984, ...,     1,     1,     1],\n",
            "       [    2,     0, 20930, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,   819, 21154,     2],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1106,   854, ...,     1,     1,     1],\n",
            "       [    0,  1213,    67, ...,  4091, 48981,     2],\n",
            "       [    0,   170,   694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   102, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 34788, ...,     1,     1,     1],\n",
            "       [    2,     0, 35660, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0, 1121,  171, ...,  347,   12,    2],\n",
            "       [   0, 1121, 1285, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  713,   16, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170, 9637, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 33020, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42713,     6, ...,     1,     1,     1],\n",
            "       [    0,  4771,  3109, ...,     1,     1,     1],\n",
            "       [    0, 44908,  4843, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35716,    87, ...,    11, 37365,     2],\n",
            "       [    0,   133, 39135, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13755, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  5393, ...,     1,     1,     1],\n",
            "       [    0,  1121,  6477, ...,     1,     1,     1],\n",
            "       [    0,   243,    64, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 30872,   724, ...,     1,     1,     1],\n",
            "       [    0, 12444,   857, ...,     1,     1,     1],\n",
            "       [    0,  9690,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  5448, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1],\n",
            "       [    0, 18377,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,  9157, 16771, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,  6647, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   102, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43253, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 30770,     6, ...,     1,     1,     1],\n",
            "       [    0,   170, 17013, ...,     1,     1,     1],\n",
            "       [    0,   133,  1850, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1],\n",
            "       [    0,   133, 13477, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 23996, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4286, ...,     1,     1,     1],\n",
            "       [    0, 44311,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,   817, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5383, 38416, ...,     1,     1,     1],\n",
            "       [    0,  1779,  3563, ...,     9,   230,     2],\n",
            "       [    0, 13863,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  1109, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2765, 24097, ...,     1,     1,     1],\n",
            "       [    0, 23055,  8738, ...,     1,     1,     1],\n",
            "       [    0,  2522,  5694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 18776, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40103, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 14721,  9179, ...,     1,     1,     1],\n",
            "       [    0,   170,  6053, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0, 13863,  3326, ...,    16,   888,     2],\n",
            "       [    0, 43872,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42124, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0,  250, 4819, ...,    1,    1,    1],\n",
            "       [   0, 2709, 4327, ...,    1,    1,    1],\n",
            "       [   0,  713,  173, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  133, 5849, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170,  311, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 24514, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42489, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,  2655, 20992,     2],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 43195,  7651, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,  1236,    15,     2],\n",
            "       [    0,  1121,  1524, ..., 45371,    15,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44188, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38416, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0, 13863,    51, ...,     1,     1,     1],\n",
            "       [    0,  3762,  1860, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 40846, ...,     1,     1,     1],\n",
            "       [    0,  1620,    52, ...,     1,     1,     1],\n",
            "       [    0,  5771,   171, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 13360,    12, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 27477, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43780, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0, 39936,   775, ...,    52,    33,     2],\n",
            "       ...,\n",
            "       [    0,  1342,  4458, ...,     1,     1,     1],\n",
            "       [    0,  3908,     5, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ..., 19282,     6,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 36949,    41, ...,     1,     1,     1],\n",
            "       [    0,  4688,   419, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,    42, ...,   775,    36,     2],\n",
            "       [    0,  9344,  1938, ...,     1,     1,     1],\n",
            "       [    0,   170,  7015, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 1694, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  5203, ...,     1,     1,     1],\n",
            "       [    0, 39531,  4400, ...,     1,     1,     1],\n",
            "       [    0, 45297,    15, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 15491, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 36542, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0,  3908,  2284, ...,   922,  4791,     2],\n",
            "       ...,\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 30597, 10244, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 48313, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   173, ...,     1,     1,     1],\n",
            "       [    0, 40566,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  1548, ...,     1,     1,     1],\n",
            "       [    0, 23271,     9, ..., 42472, 26070,     2],\n",
            "       [    0, 48454,    12, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41933, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   170,   240, ...,     1,     1,     1],\n",
            "       [    0, 43714,    40, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0,   133,   485, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  9685, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45336, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   486, ...,     1,     1,     1],\n",
            "       [    0,   133, 28894, ...,     1,     1,     1],\n",
            "       [    0, 38386,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   970,    16, ...,  3364,     5,     2],\n",
            "       [    0,   713, 12360, ...,     1,     1,     1],\n",
            "       [    0,  1121,   485, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  1034, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0, 29182,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0, 18377,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,   170,  2883, ...,     1,     1,     1],\n",
            "       [    0,   170,  2639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  5848, ...,    36, 13424,     2],\n",
            "       [    0, 44863,    31, ...,     1,     1,     1],\n",
            "       [    0, 48684,   680, ..., 20145,  4007,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 49360,    11, ...,  6068,   600,     2],\n",
            "       [    0, 45942,  6448, ...,     1,     1,     1],\n",
            "       [    0, 30383, 26713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 41542,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1285, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45356, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40884, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1213,  1157, ..., 20910,    73,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0,  1779,  1058, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 39972,    52, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 28588, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     1,     1,     1],\n",
            "       [    0, 20319,  2408, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   800, ...,     1,     1,     1],\n",
            "       [    0,  2709,    55, ...,     1,     1,     1],\n",
            "       [    0, 10653,   428, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 243, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    97, ...,     1,     1,     1],\n",
            "       [    0,  4528, 41885, ...,     1,     1,     1],\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,   170,  1455, ...,     1,     1,     1],\n",
            "       [    0,  1620,    41, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10127, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   243, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250,  1353, ...,     1,     1,     1],\n",
            "       [    0,   713,  3315, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  1989, ..., 14612, 26070,     2],\n",
            "       [    0,  3972,  1100, ...,     1,     1,     1],\n",
            "       [    0,  5320, 10074, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44466, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44863, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   133, 32809, ...,     1,     1,     1],\n",
            "       [    0,     6,  3023, ...,  1558, 15421,     2],\n",
            "       ...,\n",
            "       [    0, 10653,   428, ...,     1,     1,     1],\n",
            "       [    0, 20861, 44871, ...,     1,     1,     1],\n",
            "       [    0,   713,   839, ...,     8,    63,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   347, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 48455, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,     1,     1,     1],\n",
            "       [    0, 21438,   520, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522, 11909, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,   133, 16681, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,   936, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  4528,  8369, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 27331,   937, ...,     1,     1,     1],\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 21680, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  6209,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 15393, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0, 20086, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3972,     5, ...,     1,     1,     1],\n",
            "       [    0,   170, 24934, ...,     1,     1,     1],\n",
            "       [    0,  2522, 39030, ...,  3907,     4,     2],\n",
            "       ...,\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       [    0,   170,   892, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     6,   549,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   176, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 47515, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45566, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   574,  3439, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   163, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,    43,   396,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133, 15306, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ..., 12612,   534,     2],\n",
            "       [    0,  3972,  1306, ...,     1,     1,     1],\n",
            "       [    0, 19847,  1239, ...,  6315, 36173,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4993, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 46159, 43141, ...,     1,     1,     1],\n",
            "       [    0, 10777,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121, 14117, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 13863,  1337, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42200,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 565, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 28084,     7, ...,     1,     1,     1],\n",
            "       [    0,  1121,   144, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 38416,    29, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,  1121,  5709, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 25382, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,   511, 22772,     2],\n",
            "       [    0,  3762,     9, ...,     1,     1,     1],\n",
            "       [    0,   133,   986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   172, ...,     1,     1,     1],\n",
            "       [    0,  3972, 33942, ...,   892,  2939,     2],\n",
            "       [    0, 45875,     6, ...,    31,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 1121, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  717, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 15393, ...,    11,   130,     2],\n",
            "       [    0, 20867,  7316, ...,     1,     1,     1],\n",
            "       [    0,   713,  5665, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  9058, 24454, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44426, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771, 10364, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,   209, ...,     1,     1,     1],\n",
            "       [    0,   133,  8611, ...,     1,     1,     1],\n",
            "       [    0,  3972, 19893, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44908, ...,     1,     1,     1],\n",
            "       [    2,     0, 34002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 19186, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23803,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133,   538, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0,  5771,   144, ...,     1,     1,     1],\n",
            "       [    0,  2765,  2623, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 19163, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  3034, ...,     1,     1,     1],\n",
            "       [    0,  1121,  2171, ...,     1,     1,     1],\n",
            "       [    0,  3972,     5, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0, 17425, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   448, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   387,   293, ...,    16,   505,     2],\n",
            "       [    0, 21518,  1537, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 45628, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2409,   114, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288,  8150, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,    45,   946,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,   211, ...,   683,    36,     2],\n",
            "       [    0,   250,   194, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1620,   251, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,   740,     6,     2],\n",
            "       [    0,  1121,   103, ...,   255,     8,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 28062, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17312, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38741, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 35166, 37700, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,     1,     1,     1],\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 24989,  7373, ...,     1,     1,     1],\n",
            "       [    0,  9157, 37794, ...,     1,     1,     1],\n",
            "       [    0,   717,  4182, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10928, ...,     1,     1,     1],\n",
            "       [    2,     0, 15622, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4897, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   387,   293, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771,   419, ...,   468,   321,     2],\n",
            "       ...,\n",
            "       [    0,   530,   495, ...,     1,     1,     1],\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,  2522, 40150, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 9685, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4554, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11913, 26739, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35490,     5, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       [    0,   713,  5044, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   338, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,  1365, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,  1121,   171, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 48812,  2577, ...,    14, 20070,     2],\n",
            "       [    0,  5771,   608, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,    14,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 10516, ...,     1,     1,     1],\n",
            "       [    0,   713,  1421, ...,   163,  2688,     2],\n",
            "       [    0,  3762,    16, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,   133,  2270, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12444, ...,     1,     1,     1],\n",
            "       [    2,     0, 22011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250, 17309, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9322, ...,     1,     1,     1],\n",
            "       [    0,  3762,   169, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288, 15380, ...,     1,     1,     1],\n",
            "       [    0,   133,  7626, ...,     1,     1,     1],\n",
            "       [    0,   170,   304, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 26412, ...,     1,     1,     1],\n",
            "       [    2,     0, 46101, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,    10, ...,   204,     6,     2],\n",
            "       [    0,  9690,  1202, ...,     1,     1,     1],\n",
            "       [    0,  7605,   209, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2571,  6018, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   250, 31809, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23295, 37465, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3762, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1620,    10, ...,     1,     1,     1],\n",
            "       [    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0,   170,    40, ..., 13956,  1916,     2],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 19192,    52, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(7, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  1000, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "96a37571-a85b-46d0-822c-15337ad2a4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_model"
      ],
      "metadata": {
        "id": "dyGROt7TwXn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "fec0f7cd-d17d-4c56-fb64-c523f8dc25b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n",
            "81/81 [==============================] - 3557s 44s/step - loss: 3.8539 - val_loss: 3.3430 - rouge1: 38.3698 - rouge2: 10.1688 - rougeL: 22.2593 - rougeLsum: 31.8182 - gen_len: 88.4506\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3040s 38s/step - loss: 3.4917 - val_loss: 3.2807 - rouge1: 40.0500 - rouge2: 10.8711 - rougeL: 22.9560 - rougeLsum: 33.0063 - gen_len: 82.0000\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3153s 39s/step - loss: 3.3288 - val_loss: 3.2417 - rouge1: 39.2427 - rouge2: 10.4373 - rougeL: 22.8725 - rougeLsum: 32.7607 - gen_len: 82.4815\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3104s 39s/step - loss: 3.1818 - val_loss: 3.2276 - rouge1: 40.0325 - rouge2: 11.1820 - rougeL: 23.2123 - rougeLsum: 33.2422 - gen_len: 84.0309\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3291s 41s/step - loss: 3.0478 - val_loss: 3.2148 - rouge1: 40.4019 - rouge2: 10.9217 - rougeL: 23.2547 - rougeLsum: 33.4422 - gen_len: 85.5432\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3194s 40s/step - loss: 2.9362 - val_loss: 3.2262 - rouge1: 39.8779 - rouge2: 10.3569 - rougeL: 22.9050 - rougeLsum: 32.9253 - gen_len: 81.9753\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3124s 39s/step - loss: 2.8306 - val_loss: 3.2327 - rouge1: 40.3397 - rouge2: 10.9453 - rougeL: 23.1704 - rougeLsum: 33.6539 - gen_len: 83.5309\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3146s 39s/step - loss: 2.7277 - val_loss: 3.2288 - rouge1: 39.9447 - rouge2: 10.5412 - rougeL: 22.8769 - rougeLsum: 33.1718 - gen_len: 85.0062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "f09db19d-85fd-44b8-d76e-3ee5168e9405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c25434ec-c808-43da-b3df-579bfce352b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "699df823-a6bf-4496-f7a2-78909e82cba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {'sampling-norep-v0' : 'sampling-norep-v0/',\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              #  'greedy-norep-v3' : 'greedy-norep-v3/',\n",
        "               'sampling-norep-v3' : 'sampling-norep-v3'}\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Tjj2cjL2mE",
        "outputId": "184d644e-2892-4f36-e503-667824846e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v0/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2,\n",
            "  \"max_length\": 150,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.2,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'\n",
        "save_path = save_paths[name]\n",
        "\n",
        "with open(save_path + '/training_history.json', 'r') as file:\n",
        "    loaded_history = json.load(file)\n",
        "\n",
        "H = History()\n",
        "H.history = loaded_history\n",
        "\n",
        "\n",
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/figure.png')"
      ],
      "metadata": {
        "id": "CEUKdl4cdUPx",
        "outputId": "dd6c46b3-440c-4144-9a18-40cbf4278f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'"
      ],
      "metadata": {
        "id": "AQseeydBMTlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:5],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][6:8],\n",
        "                                        generation_config=model.generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "DXuVco4oyC_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j6DcwmJShR",
        "outputId": "f366e627-2a4f-4cae-9161-273836d3ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet framework for in-and out-of-distribution classification. This paper proposes a variational approach to solve the uncertainty estimation problem in deep neural networks by considering the label-level distribution of image input and output labels. The authors propose a new uncertainty metric for deep neural network classification that is more robust than existing uncertainty measures. This article proposes a novel variational method for solving uncertainty estimation on deep neural nets.',\n",
              " 'An unsupervised method for analyzing the contribution of individual neurons to NMT models. This paper proposes a novel approach to interpret language pairs using neural networks, and proposes a new method for measuring the contributions of each neuron to the model. The authors propose a novel method for learning languages from neural networks that can be used to control the translation performance of language pairs.',\n",
              " 'A deep diagonal-circulant ReLU network that can be decomposed into products of diagonal and circulant matrices This paper proposes to replace the weight matrix of a fully connected layer with a new type of matrix. The authors propose a method for building deep ReLU networks based on a combination of diagonal/circular matrices with low rank approximators in order to improve performance.',\n",
              " 'Explicit cognitive theory or analogy-like computation in neural networks This paper proposes a novel approach to solving complex examples of visual and symbolic representations. The authors propose an approach to the problem of visual analogy by proposing a new model that learns to contrast abstract relational structures with visual representations. This work proposes a method for learning to compare different representations of objects, which can be used to solve complex analogical problems such as visual analogy.',\n",
              " 'A novel concept annotation task for medical time series data. This paper proposes a novel method of predicting and localizing medical concepts by modeling the medical context data as input. The authors propose a novel approach to the problem of identifying medical concepts in medical time-series data, which can be used to predict and localize medical concepts. This article introduces a novel framework for understanding medical concepts using medical context information.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMr_neO6pJNK",
        "outputId": "25d85c44-b7e6-4820-927c-54279cb4c854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The study proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This study proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The article investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TReOxk1fk1cx",
        "outputId": "eea11b79-2cc4-44e3-9121-a817e0d65fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The article proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This article proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = False\n",
        "model.generation_config.temperature = 1\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "                                          tokenizer.convert_tokens_to_ids('Ġpropose'),\n",
        "                                           tokenizer.convert_tokens_to_ids('Ġproposes'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "\n",
        "model.generation_config.num_beam_groups = 4\n",
        "model.generation_config.diversity_penalty = 0.7\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.3\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r4mAa8_-ql",
        "outputId": "a4abf904-55d4-42b6-9cbe-7b5095a3ea61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"diversity_penalty\": 0.7,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.3,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    15393,\n",
            "    21037,\n",
            "    32687\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('they proposes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EysouFNL26dN",
        "outputId": "decc56fd-e82f-48df-89e2-9b621383fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['they', 'Ġproposes']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisXx8nuVDQ4",
        "outputId": "f70b8744-cbe0-4319-fc58-4bc80c0ff137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: 0/202\n",
            "[INFO]: 10/202\n",
            "[INFO]: 20/202\n",
            "[INFO]: 30/202\n",
            "[INFO]: 40/202\n",
            "[INFO]: 50/202\n",
            "[INFO]: 60/202\n",
            "[INFO]: 70/202\n",
            "[INFO]: 80/202\n",
            "[INFO]: 90/202\n",
            "[INFO]: 100/202\n",
            "[INFO]: 110/202\n",
            "[INFO]: 120/202\n",
            "[INFO]: 130/202\n",
            "[INFO]: 140/202\n",
            "[INFO]: 150/202\n",
            "[INFO]: 160/202\n",
            "[INFO]: 170/202\n",
            "[INFO]: 180/202\n",
            "[INFO]: 190/202\n",
            "[INFO]: 200/202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFgiii-sbTK4",
        "outputId": "41684db4-b656-4232-d3b5-51f1c72b8369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    170\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"early_stopping\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"repetition_penalty\": 1.8,\n",
              "  \"suppress_tokens\": [\n",
              "    1698,\n",
              "    32687\n",
              "  ]\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bNZCUqSvN7NV",
        "outputId": "bc3472f2-1429-4206-dcf8-b0d504c53db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/greedy-norep-v5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "beb7244c-9a96-46aa-e5a8-66046cfd17bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       1.420792\n",
              "std       18.729447\n",
              "min      -39.000000\n",
              "25%      -12.000000\n",
              "50%        0.500000\n",
              "75%       14.000000\n",
              "max       53.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 6\n",
        "print('ORIGINAL:' + tokenized_data['test']['target'][i])\n",
        "print('FINE TUNED MODEL:' + tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[i])\n",
        "print('PRETRAINED MODEL:' + tokenizer.batch_decode(pretrained_generated_ids, skip_special_tokens=True)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzU40R-ALuzq",
        "outputId": "7e6ad700-beac-4a0d-f100-174353d2d63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL:OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work\n",
            "FINE TUNED MODEL:Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. We show that Outlier Exposure can improve calibration performance in this realistic setting.\n",
            "PRETRAINED MODEL:However, when there is a distribution mismatch, deep neural network classifiers tend to give\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 28\n",
        "print(summaries.iloc[m, 0])\n",
        "print(summaries.iloc[m, 1])"
      ],
      "metadata": {
        "id": "lcWyZXvGLKcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1428815a-8c86-41c0-b4c1-7c39c492acdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space.  This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.\n",
            "We extend the reinforcement learning paradigm to a d-dimensional hypercube and show that quantile regression is capable of training orders of magnitudes faster in high dimensional metric spaces. This paper proposes a method to train a deep neural network to approximate the quantile function of the optimal action distribution. The authors propose a new reinforcement learning algorithm to train convolutional neural networks with quantile functions, showing that it can be used to train orders of magnitude faster on vector rewards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "-_h7Z8KBp09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small', errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 512\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "MwmAPTxGp28R",
        "outputId": "15d72dfc-c581-4263-bf36-a603bdae1fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "27b9b6005f5b46f9b5c303bf37877c39",
            "6bfc2e5ae9cd472ca9ac49b8744d630a",
            "6186710be4644795b62da8f5c586bba4",
            "c45669993f11488d86d6938d0b32d4f1",
            "91a4b86267ec4585ad2aa13ca0e87bfe",
            "210b2833d2f14aba9b284ccf98c39b80",
            "31082b98fd474800a7887ef2a91d6739",
            "65a7128c40d7412ca728488ae0d92281",
            "8611318bf21c453883ab71e7407ef845",
            "7a13238efc04431497e9479ec09f713d",
            "26cfca531d81444b9a8da85b9112343e",
            "ae904c0afcf448668e446591b46327f2",
            "00ce80c14d2c4a778fb4791e20cd3d49",
            "34086af13e764d8586a5bb8244d40096",
            "fb36800bd5074ed4bf563b6c33b0381d",
            "da9a50cd3f8348f1ae49e9743e1b5097",
            "0466d9c4830a4e5594df41baf6067c27",
            "e1609f376ee9456a89c0c78ff256ddb7",
            "72f3dcc1172e47dbae6c88e5ab144639",
            "55c88cc127a24ea58c629149130ce29c",
            "df600ae2684b453cbc3dee1562301fab",
            "60df4c9674ca491489f13760935af392",
            "f529573287ea4fa9bc71688e983d035d",
            "8c365ebc9a554db99eab64f409693ebb",
            "02dfa71d72ce4f0d8f9951b7febdd878",
            "89930e4dae6f4a78980854b143b78365",
            "38e65e56f4cb4d5f8b8a24bcedadadac",
            "1a9aacd88a764fd994e6677cee734cb1",
            "7ed8cc6a846b49fc997d2598f77e8f91",
            "666a346e9cc04d4899390c241cb851c1",
            "52ca486a2ee24ff590d420f3e7c9a524",
            "978f89b8dd064dbbb73116c24e508731",
            "412b20600d4e4ea8aafad6ffd770c379",
            "81b4cde36d644a22be7cf88e1f4d8235",
            "78f863dbd28e4709a530e685afa7e3b4",
            "ef4a7d03d5c44373b6660853ca3de3a9",
            "66bbca9efa3d4292b2a86fb5e888a73a",
            "1da72f2ebf8849eb99e04a4cd94232c0",
            "5549954a24ed42e9b018fee3bdcfc862",
            "8e619ab01e5b4eae8bbfefdcb6a51abf",
            "cd6718f592d6427cb1acca3c0d0c4f2b",
            "672817b7dac6487f8b2f07117e0bf885",
            "048e7ad8c97744fe9c1f644c9aef68c6",
            "6c025683c8824caab558e94c8d4456ab",
            "a0dc3a4aaf0c49b8867847c8a3dea225",
            "bc2843765e1145d6b69dac291837a195",
            "e28208986046499e96c16f0e51aca6a5",
            "72458bf758f74174904ad1dbee3d87cc",
            "97da3ace30904546bd42abdf2ded52f4",
            "fe7a3943e12b41c09a00c2b3f0976327",
            "34889af974d94aa58ec9ba1d2d925f27",
            "d0fff40b95ac41cc8501f6834139a8a8",
            "10e443085c92421c8039026a9ec3fde2",
            "de83b1bd03234e0895e7801e336f0977",
            "489e173a0f9d423aa6ebe21b2d45dfd6",
            "8f340715989b4150a09b7d5a4e42a68d",
            "2d852c17b415428c9f23942eb85af5cd",
            "aad43bdd0bdc4140bf19652524d16293",
            "9d9f062f1cfb4fa482ab21a4e5b7e3f3",
            "ba6d0c8fbab5440a86d0ee712ba495ba",
            "d326570eeef141b6a71f8aba5a948e72",
            "b5c55c7d0b4646acbb55f5827c0b1f1a",
            "1309dddb5ec04f9092015a1f80cf777a",
            "0e25af4b878d4a15930fea3337a415df",
            "5dda1f6c845a4ca1b255cb75bfceb3be",
            "2ef0870d6cd946c58a1fe5ab4dfd77f0"
          ]
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27b9b6005f5b46f9b5c303bf37877c39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae904c0afcf448668e446591b46327f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f529573287ea4fa9bc71688e983d035d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b4cde36d644a22be7cf88e1f4d8235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0dc3a4aaf0c49b8867847c8a3dea225"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f340715989b4150a09b7d5a4e42a68d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "\n",
        "model.generation_config.max_length = 100\n",
        "model.generation_config.min_length = 80\n",
        "model.generation_config.num_beams = 4\n",
        "model.generation_config.length_penalty = 2.0\n",
        "\n",
        "\n",
        "# model.generation_config.do_sample = True\n",
        "# model.generation_config.temperature = 0.5\n",
        "bad_words = ['We', 'we', \"propose\", 'authors']\n",
        "model.generation_config.bad_words_ids=tokenizer(bad_words, add_special_tokens=False).input_ids\n",
        "# model.generation_config.suppress_tokens = [\n",
        "#     # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "#                                           # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "name_model = 'greedy-norep-v1/'\n",
        "\n",
        "print(model.config)\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "_mad6Hc1ttRX",
        "outputId": "2e44c5f3-6d7c-4f00-eeaf-583614ff2a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8de9b74fbb2849ae8bad2054b457ad27",
            "bbff7e02ea7c487a8ff99599f5cec581",
            "b259df54bd694f069dd0f724910020fc",
            "c4040aa13b444733a632bad7ab3cadcc",
            "581135a155304b28a9f5a874dac77643",
            "fc96af9798df470e9cbd18323172bded",
            "c1ccc49a65244d10af06029878d6f7fe",
            "e591d76340be48b9a717da7e88088bf5",
            "40abdb48af2d45789a91592a51ab219e",
            "55a331a537314e2ea375cdc9f5da39a2",
            "e67dab0677d14fb698aa944e04fbffcf",
            "acd42a53c4dc4302998359008a0892a7",
            "23c759d031854eef8580a5817896f765",
            "4dce043c3cd94c288c47866d9906d58d",
            "20f444d4fa784c6aaaa1ce8bba3944fe",
            "4cb73c57973745f1942bea2478f58daa",
            "389fd726e58b433c9b8fea10e133a179",
            "e47c2a209e6d449690c8d793191a71e0",
            "6097f2fe68224f09bf19f047cf90de9c",
            "d598cdf492ab40999f5a63e4cf40e43f",
            "40ac1964298d48a79fd7fdf9df6a7502",
            "7b92ed131f084b5982768125a37698b0"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8de9b74fbb2849ae8bad2054b457ad27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acd42a53c4dc4302998359008a0892a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"google-t5/t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "GenerationConfig {\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      101\n",
            "    ],\n",
            "    [\n",
            "      62\n",
            "    ],\n",
            "    [\n",
            "      4230\n",
            "    ],\n",
            "    [\n",
            "      5921\n",
            "    ]\n",
            "  ],\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"diversity_penalty\": 0.5,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 100,\n",
            "  \"min_length\": 80,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 1.8\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "6XdKgWLNwgRD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "6PGwmRZ2yS78"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yvPZ6vubyYl1",
        "outputId": "b00f4faa-c14f-4890-f4c0-eb8628dfded2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  16449536  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  35330816  \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  41625344  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60506624 (230.81 MB)\n",
            "Trainable params: 60506624 (230.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + name_model"
      ],
      "metadata": {
        "id": "HceVCymny0eR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "p_wTT5KMyi86"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 30\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "VeSspMbVyp_q",
        "outputId": "1db1f39d-418f-4ddf-d633-69bf81e654fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7fecfaac7d90> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7fecfaac7d90> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - 2525s 31s/step - loss: 4.3423 - val_loss: 2.4052 - rouge1: 29.2941 - rouge2: 5.1047 - rougeL: 16.6952 - rougeLsum: 23.6122 - gen_len: 91.7222\n",
            "Epoch 2/30\n",
            "81/81 [==============================] - 2387s 30s/step - loss: 2.5262 - val_loss: 2.2012 - rouge1: 29.5408 - rouge2: 5.4763 - rougeL: 16.8709 - rougeLsum: 23.6416 - gen_len: 90.3704\n",
            "Epoch 3/30\n",
            "81/81 [==============================] - 2454s 31s/step - loss: 2.2226 - val_loss: 2.1392 - rouge1: 30.7530 - rouge2: 5.8785 - rougeL: 17.4531 - rougeLsum: 24.5442 - gen_len: 91.7469\n",
            "Epoch 4/30\n",
            "81/81 [==============================] - 2449s 31s/step - loss: 2.0407 - val_loss: 2.0969 - rouge1: 31.5789 - rouge2: 6.1595 - rougeL: 17.9123 - rougeLsum: 25.1060 - gen_len: 91.8086\n",
            "Epoch 5/30\n",
            "81/81 [==============================] - 2457s 31s/step - loss: 1.9592 - val_loss: 2.0651 - rouge1: 31.7063 - rouge2: 6.1397 - rougeL: 17.8763 - rougeLsum: 25.3624 - gen_len: 91.1173\n",
            "Epoch 6/30\n",
            "81/81 [==============================] - 2432s 30s/step - loss: 1.9111 - val_loss: 2.0403 - rouge1: 32.5285 - rouge2: 6.4553 - rougeL: 18.4778 - rougeLsum: 26.3889 - gen_len: 90.9136\n",
            "Epoch 7/30\n",
            "81/81 [==============================] - 2618s 33s/step - loss: 1.8738 - val_loss: 2.0193 - rouge1: 33.9764 - rouge2: 7.0239 - rougeL: 19.1468 - rougeLsum: 27.3758 - gen_len: 91.3765\n",
            "Epoch 8/30\n",
            "81/81 [==============================] - 2476s 31s/step - loss: 1.8487 - val_loss: 2.0001 - rouge1: 33.7493 - rouge2: 6.8762 - rougeL: 19.0695 - rougeLsum: 27.1478 - gen_len: 90.6358\n",
            "Epoch 9/30\n",
            "81/81 [==============================] - 2491s 31s/step - loss: 1.8250 - val_loss: 1.9836 - rouge1: 34.8342 - rouge2: 7.0412 - rougeL: 19.6666 - rougeLsum: 28.1535 - gen_len: 91.0000\n",
            "Epoch 10/30\n",
            "81/81 [==============================] - 2488s 31s/step - loss: 1.8049 - val_loss: 1.9690 - rouge1: 34.9179 - rouge2: 7.4060 - rougeL: 19.5364 - rougeLsum: 28.1451 - gen_len: 91.3086\n",
            "Epoch 11/30\n",
            "81/81 [==============================] - 2459s 31s/step - loss: 1.7907 - val_loss: 1.9550 - rouge1: 34.8221 - rouge2: 7.3745 - rougeL: 19.5166 - rougeLsum: 28.5675 - gen_len: 90.5617\n",
            "Epoch 12/30\n",
            "81/81 [==============================] - 2565s 32s/step - loss: 1.7737 - val_loss: 1.9443 - rouge1: 34.8110 - rouge2: 7.5478 - rougeL: 19.4081 - rougeLsum: 28.2546 - gen_len: 91.4198\n",
            "Epoch 13/30\n",
            "81/81 [==============================] - 2533s 32s/step - loss: 1.7589 - val_loss: 1.9337 - rouge1: 35.1414 - rouge2: 7.5863 - rougeL: 19.6511 - rougeLsum: 28.6192 - gen_len: 91.8519\n",
            "Epoch 14/30\n",
            "81/81 [==============================] - 2485s 31s/step - loss: 1.7450 - val_loss: 1.9255 - rouge1: 35.1212 - rouge2: 7.6074 - rougeL: 19.7350 - rougeLsum: 28.9983 - gen_len: 92.6358\n",
            "Epoch 15/30\n",
            "81/81 [==============================] - 2452s 31s/step - loss: 1.7315 - val_loss: 1.9176 - rouge1: 35.2364 - rouge2: 7.7789 - rougeL: 19.7926 - rougeLsum: 28.8973 - gen_len: 92.8889\n",
            "Epoch 16/30\n",
            "81/81 [==============================] - 2510s 31s/step - loss: 1.7220 - val_loss: 1.9107 - rouge1: 35.2891 - rouge2: 7.9291 - rougeL: 20.0158 - rougeLsum: 29.0782 - gen_len: 92.6049\n",
            "Epoch 17/30\n",
            "81/81 [==============================] - 2533s 32s/step - loss: 1.7143 - val_loss: 1.9048 - rouge1: 35.3028 - rouge2: 7.6947 - rougeL: 19.8848 - rougeLsum: 28.7566 - gen_len: 92.4321\n",
            "Epoch 18/30\n",
            "81/81 [==============================] - 2444s 31s/step - loss: 1.7029 - val_loss: 1.8998 - rouge1: 35.4522 - rouge2: 8.0794 - rougeL: 19.9789 - rougeLsum: 28.9959 - gen_len: 92.6420\n",
            "Epoch 19/30\n",
            "81/81 [==============================] - 2383s 30s/step - loss: 1.6928 - val_loss: 1.8950 - rouge1: 35.3874 - rouge2: 7.9949 - rougeL: 19.5866 - rougeLsum: 28.7344 - gen_len: 92.7963\n",
            "Epoch 20/30\n",
            "81/81 [==============================] - 2512s 31s/step - loss: 1.6843 - val_loss: 1.8896 - rouge1: 35.7366 - rouge2: 8.1378 - rougeL: 19.9689 - rougeLsum: 28.9632 - gen_len: 92.6049\n",
            "Epoch 21/30\n",
            "81/81 [==============================] - 2442s 30s/step - loss: 1.6775 - val_loss: 1.8860 - rouge1: 36.0606 - rouge2: 7.9124 - rougeL: 20.4265 - rougeLsum: 29.2671 - gen_len: 93.2593\n",
            "Epoch 22/30\n",
            "81/81 [==============================] - 2551s 32s/step - loss: 1.6706 - val_loss: 1.8817 - rouge1: 35.9857 - rouge2: 8.0945 - rougeL: 20.2373 - rougeLsum: 29.1073 - gen_len: 92.6111\n",
            "Epoch 23/30\n",
            "81/81 [==============================] - 2381s 30s/step - loss: 1.6639 - val_loss: 1.8785 - rouge1: 35.9384 - rouge2: 8.0422 - rougeL: 20.4651 - rougeLsum: 29.3445 - gen_len: 93.9074\n",
            "Epoch 24/30\n",
            "81/81 [==============================] - 2368s 30s/step - loss: 1.6538 - val_loss: 1.8752 - rouge1: 36.2085 - rouge2: 8.1606 - rougeL: 20.4365 - rougeLsum: 29.5609 - gen_len: 93.9753\n",
            "Epoch 25/30\n",
            "81/81 [==============================] - 2664s 33s/step - loss: 1.6456 - val_loss: 1.8720 - rouge1: 36.3367 - rouge2: 8.2687 - rougeL: 20.3637 - rougeLsum: 29.7577 - gen_len: 93.5617\n",
            "Epoch 26/30\n",
            "81/81 [==============================] - 2493s 31s/step - loss: 1.6376 - val_loss: 1.8693 - rouge1: 36.3287 - rouge2: 7.8971 - rougeL: 20.0677 - rougeLsum: 29.4013 - gen_len: 92.6790\n",
            "Epoch 27/30\n",
            "81/81 [==============================] - 2357s 29s/step - loss: 1.6350 - val_loss: 1.8666 - rouge1: 36.2762 - rouge2: 8.1158 - rougeL: 20.2910 - rougeLsum: 29.6342 - gen_len: 93.9136\n",
            "Epoch 28/30\n",
            "81/81 [==============================] - 2374s 30s/step - loss: 1.6244 - val_loss: 1.8640 - rouge1: 36.7023 - rouge2: 8.1579 - rougeL: 20.3862 - rougeLsum: 29.8861 - gen_len: 93.3951\n",
            "Epoch 29/30\n",
            "81/81 [==============================] - 2383s 30s/step - loss: 1.6223 - val_loss: 1.8617 - rouge1: 36.8272 - rouge2: 8.2257 - rougeL: 20.5599 - rougeLsum: 29.8867 - gen_len: 93.1296\n",
            "Epoch 30/30\n",
            "81/81 [==============================] - 2426s 30s/step - loss: 1.6153 - val_loss: 1.8595 - rouge1: 36.2232 - rouge2: 8.1085 - rougeL: 19.9293 - rougeLsum: 29.6269 - gen_len: 93.7222\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v1/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v1/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v1/spiece.model',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/greedy-norep-v1/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "VMgzhMiysJ1o",
        "outputId": "a0971184-93a6-479b-9c24-dffb2dd24e9b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config"
      ],
      "metadata": {
        "id": "-dD2Lsh95H9a",
        "outputId": "bbc87417-43d5-402e-86b0-e85953026ff7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    1326\n",
              "  ],\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"repetition_penalty\": 1.8\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = ['We',\"we\", \"propose\", 'authors']\n",
        "bad_words_ids=tokenizer(bad_words, add_special_tokens=False).input_ids"
      ],
      "metadata": {
        "id": "T5Tvn0_691lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "KqOLYlN5AsTr",
        "outputId": "4d5b00f3-388c-4ce6-f8ed-14792d8310ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This article introduces a variational Dirichlet algorithm for deep neural network classification problem. The paper presents a new method for detecting out-of-distribution images by using the label-wise distribution P(y) over the simplex S k. This article examines the image classification problem with image input as x and output label as y. A variational dirichlet framework that can greatly widen the distance between in-and out of-',\n",
              " 'This study introduces a new method for analyzing the contribution of individual neurons to NMT models by modifying the activation of the tense neurons from the previous section TAB2. The paper presents a novel translation control technique that can be used to predict tokens inside/outside of parentheses, quotes, or brackets. This work explores the role of linguistically interpretable neurons in translating neural networks into vector representations.',\n",
              " 'This study introduces a new neural network architecture that can approximate a deep ReLU network in which the dense matrices are of low rank. This article presents a novel method for compressing the network by reducing the memory, at the level of individual weights, and using sparse representations BID8 BID9 BID19 to replace the weight matrix of a fully connected layer with more compact circulant layers from fully connected layers.',\n",
              " 'This study introduces a new neural network that learns from random candidate answers by using the same domain as the correct answer. A model trained with random candidates performs very poorly on the more challenging contrasting test questions, which suggests that models trained in the normal regime overfit to a strategy that bears no resemblance to human-like analogical reasoning. The paper presents a novel approach for comparing abstract relational structures to semantically plausible ones.',\n",
              " 'This article introduces a concept annotation task for the medical time series data, which is used to predict and localize medical concepts by using the time series dataset as input. This work presents a method for predicting and localizing medical concepts with the help of generating text summaries such as trends from time series Data. The paper introduces an approach to generating medical concepts that can be used to identify medical concepts in a given time series setting.',\n",
              " 'This article introduces a multi-way encoding approach that obstructs the adversarial gradient search. This study presents a new attack for a binary classification problem, which uses a one-hot (1of K) encoder to train a model to misclassify an input from the green ground-truth class to the blue class, or vice versa.',\n",
              " 'This article introduces a new network that can detect patterns regardless of how they are rotated over the sphere. This paper presents a way to define the spherical correlation in the higher layers of a CNN BID4 by using a rotation operator L R that takes a function f and produces a rotated function L R f by composing it with the rotation R DISPLAYFORM1.',\n",
              " 'This study introduces a new model for multi-agent systems that can be used to predict the forward dynamics of multiple agents, and provides insights into the inner workings of each agent. The paper presents a class of analysis tools for understanding the behavior of different agents and how agents learn to operate in them. This work explores the behavioral dynamics of multi-Agent systems, which are ill-equipped to characterize various agents as a whole.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_words = ['We',\"we\", \"propose\"]\n",
        "tokenizer(bad_words, add_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74DE1w0W4abC",
        "outputId": "2ae18c5c-0195-4d43-8b40-915e2190b583"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 1], [62, 1], [4230, 1]], 'attention_mask': [[1, 1], [1, 1], [1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('authors')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97pM4w9azMcD",
        "outputId": "0a61c3c2-6d92-4aae-8552-23b73dc76202"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁authors']"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_ids(['_We'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztw1MN_aup37",
        "outputId": "bcd9dbb6-0143-4b12-f9c8-c847cfc50129"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"We is an example sentence.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "tokenized_input = tokenizer(input_text, return_tensors=\"tf\")\n",
        "\n",
        "# Print the tokenized input\n",
        "print(\"Tokenized input:\", tokenized_input)\n",
        "\n",
        "# Generate output based on the tokenized input\n",
        "output = model.generate(**tokenized_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl577YvIwQAs",
        "outputId": "fbf5fb02-a2e8-498a-8efd-b26e58366ee9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized input: {'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[ 101,   19,   46,  677, 7142,    5,    1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "8eNz9_UkxorM",
        "outputId": "a66c086c-4698-422a-ae3c-8159bfa709c1"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a example example sentence for example. The example sentence is an example sentence, and it is based on a simple example sentence in this example sentence. It is one example sentence of example sentence to a sentence that is used as a examples sentence for Example Example Example For Example Example In An Example Example To Example Example A Example Example Examples Example Example Uses Example Example sentences.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_ids(['▁We',\n",
        " '▁use',\n",
        " '▁it',\n",
        " 'er',\n",
        " 'ative',\n",
        " '▁pruning',\n",
        " '▁as',\n",
        " '▁',\n",
        " 'a',\n",
        " '▁proxy',\n",
        " '▁for',\n",
        " '▁the',\n",
        " '▁speed',\n",
        " '▁at',\n",
        " '▁which',\n",
        " '▁',\n",
        " 'a',\n",
        " '▁network',\n",
        " '▁learn',\n",
        " 's',\n",
        " '.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8TYcDO5wHw8",
        "outputId": "204b397a-93ac-4197-8be4-700555a0f738"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 169,\n",
              " 34,\n",
              " 49,\n",
              " 1528,\n",
              " 31858,\n",
              " 38,\n",
              " 3,\n",
              " 9,\n",
              " 19784,\n",
              " 21,\n",
              " 8,\n",
              " 1634,\n",
              " 44,\n",
              " 84,\n",
              " 3,\n",
              " 9,\n",
              " 1229,\n",
              " 669,\n",
              " 7,\n",
              " 5]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2BXUHuhyWyyh",
        "outputId": "ec7115b9-52d5-49bb-aa6a-e433dce70cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. the watermark must be secure against adversarial attacks and leave no tangible footprints in the target DNN. a retraining procedure resembles 'adversarial training' BID16. a retraining procedure resembles 'adversarial training' BID16.\",\n",
              " '',\n",
              " '',\n",
              " 'we hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the BatchNorm layer. we consider a standard classification task for data, having underlying distribution denoted as D, over the pair of examples x  R n and corresponding true labels y  1, 2,..., k where k represents different labels.',\n",
              " 'a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive.',\n",
              " '',\n",
              " 'DPQ-VQ is a more evenly distributed code utilization, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:8]"
      ],
      "metadata": {
        "id": "xNqaRwbOzUVD",
        "outputId": "8f0c5a88-4ccc-461f-f339-335d52d8e792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This paper presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The paper proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This paper proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The article addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.',\n",
              " 'We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This paper proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines',\n",
              " 'We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts This article focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up',\n",
              " \"Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations\"]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.do_lower_case"
      ],
      "metadata": {
        "id": "6QMQKmjqFkHL",
        "outputId": "4ed990f5-4663-46a2-cebf-d851ddb2d147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'T5Config' object has no attribute 'do_lower_case'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1d79c395fff9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'T5Config' object has no attribute 'do_lower_case'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('hello')\n"
      ],
      "metadata": {
        "id": "-MTiOKDZGJRk",
        "outputId": "b0bfdba3-3d3b-4840-ae0d-6c9550086d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21820, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('Hello')"
      ],
      "metadata": {
        "id": "Glz2s6NaHWTa",
        "outputId": "9847b24e-40cd-4d82-8889-69b9d5219511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8774, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmf68b93HqIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlOtOkxuR6vrZoQ4V4+J8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9100de41ec6946c495c032b05d90913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40d035dada3347e2963c183d035d445c",
              "IPY_MODEL_ecd31a14240f408b82b5224d97694ab7",
              "IPY_MODEL_7afeab237ced4f01b70361e2a3ce7552",
              "IPY_MODEL_5b7408314641483980402784564e62e6",
              "IPY_MODEL_c4a0eef84a5e4061884e43d2a09da02a"
            ],
            "layout": "IPY_MODEL_b114a3fe857949408305dde0c9436974"
          }
        },
        "40d035dada3347e2963c183d035d445c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "placeholder": "​",
            "style": "IPY_MODEL_52801cc2a7a646109c4a7141ef768900",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ecd31a14240f408b82b5224d97694ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_db74bb5ffc4f4900b17aaf842a618019",
            "placeholder": "​",
            "style": "IPY_MODEL_6deef8837ace4d94b5b803666ff4aa5a",
            "value": ""
          }
        },
        "7afeab237ced4f01b70361e2a3ce7552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2e00c125b4d942368e702682002f94ff",
            "style": "IPY_MODEL_571b4b08342141c798092b6b23ab410a",
            "value": true
          }
        },
        "5b7408314641483980402784564e62e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ca46452e63a34d99865718ab86726748",
            "style": "IPY_MODEL_493ec2745ddf4bf2bdce528219a1b309",
            "tooltip": ""
          }
        },
        "c4a0eef84a5e4061884e43d2a09da02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c8673d4bac474d9c5799e3ee02bf6b",
            "placeholder": "​",
            "style": "IPY_MODEL_f44df68731c14e24914d1c8816c8edce",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b114a3fe857949408305dde0c9436974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f2ddbf8c4aaf42fba6f967bf32d45ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52801cc2a7a646109c4a7141ef768900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db74bb5ffc4f4900b17aaf842a618019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6deef8837ace4d94b5b803666ff4aa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e00c125b4d942368e702682002f94ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571b4b08342141c798092b6b23ab410a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca46452e63a34d99865718ab86726748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "493ec2745ddf4bf2bdce528219a1b309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d3c8673d4bac474d9c5799e3ee02bf6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44df68731c14e24914d1c8816c8edce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7af630999e4b40f6a160b5f1b2ba5fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7fb664ef7274f14bcc1abce5cb6037e",
              "IPY_MODEL_a41b2cb057d74491a2a739bc994c3bbe",
              "IPY_MODEL_1a457d6d7236467aafa5b8238fc14767"
            ],
            "layout": "IPY_MODEL_8dd47d575244433385fe683b503bcc54"
          }
        },
        "b7fb664ef7274f14bcc1abce5cb6037e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4d7e908b14b40eda3892d03411119f6",
            "placeholder": "​",
            "style": "IPY_MODEL_ef02dbb55af04b2db8b33c10b44561dd",
            "value": "Downloading builder script: "
          }
        },
        "a41b2cb057d74491a2a739bc994c3bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff10561189024511af8a953c42345f13",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_268acb4ba1974301a643be4d3e36101c",
            "value": 2169
          }
        },
        "1a457d6d7236467aafa5b8238fc14767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f152197af28746e0be359c10d6207030",
            "placeholder": "​",
            "style": "IPY_MODEL_b130c8b9e88b4333983ccc93d745f6df",
            "value": " 5.65k/? [00:00&lt;00:00, 378kB/s]"
          }
        },
        "8dd47d575244433385fe683b503bcc54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4d7e908b14b40eda3892d03411119f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef02dbb55af04b2db8b33c10b44561dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff10561189024511af8a953c42345f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "268acb4ba1974301a643be4d3e36101c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f152197af28746e0be359c10d6207030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b130c8b9e88b4333983ccc93d745f6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4d28b89ed4648cdbfcfa1dc2d08b7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2b2a5fe93f9415980f831986473d455",
              "IPY_MODEL_3bd92aabd33a453890f6d77c0c9ce58e",
              "IPY_MODEL_b38d08cbb29643628b5a57f18d8c07e0"
            ],
            "layout": "IPY_MODEL_dbf959cfc5194f3f8631910922da9274"
          }
        },
        "d2b2a5fe93f9415980f831986473d455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b05b5bf33f443c9f7f52b9e39ab4f1",
            "placeholder": "​",
            "style": "IPY_MODEL_1124f43d738941eea0f1b2ee1ec54b40",
            "value": "vocab.json: 100%"
          }
        },
        "3bd92aabd33a453890f6d77c0c9ce58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e65538d1d0c49b9af091c304166612a",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33f64d780a6d45cc97f7b50f2255a6ba",
            "value": 898823
          }
        },
        "b38d08cbb29643628b5a57f18d8c07e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2f3b7212371421fb43818c2680f8c2b",
            "placeholder": "​",
            "style": "IPY_MODEL_813bd197893b4b819661b8775e72b314",
            "value": " 899k/899k [00:00&lt;00:00, 8.37MB/s]"
          }
        },
        "dbf959cfc5194f3f8631910922da9274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b05b5bf33f443c9f7f52b9e39ab4f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1124f43d738941eea0f1b2ee1ec54b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e65538d1d0c49b9af091c304166612a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33f64d780a6d45cc97f7b50f2255a6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2f3b7212371421fb43818c2680f8c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "813bd197893b4b819661b8775e72b314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f372eecdb7547d1a24daec8cd505b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c87abf7642b84ae790b0e746646b28b4",
              "IPY_MODEL_f7fcc25b1d4a415abd27212aa94d09e6",
              "IPY_MODEL_44c2c811c62a4e94bcae4f14f27170a2"
            ],
            "layout": "IPY_MODEL_3e2b8be15fcc49a2a4afc96956a1de1c"
          }
        },
        "c87abf7642b84ae790b0e746646b28b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_320cab01ec50434b8a812793a37e06bd",
            "placeholder": "​",
            "style": "IPY_MODEL_faeb3aa6f7664fe998b0b8ed0f51e6cd",
            "value": "merges.txt: 100%"
          }
        },
        "f7fcc25b1d4a415abd27212aa94d09e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ae179f184f6495d9c768305caeb2178",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab0422df0fe74f169ea0ea6c2bef1a1a",
            "value": 456318
          }
        },
        "44c2c811c62a4e94bcae4f14f27170a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90760b25130b47d9b78007582b9a043d",
            "placeholder": "​",
            "style": "IPY_MODEL_f2fe25adc28649b6823a1eea840b9b39",
            "value": " 456k/456k [00:00&lt;00:00, 16.8MB/s]"
          }
        },
        "3e2b8be15fcc49a2a4afc96956a1de1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "320cab01ec50434b8a812793a37e06bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faeb3aa6f7664fe998b0b8ed0f51e6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ae179f184f6495d9c768305caeb2178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab0422df0fe74f169ea0ea6c2bef1a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90760b25130b47d9b78007582b9a043d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2fe25adc28649b6823a1eea840b9b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f492ec3a35dd438081e6a57fc3d06eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9513f84be238471088846023d5e6ad84",
              "IPY_MODEL_d4340e9b05b6494796d94251ba269628",
              "IPY_MODEL_09573227d7ba4a608af773bcb763a571"
            ],
            "layout": "IPY_MODEL_ea19de9f580c4f4a904cb2f4bf744eab"
          }
        },
        "9513f84be238471088846023d5e6ad84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ac0c0873234d93afcb930ccb93e3be",
            "placeholder": "​",
            "style": "IPY_MODEL_0b9caf83693548c7a27a7b69d178b074",
            "value": "tokenizer.json: 100%"
          }
        },
        "d4340e9b05b6494796d94251ba269628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c97540d5984a29ad5e14d930698000",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f77d62be823a47d3acce116e46e86318",
            "value": 1355863
          }
        },
        "09573227d7ba4a608af773bcb763a571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6b266ce44d54509a0201847761d00cc",
            "placeholder": "​",
            "style": "IPY_MODEL_2f97b2a1032942c9a15e4d716d7ec9d6",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 12.4MB/s]"
          }
        },
        "ea19de9f580c4f4a904cb2f4bf744eab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ac0c0873234d93afcb930ccb93e3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9caf83693548c7a27a7b69d178b074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0c97540d5984a29ad5e14d930698000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f77d62be823a47d3acce116e46e86318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6b266ce44d54509a0201847761d00cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f97b2a1032942c9a15e4d716d7ec9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69941b69a1e244989afcf0e53aa2d4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18bc9f8a06d140cb8d50e2ca81c8b731",
              "IPY_MODEL_86819f5875854efcbbb5c151c06692fe",
              "IPY_MODEL_270b8e1b56da456fa45b67aab3ea5aab"
            ],
            "layout": "IPY_MODEL_7183b140556d430e8b785391603a643e"
          }
        },
        "18bc9f8a06d140cb8d50e2ca81c8b731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468b186d3ce8438abd033520d23e63e3",
            "placeholder": "​",
            "style": "IPY_MODEL_2fece72c7e06476fab3cb917c66deaf3",
            "value": "config.json: 100%"
          }
        },
        "86819f5875854efcbbb5c151c06692fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c8162823bd494aa02f3ff0d7ef7404",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afdb3d34f3264d0f8ad1b6244edec1af",
            "value": 1716
          }
        },
        "270b8e1b56da456fa45b67aab3ea5aab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f5258623e948c28535e88a49cd5ccb",
            "placeholder": "​",
            "style": "IPY_MODEL_ac67f89ed3cc4a05a6a4b1d48abc2a27",
            "value": " 1.72k/1.72k [00:00&lt;00:00, 147kB/s]"
          }
        },
        "7183b140556d430e8b785391603a643e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468b186d3ce8438abd033520d23e63e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fece72c7e06476fab3cb917c66deaf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51c8162823bd494aa02f3ff0d7ef7404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdb3d34f3264d0f8ad1b6244edec1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8f5258623e948c28535e88a49cd5ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac67f89ed3cc4a05a6a4b1d48abc2a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a7371cf82074d2db890323fc76aacb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_743aa9e8a1e941e7b4e3fe657ed0acff",
              "IPY_MODEL_6caf976daaa44b46bf2a49511c835d73",
              "IPY_MODEL_eb24ecc8196a4537b8cbd46491d14af2"
            ],
            "layout": "IPY_MODEL_85dad04b66c642e099fdf97b57035633"
          }
        },
        "743aa9e8a1e941e7b4e3fe657ed0acff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2498e547c967491e809adbb3f07ae203",
            "placeholder": "​",
            "style": "IPY_MODEL_e2f2e58c939345aca2fac6d087a8c6ba",
            "value": "Map: 100%"
          }
        },
        "6caf976daaa44b46bf2a49511c835d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9364cc4e3a849b888d661b23d8ce77b",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d60b054454543e987b45cd1721ba735",
            "value": 647
          }
        },
        "eb24ecc8196a4537b8cbd46491d14af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20213a2ac43b4708ab4c67630f9afe8e",
            "placeholder": "​",
            "style": "IPY_MODEL_c436cda1061245ad8dbf9e0cf191a0ee",
            "value": " 647/647 [00:04&lt;00:00, 145.46 examples/s]"
          }
        },
        "85dad04b66c642e099fdf97b57035633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2498e547c967491e809adbb3f07ae203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2f2e58c939345aca2fac6d087a8c6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9364cc4e3a849b888d661b23d8ce77b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d60b054454543e987b45cd1721ba735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20213a2ac43b4708ab4c67630f9afe8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c436cda1061245ad8dbf9e0cf191a0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cfc504e4ded48e98e12e825fbca1041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_282d947990f644509171a568a31498dc",
              "IPY_MODEL_2f1ed1e53c4d44a3a23cfd1a29fbb4f8",
              "IPY_MODEL_73cc9fb106554137b1d6315cce7c5a7e"
            ],
            "layout": "IPY_MODEL_b99a4e7dc8234a4c9bf14ecd11328c7c"
          }
        },
        "282d947990f644509171a568a31498dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f48d93091a4942b6b6148ead197fadf9",
            "placeholder": "​",
            "style": "IPY_MODEL_ffac937b9a634ef49dd57df0dca106aa",
            "value": "Map: 100%"
          }
        },
        "2f1ed1e53c4d44a3a23cfd1a29fbb4f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a080cede88b8402d88f59e1b0b31fee6",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5c646dcd6ca445ab40d1286fb3b8f8f",
            "value": 162
          }
        },
        "73cc9fb106554137b1d6315cce7c5a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce2243e57984549bf2eb04829eb2e5b",
            "placeholder": "​",
            "style": "IPY_MODEL_25f58819cdf7432ebf55e15b3207eb6e",
            "value": " 162/162 [00:01&lt;00:00, 130.84 examples/s]"
          }
        },
        "b99a4e7dc8234a4c9bf14ecd11328c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48d93091a4942b6b6148ead197fadf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffac937b9a634ef49dd57df0dca106aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a080cede88b8402d88f59e1b0b31fee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5c646dcd6ca445ab40d1286fb3b8f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ce2243e57984549bf2eb04829eb2e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f58819cdf7432ebf55e15b3207eb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30200b78063e4e1c9ec387156fa1a14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b73afe522ac451088745530d61f89fb",
              "IPY_MODEL_752f6f030fc549bb9e8151cafbd56712",
              "IPY_MODEL_9cbfe0e9a50d4058a3369c586ebc472d"
            ],
            "layout": "IPY_MODEL_5200594676784c08b8cf0f23e10d30bc"
          }
        },
        "9b73afe522ac451088745530d61f89fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e4ae7851f9d42859febfc2a4c7b99ee",
            "placeholder": "​",
            "style": "IPY_MODEL_35bc7f44805a449f850879134d8bdfc6",
            "value": "Map: 100%"
          }
        },
        "752f6f030fc549bb9e8151cafbd56712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9bb9601314849559d013ebf13c77862",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6eabb24ff5a34451b68a4c9a5b18ca51",
            "value": 203
          }
        },
        "9cbfe0e9a50d4058a3369c586ebc472d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f91d15c638f54ae2970720ef37b565e4",
            "placeholder": "​",
            "style": "IPY_MODEL_031ddb2105334b04bef6d4bcc5d4371f",
            "value": " 203/203 [00:01&lt;00:00, 133.77 examples/s]"
          }
        },
        "5200594676784c08b8cf0f23e10d30bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4ae7851f9d42859febfc2a4c7b99ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35bc7f44805a449f850879134d8bdfc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9bb9601314849559d013ebf13c77862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eabb24ff5a34451b68a4c9a5b18ca51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f91d15c638f54ae2970720ef37b565e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031ddb2105334b04bef6d4bcc5d4371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27b9b6005f5b46f9b5c303bf37877c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bfc2e5ae9cd472ca9ac49b8744d630a",
              "IPY_MODEL_6186710be4644795b62da8f5c586bba4",
              "IPY_MODEL_c45669993f11488d86d6938d0b32d4f1"
            ],
            "layout": "IPY_MODEL_91a4b86267ec4585ad2aa13ca0e87bfe"
          }
        },
        "6bfc2e5ae9cd472ca9ac49b8744d630a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210b2833d2f14aba9b284ccf98c39b80",
            "placeholder": "​",
            "style": "IPY_MODEL_31082b98fd474800a7887ef2a91d6739",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6186710be4644795b62da8f5c586bba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65a7128c40d7412ca728488ae0d92281",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8611318bf21c453883ab71e7407ef845",
            "value": 2324
          }
        },
        "c45669993f11488d86d6938d0b32d4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a13238efc04431497e9479ec09f713d",
            "placeholder": "​",
            "style": "IPY_MODEL_26cfca531d81444b9a8da85b9112343e",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 184kB/s]"
          }
        },
        "91a4b86267ec4585ad2aa13ca0e87bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "210b2833d2f14aba9b284ccf98c39b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31082b98fd474800a7887ef2a91d6739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65a7128c40d7412ca728488ae0d92281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8611318bf21c453883ab71e7407ef845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a13238efc04431497e9479ec09f713d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26cfca531d81444b9a8da85b9112343e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae904c0afcf448668e446591b46327f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00ce80c14d2c4a778fb4791e20cd3d49",
              "IPY_MODEL_34086af13e764d8586a5bb8244d40096",
              "IPY_MODEL_fb36800bd5074ed4bf563b6c33b0381d"
            ],
            "layout": "IPY_MODEL_da9a50cd3f8348f1ae49e9743e1b5097"
          }
        },
        "00ce80c14d2c4a778fb4791e20cd3d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0466d9c4830a4e5594df41baf6067c27",
            "placeholder": "​",
            "style": "IPY_MODEL_e1609f376ee9456a89c0c78ff256ddb7",
            "value": "spiece.model: 100%"
          }
        },
        "34086af13e764d8586a5bb8244d40096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f3dcc1172e47dbae6c88e5ab144639",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55c88cc127a24ea58c629149130ce29c",
            "value": 791656
          }
        },
        "fb36800bd5074ed4bf563b6c33b0381d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df600ae2684b453cbc3dee1562301fab",
            "placeholder": "​",
            "style": "IPY_MODEL_60df4c9674ca491489f13760935af392",
            "value": " 792k/792k [00:00&lt;00:00, 10.5MB/s]"
          }
        },
        "da9a50cd3f8348f1ae49e9743e1b5097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0466d9c4830a4e5594df41baf6067c27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1609f376ee9456a89c0c78ff256ddb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72f3dcc1172e47dbae6c88e5ab144639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55c88cc127a24ea58c629149130ce29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df600ae2684b453cbc3dee1562301fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60df4c9674ca491489f13760935af392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f529573287ea4fa9bc71688e983d035d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c365ebc9a554db99eab64f409693ebb",
              "IPY_MODEL_02dfa71d72ce4f0d8f9951b7febdd878",
              "IPY_MODEL_89930e4dae6f4a78980854b143b78365"
            ],
            "layout": "IPY_MODEL_38e65e56f4cb4d5f8b8a24bcedadadac"
          }
        },
        "8c365ebc9a554db99eab64f409693ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a9aacd88a764fd994e6677cee734cb1",
            "placeholder": "​",
            "style": "IPY_MODEL_7ed8cc6a846b49fc997d2598f77e8f91",
            "value": "tokenizer.json: 100%"
          }
        },
        "02dfa71d72ce4f0d8f9951b7febdd878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_666a346e9cc04d4899390c241cb851c1",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52ca486a2ee24ff590d420f3e7c9a524",
            "value": 1389353
          }
        },
        "89930e4dae6f4a78980854b143b78365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978f89b8dd064dbbb73116c24e508731",
            "placeholder": "​",
            "style": "IPY_MODEL_412b20600d4e4ea8aafad6ffd770c379",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 18.9MB/s]"
          }
        },
        "38e65e56f4cb4d5f8b8a24bcedadadac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a9aacd88a764fd994e6677cee734cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed8cc6a846b49fc997d2598f77e8f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "666a346e9cc04d4899390c241cb851c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52ca486a2ee24ff590d420f3e7c9a524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "978f89b8dd064dbbb73116c24e508731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "412b20600d4e4ea8aafad6ffd770c379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81b4cde36d644a22be7cf88e1f4d8235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78f863dbd28e4709a530e685afa7e3b4",
              "IPY_MODEL_ef4a7d03d5c44373b6660853ca3de3a9",
              "IPY_MODEL_66bbca9efa3d4292b2a86fb5e888a73a"
            ],
            "layout": "IPY_MODEL_1da72f2ebf8849eb99e04a4cd94232c0"
          }
        },
        "78f863dbd28e4709a530e685afa7e3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5549954a24ed42e9b018fee3bdcfc862",
            "placeholder": "​",
            "style": "IPY_MODEL_8e619ab01e5b4eae8bbfefdcb6a51abf",
            "value": "Map: 100%"
          }
        },
        "ef4a7d03d5c44373b6660853ca3de3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd6718f592d6427cb1acca3c0d0c4f2b",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_672817b7dac6487f8b2f07117e0bf885",
            "value": 647
          }
        },
        "66bbca9efa3d4292b2a86fb5e888a73a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_048e7ad8c97744fe9c1f644c9aef68c6",
            "placeholder": "​",
            "style": "IPY_MODEL_6c025683c8824caab558e94c8d4456ab",
            "value": " 647/647 [00:02&lt;00:00, 224.47 examples/s]"
          }
        },
        "1da72f2ebf8849eb99e04a4cd94232c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5549954a24ed42e9b018fee3bdcfc862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e619ab01e5b4eae8bbfefdcb6a51abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd6718f592d6427cb1acca3c0d0c4f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672817b7dac6487f8b2f07117e0bf885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "048e7ad8c97744fe9c1f644c9aef68c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c025683c8824caab558e94c8d4456ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0dc3a4aaf0c49b8867847c8a3dea225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc2843765e1145d6b69dac291837a195",
              "IPY_MODEL_e28208986046499e96c16f0e51aca6a5",
              "IPY_MODEL_72458bf758f74174904ad1dbee3d87cc"
            ],
            "layout": "IPY_MODEL_97da3ace30904546bd42abdf2ded52f4"
          }
        },
        "bc2843765e1145d6b69dac291837a195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe7a3943e12b41c09a00c2b3f0976327",
            "placeholder": "​",
            "style": "IPY_MODEL_34889af974d94aa58ec9ba1d2d925f27",
            "value": "Map: 100%"
          }
        },
        "e28208986046499e96c16f0e51aca6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0fff40b95ac41cc8501f6834139a8a8",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10e443085c92421c8039026a9ec3fde2",
            "value": 162
          }
        },
        "72458bf758f74174904ad1dbee3d87cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de83b1bd03234e0895e7801e336f0977",
            "placeholder": "​",
            "style": "IPY_MODEL_489e173a0f9d423aa6ebe21b2d45dfd6",
            "value": " 162/162 [00:00&lt;00:00, 228.00 examples/s]"
          }
        },
        "97da3ace30904546bd42abdf2ded52f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe7a3943e12b41c09a00c2b3f0976327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34889af974d94aa58ec9ba1d2d925f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0fff40b95ac41cc8501f6834139a8a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10e443085c92421c8039026a9ec3fde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de83b1bd03234e0895e7801e336f0977": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "489e173a0f9d423aa6ebe21b2d45dfd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f340715989b4150a09b7d5a4e42a68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d852c17b415428c9f23942eb85af5cd",
              "IPY_MODEL_aad43bdd0bdc4140bf19652524d16293",
              "IPY_MODEL_9d9f062f1cfb4fa482ab21a4e5b7e3f3"
            ],
            "layout": "IPY_MODEL_ba6d0c8fbab5440a86d0ee712ba495ba"
          }
        },
        "2d852c17b415428c9f23942eb85af5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d326570eeef141b6a71f8aba5a948e72",
            "placeholder": "​",
            "style": "IPY_MODEL_b5c55c7d0b4646acbb55f5827c0b1f1a",
            "value": "Map: 100%"
          }
        },
        "aad43bdd0bdc4140bf19652524d16293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1309dddb5ec04f9092015a1f80cf777a",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e25af4b878d4a15930fea3337a415df",
            "value": 203
          }
        },
        "9d9f062f1cfb4fa482ab21a4e5b7e3f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dda1f6c845a4ca1b255cb75bfceb3be",
            "placeholder": "​",
            "style": "IPY_MODEL_2ef0870d6cd946c58a1fe5ab4dfd77f0",
            "value": " 203/203 [00:00&lt;00:00, 222.64 examples/s]"
          }
        },
        "ba6d0c8fbab5440a86d0ee712ba495ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d326570eeef141b6a71f8aba5a948e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c55c7d0b4646acbb55f5827c0b1f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1309dddb5ec04f9092015a1f80cf777a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e25af4b878d4a15930fea3337a415df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5dda1f6c845a4ca1b255cb75bfceb3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ef0870d6cd946c58a1fe5ab4dfd77f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8de9b74fbb2849ae8bad2054b457ad27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbff7e02ea7c487a8ff99599f5cec581",
              "IPY_MODEL_b259df54bd694f069dd0f724910020fc",
              "IPY_MODEL_c4040aa13b444733a632bad7ab3cadcc"
            ],
            "layout": "IPY_MODEL_581135a155304b28a9f5a874dac77643"
          }
        },
        "bbff7e02ea7c487a8ff99599f5cec581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc96af9798df470e9cbd18323172bded",
            "placeholder": "​",
            "style": "IPY_MODEL_c1ccc49a65244d10af06029878d6f7fe",
            "value": "config.json: 100%"
          }
        },
        "b259df54bd694f069dd0f724910020fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e591d76340be48b9a717da7e88088bf5",
            "max": 1206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40abdb48af2d45789a91592a51ab219e",
            "value": 1206
          }
        },
        "c4040aa13b444733a632bad7ab3cadcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55a331a537314e2ea375cdc9f5da39a2",
            "placeholder": "​",
            "style": "IPY_MODEL_e67dab0677d14fb698aa944e04fbffcf",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 96.5kB/s]"
          }
        },
        "581135a155304b28a9f5a874dac77643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc96af9798df470e9cbd18323172bded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1ccc49a65244d10af06029878d6f7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e591d76340be48b9a717da7e88088bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40abdb48af2d45789a91592a51ab219e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55a331a537314e2ea375cdc9f5da39a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e67dab0677d14fb698aa944e04fbffcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acd42a53c4dc4302998359008a0892a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23c759d031854eef8580a5817896f765",
              "IPY_MODEL_4dce043c3cd94c288c47866d9906d58d",
              "IPY_MODEL_20f444d4fa784c6aaaa1ce8bba3944fe"
            ],
            "layout": "IPY_MODEL_4cb73c57973745f1942bea2478f58daa"
          }
        },
        "23c759d031854eef8580a5817896f765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_389fd726e58b433c9b8fea10e133a179",
            "placeholder": "​",
            "style": "IPY_MODEL_e47c2a209e6d449690c8d793191a71e0",
            "value": "model.safetensors: 100%"
          }
        },
        "4dce043c3cd94c288c47866d9906d58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6097f2fe68224f09bf19f047cf90de9c",
            "max": 242043056,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d598cdf492ab40999f5a63e4cf40e43f",
            "value": 242043056
          }
        },
        "20f444d4fa784c6aaaa1ce8bba3944fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40ac1964298d48a79fd7fdf9df6a7502",
            "placeholder": "​",
            "style": "IPY_MODEL_7b92ed131f084b5982768125a37698b0",
            "value": " 242M/242M [00:01&lt;00:00, 225MB/s]"
          }
        },
        "4cb73c57973745f1942bea2478f58daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389fd726e58b433c9b8fea10e133a179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e47c2a209e6d449690c8d793191a71e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6097f2fe68224f09bf19f047cf90de9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d598cdf492ab40999f5a63e4cf40e43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40ac1964298d48a79fd7fdf9df6a7502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b92ed131f084b5982768125a37698b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}