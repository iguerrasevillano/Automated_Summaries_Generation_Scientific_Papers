{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/main/TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "70c8ba82-60a0-42c8-ea95-9a05b2532732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=0a37d6509e1a19cd90a96ec9fa52cdd5fe024c624330e8a524626a449037f94c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "98fda713-f163-4ee9-ad57-a53ed94887f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-1-c5cb458fe86c>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TFBartForConditionalGeneration, BartTokenizer, T5Tokenizer, pipeline, TFT5ForConditionalGeneration\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect w/ HuggingFace HUB\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "9100de41ec6946c495c032b05d90913d",
            "40d035dada3347e2963c183d035d445c",
            "ecd31a14240f408b82b5224d97694ab7",
            "7afeab237ced4f01b70361e2a3ce7552",
            "5b7408314641483980402784564e62e6",
            "c4a0eef84a5e4061884e43d2a09da02a",
            "b114a3fe857949408305dde0c9436974",
            "f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "52801cc2a7a646109c4a7141ef768900",
            "db74bb5ffc4f4900b17aaf842a618019",
            "6deef8837ace4d94b5b803666ff4aa5a",
            "2e00c125b4d942368e702682002f94ff",
            "571b4b08342141c798092b6b23ab410a",
            "ca46452e63a34d99865718ab86726748",
            "493ec2745ddf4bf2bdce528219a1b309",
            "d3c8673d4bac474d9c5799e3ee02bf6b",
            "f44df68731c14e24914d1c8816c8edce"
          ]
        },
        "id": "wpR8O7wbdMdI",
        "outputId": "d0ef909e-6736-48c6-bf15-0c9b0cbe318a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9100de41ec6946c495c032b05d90913d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "2f0c2eaf-422f-4a8b-c4ab-ac348d539352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "1I5H39lnQW9o",
        "outputId": "8b619c50-f79e-40db-aa32-902cb97dccd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source    paper_id  \\\n",
              "582   The Wasserstein probability metric has receive...   S1m6h21Cb   \n",
              "1078  According to parallel distributed processing (...   HJXOfZ-AZ   \n",
              "1216  The ability of a classifier to recognize unkno...   Hy7EPh10W   \n",
              "948   We introduce a hierarchical model for efficien...   Hkc-TeZ0W   \n",
              "932   Recently there has been a surge of interest in...  HJeKCi0qYX   \n",
              "\n",
              "                                                 target  \\\n",
              "582   The Wasserstein distance is hard to minimize w...   \n",
              "1078  Local codes have been found in feed-forward ne...   \n",
              "1216  We propose to solve a problem of simultaneous ...   \n",
              "948   We introduce a hierarchical model for efficien...   \n",
              "932   A generic framework to scale existing graph em...   \n",
              "\n",
              "                                                  title  number_words_target  \\\n",
              "582   The Cramer Distance as a Solution to Biased Wa...                   68   \n",
              "1078  When and where do feed-forward neural networks...                   50   \n",
              "1216                         Novelty Detection with GAN                   70   \n",
              "948           A Hierarchical Model for Device Placement                   75   \n",
              "932   MILE: A Multi-Level Framework for Scalable Gra...                   54   \n",
              "\n",
              "                                     extractive_summary  \n",
              "582   In this paper we provide a clue as to why that...  \n",
              "1078  And, as this data is structured to have some i...  \n",
              "1216  We show that a multi-class discriminator train...  \n",
              "948   Given that graph partitioning is a well-studie...  \n",
              "932   Specifically, we ask: 1) Can we scale up the e...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4a90b32-6dbb-46b5-bba1-6c2819f9153c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>The Wasserstein probability metric has receive...</td>\n",
              "      <td>S1m6h21Cb</td>\n",
              "      <td>The Wasserstein distance is hard to minimize w...</td>\n",
              "      <td>The Cramer Distance as a Solution to Biased Wa...</td>\n",
              "      <td>68</td>\n",
              "      <td>In this paper we provide a clue as to why that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>According to parallel distributed processing (...</td>\n",
              "      <td>HJXOfZ-AZ</td>\n",
              "      <td>Local codes have been found in feed-forward ne...</td>\n",
              "      <td>When and where do feed-forward neural networks...</td>\n",
              "      <td>50</td>\n",
              "      <td>And, as this data is structured to have some i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1216</th>\n",
              "      <td>The ability of a classifier to recognize unkno...</td>\n",
              "      <td>Hy7EPh10W</td>\n",
              "      <td>We propose to solve a problem of simultaneous ...</td>\n",
              "      <td>Novelty Detection with GAN</td>\n",
              "      <td>70</td>\n",
              "      <td>We show that a multi-class discriminator train...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>We introduce a hierarchical model for efficien...</td>\n",
              "      <td>Hkc-TeZ0W</td>\n",
              "      <td>We introduce a hierarchical model for efficien...</td>\n",
              "      <td>A Hierarchical Model for Device Placement</td>\n",
              "      <td>75</td>\n",
              "      <td>Given that graph partitioning is a well-studie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>932</th>\n",
              "      <td>Recently there has been a surge of interest in...</td>\n",
              "      <td>HJeKCi0qYX</td>\n",
              "      <td>A generic framework to scale existing graph em...</td>\n",
              "      <td>MILE: A Multi-Level Framework for Scalable Gra...</td>\n",
              "      <td>54</td>\n",
              "      <td>Specifically, we ask: 1) Can we scale up the e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4a90b32-6dbb-46b5-bba1-6c2819f9153c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4a90b32-6dbb-46b5-bba1-6c2819f9153c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4a90b32-6dbb-46b5-bba1-6c2819f9153c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-928e110e-9f91-47ee-b09e-329b2361b29d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-928e110e-9f91-47ee-b09e-329b2361b29d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-928e110e-9f91-47ee-b09e-329b2361b29d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"According to parallel distributed processing (PDP) theory in psychology, neural networks (NN) learn distributed rather than interpretable localist representations. This view has been held so strongly that few researchers have analysed single units to determine if this assumption is correct. However, recent results from psychology, neuroscience and computer science have shown the occasional existence of local codes emerging in artificial and biological neural networks. In this paper, we undertake the first systematic survey of when local codes emerge in a feed-forward neural network, using generated input and output data with known qualities. We find that the number of local codes that emerge from a NN follows a well-defined distribution across the number of hidden layer neurons, with a peak determined by the size of input data, number of examples presented and the sparsity of input data. Using a 1-hot output code drastically decreases the number of local codes on the hidden layer. The number of emergent local codes increases with the percentage of dropout applied to the hidden layer, suggesting that the localist encoding may offer a resilience to noisy networks. This data suggests that localist coding can emerge from feed-forward PDP networks and suggests some of the conditions that may lead to interpretable localist representations in the cortex. The findings highlight how local codes should not be dismissed out of hand. Local neural network models, which are often argued to be biologically implausible, have nevertheless been built or discussed by psychologists such as BID11 ; BID16 ; BID13 , and a few researchers like BID1 have done single neuron probing studies (the equivalent of the neuroscience approach) on their neural networks. However, as parallel distributed processing (PDP) neural networks (NN), as discussed by BID17 b) and BID15 , are generally assumed to learn distributed encodings across all situations, it is often believed that a single neuron in an artificial neural network is not interpretable, and experiments to test if this is true are rarely performed. Recently, however, there has been evidence emerging from neuroscience and modern artificial neural networks that demonstrate the existence of interpretable, local codes. BID10 argued that the neurons in the hippocampus codes for information in a highly selective manner in order to learn quickly without forgetting (catastrophic interference), and BID4 argued that some neurons in cortex are highly selective in order to encode multiple items at the same time in shortterm memory (solving the so-called superposition catastrophe). BID14 reported single cells that fire frequently in response to one stimulus, which suggests that individual neurons can be usefully interpreted. Localist codes have been found in artificial neural networks, see BID2 for full reviews, some examples are Le (2013); BID7 ). BID4 have shown that PDP models learn localist codes when trained to co-activate multiple items at the same time. Deep networks learn selective codes under some conditions. For example, BID8 's found quote mark detectors in RNNs. And there is also some evidence from feed-forward models. For example, BID12 have found that probing individual hidden layer neurons (HLNs) with noise and using activation maximisation they can produce a picture of what that neuron will respond most to, and from this they identified HLNs that act as feature detectors, such as those that only responding to creases (in clothing) or eyes or faces and so on. We would like to elucidate the conditions in which simple networks learn selective units, as this may provide further insight into the conditions in which neurons in cortex respond selectively, and, as we expect such codes are learned for sound information theoretic reasons, we expect that these conditions will also apply to when neural networks might learn them as well. Thus, in this paper, we undertake a study of simple feed-forward neural networks to investigate whether local codes, LCs, do actually emerge in PDP networks, and (as we shall show that they do) we then look at what inhibits or promotes the emergence of LCs by designing input and output data with known properties. And, as this data is structured to have some invariance within a class and some randomness, it is proposed that these experiments could as be modelling the layers within the deep neural network above those which transform the input data from pixel space to feature space. To be clear, we consider a neuron to be interpretable if probing of the activation state of it could give correct and useful information about the classification of the input. We look for information about the presence or absence of a category in the hidden layer (category selective HLNs). We separate the qualitative measure (selective) of whether or not a HLN encodes category presence, from the quantitative measure of how much the HLN responds to a category (selectivity). Thus, a HLN is selective if it encodes the presence/absence of a category. Examples are shown in figure 1, as the neuron encodes the presence or absence of the category shown as red circles, equivalently, it could be claimed that these neurons are selective for that category. As biological neurons use energy to encode information, a selective neuron is usually 'selectively on' (see figure 1(left)), but as there is no energy cost in neural networks 'selectively off' units (figure 1(right)) have also been observed. We use the word 'selectivity' as a quantitative measure of the difference between activations for the two categories, A and not-A, where A is the category a neuron is selective for (and not-A being all other categories). Specifically: DISPLAYFORM0 The important point is the qualitative measure of whether or not a neuron is selective, not how much it is selective by, as we are interested in counting the number of local codes that emerge. Note that the chance that all the members of A would emerge disjoint from the members of not-A is 50 50 / 500 50 is tiny (4.32 \\u00d7 10 \\u221271 ). Furthermore, we found that the selectivity increased with training as the neural network minimised the loss function, but that the number of selective codes did not change once the neural network achieved 100% accuracy. A criticism often made of a grandmother cell hypothesis is that even if a cell fires consistently to a single class, it is not possible to know that it would not have fired to a stimulus that was not presented. For example, although BID14 found a neuron that responded selectively to images of Jennifer Aniston, the authors only presented approximately 100 images to the human participant, and it is possible that other non-tested images would also drive the neuron. BID20 estimated that between 50-150 other images would drive this neuron. Obviously an experimenter cannot present every possible combination of visual inputs to a patient. However, in neural networks with small datasets, we can present all the possible stimuli to the network. We consider a neuron to be selective if it is selective over all the data it is reasonable to expect the network to differentiate between. For example, it is reasonable to do the test over all training data, and it is reasonable to do it over all test and all verification data or even other data of a similar form (such as different photos of the same class), and choosing what constitutes a reasonable set of data is a decision to be made by the experimenters and reviewers. In this work, we chose to use a simple pattern classification task, rather than an image recognition task, as we could then test the NN with all possible patterns. Data design Data input to a neural network can be understood as a code, {C x }, with each trained input data vector designated as a codeword, C x . The size of the code is related to the number of codewords (i.e. the size of the training set), n x . L x is the length of the codeword, and is 500bits in this paper. We used a binary alphabet, and the number of '1's in a codeword is the weight, w x of that codeword (this weight definition is not the same as connection weights in the neural network).Figure 1: Examples of interpretable local codes found in a distributed network. Left: a selectively on unit with a selectivity of \\u223c +0.12; Right: selectively off unit with a selectivity of \\u223c \\u22120.2. Red circles belong to a single category, blue stars are all the members of all other categories, the x-axis is the activation of a hidden layer neuron (HLN) and points are jittered randomly around 1 on the y-axis for ease of viewing. There is a clear separation between activations for the class depicted in red (A) and all other activations (not-A), thus examination of the activations of these units would reveal the presence or absence of the red class. To create a set of n P classes with a known structural similarity, the procedure in figure 2 was followed. We start with a set of n P prototypes, {P x , 1 \\u2264 x \\u2264 n P }, with blocks of '1's of length L P /n P , called prototype blocks, which code for a class. For example, if L x were 12 and n P were 3: P 1 = [111100000000] and P 2 = [000011110000], P 3 = [000000001111], and this would gives prototypes that are a Hamming distance of 8 apart (Hamming distance is the number of bits which must be switched to change one codeword into another), and thus we know that our prototypes span input-data space. To create members of each class, the prototype is used as a mask, with the '0' blocks replaced by blocks from a random vector, R x . The weight of the random vectors, w R can be tuned to ensure that a set of vectors randomly clustered around the prototype vector are generated, such that members of the same category are closer to each other than those of the other categories (N.B. the prototypes are not included as members of the category). A more realistic dataset is created by allowing the prototypes to be perturbed so that a percentage of the prototype block is randomly switched to '0's each time a new codeword is created, in accordance with the perturbation rate (see P 2 in figure 2 ). This method creates a code with a known number of invariant bits (bits that are the same) between codewords in the same category. For example, in figure 2, codewords C 1 and C 2 were both derived from P 1 , and have a Hamming distance of 6, where as C 1 and C 4 are in different classes and have a Hamming distance of 8. Note that the difference between these numbers is much bigger in our experiments as we used vectors of length 500 split into 10 categories). We define 'sparseness' of a vector, S x , as the fraction of bits that are '1's. Furthermore, local codes were highly unlikely to appear in the input codes and we did not observe any in a few random checks. Neural network design The neural networks are three-layer feed-forward networks, with L x = 500 input neurons, n HLN hidden layer neurons (HLN) and either 50 or 10 output neurons. All input vectors were 500bits long and were matched to 10 output classes, with the output encoded either as a 50bit long distributed vector (with weight 25) or a 10bit long 1-hot output vector. These networks are intended to also model single layers within a deeper and more complex network, and thus no form of softmax activation on the output was used. For most of the data, we chose to use sigmoidal activation functions for ease of understanding (activation is strictly limited between 0 and 1), but ReLU neurons were also tested. All networks had 100% accuracy on the task (with an output of more than 0.9 being taken as a one and less than 0.1 as a zero, although the networks were closer than these limits at the end of training). The networks were set-up in Keras BID5 ) using a Tensorflow BID0 ) back-end, and run on an Titan X Nvidia GPU under Ubuntu Linux. Each plotted data point comes from an average of at least 10 trained networks, with error bars calculated as the standard error from the measured data. Neural networks were trained for 45,000 epochs. Neurons were counted as a local code if the selectivity was above 0.05, most neurons were higher. Unless stated differently in the experiment, neural networks were run with sigmoidal neurons, 500 500bit long vectors separated into 10 classes, with 50bit long distributed output codes, no perturbation of the prototype block code and no dropout applied (these are the conditions for the black-dashed data in figures 3, 4 and 6 and this dataset is used as our standard to compare to).Figure 2: Schematic for building a random code with known properties. Black circles represent ones, white circles represent zeros. Class prototypes are made (P 1 , P 2 , and P 3 ) with length L P ; the number of prototypes are n p , which is three in this example, their weight is four, with a sparseness number, S p of 1 3 . Random vectors, R x , are made, as shown, these have length L R and there are n R of them; they have weight, w R of two and sparseness number, S R , of 1 4 . To assemble a new codeword, a prototype is chosen, this example, P 2 , and 'perturbation' errors are applied, in this example, the perturbation rate is 1 4 , so a single one is turned to a zero in the modified prototype (P 2 ). A random vector, in this example R 1 , is then generated, split into blocks and added to the parts of the prototype (P 1 ) that were zero (the random vectors cannot overwrite a random decay in P ). The process is repeated to create an input 'code' with n x codewords of length L x , where n x = n R and L x = L P . If the decay values is sufficiently low, members of each class, as they are based on the same prototype are more similar to each other than codewords in other classes. First, we should make that point that finding a single interpretable local code, such as those shown in figure 1, refutes the idea that neural networks do not have interpretable or locally encoded units. The number of local codes is not large, usually between 0-10% of the hidden layer, but this is not insignificant. Furthermore, the number of local codes is tuned by the size the hidden layer, with a peak in the number of local codes seen at n HLN =1000 for the standard data set, and a peak in the percentage of HLNs which are local codes seen at n HLN = 500 (data not shown): dashed and dot-dashed grey lines are drawn at these points in all relevant figures. Figure 3(left) shows the standard data (black-dashed line) with varying sizes of input data codes (n x is the number of training examples). More local codes emerge with fewer examples per class, perhaps suggesting that the fewer examples there are, the more efficient it is to learn a short-cut for that class (and the way we set the code up with a prototype block means that there is a short-cut for the neural network to learn). With n x = 1000 the peak is shifted to the left, possibly due to the existence of more different types of solution (see section 3.3.1 for discussion). The effect of sparseness is given in FIG0 . There are many more local codes when the sparseness is very low, which suggests that the bigger a proportion of the vector is the prototype block, the more drive there is to learn local codes. However, this process is not linear, with the S R of 1 9 given very many more codes. Interestingly, the range of Hamming distances between any two members of the same class does not overlap at all with the range of the Hamming distances possible between any two members of any two different classes, whereas, if S R of 2 9 or 3 9 these two sets overlap. . Right: The effect of changing the sparseness of the random blocks of the vector, S R . As the prototype vector in all these cases has a sparseness of 1 / 10 , the weight of the random w R and prototype, w P , parts of the codeword and the codewords are 500bits long, note that purple: S x = 0.2, w R =50, w P = 50; green: S x = 0.3, w R =100, w P = 50; black dashed: S x = 0.4, w R =150, w P = 50 Gray dashed and dot-dashed lines are drawn at n HLN =500 and 1000 respectfully. Switching from sigmoidal HLNs with a sigmoidal activation function to a rectified linear units (ReLU) neurons; Right: Switching from a distributed to a 1-hot output encoding. Note that the black dashed data is the same as in FIG0 . Switching to ReLU gives slightly more LCs, likely due to the fact that ReLUs train quicker and all experiments were stopped after 45,000 epochs. Using a 1-hot output code drastically reduces the need for local codes to emerge in the hidden layer. FIG1 shows the effect of changing the HLN activation function and the effect of an 1-hot output encoding. ReLU neurons generally produce a few more LCs, and have a peak shifted to a lower n HLN . As all the neural networks were trained for a specified number of steps, this finding may be due to ReLU neurons training quicker (more LCs emerge with longer training, which could be due to LCs being a more efficient solution or more likely due to more proto-selective codes passing the threashold for inclusion). DISPLAYFORM0 We chose to use distributed output codes to prevent the possibility that the 1-hot output encoding (which are local codes) would artificially induce local codes in the hidden layer of the network. As shown in figure 4(right), the effect was the opposite with very few local codes emerging if there were a local codes at the output. This suggests that local codes may offer some advantage in a system with distributed inputs and outputs, thus the NN finds them when they are not provided, but that only a small number is needed, so few are added if they are provided. This suggests that emergent local codes are highly unlikely to be found in the penultimate layer of deep networks. Figure 5 : Perturbing the prototype part of the code word decreases the drive to learn local codes. Perturbation, P , is measured as the number of bits in prototype block code that are randomly flipped. Data is shown for 500 and 1000 HLNs. No L.C.s emerge when the weight of the prototype block code is twice that of the random blocks. The random part of the vector already adds some noise on top of the invariant features shared by a class (which is the prototype block), here we demonstrate that the existence of local codes is predicated on there being shared invariant features within a class. FIG0 3.1 plots the decrease in the number of local codes against the perturbation rate, P , where P is defined as the number of bits randomly flipped to zero over the length of the prototype block for the two networks marked with vertical dashed lines in the previous figures. The curves are not well fit by an exponetial, the fits shown here are: (HLN=500) 20.62 -30.65 \\u221a P , with R 2 = 0.984; (HLN=1000) 19.72 \\u2212 32.29 \\u221a x with R 2 = 0.977, and these data were fit up to P =0.4 where the curve crosses zero. Interestingly, the point at which these curves cross the abscissa is the point when the weight of the prototype block is still twice that of the random blocks, i.e. if the random blocks are 33.3% ones, when the prototype block is \\u224860% ones, there is no drive to learn any local codes. This experiment shows that local codes can only emerge (at least in our data here) if there is some factor in common between the categories. This suggests that local codes are unlikely to be found at the very bottom of deep neural networks, where the data has fewer invariant features. However, as neural networks work by re-encoding information in terms of found features, there are class-shared invariants at the higher levels. Furthermore, this does raise the interesting question of whether more local codes will be found in categories that are similar (i.e. a network learning the difference between dog and cat pictures) than in categories that are more random (such as two categories made of random mixes of dog and cat photos), and thus, can the existence of emergent local codes be a measure of true categorical similarity? Or conversely, would a neural network trained on such random data find some invariant features nonetheless? As more LCs were seen in smaller datasets, and irrelevant invariant features are more likely in smaller categories, this suggests that this could be correct. Dropout (see BID19 ) is a common training technique where a percentage of a layer's neurons are 'dropped out' of the network (their connections are set to zero) during training to prevent over-fitting, however, dropout can also be viewed as a type of training noise. Dropout values of 0%, 20%, 50%, 70% and 90% were applied to the training of the standard network, and the networks were trained repeatedly (220, 310, 260, 250 and 250 times respectively) . Results are shown in FIG2 and table 1. There are always solutions with close to zero local codes, but the expected (mean) and maximum number roughly increases with an increasing percentage of dropped out neurons. Generally, dropout percentages in the range of 20-50% are used in training, and these probability distribution functions (PDFs), like 0% peak, are also joint peaks, with the 20% data having more of the lower solution and the 50% having more of the higher one. Droping out more than 50% of the network is not generally used as it slows down training. However, with these higher values, the range of solutions is much higher (as evidence by a higher variance and range of the number of local codes), which is expected as dropout forces the network to adopt a range of solution sub-networks, the increase in local codes suggest that localised encoding offers some protection against noise. At first glance, this might seem unlikely, as distributed patterns are claimed to be more resilient against failure. However, say we had a 20% dropout rate, a fully distributed encoding, would be affected by dropout 100% of the time, losing 20% of its information, whereas a localised encoding would be unaffected 80% of the time (although 20% of the time it would lose all data), and further resilience can be provided if duplicate local codes were used for the same class. Note that, as only 10 classes were used, the large number of local codes, especially for the high dropout values, suggests there are multiple LCs for each category. These results suggest that, for a noisy network, solutions involving some duplicate localised codes are useful methods for dealing with uncertainty. : Increasing dropout increases the number of local codes. Probability density function (PDF), left, and cumulative probability density function (CDF), right, of the number of local codes that emerge in repeated neural networks trained with dropout, percentage of the hidden layer neurons dropped out given in the legend. As the dropout percentage increases, generally, the mean and range of local codes found increases, suggesting that localized encoding by the network offers some advantage against noise. We have demonstrated that interpretable localised encodings of the presence/absence of some categories can emerge from the hidden layer of a feed-forward neural network. As the number of local codes follows a well-defined pattern with the size of the hidden layer, and it is affected by modifications of the input and output data, it suggests that the number of local codes is related to the computing capacity of the neural network and the difficulty of the problem presented to it, suggesting that the local codes offer some modification to the computing power of the neural network. Furthermore, as the hidden layer size increases, there is so much extra capacity that local codes are not needed. Our results suggest that local codes require more effort to train, but offer more efficient use of the available capacity. As the number of local codes shared invariants within a categories, it does imply that the local codes have some function associated with recognising these invariants. As the average number and range of LCs generally increases with dropout, and the LCs are repressed by a fully locally encoded output layer, it suggests that some local codes are good to have, and that number increases with noisy networks. The fact that the dropout data seems to contain multiple overlapping peaks, and, in our tests, peak numbers of LCs are seen at 500 and 1000 (and 2000 for the S R = 1 9 data) HLNs implies that there are more than one qualitative approaches for the neural network to solve the problem, and tuning the problem and neural network parameters nudges the solution to different distributions of local codes. Do these simple networks tell us anything about deep neural networks? The data presented here was designed to have invariant feature 'short-cuts' that the neural network could make use of in classifying input data into classes and the argument could well be made that the data passed between layers of a deep neural network is not of the same quality. Whilst an obvious next experiment for us is to investigate the qualities of the data passed within a neural network, preliminary feed-forward neural network training on standard simple neural network data (such as the Iris dataset from Fisher (1936)) results also show the emergence of local codes when there is a 'short-cut' in the data (publication in preparation). The observations that local codes are seen under dropout, with distributed input and output codes, when there are invariant features and local codes are inhibited with 1-hot output encodings, suggests that local codes might be found in a the middle and higher layers of a deep network, and not the penultimate layer where the 1-hot output could inhibit them or the early layers where invariant features common to a class have not yet been identified. Another interesting question is whether the local codes might have a diagnostic use, for example, is it the case that they increased in networks that generalise or are they, perhaps, an indicator of over-training? Answering this would also highlight when and where we should expect invariants in the data, as learning an invariant feature, such as, 'presence of eyes implies presence of face', could help with generalisation and classification, however learning an irrelevant invariant feature, such as, presence of 'blue sky implies tanks' would not. Discriminating a blue-sky selective neuron from a tank-selective neuron in such a case would require careful thought about what we should consider reasonable data to test for selectivity.\",\n          \"Recently there has been a surge of interest in designing graph embedding methods. Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements. In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework \\u2013 a generic methodology allowing contemporary graph embedding methods to scale to large graphs. MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph. It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a novel graph convolution neural network that it learns. The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them. We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs. Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while also often generating embeddings of better quality for the task of node classification. MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation. In recent years, graph embedding has attracted much interest due to its broad applicability for various tasks BID17 BID10 . However, such methods rarely scale to large datasets (e.g., graphs with over 1 million nodes) since they are computationally expensive and often memory intensive. For example, random-walkbased embedding techniques require a large amount of CPU time to generate a sufficient number of walks and train the embedding model. As another example, embedding methods based on matrix factorization, including GraRep BID1 and NetMF BID18 , requires constructing an enormous objective matrix (usually much denser than adjacency matrix), on which matrix factorization is performed. Even a medium-size graph with 100K nodes can easily require hundreds of GB of memory using those methods. On the other hand, many graph datasets in the real world tend to be large-scale with millions or even billions of nodes. To the best of our knowledge, none of the existing efforts examines how to scale up graph embedding in a generic way. We make the first attempt to close this gap. We are also interested in the related question of whether the quality of such embeddings can be improved along the way. Specifically, we ask: 1) Can we scale up the existing embedding techniques in an agnostic manner so that they can be directly applied to larger datasets?2) Can the quality of such embedding methods be strengthened by incorporating the holistic view of the graph?To tackle these problems, we propose a MultI-Level Embedding (MILE) framework for graph embedding. Our approach relies on a three-step process: first, we repeatedly coarsen the original graph into smaller ones by employing a hybrid matching strategy; second, we compute the embeddings on the coarsest graph using an existing embedding techniquesand third, we propose a novel refinement model based on learning a graph convolution network to refine the embeddings from the coarsest graph to the original graph -learning a graph convolution network allows us to compute a refinement procedure that levers the dependencies inherent to the graph structure and the embedding method of choice. To summarize, we find that:\\u2022 MILE is generalizable : Our MILE framework is agnostic to the underlying graph embedding techniques and treats them as black boxes.\\u2022 MILE is scalable : MILE can significantly improve the scalability of the embedding methods (up to 30-fold), by reducing the running time and memory consumption.\\u2022 MILE generates high-quality embeddings : In many cases, we find that the quality of embeddings improves by levering MILE (in some cases is in excess of 10%). Many techniques for graph or network embedding have been proposed in recent years. DeepWalk and Node2Vec generate truncated random walks on graphs and apply the Skip Gram by treating the walks as sentences BID17 BID7 . LINE learns the node embeddings by preserving the first-order and second-order proximities . Following LINE, SDNE leverages deep neural networks to capture the highly non-linear structure . Other methods construct a particular objective matrix and use matrix factorization techniques to generate embeddings, e.g., GraRep BID1 and NetMF BID18 . This also led to the proliferation of network embedding methods for information-rich graphs, including heterogeneous information networks BID5 and attributed graphs BID16 BID15 BID22 BID13 .On the other hand, there are very few efforts, focusing on the scalability of network embedding BID23 BID11 . First, such efforts are specific to a particular embedding strategy and do not generalize. Second, the scalability of such efforts is limited to moderately sized datasets. Finally, and notably, these efforts at scalability are actually orthogonal to our strategy and can potentially be employed along with our efforts to afford even greater speedup. The closest work to this paper is the very recently proposed HARP BID3 , which proposes a hierarchical paradigm for graph embedding based on iterative learning methods (e.g., DeepWalk and Node2Vec). However, HARP focuses on improving the quality of embeddings by using the learned embeddings from the previous level as the initialized embeddings for the next level, which introduces a huge computational overhead. Moreover, it is not immediately obvious how a HARP like methodology would be extended to other graph embedding techniques (e.g., GraRep and NetMF) in an agnostic manner since such an approach would necessarily require one to modify the embedding methods to preset their initialized embeddings. In this paper, we focus on designing a general-purpose framework to scale up embedding methods treating them as black boxes. Let G = (V, E) be the input graph where V and E are respectively the node set and edge set. Let A be the adjacency matrix of the graph and we assume G is undirected, though our problem can be easily extended BID4 BID6 BID19 to directed graph. We first define graph embedding:Definition 3.1 Graph Embedding Given a graph G = (V, E) and a dimensionality d (d |V |), the problem of graph embedding is to learn a d-dimension vector representation for each node in G so that graph properties are best preserved. Following this, a graph embedding method is essentially a mapping function f : R |V |\\u00d7|V | \\u2192 R |V |\\u00d7d , whose input is the adjacency matrix A (or G) and output is a lower dimension matrix. Motivated by the fact that the majority of graph embedding methods cannot scale : MILE framework to large datasets, we seek to speed up existing graph embedding methods without sacrificing quality. We formulate the problem as:Given a graph G = (V, E) and a graph embedding method f (\\u00b7), we aim to realize a strengthened graph embedding methodf (\\u00b7) so that it is more scalable than f (\\u00b7) while generating embeddings of comparable or even better quality. MILE framework consists of three key phases: graph coarsening, base embedding, and embeddings refining. FIG1 shows the overview. In this phase, the input graph G (or G 0 ) is repeatedly coarsened into a series of smaller graphs DISPLAYFORM0 In order to coarsen a graph from G i to G i+1 , multiple nodes in G i are collapsed to form super-nodes in G i+1 , and the edges incident on a super-node are the union of the edges on the original nodes in G i . Here the set of nodes forming a super-node is called a matching. We propose a hybrid matching technique containing two matching strategies that can efficiently coarsen the graph while retaining the global structure. An example is shared in FIG3 . presents the adjacency matrix A0 of the input graph, the matching matrix M0,1 corresponding to the SEM and NHEM matchings, and the derivation of the adjacency matrix A1 of the coarsened graph using Eq. 2.Structural Equivalence Matching (SEM) : Given two vertices u and v in an unweighted graph G, we call they are structurally equivalent if they are incident on the same set of neighborhoods. In FIG3 , node D and E are structurally equivalent. The intuition of matching structually equivalent nodes is that if two vertices are structurally equivalent, then their node embeddings will be similar. : Heavy edge matching is a popular matching method for graph coarsening BID12 . For an unmatched node u in G i , its heavy edge matching is a pair of vertices (u, v) such that the weight of the edge between u and v is the largest. In this paper, we propose to normalize the edge weights when applying heavy edge matching using the formula as follows DISPLAYFORM0 Here, the weight of an edge is normalized by the degree of the two vertices on which the edge is incident. Intuitively, it penalizes the weights of edges connected with high-degree nodes. As we will show in Sec. 4.3, this normalization is tightly connected with the graph convolution kernel. Hybrid Matching Method : We use a hybrid of two matching methods above for graph coarsening. To construct G i+1 from G i , we first find out all the structural equivalence matching (SEM) M 1 , where G i is treated as an unweighted graph. This is followed by the searching of the normalized heavy edge matching (NHEM) M 2 on G i . Nodes in each matching are then collapsed into a super-node in G i+1 . Note that some nodes might not be matched at all and they will be directly copied to G i+1 .Formally, we build the adjacency matrix A i+1 of G i+1 through matrix operations. To this end, we define the matching matrix storing the matching information from graph G i to G i+1 as a binary matrix M i,i+1 \\u2208 {0, 1} |Vi|\\u00d7|Vi+1| . The r-th row and c-th column of M i,i+1 is set to 1 if node r in G i will be collapsed to super-node c in G i+1 , and is set to 0 if otherwise. Each column of M i,i+1 represents a matching with the 1s representing the nodes in it. Each unmatched vertex appears as an individual column in M i,i+1 with merely one entry set to 1. Following this formulation, we construct the adjacency matrix of G i+1 by using DISPLAYFORM1 The size of the graph reduces drastically after each iteration of coarsening, halving the size of the graph in the best case. We coarsen the graph for m iterations and apply the graph embedding method f (\\u00b7) on the coarsest graph G m . Denoting the embeddings on G m as E m , we have E m = f (G m ). Since our framework is agnostic to the adopted graph embedding method, we can use any graph embedding algorithm for base embedding. The final phase of MILE is the embeddings refinement phase. Given a series of coarsened DISPLAYFORM0 .., M m\\u22121,m , and the node embeddings E m on G m , we seek to develop an approach to derive the node embeddings of G 0 from G m . To this end, we first study an easier subtask: given a graph G i , its coarsened graph G i+1 , the matching matrix M i,i+1 and the node embeddings E i+1 on G i+1 , how to infer the embeddings E i on graph G i . Once we solved this subtask, we can then iteratively apply the technique on each pair of consecutive graphs from G m to G 0 and eventually derive the node embeddings on G 0 . In this work, we propose to use a graph-based neural network model to perform embeddings refinement. Since we know the matching information between the two consecutive graphs G i and G i+1 , we can easily project the node embeddings from the coarse-grained graph G i+1 to the fine-grained graph G i using DISPLAYFORM0 In this case, embedding of a super-node is directly copied to its original node(s). We call E p i the projected embeddings from G i+1 to G i , or simply projected embeddings without ambiguity. While this way of simple projection maintains some information of node embeddings, it has obvious limitations that nodes will share the same embeddings if they are matched and collapsed into a super-node during the coarsening phase. This problem will be more serious when the embedding refinement is performed iteratively from G m , ..., G 0 . To address this issue, we propose to use a graph convolution network for embedding refinement. Specifically, we design a graph-based neural network model DISPLAYFORM1 , which derives the embeddings E i on graph G i based on the projected embeddings E p i and the graph adjacency matrix A i .Given graph G with adjacency matrix A, we consider the fast approximation of graph convolution from BID13 . The k-th layer of this neural network model is DISPLAYFORM2 where \\u03c3(\\u00b7) is an activation function, \\u0398 (k) is a layer-specific trainable weight matrix, and DISPLAYFORM3 In this paper, we define our embedding refinement model as a l-layer graph convolution model DISPLAYFORM4 The architecture of the refinement model is shown in FIG1 . The intuition behind this refinement model is to integrate the structural information of the current graph G i into the projected embedding E p i by repeatedly performing the spectral graph convolution. Each layer of graph convolution network in Eq. 4 can be regarded as one iteration of embedding propagation in the graph following the re-normalized adjacency matrixD DISPLAYFORM5 . Note that this re-normalized matrix is well aligned with the way we conduct normalized heavy edge matching in Eq. 1. We next discuss how the weight matrix \\u0398 (k) is learned. The learning of the refinement model is essentially learning \\u0398 (k) for each k \\u2208 [1, l] according to Eq. 4. Here we study how to design the learning task and construct the loss function. Since the graph convolution model H (l) (\\u00b7) aims to predict the embeddings E i on graph G i , we can directly run a base embedding on G i to generate the \\\"ground-truth\\\" embeddings and use the difference between these embeddings and the predicted ones as the loss function for training. We propose to learn \\u0398 (k) on the coarsest graph and reuse them across all the levels for refinement. Specifically, we can define the loss function as the mean square error as follows DISPLAYFORM0 We refer to the learning task associated with the above loss function as double-base embedding learning. We point out, however, there are two key drawbacks to this method. First of all, the above loss function requires one more level of coarsening to construct G m+1 and an extra base embedding on G m+1 . These two steps, especially the latter, introduce nonnegligible overheads to the MILE framework, which contradicts our motivation of scaling up graph embedding. More importantly, E m might not be a desirable \\\"ground truth\\\" for the refined embeddings. This is because most of the embedding methods are invariant to an orthogonal transformation of the embeddings, i.e., the embeddings can be rotated by an arbitrary orthogonal matrix BID8 . In other words, the embedding spaces of graph G m and G m+1 can be totally different since the two base embeddings are learned independently. Even if we follow the paradigm in BID3 and conduct base embedding on G m using the simple projected embeddings from G m+1 (E p m ) as initialization, the embedding space does not naturally generalize and can drift during re-training. One possible solution is to use an alignment procedure to force the embeddings to be aligned between the two graphs BID9 . But it could be very expensive. In this paper, we propose a very simple method to address the above issues. Instead of conducting an additional level of coarsening, we construct a dummy coarsened graph by simply copying G m , i.e., M m,m+1 = I and G m+1 = G m . By doing this, we not only reduce one iteration of graph coarsening, but also avoid performing base embedding on G m+1 simply because E m+1 = E m . Moreover, the embeddings of G m and G m+1 are guaranteed to be in the same space in this case without any drift. With this strategy, we change the loss function for model learning as follows DISPLAYFORM1 With the above loss function, we adopt gradient descent with back-propagation to learn the parameters DISPLAYFORM2 In the subsequent refinement steps, we apply the same set of parameters \\u0398 (k) to infer the refined embeddings. We point out that the training of the refinement model is rather efficient as it is done on the coarsest graph. The embeddings refinement process involves merely sparse matrix multiplications using Eq. 5 and is relatively affordable compared to conducting embedding on the original graph. With these different components, we summarize the whole algorithm of our MILE framework in Algorithm 1. The appendix contains the time complexity of the algorithm in Section A.2 Input: A input graph G0 = (V0, E0), # coarsening levels m, and a base embedding method f (\\u00b7). Output: Graph embeddings E0 on G0.1: Coarsen G0 into G1, G2, ..., Gm using proposed hybrid matching method. 2: Perform base embedding on the coarsest graph Gm (See Section. 4.2). 3: Learn the weights \\u0398 (k) using the loss function in Eq. 7. 4: for i = (m \\u2212 1)...0 do 5:Compute the projected embeddings E p i on Gi. 6:Use Eq. 4 and Eq. 5 to compute refined embeddings Ei. 7: Return graph embeddings E0 on G0. The datasets used in our experiments is shown in Table 1 . Yelp dataset is preprocessed by us following similar procedures in BID11 1 . To demonstrate that MILE can work with different graph embedding methods , we explore several popular methods for graph embedding, mainly, DeepWalk BID17 , Node2vec BID7 , Line , GraRep BID1 and NetMF BID18 .To evaluate the quality of the embeddings, we follow the typical method in existing work to perform multi-label node classification BID17 BID7 We first evaluate the performance of our MILE framework when applied to different graph embedding methods. FIG4 summarizes the performance of MILE on different datasets with various base embedding methods on various coarsening levels 2 (exact numbers can be seen in TAB3 of Appendix). Note that m=0 corresponds to original embedding method. We make the following observations:\\u2022 MILE is scalable. MILE greatly boosts the speed of the explored embedding methods. With a single level of coarsening (m=1), we are able to achieve speedup ranging from 1.5\\u00d7 to 3.4\\u00d7 (on PPI, Blog, and Flickr) while improving qualitative performance. Larger speedups are typically observed on GraRep and NetMF. Increasing the coarsening level m to 2, the speedup increases further (up to 14.4\\u00d7), while the quality of the embeddings is comparable with the original methods reflected by Micro-F1. On YouTube, for the coarsening levels 6 and 8, we observe more than 10\\u00d7 speedup for DeepWalk, Node2Vec and LINE. For NetMF on YouTube, the speedup is even larger -original NetMF runs out of memory within 9.5 hours while MILE (NetMF) only takes around 20 minutes (m = 8).\\u2022 MILE improves quality. For the smaller coarsening levels across all the datasets and methods, MILE-enhanced embeddings almost always offer a qualitative improvement over , MILE in addition to being much faster can still improve, qualitatively, over the original methods on most of the datasets, e.g., MILE(NetMF, m = 2) NETMF on PPI, Blog, and Flickr. We conjecture the observed improvement on quality is because the embeddings begin to rely on a more holistic view of the graph. DISPLAYFORM0 \\u2022 MILE supports multiple embedding strategies. We make some embedding-specific observations here. We observe that MILE consistently improves both the quality and the efficiency of NetMF on all four datasets (for YouTube, NetMF runs out of memory). For the largest dataset, the speedups afforded exceed 30-fold. We observe that for GraRep, while speedups with MILE are consistently observed, the qualitative improvements, if any, are smaller (for both YouTube and Flickr, the base method runs out of memory). For Line, even though its time complexity is linear to the number of edges , applying MILE framework on top of it still generates significant speed-up (likely due to the fact that the complexity of Line contains a larger constant factor k than MILE). On the other hand, MILE on top of Line generates better quality of embeddings on PPI and YouTube while falling a bit short on Blog and Flickr. For DeepWalk and Node2Vec, we again observe consistent improvements in scalability (up to 11-fold on the larger datasets) as well as quality using MILE with a few levels of coarsening. However, when the coarsening level is increased, the additional speedup afforded (up to 17-fold) comes at a mixed cost to quality (micro-F1 drops slightly).\\u2022 Impact of varying coarsening levels on MILE. When coarsening level m is small, MILE tends to significantly improve the quality of embeddings while taking much less time. From m = 0 to m = 1, we see a clear jump of the Micro-F1 score on all the datasets across the base embedding methods. This observation is more evident on larger datasets (Flickr and YouTube). On YouTube, MILE (DeepWalk) with m=1 increases the Micro-F1 score by 5.3% while only consuming half of the time compared to the original DeepWalk. MILE (DeepWalk) continues to generate embeddings of better quality than DeepWalk until m = 7, where the speedup is 13\\u00d7. As the coarsening level m in MILE increases, the running time drops dramatically while the quality of embeddings only decreases slightly. The running time decreases at an almost exponential rate (logarithm scale on the y-axis in the second row of FIG4 ). On the other hand, the Micro-F1 score descends much more slowly (the first row of FIG4 ). most of which are still better than the original methods. This shows that MILE can not only consolidates the existing embedding methods, but also provides nice trade-off between effectiveness and efficency.0 Comparing MILE with HARP HARP is a multi-level method primarily for improving the quality of graph embeddings. We compare HARP with our MILE framework using DeepWalk and Node2vec as the base embedding methods 3 . TAB2 shows the performance of these two methods on the four datasets (coarsening level is 1 on PPI/Blog/Flickr and 6 on YouTube). From the table we can observe that MILE generates embeddings of comparable quality with HARP. MILE performs much better than HARP on PPI and Blog, marginally better on Flickr and marginally worse on YouTube. However, MILE is significantly faster than HARP on all the four datasets (e.g. on YouTube, MILE affords a 31\\u00d7 speedup). This is because HARP requires running the whole embedding algorithm on each coarsened graph, which introduces a huge computational overhead. Note that for PPI and BLOG -MILE with NetMF (not shown) as its base embeddings produces the best micro-F1 of 26.9 and 43.8, respectively. This shows another advantage of MILE -agnostic to the base embedding when compared with HARP. We now explore the scalability of MILE on the large Yelp dataset. None of the five graph embedding methods studied in this paper can successfully conduct graph embedding on Yelp within 60 hours on a modern machine with 28 cores and 128 GB RAM. Even extending the run-time deadline to 100 hours, we see DeepWalk and Line barely finish. Leveraging the proposed MILE framework now makes it much easier to perform graph embedding on this scale of datasets (see FIG5 for the results). We observe that MILE significantly reduces the running time and improves the Micro-F1 score. For example, Micro-F1 score of original DeepWalk and Line are 0.640 and 0.625 respectively, which all take more than 80 hours. But using MILE with m = 4, the micro-F1 score improves to 0.643 (DeepWalk) and 0.642 (Line) while achiving speedups of around 1.6\\u00d7. Moreover, MILE reduces the running time of DeepWalk from 53 hours (coarsening level 4) to 2 hours (coarsening level 22) while reducing the Micro-F1 score just by 1% (from 0.643 to 0.634). Meanwhile, there is no change in the Micro-F1 score from coarsening level 4 to 10, where the running time is improved by a factor of two. These results affirm the power of the proposed MILE framework on scaling up graph embedding algorithms while generating quality embeddings. In this work, we propose a novel multi-level embedding (MILE) framework to scale up graph embedding techniques, without modifying them. Our framework incorporates existing embedding techniques as black boxes, and significantly improves the scalability of extant methods by reducing both the running time and memory consumption. Additionally, MILE also provides a lift in the quality of node embeddings in most of the cases. A fundamental contribution of MILE is its ability to learn a refinement strategy that depends on both the underlying graph properties and the embedding method in use. In the future, we plan to generalize MILE for information-rich graphs and employing MILE for more applications. The details about the datasets used in our experiments are :\\u2022 PPI is a Protein-Protein Interaction graph constructed based on the interplay activity between proteins of Homo Sapiens, where the labels represent biological states.\\u2022 Blog is a network of social relationship of bloggers on BlogCatalog and the labels indicate interests of the bloggers.\\u2022 Flickr is a social network of the contacts between users on flickr.com with labels denoting the interest groups.\\u2022 YouTube is a social network between users on YouTube, where labels represent genres of groups subscribed by users.\\u2022 Yelp is a social network of friends on Yelp and labels indicate the business categories on which the users review. Baseline Methods: To demonstrate that MILE can work with different graph embedding methods, we explore several popular methods for graph embedding.\\u2022 DeepWalk (DW) BID17 : Following the original work BID17 , we set the length of random walks as 80, number of walks per node as 10, and context windows size as 10.\\u2022 Node2Vec (NV) BID7 : We use the same setting as DeepWalk for those common hyper-parameters while setting p = 4.0 and q = 1.0, which we found empirically to generate better results across all the datasets.\\u2022 Line (LN) : This method aims at preserving first-order and secondorder proximities and has been applied on large-scale graph. We learn the first-order and second-order embeddings respectively and concatenate them to a unified embedding.\\u2022 GraRep (GR) BID1 : This method considers different powers (up to k) of the adjacency matrix to preserve higher-order graph proximity for graph embedding. It uses SVD decomposition to generate the low-dimensional representation of nodes. We set k = 4 as suggested in the original work.\\u2022 NetMF (NM) BID18 : It is a recent effort that supports graph embedding via matrix factorization. We set the window size to 10 and the rank h to 1024, and lever the approximate version, as suggested and reported by the authors. For all the above base embedding methods, we set the embedding dimensionality d as 128. When applying our MILE framework, we vary the coarsening levels m from 1 to 10 whenever possible. For the graph convolution network model, the self-loop weight \\u03bb is set to 0.05, the number of hidden layers l is 2, and tanh(\\u00b7) is used as the activation function, the learning rate is set to 0.001 and the number of training epochs is 200. The Adam Optimizer is used for model training. The experiments were conducted on a machine running Linux with an Intel Xeon E5-2680 CPU (28 cores, 2.40GHz) and 128 GB of RAM. We implement our MILE framework in Python. Our code and data are will be available for the replicability purpose. For all the five base embedding methods, we adapt the original code from the authors 4 . We additionally use TensorFlow package for the embeddings refinement learning component. We lever the available parallelism (on 28 cores) for each method (e.g., the generation of random walks in DeepWalk and Node2Vec, the training of the refinement model in MILE, etc.). To evaluate the quality of the embeddings, we follow the typical method in existing work to perform multi-label node classification BID17 BID7 . Specifically, after the graph embeddings are learned for nodes (label is not used for this part), we run a 10-fold cross validation using the embeddings as features and report the average Micro-F1 and average Macro-F1. We also record the end-to-end wallclock time consumed by each method for scalability comparisons. It is non-trivial to derive the exact time complexity of MILE as it is dependent on the graph structure, the chosen base embedding method, and the convergence rate of the GCN model training. Here, we provide a rough estimation of the time complexity. For simplicity, we assume the number of vertices and the number of edges are reduced by factor \\u03b1 and \\u03b2 respectively at each step of coarsening (\\u03b1 > 1.0 and \\u03b2 > 1.0), i.e., V i = BID13 , where k 1 is a small constant related to embedding dimensionality and the number of training epochs. The embedding inference part is simply sparse matrix multiplication using Eq. 4 with time complexity O(k 2 * E i ) when refining the embeddings on graph G i , where k 2 is an even smaller constant (k 2 < k 1 ). As a result, the time complexity of the whole refinement phase is O( DISPLAYFORM0 where k 3 is a small constant. Overall, for an embedding algorithm of time complexity T (V, E), the MILE framework can reduce it to be T ( DISPLAYFORM1 . This is a significant improvement considering T (V, E) is usually very large. The reduction in time complexity is attributed to the fact that we run the embedding learning and refinement model training at the coarsest graph. In addition, the overhead introduced by the coarsening phase and recursive embedding refinement is relatively small (linear to the number of edges E). Note that the constant factor k in the complexity term is usually small and we empirically found it to be in the scale of tens. Because of this, even when the complexity of the original embedding algorithm is linear to E, our MILE framework could still potentially speed up the embedding process because the complexity of MILE contains a smaller constant factor k (see Sec. 5.2 for the experiment of applying MILE on LINE).Furthermore, it is worth noting that many of the existing embedding strategies involve hyperparameters tunning for the best performance, especially for those methods based on neural networks (e.g., DeepWalk, Node2Vec, etc.). This in turn requires the algorithm to be run repeatedly -hence any savings in runtime by applying MILE are magnified across multiple runs of the algorithm with different hyper-parameter settings. The detailed information about performance evaluation is available in TAB3 : Performance of MILE. DeepWalk, Node2Vec, GraRep, and NetMF denotes the original method without using our MILE framework. m is the number of coarsening levels. The numbers within the parenthesis by the reported Micro-F1 and Macro-F1 scores are the relative percentage of change compared to the original method Numbers along with \\\"\\u00d7\\\" is the speedup compared to the original method. \\\"N/A\\\" indicates the method runs out of memory and we show the amount of running time spent when it happens. We now study the role of the design choices we make within the MILE framework related to the coarsening and refinement procedures described. To this end, we examine alternative design choices and systematically examine their performance. The alternatives we consider are:\\u2022 Random Matching (MILE-rm): For each iteration of coarsening, we repeatedly pick a random pair of connected nodes as a match and merge them into a super-node until no more matching can be found. The rest of the algorithm is the same as our MILE.\\u2022 Simple Projection (MILE-proj): We replace our embedding refinement model with a simple projection method. In other words, we directly copy the embedding of a super-node to its original node(s) without any refinement (see Eq. 3).\\u2022 Averaging Neighborhoods (MILE-avg): For this baseline method, the refined embedding of each node is a weighted average node embeddings of its neighborhoods (weighted by the edge weights). This can be regarded as an embeddings propagation method. We add self-loop to each node 6 and conduct the embeddings propagation for two rounds.\\u2022 Untrained Refinement Model (MILE-untr): Instead of training the refinement model to minimize the loss defined in Eq. 7, this baseline merely uses a fixed set of values for parameters \\u0398 (k) without training (values are randomly generated; other parts of the model in Eq. 4 are the same, including\\u00c3 andD). Table 4 : Comparisons of graph embeddings between MILE and its variants. Except for the original methods (DeepWalk and NetMF), the number of coarsening level m is set to 1 on PPI/Blog/Flickr and 6 on YouTube. Mi-F1 is the Micro-F1 score in 10 \\u22122 scale while Time column shows the running time of the method in minutes. \\\"N/A\\\" denotes the method consumes more than 128 GB RAM.\\u2022 Double-base Embedding for Refinement Training (MILE-2base): This method replaces the loss function in Eq. 7 with the alternative one in Eq. 6 for model training. It conducts one more layer of coarsening and base embedding (level m + 1), from which the embeddings are projected to level m and used as the input for model training.\\u2022 GraphSAGE as Refinement Model (MILE-gs): It replaces the graph convolution network in our refinement method with GraphSAGE BID8 7 . We choose max-pooling for aggregation and set the number of sampled neighbors as 100, as suggested by the authors. Also, concatenation is conducted instead of replacement during the process of propagation. Table 4 shows the comparison of performance on these methods across the four datasets. Here, we focus on using DeepWalk and NetMF for base embedding with a smaller coarsening level (m = 1 for PPI, Blog, and Flickr; m = 6 for YouTube). Results are similar for the other embedding options we consider. We hereby summarize the key information derived from Table 4 as follows:\\u2022 The matching methods used within MILE offer a qualitative benefit at a minimal cost to execution time. Comparing MILE with MILE-rm for all the datasets, we can see that MILE generates better embeddings than MILE-rm using either DeepWalk or NetMF as the base embedding method. Though MILE-rm is slightly faster than MILE due to its random matching, its Micro-F1 score and Macro-F1 score are consistently lower than of MILE.\\u2022 The graph convolution based refinement learning methodology in MILE is particularly effective. Simple projection-based MILE-proj, performs significantly worse than MILE. The other two variants (MILE-avg and MILE-untr) which do not train the refinement model at all, also perform much worse than the proposed method. Note MILE-untr is the same as MILE except it uses a default set of parameters instead of learning those parameters. Clearly, the model learning part of our refinement method is a fundamental contributing factor to the effectiveness of MILE. Through training, the refinement model is tailored to the specific graph under the base embedding method in use. The overhead cost of this learning (comparing MILE with MILE-untr), can vary depending on the base embedding employed (for instance on the YouTube dataset, it is an insignificant 1.2% on DeepWalk -while being up to 20% on NetMF) but is still worth it due to qualitative benefits (Micro-F1 up from 30.2 to 40.9 with NetMF on YouTube).\\u2022 Graph convolution refinement learning outperforms GraphSAGE. Replacing the graph convolution network with GraphSAGE for embeddings refinement, MILE-gs does not perform as well as MILE. It is also computationally more expensive, partially due to its reliance on embeddings concatenation, instead of replacement, during the process the embeddings propagation (higher model complexity).\\u2022 Double-base embedding learning is not effective. In Sec. 4.3, we discuss the issues with unaligned embeddings of the double-base embedding method for the refinement model learning. The performance gap between MILE and MILE-2base in Table 4 provides empirical evidence supporting our argument. This gap is likely caused by the fact that the base embeddings of level m and level m + 1 might not lie in the same embedding space (rotated by some orthogonal matrix) BID8 . As a result, using the projected embeddings E p m as input for model training (MILE-2base) is not as good as directly using E m (MILE). Moreover, Table 4 shows that the additional round of base embedding in MILE-2base introduces a non-trivial overhead. On YouTube, the running time of MILE-2base is 1.6 times as much as MILE. We also study the impact of MILE on reducing memory consumption. For this purpose, we focus on MILE (GraRep) and MILE (NetMF), with GraRep and NetMF as base embedding methods respectively. Both of these are embedding methods based on matrix factorization, which possibly involves a dense objective matrix and could be rather memory expensive. We do not explore DeepWalk and Node2Vec here since their embedding learning methods generate truncated random walks (training data) on the fly with almost negligible memory consumption (compared to the space storing the graph and the embeddings). FIG7 shows the memory consumption of MILE (GraRep) and MILE(NetMF) as the coarsening level increases on Blog (results on other datasets are similar). We observe that MILE significantly reduces the memory consumption as the coarsening level increases. Even with one level of coarsening, the memory consumption of GraRep and NetMF reduces by 64% and 42% respectively. The dramatic reduction continues as the coarsening level increases until it reaches 4, where the memory consumption is mainly contributed by the storage of the graph and the embeddings. This memory reduction is consistent with our intuition, since both # rows and # columns in the objective matrix reduce almost by half with one level of coarsening. A.6 MILE Drilldown: Discussion on reusing \\u0398 (k) across all levels Similar to GCN, \\u0398 (k) is a matrix of filter parameters and is of size d * d (where d is the embedding dimensionality). Eq. 4 in this paper defines how the embeddings are propagated during embedding refinements, parameterized by \\u0398 (k) . Intuitively, \\u0398 (k) defines how different embedding dimensions interact with each other during the embedding propagation. This interaction is dependent on graph structure and base embedding method, which can be learned from the coarsest level. Ideally, we would like to learn this parameter \\u0398 (k) on every two consecutive levels. But this is not practical since this could be expensive as the graph get more fine-grained (and defeat our purpose of scaling up graph embedding). This trick of \\\"sharing\\\" parameters across different levels is the trade-off between efficiency and effectiveness. To some extent, it is similar to the original GCN BID13 , where the authors share the same filter parameters \\u0398 (k) over the whole graph (as opposed to using different \\u0398 (k) for different nodes; see Eq (6) and FORMULA11 in BID13 ). Moreover, we empirically found this works good enough and much more efficient. Table 4 shows that if we do not share \\u0398 (k) values and use random values for \\u0398 (k) during refinements, the quality of embedding is much worse (see baseline MILE-untr).A.7 MILE Drilldown: Discussion on choice of embedding methodsWe wish to point out that we chose the base embedding methods as they are either recently proposed NetMF (introduced in 2018) or are widely used (DeepWalk, Node2Vec, LINE). By showing the performance gain of using MILE on top of these methods, we want to ensure the contribution of this work is of broad interest to the community. We also want to reiterate that these methods are quite different in nature:\\u2022 DeepWalk (DW) and Node2vec (N2V) rely on the use of random walks for latent representation of features.\\u2022 LINE learns an embedding that directly optimizes a carefully constructed objective function that preserves both first/second order proximity among nodes in the embedding space.\\u2022 GraRep constructs multiple objective matrices based on high orders of random walk laplacians, factories each objective matrix to generate embeddings and then concatenates the generated embeddings to form final embedding.\\u2022 NetMF constructs an objective matrix based on random walk Laplacian and factorizes the objective matrix in order to generate the embeddings. Indeed NetMF BID18 BID14 with an appropriately constructed objective matrix has been shown to approximate DW, N2V and LINE allowing such be conducting implicit matrix factorization of approximated matrices. There are limitations to such approximations (shown in a related context by BID0 ) -the most important one is the requirement of a sufficiently large embedding dimensionality. Additionally, we note that while unification is possible under such a scenario, the methods based on matrix factorization are quite different from the original methods and do place a much larger premium on space (memory consumption) -in fact this is observed by the fact we are unable to run NetMF and GraRep in many cases without incorporating them within MILE.A.8 MILE Drilldown: Discussion on extending MILE to directed graphs Note that as pointed out by BID4 , one can construct random-walk Laplacians for a directed graph thus incorporating approaches like NetMF to accommodate such solutions. Another simple solution is to symmetrize the graph while accounting for directionality. Once the graph is symmetrized, any of the embedding strategies we discuss can be employed within the MILE framework (including the coarsening technique). There are many ideas for symmetrization of directed graphs (see for example work described by BID6 or BID19 .A.9 MILE Drilldown: Discussion on effectiveness of SEM The effectiveness of structurally equivalent matching (SEM) is highly dependent on graph structure but in general 5% -20% of nodes are structurally equivalent (most of which are low-degree nodes). For example, during the first level of coarsening, YouTube has 172,906 nodes (or 86,453 pairs) out of 1,134,890 nodes that are found to be SEM (around 15%); Yelp has 875,236 nodes (or 437,618 pairs) out of 8,938,630 nodes are SEM (around 10%). In fact, more nodes are involved in SEM as SEM is run iteratively at each coarsening level.\",\n          \"The ability of a classifier to recognize unknown inputs is important for many classification-based systems. We discuss the problem of simultaneous classification and novelty detection, i.e. determining whether an input is from the known set of classes and from which specific class, or from an unknown domain and does not belong to any of the known classes. We propose a method based on the Generative Adversarial Networks (GAN) framework. We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. We approximate that generator with a mixture generator trained with the Feature Matching loss and empirically show that the proposed method outperforms conventional methods for novelty detection. Our findings demonstrate a simple, yet powerful new application of the GAN framework for the task of novelty detection. In recent years we have witnessed incredible progress in AI, largely due to the success of Deep Learning, and more specifically Supervised Deep Learning. One of the basic requirements from a good supervised learning algorithm is generalization -the ability to classify input that is reasonably similar to the training data. However, usually there are no requirements whatsoever on how the classifier should behave for new types of input that differ substantially from the data that are available during training. In fact for such novel input the algorithm will produce erroneous output and classify it as one of the classes that were available to it during training. Ideally, we would like that the classifier, in addition to its generalization ability, be able to detect novel inputs, or in other words, we would like the classifier to say, \\\"I don't know.\\\" Novelty detection can be defined as the task of recognizing that test data differs in some manner from the data that was available during training. The problem of novelty detection arises in many fields and is closely related, but not identical, to the problems of anomaly detection (also outlier detection), where the goal is to recognize anomalous examples in a dataset BID2 . Novelty detection is a hard problem that has received relatively little attention in the ML literature. Nevertheless, in our opinion novelty detection should be a central part of every recognition system. For a comprehensive in-depth review of the topic of novelty detection, we refer the reader to BID18 . Popular conventional novelty detection methods such as Probabilistic methods BID18 , which perform density estimation of the examples from a nominal class, or Domainbased methods BID23 ; BID27 , which re-formulate novelty detection as a \\\"one-class classification\\\" problem and define a boundary around the nominal class, do not scale well to large high-dimensional datasets. Another class of novelty detection methods is distance-based methods, which assume that the nominal data are tightly clustered, while novel data occur far from their nearest neighbors. These methods require computationally expensive clustering or nearest neighbors search. One of the major drawbacks of conventional novelty detection methods is that at test time, they are separated from the classification algorithm and therefore increase the overall computational and design complexity of the system. Furthermore, combining novelty detection with a multi-class classifier into a single algorithm may leverage important intra and inter-class information in the training data which can benefit both tasks. In this paper we are interested in methods for simultaneous classification and novelty detection. A popular heuristic approach to simultaneous classification and novelty detection is thresholding the maximum of estimated class probability BID9 , or alternatively, to threshold the entropy of the estimated probability distribution. Another practical approach is to collect \\\"backgroundclass\\\" samples, that is, samples that are not from the nominal set and hopefully represent novel data. In this case, novelty detection can be reduced to a supervised learning problem. Unfortunately, this solution requires collecting a large set of background inputs -a time-consuming task and moreover, it is very difficult to sample a large enough background class that will represent all possible novel examples. We embrace the \\\"background-class\\\" sampling idea and ask is it possible to generate novel data? To the best of our knowledge, no method for novelty detection is based on generating novel examples. In this paper we examine whether Generative Adversarial Networks (GAN), a popular generative framework that can be used to generate novel examples. The GAN framework was proposed by BID8 , as a generative modeling method, mostly used for generating realistic samples of natural images. More specifically, GAN is an approach to generative modeling where two models are trained simultaneously: a generator and a discriminator. The task of the discriminator is to classify an input as either the output of the generator (\\\"fake\\\" data), or actual samples from the underlying data distribution (\\\"real\\\" data). The goal of the generator is to produce outputs that are classified by the discriminator as \\\"real\\\", or as coming from the underlying data distribution. In some formulations of GANs Odena (2016); BID25 , the discriminator is trained to classify data not only into two classes \\\"real\\\" and \\\"fake\\\", but rather into multiple classes. If the \\\"real\\\" data consists of K classes, then the output of the discriminator is K + 1 class probabilities where K probabilities corresponds to K known classes, and the K + 1 probability correspond to the \\\"fake\\\" class. In this paper, we propose to use a multi-class GAN framework for simultaneous classification and novelty detection. If during training the generator generates a mixture of nominal data and novel data, the multi-class discriminator learns to discriminate novel data from nominal data and essentially became a novelty detector. At test time, when the discriminator classifies a real example to the K + 1 th class, i.e., class which represented \\\"fake examples\\\" during training, this the example is most likely a novel example and not from one of the K nominal classes. In fact, we prove that in this case the discriminator become an optimal novelty detector (for a given false positive rate). We approximate such a mixture generator with a generator trained with specifically designed loss functions. In Section 2 we provide background and a theoretical justification to our proposed method and its connection to existing novelty detection methods. We validate the proposed method by a set of experiments in Section 3. 2.1 NOVELTY DETECTION In the novelty detection task, we assume that during inference, the classifier may be tested on data samples, which do not belong to the nominal data distribution p data (x), or in other words, novel data from p novel (x) and not from one of the K classes that the classifier is supposed to classify. If we had access to both p data (x) and p novel (x), the densities of the nominal data and novel data, then according to Neyman-Pearson's Lemma BID14 , the optimal novelty detection test for a given false positive rate \\u03b1 can be achieved by thresholding the likelihood ratio DISPLAYFORM0 at an appropriate value. In practice we don't know both distributions. A \\\"second best\\\" solution would have to have access to both nominal and novel examples, then novelty detection can be reduced to a supervised binary classification problem. However, in practice we do not have access to novel examples. One of the standard approaches to novelty detection is to estimate a level set of the nominal density p data (x) > \\u03b1, and to declare test points outside of the estimated level set as novel. Density level set estimation is equivalent to assuming that novelties are uniformly distributed on the support of p data (x). Unfortunately, these methods are difficult to implement since they require estimating the high-dimensional density of the nominal data. Level set methods are closely related to one-class classification methods BID23 ; BID27 and in general can be reduced to binary classification problem between nominal data and artificially generated sample from uniform distribution on the data domain, as was discussed by BID26 . BID0 discussed a Semi-Supervised Novelty Detection (SSND) problem, where in addition to the training dataset of nominal examples we have access to an unlabeled dataset that contains a mixture of nominal and novel examples. The authors showed that in this case it is possible to obtain an optimal (for a given false positive rate) novelty detector. Indeed, if we have access to nominal data distribution p data (x) and a mixture of the form DISPLAYFORM1 which leads to an appropriately scaled optimal novelty detector. Therefore, in the SSND case, novelty detection can still be reduced to a supervised classification problem. This is in contrast to the density estimation problem in which no novel data is available. Of course the problem remains to estimate the likelihood ratio from the training data. While semi-supervised novelty detection provides interesting insights, how can these ideas be applied to the fully unsupervised novelty detection problem where only nominal data are available during training? Level set methods can be solved by generating artificial examples from uniform distribution on data domain and formulating binary classification problem. But, is it possible to generate artificial examples from some non-uniform distribution which better represent novel data than uniform distribution? and Is it possible to generate examples from the mixture of nominal and non-uniform distributions to apply the SSND ideas? And finally, is it possible to combine these ideas for novelty detection with multi-class classification? In the next sections we demonstrate how we can leverage the GAN framework to generate examples from the mixture of nominal and non-uniform distributions and use the GAN's discriminator as a simultaneous multi-class classifier and novelty detector. Generative Adversarial Networks Goodfellow et al. FORMULA1 is a recently proposed approach for generative modeling. The main idea behind GANs is to have two competing differentiable functions, usually implemented as neural network models. One model, which is called the generator G(z; \\u03b8 G ), maps a noise sample z sampled from some prior distribution p(z) to the \\\"fake\\\" sample x = G(z; \\u03b8 G ); x should be similar to a \\\"real\\\" sample sampled from the nominal data distribution p data (x). The objective of the other model called the discriminator D(x; \\u03b8 D ) is to correctly distinguish generated samples from the training data which samples p data (x). This is a minimax game between the two models with a solution at the Nash equilibrium. For an excellent overview on GANs, see BID7 . Unfortunately, there is no closed-form solution for such problems. Therefore, the solution is approximated using an iterative gradient-based optimization of the generator and the discriminator functions. The discriminator is optimized by maximizing: DISPLAYFORM0 and the generator is optimized by minimizing: DISPLAYFORM1 For a fixed generator G(z) we can analytically derive that the optimal discriminator will take the form: DISPLAYFORM2 Since the introduction of Generative Adversarial Networks by BID8 , multiple variants of GANs were published in the literature. GANs were applied to various interesting tasks such as realistic image generation BID19 , text-to-image generation BID20 , video generation BID28 , image-to-image generation BID10 , image inpainting BID17 , super-resolution BID13 , and many more. In almost all of these applications, only the generator G is used at a test time, while the discriminator D is trained for the sake of training the generator and is discarded at test time. In contrast, introduced a GAN based semi-supervised classifier (SSL-GAN).They showed that in case where only a small fraction of the real examples have labels, and the bulk of the real data is unlabeled, their GAN framework is able to train a powerful multi-class classifier, which is the discriminator D. In order to improve the GAN convergence, proposed to optimize the generator by minimizing a Feature-Matching loss: DISPLAYFORM3 where f (x) is an intermediate layer of the fixed discriminator used as a feature representation of x. Optimizing a generator using the Feature Matching loss results in samples which are not of the best visual quality, but the resulting multi-class discriminator performs well for supervised classification and for semi-supervised classification as well. In the semi-supervised settings of multi-class classification, showed that SSL-GAN trained with the Feature Matching loss (eq.5) improves the classification accuracy of the discriminator. This improvement occurs even though the generated samples are not of the best visual quality. BID5 addressing the question \\\"why does the Feature Matching loss improves the semi-supervised discriminator\\\" made two important observations: In Proposition 1 BID5 show that when the generator is perfect, i.e. p g (x) = p data (x), it does not improve the generalization performance of the SSL-GAN discriminator over the supervised learning training without GAN. In Proposition 2 BID5 defined the complement generator, a generator which generates samples with feature representation f (x) distributed in a complementary region to the distribution support of the features of the real data. They show that under mild assumptions when training a multi-class discriminator with the complement generator the discriminator places the real-classes boundaries in low-density areas of the features distributions of the real data. The conclusion is that only a bad generator, where p g (x) = p data (x) and which generates samples outside of the high-density areas of the real data distribution, is able to improve semi-supervised learning as was demonstrated by .The results of and BID5 led us to propose the following definition of a mixture generator:Definition 1. Mixture generator is a generator with a mixture distribution p g (x) = \\u03c0p other (x) + (1 \\u2212 \\u03c0)p data (x) of the true data distribution p data (x) and some other distribution p other (x), such that there exists a non-empty region \\u2126 where {\\u2200x \\u2208 \\u2126, p other (x) > p data (x), p data (x) \\u2264 }, for some > 0.In other words the mixture generator is a generator that generates a mixture of true data distribution p data (x) and some other data p other (x), where at least part of the p other (x) probability mass is concentrated in lower-density regions of p data (x).In general, any generator distribution p g (x) that is different from p data (x), can be represented as a unique mixture of p data (x) and some other distribution, which by itself cannot be represented as a (nontrivial) mixture of p data (x) with another distribution (see Proposition 5 by BID0 ). However not any generator with p g (x) = p data (x) is a mixture generator. The requirement is that at least part of the p other (x) probability mass is concentrated in lower-density regions of p data (x) excludes cases where p g (x) = p data (x) but generates samples in high-density regions of p data (x) as happens in the case of mode collapse. The mixture generator can be viewed as a relaxed version of the theoretical complement generator defined by BID5 : every complement generator is a mixture generator with \\u03c0 = 1 and where p other (x) and p data (x) have disjoint support, but the opposite is not true. We can also consider the degenerate mixture generator: in the degenerate case p other (x) is a uniform distribution over the real data domain and \\u03c0 = 1 (assuming p data (x) is not uniform by itself). In this case no learning for such generator is required, and the generator only need to be able to generate uniformly distributed samples in data domain. For novelty detection we would like the generator to generate samples from the (unknown) distribution of the novel data p novel (x). This allows us to solve the novelty detection problem using the following observation:Proposition 1. For a fixed mixture generator G, if p other (x) defines a distribution of the novel data, i.e. p other (x) = p novel (x), then the optimal discriminator D * G (x) is also an optimal (for a given false positive rate) novelty detector. The proof of Proposition 1 follows trivially from the definition of the mixture generator, eq. 4 of the optimal discriminator for a fixed generator, and eq. 1 of the optimal novelty detector as in the SSND case. Indeed, if p g (x) = \\u03c0p novel (x) + (1 \\u2212 \\u03c0)p data (x) then we can invert eq. 4 and we have DISPLAYFORM0 which as in eq. 1 is an optimal novelty detector for a given false positive rate. The question is which mixture generator is capable of generating data from the unknown distribution p novel (x) of the real novel data. Unfortunately, without any knowledge of the specific distribution and without labeled real examples from this distribution, or at least unlabeled examples from a mixture of real novel data and nominal data, it is impossible to design a generator which will generate samples from it 1 . Nevertheless, we can train a mixture generator with specific properties of p other (x) which will help novelty detection. In the case of the degenerate mixture generator, i.e. when the generator generates samples uniformly distributed over data domain, the discriminator in eq. 6 is a level set novelty detector. As we mentioned in section 2.1 such reduction of the level-set novelty detection to a classification problem is well known in the literature BID26 and references therein). However, it is clear that sampling novel examples from a uniform distribution in a very high dimensional space is not an efficient strategy for training novelty detector. Therefore, we want a mixture generator that will generate novel data distributed in nearby surrounding or at low-density regions of the true data manifold. To train such a mixture generator we need a loss function that encourage the generator to do that. For example BID5 proposed a loss function based on the Kullback-Leibler (KL) divergence between the distributions of the generator distribution and distribution of the data. More specifically they propose to minimize: DISPLAYFORM1 where H(\\u00b7) is the entropy function and I[\\u00b7] is the indicator function. This loss function is designed to produce a generator with a distribution which on one hand has support which does not intersect with high density regions of the real data (second term), but still close to the data manifold (Feature Matching loss as a third term). Unfortunately, to train a generator with the loss function in eq. 7, we need to estimate p data (x), the same problem which we wanted to avoid in conventional novelty detection methods. In their paper BID5 experimentally demonstrated on two synthetic datasets that a generator that is trained with a Feature Matching loss eq. 5 is able to generates both samples that fall onto the data manifold, and samples which are scattered in the nearby surrounding of the data manifold, i.e. a mixture of real data and a data outside of data manifold. The empirical results of BID5 suggest that a generator that is trained with the Feature Matching loss only, is by itself a mixture generator. Moreover, we know from the results of BID5 that when the generator generates samples that are scattered around and in the low-density areas of the data manifold, the discriminator classification boundaries becomes tighter, thus improving the discriminator's ability to detect real novelties. In Section 3 we empirically demonstrate that when a multi-class discriminator trained with a mixture generator that was trained with the Feature Matching loss, has an impressive ability to detect real novel examples. To summarize, we propose to train a GAN in the multi-class setting with the Feature Matching loss or some other similar loss which facilitates the generator to be a mixture generator as in Definition 1 to solve the problem of simultaneous classification and novelty detection. When the multi-class discriminator of the network classifies an example as \\\"fake\\\", it means that most likely this example is a novel example, otherwise the class with the highest probability is the classification result. The novelty detection scores are the \\\"fake\\\" class probability or a related quantity such as DISPLAYFORM2 , the ratio between \\\"real\\\" and \\\"fake\\\" class-probabilities. The novelty detection scores are thresholded to achieve a desired false positive rate. The GAN based training produce a unified multi-class classifier and novelty detector with minimal additional computation overhead at inference time. Moreover, using unlabeled samples as in improve both the multi-class classifier and the novelty detector. We perform an experimental evaluation of the proposed GAN-based novelty detection method and compare it to several methods for simultaneous classification and novelty detection on the MNIST and CIFAR10 datasets. As a comparison metric we use the Area Under the Receiver Operating Characteristic curve (AUROC) of novelty detection scores. The first method that we compare to is based on the estimated class probabilities of a multi-class classifier. BID9 suggested to use the maximum of the estimated (softmax) probability as a baseline for out-of-distribution data (i.e. novelty) detection score. Another popular novelty score is the entropy of the estimated probabilities, which takes into account all the predicted class probabilities. These scores are highly correlated. In addition, we compare to methods that rely on nearest-neighbors distance analysis in feature space derived from the trained multi-class classifier (e.g. last convolutional layer of CNN). We use normalized k-NN distance as a novelty score as described by BID6 . The kNN novelty score of a test data point x is a distance to the k th nearest neighbor in the nominal training dataset DISPLAYFORM0 For k = 1, d is an l 2 distance in a feature space. For k > 1, d denotes the average distance to the k nearest neighbors. We evaluate this method using k = {1, 5}.Additionally we experimented with the OCSVM and the SVDD methods BID23 ; BID27 implemented in LIBSVM library BID3 . We tried various kernels and parameters values, but failed to achieve competitive results, even relative to the kNN methods. Our findings are supported by BID6 where kNN methods outperformed domain-based methods on multiple datasets. The fact that domain-based method struggling in applications involving high-dimensional spaces was also noted by BID18 .For the GAN based novelty detection score which we call ND-GAN, we trained the SSL-GAN model as described by using modified publicly available implementation of SSL-GAN 2 and using Theano Theano Development Team (2016). The mixture generator was trained with the Feature-Matching loss eq.5, and the novelty score of the ND-GAN was defined as the ratio: DISPLAYFORM1 , the ratio between\\\"real\\\" class and \\\"fake\\\" class probabilities. To make a fair comparison, when comparing to the competing methods, we trained the multi-class classifiers with the same architectures as the discriminator. We performed two experiments with MNIST dataset, novelty detection from other datasets and a holdout novelty detection test. First, we identify novelties with respect to the MNIST datasets by running MNIST trained classifiers on the Omniglot dataset of images of handwritten characters from BID12 , and notMNIST dataset images typeface characters from BID1 . 10000 MNIST test images are used as positive examples, while images from Omniglot and notMNIST datasets are used as a negative examples. the generator and the discriminator have 5 fully connected hidden layers each. Weight normalization BID21 was used and Gaussian noise was added to the output of each layer of the discriminator. To evaluate the other methods (kNN, entropy and max-probability), we trained a standard supervised network with the same architecture. The k-NN distances were computed from the 250 dimensional feature vectors of the last fully connected layer. For the ICDAR 2003 datasets the following ambiguous letters {o, O, i, I, z, Z, S, s,l} were excluded from the experiment. The ROC curves of the experiments are depicted in FIG0 . We see that the ND-GAN novelty score outperforms the other novelty detection methods in all experiments. In the holdout experiment, we train an SSL-GAN model and multi-class classifier with the same architecture as discriminator for each of the ten digits. Every model is trained on nine out of the ten classes. During testing, we compute the novelty scores for all of the classes, including the holdout class which is considered to be novel (negative). For the holdout class testing is performed on both the train and test examples in order to balance the number of nominal and novel examples. In Table 1 we present the AUROC of ND-GAN and the other competing methods. We see that in seven out of ten holdout experiments, ND-GAN outperforms the other methods, as well as the mean result. In the CIFAR experiment we train classifiers on the CIFAR10 train datatset and test for novelties on images from the CIFAR10 test datasets (nominal examples) and the CIFAR100 dataset (novel examples) Krizhevsky (2009) . The CIFAR100 dataset contains 100 fine and 20 coarse categories, which differ from the CIFAR10 categories (small overlaps in the data such as lion-cat, wolf-dog, truck-bus, etc. contributes equally to the error of all of the methods). For the ND-GAN novelty detector, we employ the architecture as in the SSL-GAN framework with 3000 labeled examples from each class. For the discriminator a 9 layer deep convolutional network with dropout and weight normalization was used. The generator has a 4 layer deep CNN with batch normalization. For the other methods (kNN, entropy and max-probability) the same architecture as in the discriminator was used. The k-NN distances are computed from the 192 dimensional feature Figure 2: CIFAR10 vs CIFAR100 experiment: The CIFAR10-train-set was used to train the model. The CIFAR10 test-set was used as a nominal data and the CIFAR100 test-set as a novel data.vectors of last convolution layer. Figure 2 depicts a ROC curves in which the CIFAR10 test set (nominal examples) is compared against the whole CIFAR100 test set (novel examples). We see that the ND-GAN novelty score performs comparably to other methods. We also compare each of the 20 coarse categories in CIFAR100 separately. The results of this experiment are presented in TAB1 . We see that in 13 out of 20 coarse CIFAR100 categories ND-GAN outperforms the other methods. The ability to identify novelties or say \\\"I don't know\\\" is an important tool for many classificationbased systems. In this work, we propose to solve a problem of simultaneous classification and novelty detection within the GAN framework. We propose to use a GAN with a mixture generator to turn this problem into a supervised learning problem without collecting \\\"background-class\\\" data. In the case where a mixture generator generates samples from a mixture of nominal data distribution and novel data distribution, we showed that the GAN's discriminator is an optimal novelty detector. We approximate that generator with a mixture generator trained with the Feature Matching loss. This mixture generator generates samples scattered around and in the low-density areas of the data manifold, and this makes a multi-class discriminator a powerful novelty detector. We empirically validate that the performance of the proposed solution is comparable to several popular novelty detection methods, and sometimes outperforms them. Clearly, evaluation of the proposed framework on more challenging datasets is required. As a future research direction we would like to search for new loss functions for mixture generators that will enrich the generator distribution and will improve novelty detection. Classification with asymmetric label noise problem BID24 is closely related to semi-supervised novelty detection, and it will be interesting to see whether the GAN framework can be used to solve this problem. Finally, it is interesting to see if the suggested framework can be applied to detecting the adversarial examples.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"HJXOfZ-AZ\",\n          \"HJeKCi0qYX\",\n          \"Hy7EPh10W\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Local codes have been found in feed-forward neural networks A method for determining to what degree individual neurons in a hidden layer of an MLP encode a localist code, which is studied for different input representations. Studies the development of localist representations in the hidden layers of feed-forward neural networks.\",\n          \"A generic framework to scale existing graph embedding techniques to large graphs. This paper proposes a multi-level embedding framework to be applied on top of existing network embedding methods in order to scale to large scale networks with faster speed. The authors propose a three-stage framework for large-scale graph embedding with improved embedding quality.\",\n          \"We propose to solve a problem of simultaneous classification and novelty detection within the GAN framework. Proposes a GAN to unify classification and novelty detection. The paper presents a method for novelty detection based on a multi-class GAN which is trained to output images generated from a mixture of the nominal and novel distributions. The paper proposes a GAN for novelty detection using a mixture generator with feature matching loss\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"When and where do feed-forward neural networks learn localist representations?\",\n          \"MILE: A Multi-Level Framework for Scalable Graph Embedding\",\n          \"Novelty Detection with GAN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 50,\n        \"max\": 75,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          50,\n          54,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"And, as this data is structured to have some invariance within a class and some randomness, it is proposed that these experiments could as be modelling the layers within the deep neural network above those which transform the input data from pixel space to feature space. Specifically: DISPLAYFORM0 The important point is the qualitative measure of whether or not a neuron is selective, not how much it is selective by, as we are interested in counting the number of local codes that emerge. Furthermore, local codes were highly unlikely to appear in the input codes and we did not observe any in a few random checks. Unless stated differently in the experiment, neural networks were run with sigmoidal neurons, 500 500bit long vectors separated into 10 classes, with 50bit long distributed output codes, no perturbation of the prototype block code and no dropout applied (these are the conditions for the black-dashed data in figures 3, 4 and 6 and this dataset is used as our standard to compare to).Figure 2: Schematic for building a random code with known properties. First, we should make that point that finding a single interpretable local code, such as those shown in figure 1, refutes the idea that neural networks do not have interpretable or locally encoded units. Furthermore, the number of local codes is tuned by the size the hidden layer, with a peak in the number of local codes seen at n HLN =1000 for the standard data set, and a peak in the percentage of HLNs which are local codes seen at n HLN = 500 (data not shown): dashed and dot-dashed grey lines are drawn at these points in all relevant figures. ReLU neurons generally produce a few more LCs, and have a peak shifted to a lower n HLN . As all the neural networks were trained for a specified number of steps, this finding may be due to ReLU neurons training quicker (more LCs emerge with longer training, which could be due to LCs being a more efficient solution or more likely due to more proto-selective codes passing the threashold for inclusion). FIG0 3.1 plots the decrease in the number of local codes against the perturbation rate, P , where P is defined as the number of bits randomly flipped to zero over the length of the prototype block for the two networks marked with vertical dashed lines in the previous figures. Furthermore, this does raise the interesting question of whether more local codes will be found in categories that are similar (i.e. a network learning the difference between dog and cat pictures) than in categories that are more random (such as two categories made of random mixes of dog and cat photos), and thus, can the existence of emergent local codes be a measure of true categorical similarity? However, with these higher values, the range of solutions is much higher (as evidence by a higher variance and range of the number of local codes), which is expected as dropout forces the network to adopt a range of solution sub-networks, the increase in local codes suggest that localised encoding offers some protection against noise. As the average number and range of LCs generally increases with dropout, and the LCs are repressed by a fully locally encoded output layer, it suggests that some local codes are good to have, and that number increases with noisy networks. The fact that the dropout data seems to contain multiple overlapping peaks, and, in our tests, peak numbers of LCs are seen at 500 and 1000 (and 2000 for the S R = 1 9 data) HLNs implies that there are more than one qualitative approaches for the neural network to solve the problem, and tuning the problem and neural network parameters nudges the solution to different distributions of local codes. The data presented here was designed to have invariant feature 'short-cuts' that the neural network could make use of in classifying input data into classes and the argument could well be made that the data passed between layers of a deep neural network is not of the same quality. Whilst an obvious next experiment for us is to investigate the qualities of the data passed within a neural network, preliminary feed-forward neural network training on standard simple neural network data (such as the Iris dataset from Fisher (1936)) results also show the emergence of local codes when there is a 'short-cut' in the data (publication in preparation). The observations that local codes are seen under dropout, with distributed input and output codes, when there are invariant features and local codes are inhibited with 1-hot output encodings, suggests that local codes might be found in a the middle and higher layers of a deep network, and not the penultimate layer where the 1-hot output could inhibit them or the early layers where invariant features common to a class have not yet been identified.\",\n          \"Specifically, we ask: 1) Can we scale up the existing embedding techniques in an agnostic manner so that they can be directly applied to larger datasets?2) Can the quality of such embedding methods be strengthened by incorporating the holistic view of the graph?To tackle these problems, we propose a MultI-Level Embedding (MILE) framework for graph embedding. To summarize, we find that:\\u2022 MILE is generalizable : Our MILE framework is agnostic to the underlying graph embedding techniques and treats them as black boxes.\\u2022 MILE is scalable : MILE can significantly improve the scalability of the embedding methods (up to 30-fold), by reducing the running time and memory consumption.\\u2022 MILE generates high-quality embeddings : In many cases, we find that the quality of embeddings improves by levering MILE (in some cases is in excess of 10%). Moreover, it is not immediately obvious how a HARP like methodology would be extended to other graph embedding techniques (e.g., GraRep and NetMF) in an agnostic manner since such an approach would necessarily require one to modify the embedding methods to preset their initialized embeddings. Here the set of nodes forming a super-node is called a matching. While this way of simple projection maintains some information of node embeddings, it has obvious limitations that nodes will share the same embeddings if they are matched and collapsed into a super-node during the coarsening phase. The k-th layer of this neural network model is DISPLAYFORM2 where \\u03c3(\\u00b7) is an activation function, \\u0398 (k) is a layer-specific trainable weight matrix, and DISPLAYFORM3 In this paper, we define our embedding refinement model as a l-layer graph convolution model DISPLAYFORM4 The architecture of the refinement model is shown in FIG1 . Since the graph convolution model H (l) (\\u00b7) aims to predict the embeddings E i on graph G i , we can directly run a base embedding on G i to generate the \\\"ground-truth\\\" embeddings and use the difference between these embeddings and the predicted ones as the loss function for training. With this strategy, we change the loss function for model learning as follows DISPLAYFORM1 With the above loss function, we adopt gradient descent with back-propagation to learn the parameters DISPLAYFORM2 In the subsequent refinement steps, we apply the same set of parameters \\u0398 (k) to infer the refined embeddings. The details about the datasets used in our experiments are :\\u2022 PPI is a Protein-Protein Interaction graph constructed based on the interplay activity between proteins of Homo Sapiens, where the labels represent biological states.\\u2022 Blog is a network of social relationship of bloggers on BlogCatalog and the labels indicate interests of the bloggers.\\u2022 Flickr is a social network of the contacts between users on flickr.com with labels denoting the interest groups.\\u2022 YouTube is a social network between users on YouTube, where labels represent genres of groups subscribed by users.\\u2022 Yelp is a social network of friends on Yelp and labels indicate the business categories on which the users review. Baseline Methods: To demonstrate that MILE can work with different graph embedding methods, we explore several popular methods for graph embedding.\\u2022 DeepWalk (DW) BID17 : Following the original work BID17 , we set the length of random walks as 80, number of walks per node as 10, and context windows size as 10.\\u2022 Node2Vec (NV) BID7 : We use the same setting as DeepWalk for those common hyper-parameters while setting p = 4.0 and q = 1.0, which we found empirically to generate better results across all the datasets.\\u2022 Line (LN) : This method aims at preserving first-order and secondorder proximities and has been applied on large-scale graph. The reduction in time complexity is attributed to the fact that we run the embedding learning and refinement model training at the coarsest graph. Because of this, even when the complexity of the original embedding algorithm is linear to E, our MILE framework could still potentially speed up the embedding process because the complexity of MILE contains a smaller constant factor k (see Sec. 5.2 for the experiment of applying MILE on LINE).Furthermore, it is worth noting that many of the existing embedding strategies involve hyperparameters tunning for the best performance, especially for those methods based on neural networks (e.g., DeepWalk, Node2Vec, etc.). We add self-loop to each node 6 and conduct the embeddings propagation for two rounds.\\u2022 Untrained Refinement Model (MILE-untr): Instead of training the refinement model to minimize the loss defined in Eq. 7, this baseline merely uses a fixed set of values for parameters \\u0398 (k) without training (values are randomly generated; other parts of the model in Eq. 4 are the same, including\\u00c3 andD). It conducts one more layer of coarsening and base embedding (level m + 1), from which the embeddings are projected to level m and used as the input for model training.\\u2022 GraphSAGE as Refinement Model (MILE-gs): It replaces the graph convolution network in our refinement method with GraphSAGE BID8 7 . Table 4 shows that if we do not share \\u0398 (k) values and use random values for \\u0398 (k) during refinements, the quality of embedding is much worse (see baseline MILE-untr).A.7 MILE Drilldown: Discussion on choice of embedding methodsWe wish to point out that we chose the base embedding methods as they are either recently proposed NetMF (introduced in 2018) or are widely used (DeepWalk, Node2Vec, LINE). There are limitations to such approximations (shown in a related context by BID0 ) -the most important one is the requirement of a sufficiently large embedding dimensionality.\",\n          \"We show that a multi-class discriminator trained with a generator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. One of the major drawbacks of conventional novelty detection methods is that at test time, they are separated from the classification algorithm and therefore increase the overall computational and design complexity of the system. In contrast, introduced a GAN based semi-supervised classifier (SSL-GAN).They showed that in case where only a small fraction of the real examples have labels, and the bulk of the real data is unlabeled, their GAN framework is able to train a powerful multi-class classifier, which is the discriminator D. In order to improve the GAN convergence, proposed to optimize the generator by minimizing a Feature-Matching loss: DISPLAYFORM3 where f (x) is an intermediate layer of the fixed discriminator used as a feature representation of x. Optimizing a generator using the Feature Matching loss results in samples which are not of the best visual quality, but the resulting multi-class discriminator performs well for supervised classification and for semi-supervised classification as well. The conclusion is that only a bad generator, where p g (x) = p data (x) and which generates samples outside of the high-density areas of the real data distribution, is able to improve semi-supervised learning as was demonstrated by .The results of and BID5 led us to propose the following definition of a mixture generator:Definition 1. The question is which mixture generator is capable of generating data from the unknown distribution p novel (x) of the real novel data. In their paper BID5 experimentally demonstrated on two synthetic datasets that a generator that is trained with a Feature Matching loss eq. 5 is able to generates both samples that fall onto the data manifold, and samples which are scattered in the nearby surrounding of the data manifold, i.e. a mixture of real data and a data outside of data manifold. Moreover, we know from the results of BID5 that when the generator generates samples that are scattered around and in the low-density areas of the data manifold, the discriminator classification boundaries becomes tighter, thus improving the discriminator's ability to detect real novelties. In Section 3 we empirically demonstrate that when a multi-class discriminator trained with a mixture generator that was trained with the Feature Matching loss, has an impressive ability to detect real novel examples. To summarize, we propose to train a GAN in the multi-class setting with the Feature Matching loss or some other similar loss which facilitates the generator to be a mixture generator as in Definition 1 to solve the problem of simultaneous classification and novelty detection. We perform an experimental evaluation of the proposed GAN-based novelty detection method and compare it to several methods for simultaneous classification and novelty detection on the MNIST and CIFAR10 datasets. In addition, we compare to methods that rely on nearest-neighbors distance analysis in feature space derived from the trained multi-class classifier (e.g. last convolutional layer of CNN). The mixture generator was trained with the Feature-Matching loss eq.5, and the novelty score of the ND-GAN was defined as the ratio: DISPLAYFORM1 , the ratio between\\\"real\\\" class and \\\"fake\\\" class probabilities. In the holdout experiment, we train an SSL-GAN model and multi-class classifier with the same architecture as discriminator for each of the ten digits. For the holdout class testing is performed on both the train and test examples in order to balance the number of nominal and novel examples. In the case where a mixture generator generates samples from a mixture of nominal data distribution and novel data distribution, we showed that the GAN's discriminator is an optimal novelty detector. This mixture generator generates samples scattered around and in the low-density areas of the data manifold, and this makes a multi-class discriminator a powerful novelty detector.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "d82e9210-1918-402f-e8d1-5d1d74368b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "397ea569-7716-43a0-9844-6e85d48cb2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "0a16c414-610c-4c3e-f047-3fdb26828ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      shuffle=True,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        shuffle=True,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "686a38d2-0f41-4406-adb2-501f28f1688d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 8) (16, 8) (80, 8) (20, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenize data\n",
        "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# # Function in order to tokenize source and target\n",
        "# max_input_length = 1024\n",
        "\n",
        "# def tokenize_function(data):\n",
        "#   model_inputs = tokenizer(text=data['extractive_summary'], text_target=data['target'], max_length=max_input_length, truncation=True)\n",
        "#   return model_inputs\n",
        "\n",
        "# tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "VyuuPBgIsJim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "c5099645-f9f4-4e76-bf89-ea14c45d51a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "4436655f7773484fafd1cff33ef78ffb",
            "d0c8fbe4c9cc47ef9de30e6f1d7b8a00",
            "5ddedeae3c37411f809914dd40626747",
            "2b9d07ace2bf4dd6864c4e7f70d8d184",
            "365f866542054b8096f0425b740e3657",
            "88891a0283ed46ec815622f98ec2e88d",
            "f0ae46782ed54cda804a8a992bff0092",
            "9125306d57674cc79e44cbbc6fa73493",
            "7351b89d38c841699116d5aa21d2b8e6",
            "8df3401496304e62b772d6b9dfa54491",
            "34a039133bef4ba297da41e8e252412f",
            "ae83c009d6af44cbb7235c317c8959d7",
            "e4774ef2229d4eb0ab8335fecf443bfa",
            "c2a1348eca4f40ea8f9976036ab98d2c",
            "a2e1287ad5da41138aab3041744c2eb4",
            "1c129f9f527e4511ab67540e3cf838e4",
            "a3ca1257e20946749a7a6b2dc2ea7194",
            "8749f001e7a24926bffc01a5491e1e8a",
            "5e97b25494a744f7bfbf01198bb4eab0",
            "2d628a6203104fa696604ec4609eeffc",
            "ba8eea1a6051482394bcd1176906e8f7",
            "5429d5d2ce5543bfaf691acd3abae273",
            "66beca881e744a0d95d12bbcb071f9e6",
            "e32b30837b684ea1a9369c97610084b5",
            "9174c28f178e4720b081af53ace7648a",
            "148c7002ce6a4dc491ee90bd3fd53e5f",
            "a98ba1580ced44dfaf4573d89e4f20cb",
            "f277c50799e447998d6091f3d671d8ef",
            "77a6a099c602449cab53c4331133baf9",
            "2bdc6e8e939b4f04ace92239a8d59dc2",
            "a20f3e6076ee4028bae0076528f657c4",
            "a809ae1cbf4941e28a5ee6b8990f0d47",
            "582775feaacc4557a9052e37fa54d1f1",
            "79a88598be7e47e2b3df8f13fd34d4b3",
            "cb47d7607757459396206b5c29bd7584",
            "fb62bc7f31ca4f3799a11d035c75a87d",
            "7d7598e022994663a060abfaeebf8f50",
            "8389228f026241feb945dd390466b2ba",
            "33ca2f7b5b2e424aa24ae42a85478c77",
            "0d0586338ea242ac885a4167a824cfa3",
            "387c04fc5fbb421b87f9eaf0cc150e8b",
            "4aa0091c91764f6ca2e21d65c45d18eb",
            "6bcfa13acbad4f1e9fb284a68f61d9a9",
            "77f1d6f94bab409cbfadfd13c7597f3a",
            "972f7cc0237f488ab0a54befbfe85b26",
            "5bac7c1c2a544f19b3bd5b3ec89b5cfa",
            "81aff29d06114ac585119b7c0fe6dfe4",
            "e1794a5fee8b4b3cb693689b7db95cdf",
            "17f3736bee3647979f096f40113e0963",
            "56550977be954f68816b767163347f62",
            "49d7da777634415da2235bf936ee4abe",
            "3d06964dc33345948b552a48f26af805",
            "901d231559a646bf8ef44f11ad6a7d1f",
            "27e9f6cbeacb4e41a518ac76d93be103",
            "ba259db36cbf4c16bfd297cfa6d563a9",
            "55d2683ac2754716b0e0d9e3278cfe89",
            "fc2a494fbc2d4877ae995d6b628a20e9",
            "702b85d5fab64c92ad5aceb22590dad4",
            "f80148a45c1a4719a2be4a2dc2212532",
            "5e092b22d4204ac2a5460189e74181d8",
            "c7488fa5a9a0421ba176d0ff94257498",
            "503836257374431893318438c40c02f3",
            "b477dab0156d418b8628ce7eb0101f2b",
            "be4b80a5029443a687913fab23f85209",
            "4300241ad43a426389660ba557b5041e",
            "97cd20bbd1974fe49435f102ea74242d",
            "c6e6414e83b5478e9bd21b830b0fb7b7",
            "f01fcdd5a3804589b6dd9b0353298e0d",
            "2cb53e5c8fb9432795fa00dbd28a95ae",
            "7f3636d640fe4670b623e492f8a336f7",
            "01871aacaa8649f2ab1bdb072114f1c9",
            "f225c165ce93444a97f893b8ab1b4cc9",
            "152a86e33f864838af4780a780a170b6",
            "7aa29606933f4bed8700a14e1596a938",
            "b9a8843efae24b7a9e987ebc7d47e13c",
            "715ef094f367439bbaf1128eb6d41f6b",
            "cf68125bf1a14540825eab2fa30ce1fa"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4436655f7773484fafd1cff33ef78ffb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae83c009d6af44cbb7235c317c8959d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66beca881e744a0d95d12bbcb071f9e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79a88598be7e47e2b3df8f13fd34d4b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/192 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "972f7cc0237f488ab0a54befbfe85b26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/48 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55d2683ac2754716b0e0d9e3278cfe89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6e6414e83b5478e9bd21b830b0fb7b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = True\n",
        "model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "# model.generation_config.renormalize_logits = True\n",
        "\n",
        "model.config.attention_dropout = 0.1\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "name_model = 'sampling-norep-v3/'\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "2160793f-5100-4075-8d55-b3b4c91ed2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXfD8z7vdqC",
        "outputId": "90204fd9-098e-49f2-905d-a0c38187493b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartConfig {\n",
              "  \"_name_or_path\": \"facebook/bart-base\",\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"gelu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"BartModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.1,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 768,\n",
              "  \"decoder_attention_heads\": 12,\n",
              "  \"decoder_ffn_dim\": 3072,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"dropout\": 0.1,\n",
              "  \"early_stopping\": true,\n",
              "  \"encoder_attention_heads\": 12,\n",
              "  \"encoder_ffn_dim\": 3072,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_position_embeddings\": 1024,\n",
              "  \"model_type\": \"bart\",\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": true,\n",
              "  \"num_beams\": 4,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"scale_embedding\": false,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 128,\n",
              "      \"min_length\": 12,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_cnn\": {\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 142,\n",
              "      \"min_length\": 56,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_xsum\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 62,\n",
              "      \"min_length\": 11,\n",
              "      \"num_beams\": 6\n",
              "    }\n",
              "  },\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset to inspect the batches\n",
        "for batch in train_dataset.take(100):  # Take the first batch for inspection\n",
        "    print(batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CsKTRNhvqCQ",
        "outputId": "38adf07c-d213-4243-e990-5af2b6feae17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3376,  5969, ...,     1,     1,     1],\n",
            "       [    0, 44891,     7, ...,     1,     1,     1],\n",
            "       [    0,     6,   992, ...,    81, 14307,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 39936, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4195, ..., 28695,     5,     2],\n",
            "       [    0, 13863,    89, ...,     1,     1,     1],\n",
            "       [    0, 46797,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16991,     9, ...,     1,     1,     1],\n",
            "       [    0,  9690, 16894, ...,  5342,  2222,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 26039, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 15243,   484, ...,     1,     1,     1],\n",
            "       [    0, 21119,  4945, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  6680, ...,     1,     1,     1],\n",
            "       [    0,   170,  3608, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 29235, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 16215, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 47380, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1106,   215, ...,     8,  1850,     2],\n",
            "       [    0,  3972, 22016, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,   170,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 47302, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 43043, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 33731,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,  4340, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,  6448, ...,     1,     1,     1],\n",
            "       [    0,   387, 35948, ...,     1,     1,     1],\n",
            "       [    0,  1121,   937, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 39231, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   717, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   846,    12, ...,     1,     1,     1],\n",
            "       [    0, 10105,     9, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    28, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 17629, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 47032, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6373, ...,     1,     1,     1],\n",
            "       [    0, 46874,  2088, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 43123, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,  3854,     9,     2],\n",
            "       [    0,   170,    67, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13360, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 13033, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  5709, ...,   230,     6,     2],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,  8269, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709, 25342, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0,  2522,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0, 3684, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  4528,   426, ...,     1,     1,     1],\n",
            "       [    0,   250,   864, ...,     1,     1,     1],\n",
            "       [    0,   170,  2807, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  9437, ...,     1,     1,     1],\n",
            "       [    0, 40450,  9097, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3084, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46498, ...,     1,     1,     1],\n",
            "       [    2,     0, 17105, ...,     1,     1,     1],\n",
            "       [    2,     0, 46444, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,    41, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     5, 14612,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 5320, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9355, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42158, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  6243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 3762,    9, ...,    1,    1,    1],\n",
            "       [   0, 3762,  169, ...,    1,    1,    1],\n",
            "       [   0,  713,   34, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  170, 1455, ...,    1,    1,    1],\n",
            "       [   0,  170,  109, ...,    1,    1,    1],\n",
            "       [   0, 5975,  272, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 42578, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   534, ..., 37357,     5, 23341],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 18377,     5, ...,     1,     1,     1],\n",
            "       [    0, 39936,  1364, ...,     1,     1,     1],\n",
            "       [    0,   133,  4472, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   713,   892, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1966, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 14563, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 30597, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133, 30673, ...,     1,     1,     1],\n",
            "       [    0,   170, 33461, ...,     1,     1,     1],\n",
            "       [    0, 49111,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   243,    16, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42274, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 46692, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   216, ...,     1,     1,     1],\n",
            "       [    0,  9058,  1537, ...,  3854,  6533,     2],\n",
            "       [    0,  2522, 15491, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   510,  8631, ...,     1,     1,     1],\n",
            "       [    0, 45461,  6448, ...,     1,     1,     1],\n",
            "       [    0, 27728,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 46011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 41084, ...,     1,     1,     1],\n",
            "       [    2,     0,   113, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   386, ...,     1,     1,     1],\n",
            "       [    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,   713,  1639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    64, ...,     1,     1,     1],\n",
            "       [    0,   565, 26582, ...,     1,     1,     1],\n",
            "       [    0,  4528,  6448, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1],\n",
            "       [    2,     0,  8532, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,   133,  2731, ...,   141,  1365,     2],\n",
            "       [    0,  5771,   258, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 14246, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       [    0,   170,   492, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 34447, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 48293,  1836, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,  5428, 22098,     2],\n",
            "       [    0,   133,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       [    2,     0, 47744, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12592, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,    12,   170, ...,     1,     1,     1],\n",
            "       [    0,   713,   173, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,  1779,    89, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 40089, 25373, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   448,  7629, ...,     1,     1,     1],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1106,    52, ...,    33,  4163,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 25077, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  2765, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1106,    52, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 45288,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133,   434, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     7,  1807,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0, 9167, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  1197, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   717,  6486, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 33837, 10518, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 18522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,     5, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  7939,  1423, ...,  3278,    63,     2],\n",
            "       [    0,   133,   335, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42489, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 28062, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,   173, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,  1296,   114,     2],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2847,     6, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11321, 20237, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 34647, ...,     1,     1,     1],\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,    52, ...,     1,     1,     1],\n",
            "       [    0, 21461,    11, ...,     1,     1,     1],\n",
            "       [    0,  2765,  4655, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  4442, ...,     1,     1,     1],\n",
            "       [    0, 45408, 19047, ...,     1,     1,     1],\n",
            "       [    0,   713,  1548, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 6179, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0, 2709, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 38386,    10, ...,     1,     1,     1],\n",
            "       [    0,  4528, 15716, ...,     1,     1,     1],\n",
            "       [    0,   448,    36, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1850, ...,     1,     1,     1],\n",
            "       [    0, 35416,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 14484, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  9344, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3813,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,     5, ...,     1,     1,     1],\n",
            "       [    0, 44863,  1319, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,    84, ...,     1,     1,     1],\n",
            "       [    0,   133,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36984, ...,     1,     1,     1],\n",
            "       [    2,     0, 20930, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,   819, 21154,     2],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1106,   854, ...,     1,     1,     1],\n",
            "       [    0,  1213,    67, ...,  4091, 48981,     2],\n",
            "       [    0,   170,   694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   102, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 34788, ...,     1,     1,     1],\n",
            "       [    2,     0, 35660, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0, 1121,  171, ...,  347,   12,    2],\n",
            "       [   0, 1121, 1285, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  713,   16, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170, 9637, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 33020, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42713,     6, ...,     1,     1,     1],\n",
            "       [    0,  4771,  3109, ...,     1,     1,     1],\n",
            "       [    0, 44908,  4843, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35716,    87, ...,    11, 37365,     2],\n",
            "       [    0,   133, 39135, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13755, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  5393, ...,     1,     1,     1],\n",
            "       [    0,  1121,  6477, ...,     1,     1,     1],\n",
            "       [    0,   243,    64, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 30872,   724, ...,     1,     1,     1],\n",
            "       [    0, 12444,   857, ...,     1,     1,     1],\n",
            "       [    0,  9690,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  5448, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1],\n",
            "       [    0, 18377,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,  9157, 16771, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,  6647, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   102, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43253, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 30770,     6, ...,     1,     1,     1],\n",
            "       [    0,   170, 17013, ...,     1,     1,     1],\n",
            "       [    0,   133,  1850, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1],\n",
            "       [    0,   133, 13477, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 23996, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4286, ...,     1,     1,     1],\n",
            "       [    0, 44311,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,   817, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5383, 38416, ...,     1,     1,     1],\n",
            "       [    0,  1779,  3563, ...,     9,   230,     2],\n",
            "       [    0, 13863,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  1109, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2765, 24097, ...,     1,     1,     1],\n",
            "       [    0, 23055,  8738, ...,     1,     1,     1],\n",
            "       [    0,  2522,  5694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 18776, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40103, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 14721,  9179, ...,     1,     1,     1],\n",
            "       [    0,   170,  6053, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0, 13863,  3326, ...,    16,   888,     2],\n",
            "       [    0, 43872,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42124, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0,  250, 4819, ...,    1,    1,    1],\n",
            "       [   0, 2709, 4327, ...,    1,    1,    1],\n",
            "       [   0,  713,  173, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  133, 5849, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170,  311, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 24514, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42489, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,  2655, 20992,     2],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 43195,  7651, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,  1236,    15,     2],\n",
            "       [    0,  1121,  1524, ..., 45371,    15,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44188, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38416, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0, 13863,    51, ...,     1,     1,     1],\n",
            "       [    0,  3762,  1860, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 40846, ...,     1,     1,     1],\n",
            "       [    0,  1620,    52, ...,     1,     1,     1],\n",
            "       [    0,  5771,   171, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 13360,    12, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 27477, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43780, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0, 39936,   775, ...,    52,    33,     2],\n",
            "       ...,\n",
            "       [    0,  1342,  4458, ...,     1,     1,     1],\n",
            "       [    0,  3908,     5, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ..., 19282,     6,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 36949,    41, ...,     1,     1,     1],\n",
            "       [    0,  4688,   419, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,    42, ...,   775,    36,     2],\n",
            "       [    0,  9344,  1938, ...,     1,     1,     1],\n",
            "       [    0,   170,  7015, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 1694, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  5203, ...,     1,     1,     1],\n",
            "       [    0, 39531,  4400, ...,     1,     1,     1],\n",
            "       [    0, 45297,    15, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 15491, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 36542, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0,  3908,  2284, ...,   922,  4791,     2],\n",
            "       ...,\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 30597, 10244, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 48313, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   173, ...,     1,     1,     1],\n",
            "       [    0, 40566,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  1548, ...,     1,     1,     1],\n",
            "       [    0, 23271,     9, ..., 42472, 26070,     2],\n",
            "       [    0, 48454,    12, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41933, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   170,   240, ...,     1,     1,     1],\n",
            "       [    0, 43714,    40, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0,   133,   485, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  9685, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45336, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   486, ...,     1,     1,     1],\n",
            "       [    0,   133, 28894, ...,     1,     1,     1],\n",
            "       [    0, 38386,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   970,    16, ...,  3364,     5,     2],\n",
            "       [    0,   713, 12360, ...,     1,     1,     1],\n",
            "       [    0,  1121,   485, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  1034, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0, 29182,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0, 18377,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,   170,  2883, ...,     1,     1,     1],\n",
            "       [    0,   170,  2639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  5848, ...,    36, 13424,     2],\n",
            "       [    0, 44863,    31, ...,     1,     1,     1],\n",
            "       [    0, 48684,   680, ..., 20145,  4007,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 49360,    11, ...,  6068,   600,     2],\n",
            "       [    0, 45942,  6448, ...,     1,     1,     1],\n",
            "       [    0, 30383, 26713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 41542,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1285, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45356, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40884, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1213,  1157, ..., 20910,    73,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0,  1779,  1058, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 39972,    52, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 28588, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     1,     1,     1],\n",
            "       [    0, 20319,  2408, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   800, ...,     1,     1,     1],\n",
            "       [    0,  2709,    55, ...,     1,     1,     1],\n",
            "       [    0, 10653,   428, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 243, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    97, ...,     1,     1,     1],\n",
            "       [    0,  4528, 41885, ...,     1,     1,     1],\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,   170,  1455, ...,     1,     1,     1],\n",
            "       [    0,  1620,    41, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10127, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   243, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250,  1353, ...,     1,     1,     1],\n",
            "       [    0,   713,  3315, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  1989, ..., 14612, 26070,     2],\n",
            "       [    0,  3972,  1100, ...,     1,     1,     1],\n",
            "       [    0,  5320, 10074, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44466, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44863, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   133, 32809, ...,     1,     1,     1],\n",
            "       [    0,     6,  3023, ...,  1558, 15421,     2],\n",
            "       ...,\n",
            "       [    0, 10653,   428, ...,     1,     1,     1],\n",
            "       [    0, 20861, 44871, ...,     1,     1,     1],\n",
            "       [    0,   713,   839, ...,     8,    63,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   347, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 48455, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,     1,     1,     1],\n",
            "       [    0, 21438,   520, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522, 11909, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,   133, 16681, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,   936, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  4528,  8369, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 27331,   937, ...,     1,     1,     1],\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 21680, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  6209,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 15393, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0, 20086, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3972,     5, ...,     1,     1,     1],\n",
            "       [    0,   170, 24934, ...,     1,     1,     1],\n",
            "       [    0,  2522, 39030, ...,  3907,     4,     2],\n",
            "       ...,\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       [    0,   170,   892, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     6,   549,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   176, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 47515, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45566, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   574,  3439, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   163, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,    43,   396,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133, 15306, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ..., 12612,   534,     2],\n",
            "       [    0,  3972,  1306, ...,     1,     1,     1],\n",
            "       [    0, 19847,  1239, ...,  6315, 36173,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4993, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 46159, 43141, ...,     1,     1,     1],\n",
            "       [    0, 10777,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121, 14117, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 13863,  1337, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42200,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 565, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 28084,     7, ...,     1,     1,     1],\n",
            "       [    0,  1121,   144, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 38416,    29, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,  1121,  5709, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 25382, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,   511, 22772,     2],\n",
            "       [    0,  3762,     9, ...,     1,     1,     1],\n",
            "       [    0,   133,   986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   172, ...,     1,     1,     1],\n",
            "       [    0,  3972, 33942, ...,   892,  2939,     2],\n",
            "       [    0, 45875,     6, ...,    31,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 1121, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  717, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 15393, ...,    11,   130,     2],\n",
            "       [    0, 20867,  7316, ...,     1,     1,     1],\n",
            "       [    0,   713,  5665, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  9058, 24454, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44426, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771, 10364, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,   209, ...,     1,     1,     1],\n",
            "       [    0,   133,  8611, ...,     1,     1,     1],\n",
            "       [    0,  3972, 19893, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44908, ...,     1,     1,     1],\n",
            "       [    2,     0, 34002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 19186, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23803,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133,   538, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0,  5771,   144, ...,     1,     1,     1],\n",
            "       [    0,  2765,  2623, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 19163, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  3034, ...,     1,     1,     1],\n",
            "       [    0,  1121,  2171, ...,     1,     1,     1],\n",
            "       [    0,  3972,     5, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0, 17425, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   448, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   387,   293, ...,    16,   505,     2],\n",
            "       [    0, 21518,  1537, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 45628, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2409,   114, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288,  8150, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,    45,   946,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,   211, ...,   683,    36,     2],\n",
            "       [    0,   250,   194, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1620,   251, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,   740,     6,     2],\n",
            "       [    0,  1121,   103, ...,   255,     8,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 28062, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17312, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38741, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 35166, 37700, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,     1,     1,     1],\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 24989,  7373, ...,     1,     1,     1],\n",
            "       [    0,  9157, 37794, ...,     1,     1,     1],\n",
            "       [    0,   717,  4182, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10928, ...,     1,     1,     1],\n",
            "       [    2,     0, 15622, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4897, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   387,   293, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771,   419, ...,   468,   321,     2],\n",
            "       ...,\n",
            "       [    0,   530,   495, ...,     1,     1,     1],\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,  2522, 40150, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 9685, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4554, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11913, 26739, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35490,     5, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       [    0,   713,  5044, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   338, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,  1365, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,  1121,   171, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 48812,  2577, ...,    14, 20070,     2],\n",
            "       [    0,  5771,   608, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,    14,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 10516, ...,     1,     1,     1],\n",
            "       [    0,   713,  1421, ...,   163,  2688,     2],\n",
            "       [    0,  3762,    16, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,   133,  2270, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12444, ...,     1,     1,     1],\n",
            "       [    2,     0, 22011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250, 17309, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9322, ...,     1,     1,     1],\n",
            "       [    0,  3762,   169, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288, 15380, ...,     1,     1,     1],\n",
            "       [    0,   133,  7626, ...,     1,     1,     1],\n",
            "       [    0,   170,   304, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 26412, ...,     1,     1,     1],\n",
            "       [    2,     0, 46101, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,    10, ...,   204,     6,     2],\n",
            "       [    0,  9690,  1202, ...,     1,     1,     1],\n",
            "       [    0,  7605,   209, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2571,  6018, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   250, 31809, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23295, 37465, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3762, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1620,    10, ...,     1,     1,     1],\n",
            "       [    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0,   170,    40, ..., 13956,  1916,     2],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 19192,    52, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(7, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  1000, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "96a37571-a85b-46d0-822c-15337ad2a4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_model"
      ],
      "metadata": {
        "id": "dyGROt7TwXn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "fec0f7cd-d17d-4c56-fb64-c523f8dc25b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n",
            "81/81 [==============================] - 3557s 44s/step - loss: 3.8539 - val_loss: 3.3430 - rouge1: 38.3698 - rouge2: 10.1688 - rougeL: 22.2593 - rougeLsum: 31.8182 - gen_len: 88.4506\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3040s 38s/step - loss: 3.4917 - val_loss: 3.2807 - rouge1: 40.0500 - rouge2: 10.8711 - rougeL: 22.9560 - rougeLsum: 33.0063 - gen_len: 82.0000\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3153s 39s/step - loss: 3.3288 - val_loss: 3.2417 - rouge1: 39.2427 - rouge2: 10.4373 - rougeL: 22.8725 - rougeLsum: 32.7607 - gen_len: 82.4815\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3104s 39s/step - loss: 3.1818 - val_loss: 3.2276 - rouge1: 40.0325 - rouge2: 11.1820 - rougeL: 23.2123 - rougeLsum: 33.2422 - gen_len: 84.0309\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3291s 41s/step - loss: 3.0478 - val_loss: 3.2148 - rouge1: 40.4019 - rouge2: 10.9217 - rougeL: 23.2547 - rougeLsum: 33.4422 - gen_len: 85.5432\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3194s 40s/step - loss: 2.9362 - val_loss: 3.2262 - rouge1: 39.8779 - rouge2: 10.3569 - rougeL: 22.9050 - rougeLsum: 32.9253 - gen_len: 81.9753\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3124s 39s/step - loss: 2.8306 - val_loss: 3.2327 - rouge1: 40.3397 - rouge2: 10.9453 - rougeL: 23.1704 - rougeLsum: 33.6539 - gen_len: 83.5309\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3146s 39s/step - loss: 2.7277 - val_loss: 3.2288 - rouge1: 39.9447 - rouge2: 10.5412 - rougeL: 22.8769 - rougeLsum: 33.1718 - gen_len: 85.0062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "f09db19d-85fd-44b8-d76e-3ee5168e9405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c25434ec-c808-43da-b3df-579bfce352b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "699df823-a6bf-4496-f7a2-78909e82cba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {'sampling-norep-v0' : 'sampling-norep-v0/',\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              #  'greedy-norep-v3' : 'greedy-norep-v3/',\n",
        "               'sampling-norep-v3' : 'sampling-norep-v3'}\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Tjj2cjL2mE",
        "outputId": "184d644e-2892-4f36-e503-667824846e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v0/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2,\n",
            "  \"max_length\": 150,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.2,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'\n",
        "save_path = save_paths[name]\n",
        "\n",
        "with open(save_path + '/training_history.json', 'r') as file:\n",
        "    loaded_history = json.load(file)\n",
        "\n",
        "H = History()\n",
        "H.history = loaded_history\n",
        "\n",
        "\n",
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/figure.png')"
      ],
      "metadata": {
        "id": "CEUKdl4cdUPx",
        "outputId": "dd6c46b3-440c-4144-9a18-40cbf4278f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'"
      ],
      "metadata": {
        "id": "AQseeydBMTlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:5],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][6:8],\n",
        "                                        generation_config=model.generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "DXuVco4oyC_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j6DcwmJShR",
        "outputId": "f366e627-2a4f-4cae-9161-273836d3ef9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet framework for in-and out-of-distribution classification. This paper proposes a variational approach to solve the uncertainty estimation problem in deep neural networks by considering the label-level distribution of image input and output labels. The authors propose a new uncertainty metric for deep neural network classification that is more robust than existing uncertainty measures. This article proposes a novel variational method for solving uncertainty estimation on deep neural nets.',\n",
              " 'An unsupervised method for analyzing the contribution of individual neurons to NMT models. This paper proposes a novel approach to interpret language pairs using neural networks, and proposes a new method for measuring the contributions of each neuron to the model. The authors propose a novel method for learning languages from neural networks that can be used to control the translation performance of language pairs.',\n",
              " 'A deep diagonal-circulant ReLU network that can be decomposed into products of diagonal and circulant matrices This paper proposes to replace the weight matrix of a fully connected layer with a new type of matrix. The authors propose a method for building deep ReLU networks based on a combination of diagonal/circular matrices with low rank approximators in order to improve performance.',\n",
              " 'Explicit cognitive theory or analogy-like computation in neural networks This paper proposes a novel approach to solving complex examples of visual and symbolic representations. The authors propose an approach to the problem of visual analogy by proposing a new model that learns to contrast abstract relational structures with visual representations. This work proposes a method for learning to compare different representations of objects, which can be used to solve complex analogical problems such as visual analogy.',\n",
              " 'A novel concept annotation task for medical time series data. This paper proposes a novel method of predicting and localizing medical concepts by modeling the medical context data as input. The authors propose a novel approach to the problem of identifying medical concepts in medical time-series data, which can be used to predict and localize medical concepts. This article introduces a novel framework for understanding medical concepts using medical context information.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMr_neO6pJNK",
        "outputId": "25d85c44-b7e6-4820-927c-54279cb4c854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The study proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This study proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The article investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TReOxk1fk1cx",
        "outputId": "eea11b79-2cc4-44e3-9121-a817e0d65fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The article proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This article proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = False\n",
        "model.generation_config.temperature = 1\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "                                          tokenizer.convert_tokens_to_ids('Ġpropose'),\n",
        "                                           tokenizer.convert_tokens_to_ids('Ġproposes'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "\n",
        "model.generation_config.num_beam_groups = 4\n",
        "model.generation_config.diversity_penalty = 0.7\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.3\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r4mAa8_-ql",
        "outputId": "a4abf904-55d4-42b6-9cbe-7b5095a3ea61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"diversity_penalty\": 0.7,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.3,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    15393,\n",
            "    21037,\n",
            "    32687\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('they proposes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EysouFNL26dN",
        "outputId": "decc56fd-e82f-48df-89e2-9b621383fc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['they', 'Ġproposes']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisXx8nuVDQ4",
        "outputId": "f70b8744-cbe0-4319-fc58-4bc80c0ff137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: 0/202\n",
            "[INFO]: 10/202\n",
            "[INFO]: 20/202\n",
            "[INFO]: 30/202\n",
            "[INFO]: 40/202\n",
            "[INFO]: 50/202\n",
            "[INFO]: 60/202\n",
            "[INFO]: 70/202\n",
            "[INFO]: 80/202\n",
            "[INFO]: 90/202\n",
            "[INFO]: 100/202\n",
            "[INFO]: 110/202\n",
            "[INFO]: 120/202\n",
            "[INFO]: 130/202\n",
            "[INFO]: 140/202\n",
            "[INFO]: 150/202\n",
            "[INFO]: 160/202\n",
            "[INFO]: 170/202\n",
            "[INFO]: 180/202\n",
            "[INFO]: 190/202\n",
            "[INFO]: 200/202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFgiii-sbTK4",
        "outputId": "41684db4-b656-4232-d3b5-51f1c72b8369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    170\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"early_stopping\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"repetition_penalty\": 1.8,\n",
              "  \"suppress_tokens\": [\n",
              "    1698,\n",
              "    32687\n",
              "  ]\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bNZCUqSvN7NV",
        "outputId": "bc3472f2-1429-4206-dcf8-b0d504c53db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/greedy-norep-v5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "beb7244c-9a96-46aa-e5a8-66046cfd17bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       1.420792\n",
              "std       18.729447\n",
              "min      -39.000000\n",
              "25%      -12.000000\n",
              "50%        0.500000\n",
              "75%       14.000000\n",
              "max       53.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 6\n",
        "print('ORIGINAL:' + tokenized_data['test']['target'][i])\n",
        "print('FINE TUNED MODEL:' + tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[i])\n",
        "print('PRETRAINED MODEL:' + tokenizer.batch_decode(pretrained_generated_ids, skip_special_tokens=True)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzU40R-ALuzq",
        "outputId": "7e6ad700-beac-4a0d-f100-174353d2d63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL:OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work\n",
            "FINE TUNED MODEL:Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. We show that Outlier Exposure can improve calibration performance in this realistic setting.\n",
            "PRETRAINED MODEL:However, when there is a distribution mismatch, deep neural network classifiers tend to give\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 28\n",
        "print(summaries.iloc[m, 0])\n",
        "print(summaries.iloc[m, 1])"
      ],
      "metadata": {
        "id": "lcWyZXvGLKcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1428815a-8c86-41c0-b4c1-7c39c492acdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space.  This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.\n",
            "We extend the reinforcement learning paradigm to a d-dimensional hypercube and show that quantile regression is capable of training orders of magnitudes faster in high dimensional metric spaces. This paper proposes a method to train a deep neural network to approximate the quantile function of the optimal action distribution. The authors propose a new reinforcement learning algorithm to train convolutional neural networks with quantile functions, showing that it can be used to train orders of magnitude faster on vector rewards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "-_h7Z8KBp09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-base', errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "MwmAPTxGp28R",
        "outputId": "810333e4-4f20-47db-a57d-a3d634679149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "a947a76d796343fb8010d27f818fbf2f",
            "22ce83fed2d444919ba40dfb5526d90b",
            "179503fdfa3f49e6a4e8f998af5d573d",
            "c5198b7842714377acc4efeb347fcf6e",
            "b952ae1ae73f4892aa7d8eef1904e9fe",
            "6394d8d637444b4490dadb9c0dcaf830",
            "b07b49a5db434eff838e6dcec3cfc747",
            "ee1e4766dcde41d7ba690f8fdca722fc",
            "bbd28d54ee2e43f5a2e710515a782406",
            "44799cbdb0c248f8b14434970ccb551b",
            "8214d98138f041079c83d5a9507178b6",
            "f25b823e67c84984b571042ef8376c7e",
            "24d6aa12d60342a19399cf825b332e1c",
            "8962a4e16c4b4fbb846656f35a70a3a9",
            "7fe00692904040c38e5832f11b8975b9",
            "ee17540ad8714e9b903d9f9a181af961",
            "4d14a957fe724702a82ada900361de55",
            "22880aebf50e4f60b2e18fd8b00e88d3",
            "9a69a0d3006141c8a7a7f53c262294cb",
            "ee8a801d8d03459fbc058b37cb528f21",
            "e2001fdf69d5424cab0e321a1e2760ad",
            "5724c8c3c435472da70b8bf114aa6829",
            "853aa86e41dc47d4ae9e4622941f1af3",
            "83579f36b9c44813b9533c37046b2d93",
            "cd35a1f388034fe9929318cebca0e1aa",
            "3adfde6716054b4eb3b968e10bb94264",
            "ae25b03da35e465badd05b18e33c5a89",
            "c7393147bf8f497fb965e3f356a83b70",
            "2489ce6eece4479ea965ddc4a59e95bc",
            "0f3fd7bc5cb042debc5576e5d33750d6",
            "18dfab3436384c5599b39fb00654deaa",
            "53557f686a0046caa7f97ce2da392007",
            "5c8d845574a349cd9133d93d0c5c354b"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a947a76d796343fb8010d27f818fbf2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/16 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f25b823e67c84984b571042ef8376c7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "853aa86e41dc47d4ae9e4622941f1af3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google-t5/t5-base')\n",
        "\n",
        "# model.config.task_specific_params['summarization']['max_length'] = 150\n",
        "# model.config.task_specific_params['summarization']['min_length'] = 80\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 80\n",
        "model.generation_config.num_beams = 4\n",
        "model.generation_config.length_penalty = 2.0\n",
        "\n",
        "\n",
        "# model.generation_config.do_sample = True\n",
        "# model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "# model.generation_config.suppress_tokens = [\n",
        "#     # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "#                                           # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "name_model = 'v0/'\n",
        "\n",
        "print(model.config)\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "_mad6Hc1ttRX",
        "outputId": "e90f59c5-d35a-4044-a95f-c471fbf4fb4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"google-t5/t5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 150,\n",
            "      \"min_length\": 80,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.38.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 80,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "6XdKgWLNwgRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "6PGwmRZ2yS78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yvPZ6vubyYl1",
        "outputId": "85b1b4c0-caa5-4d48-cb92-af56d42ffd73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  24674304  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  109628544 \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  137949312 \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222903552 (850.31 MB)\n",
            "Trainable params: 222903552 (850.31 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + name_model"
      ],
      "metadata": {
        "id": "HceVCymny0eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "p_wTT5KMyi86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "VeSspMbVyp_q",
        "outputId": "cbbc047d-46fe-4cfb-a8f3-05e52e245838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7e4adf964a60> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7e4adf964a60> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - 1894s 23s/step - loss: 4.3853 - val_loss: 2.4166 - rouge1: 30.9318 - rouge2: 5.6016 - rougeL: 17.2004 - rougeLsum: 25.4458 - gen_len: 109.1358\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 1833s 23s/step - loss: 2.5470 - val_loss: 2.2051 - rouge1: 31.2962 - rouge2: 5.7309 - rougeL: 17.2384 - rougeLsum: 25.5281 - gen_len: 109.6420\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 1896s 24s/step - loss: 2.2275 - val_loss: 2.1377 - rouge1: 31.6351 - rouge2: 5.8724 - rougeL: 17.5436 - rougeLsum: 25.7174 - gen_len: 109.2099\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 2310s 29s/step - loss: 2.0358 - val_loss: 2.0941 - rouge1: 32.6300 - rouge2: 6.3857 - rougeL: 17.8555 - rougeLsum: 26.5149 - gen_len: 109.4259\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 2307s 29s/step - loss: 1.9544 - val_loss: 2.0609 - rouge1: 33.6372 - rouge2: 7.1342 - rougeL: 18.9671 - rougeLsum: 27.6714 - gen_len: 115.8457\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - ETA: 0s - loss: 1.9086"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config"
      ],
      "metadata": {
        "id": "-dD2Lsh95H9a",
        "outputId": "068f0275-ea5b-4f1e-e841-fcfa5b6e7d8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 0\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "KqOLYlN5AsTr",
        "outputId": "fba992f6-80ff-46c7-d879-fd6ebaf022eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. the watermark must be secure against adversarial attacks and leave no tangible footprints in the target DNN. the watermark must be able to carry a multi-bit string instead of a one-bit boolean decision (existence or not of the WM)',\n",
              " 'multiplying with W = diag(a 1,..., a n) can be implemented efficiently as f = a x. a diagonal 3D tensor is equivalent to a hypernetwork that generates a vector. a diagonal 3D tensor is equivalent to a hypernetwork that generates a vector.',\n",
              " 'a model based on LSTM was proposed but as far as each user generates only a small portion of textual data, such data by itself cannot be used for updates of the general language model. the problem of learning without forgetting (LwF) consists in re-training of existing model  on new data such that its performance on the old data does not degrade.',\n",
              " 'we hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the BatchNorm layer. we consider a standard classification task for data, having underlying distribution denoted as D, over the pair of examples x  R n and corresponding true labels y  1, 2,..., k where k represents different labels.',\n",
              " 'a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. we learn a feature extractor d = f (x) using a neural network, which maps the original input data into a feature representation. a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at',\n",
              " 'a generative model (mnist) uses gradient descent to learn the perturbation. we use a variational auto encoder BID1 to create a generative model. we also use a generative model BID3 to create a generative model. a generative model (mnist) uses gradient descent to learn the perturbation.',\n",
              " 'DPQ-VQ is a more evenly distributed code utilization, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution. DPQ-SX is a more concentrated and sparse code distribution. DPQ-SX is a more concentrated and sparse code distribution.',\n",
              " 'a new model of style transfer is based on the generative network. the method requires batches of training data with the same content but different style. the method requires batches of training data with the same content but different content. the method requires batches of data with the same content but different content. the method is based on the model of the generative network.']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "2BXUHuhyWyyh",
        "outputId": "ec7115b9-52d5-49bb-aa6a-e433dce70cae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. the watermark must be secure against adversarial attacks and leave no tangible footprints in the target DNN. a retraining procedure resembles 'adversarial training' BID16. a retraining procedure resembles 'adversarial training' BID16.\",\n",
              " '',\n",
              " '',\n",
              " 'we hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the BatchNorm layer. we consider a standard classification task for data, having underlying distribution denoted as D, over the pair of examples x  R n and corresponding true labels y  1, 2,..., k where k represents different labels.',\n",
              " 'a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive.',\n",
              " '',\n",
              " 'DPQ-VQ is a more evenly distributed code utilization, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods. DPQ-VQ is a more concentrated and sparse code distribution, compared to discrete code learning-based methods.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "UFO8_u8PaILU",
        "outputId": "3ad8e9dc-dc25-42af-f6eb-644c8f67f790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. a retraining procedure is performed locally by the model owner. the retraining process is based on the uncertainty involved in the key generation process. a retraining procedure is performed locally by the model owner. a retraining procedure is performed by the owner.',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][:8],\n",
        "                                        seed=42)\n",
        "\n",
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "24dqMol-ZvT8",
        "outputId": "01c4537c-27ea-49e2-f377-50e1e9184fda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"black-box watermarking papers embed the WM as a statistical bias in the decision boundaries of the DNN. blackmarks' WM embedding process improves the robustness of the model against adversarial attacks. the watermark must be secure against brute-force attacks and leave no tangible footprints in the target DNN. a retraining procedure resembles 'adversarial training' BID16.\",\n",
              " '',\n",
              " '',\n",
              " 'we hypothesize that the use of different normalization statistics during training and inference is the main cause of this adversarial vulnerability in the BatchNorm layer. we consider a standard classification task for data, having underlying distribution denoted as D, over the pair of examples x  R n and corresponding true labels y  1, 2,..., k where k represents different labels.',\n",
              " 'a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. we learn a feature extractor d = f (x) using a neural network, which maps the original input data into a feature representation. a method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at',\n",
              " 'a neural network identifies a tree as a pedestrian who was about to cross. a perturbation to a misclassified image must be constrained to only the parts of an image relevant to the class. we use gradient descent to learn the perturbation to the misclassified image. a generative model generates latent Sympathetic Examples (LSE)',\n",
              " 'DPQ-VQ is a DPQ-SX-based embedding matrix. it is a DPQ-SX-based model that uses a pre-trained embedding table. DPQ-VQ could have a big approximation error. DPQ-VQ could have a big approximation error.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['input_ids'][1]"
      ],
      "metadata": {
        "id": "jLsv2o6lZ-Zq",
        "outputId": "769b3333-b954-491e-e16b-889f925da66a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21603,\n",
              " 10,\n",
              " 86,\n",
              " 8,\n",
              " 495,\n",
              " 213,\n",
              " 3,\n",
              " 89,\n",
              " 11,\n",
              " 3,\n",
              " 122,\n",
              " 33,\n",
              " 3,\n",
              " 18581,\n",
              " 15,\n",
              " 41,\n",
              " 9,\n",
              " 7,\n",
              " 16,\n",
              " 8,\n",
              " 926,\n",
              " 161,\n",
              " 201,\n",
              " 224,\n",
              " 3,\n",
              " 9,\n",
              " 1229,\n",
              " 19,\n",
              " 1776,\n",
              " 7072,\n",
              " 12,\n",
              " 8,\n",
              " 15574,\n",
              " 75,\n",
              " 1528,\n",
              " 607,\n",
              " 3028,\n",
              " 756,\n",
              " 5,\n",
              " 4908,\n",
              " 102,\n",
              " 120,\n",
              " 53,\n",
              " 28,\n",
              " 549,\n",
              " 3274,\n",
              " 3,\n",
              " 25930,\n",
              " 599,\n",
              " 9,\n",
              " 209,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 233,\n",
              " 6,\n",
              " 3,\n",
              " 9,\n",
              " 3,\n",
              " 29,\n",
              " 3,\n",
              " 61,\n",
              " 54,\n",
              " 36,\n",
              " 6960,\n",
              " 8877,\n",
              " 38,\n",
              " 3,\n",
              " 89,\n",
              " 3274,\n",
              " 3,\n",
              " 9,\n",
              " 3,\n",
              " 226,\n",
              " 213,\n",
              " 5475,\n",
              " 3282,\n",
              " 10684,\n",
              " 1249,\n",
              " 13555,\n",
              " 42,\n",
              " 8,\n",
              " 10118,\n",
              " 265,\n",
              " 986,\n",
              " 556,\n",
              " 41,\n",
              " 26714,\n",
              " 120,\n",
              " 21,\n",
              " 8,\n",
              " 14387,\n",
              " 137,\n",
              " 20748,\n",
              " 1002,\n",
              " 33,\n",
              " 3115,\n",
              " 261,\n",
              " 28,\n",
              " 315,\n",
              " 3,\n",
              " 8345,\n",
              " 12,\n",
              " 273,\n",
              " 62,\n",
              " 5530,\n",
              " 270,\n",
              " 10,\n",
              " 79,\n",
              " 54,\n",
              " 18513,\n",
              " 42,\n",
              " 3,\n",
              " 24608,\n",
              " 89,\n",
              " 63,\n",
              " 824,\n",
              " 3785,\n",
              " 7,\n",
              " 11,\n",
              " 995,\n",
              " 307,\n",
              " 18,\n",
              " 5517,\n",
              " 6002,\n",
              " 11573,\n",
              " 57,\n",
              " 3,\n",
              " 13275,\n",
              " 3785,\n",
              " 7,\n",
              " 640,\n",
              " 97,\n",
              " 18,\n",
              " 7910,\n",
              " 7,\n",
              " 41,\n",
              " 15832,\n",
              " 8181,\n",
              " 53,\n",
              " 756,\n",
              " 19,\n",
              " 2348,\n",
              " 57,\n",
              " 3,\n",
              " 9,\n",
              " 2201,\n",
              " 53,\n",
              " 3760,\n",
              " 6,\n",
              " 21,\n",
              " 677,\n",
              " 137,\n",
              " 101,\n",
              " 169,\n",
              " 175,\n",
              " 7639,\n",
              " 12,\n",
              " 3,\n",
              " 19882,\n",
              " 17,\n",
              " 24,\n",
              " 298,\n",
              " 72,\n",
              " 2881,\n",
              " 6,\n",
              " 4014,\n",
              " 3,\n",
              " 9,\n",
              " 1146,\n",
              " 455,\n",
              " 6565,\n",
              " 41,\n",
              " 11600,\n",
              " 3,\n",
              " 9,\n",
              " 12938,\n",
              " 8181,\n",
              " 61,\n",
              " 429,\n",
              " 4410,\n",
              " 72,\n",
              " 6473,\n",
              " 12,\n",
              " 224,\n",
              " 1002,\n",
              " 68,\n",
              " 62,\n",
              " 103,\n",
              " 59,\n",
              " 3346,\n",
              " 1099,\n",
              " 1388,\n",
              " 16,\n",
              " 48,\n",
              " 1040,\n",
              " 11,\n",
              " 1175,\n",
              " 34,\n",
              " 12,\n",
              " 647,\n",
              " 161,\n",
              " 5,\n",
              " 3,\n",
              " 15209,\n",
              " 6,\n",
              " 3,\n",
              " 9,\n",
              " 26184,\n",
              " 220,\n",
              " 308,\n",
              " 3,\n",
              " 324,\n",
              " 7,\n",
              " 127,\n",
              " 19,\n",
              " 7072,\n",
              " 12,\n",
              " 3,\n",
              " 9,\n",
              " 6676,\n",
              " 1582,\n",
              " 1981,\n",
              " 24,\n",
              " 3806,\n",
              " 7,\n",
              " 3,\n",
              " 9,\n",
              " 12938,\n",
              " 11,\n",
              " 19,\n",
              " 3334,\n",
              " 28,\n",
              " 3,\n",
              " 9,\n",
              " 141,\n",
              " 265,\n",
              " 986,\n",
              " 556,\n",
              " 5,\n",
              " 3,\n",
              " 21900,\n",
              " 2651,\n",
              " 126,\n",
              " 10561,\n",
              " 87,\n",
              " 10905,\n",
              " 6438,\n",
              " 405,\n",
              " 59,\n",
              " 2603,\n",
              " 8,\n",
              " 1120,\n",
              " 12907,\n",
              " 603,\n",
              " 257,\n",
              " 579,\n",
              " 13,\n",
              " 24228,\n",
              " 3134,\n",
              " 7,\n",
              " 6,\n",
              " 983,\n",
              " 224,\n",
              " 14172,\n",
              " 54,\n",
              " 483,\n",
              " 8,\n",
              " 10950,\n",
              " 19712,\n",
              " 7,\n",
              " 628,\n",
              " 532,\n",
              " 356,\n",
              " 13,\n",
              " 3621,\n",
              " 24,\n",
              " 54,\n",
              " 36,\n",
              " 7283,\n",
              " 1776,\n",
              " 41,\n",
              " 4065,\n",
              " 3,\n",
              " 632,\n",
              " 3505,\n",
              " 201,\n",
              " 11,\n",
              " 8,\n",
              " 6607,\n",
              " 655,\n",
              " 13,\n",
              " 3,\n",
              " 9,\n",
              " 207,\n",
              " 12182,\n",
              " 1016,\n",
              " 41,\n",
              " 4067,\n",
              " 186,\n",
              " 8755,\n",
              " 33,\n",
              " 906,\n",
              " 201,\n",
              " 38,\n",
              " 168,\n",
              " 38,\n",
              " 669,\n",
              " 2020,\n",
              " 5,\n",
              " 290,\n",
              " 19,\n",
              " 3,\n",
              " 9,\n",
              " 4248,\n",
              " 643,\n",
              " 13,\n",
              " 6678,\n",
              " 3825,\n",
              " 15574,\n",
              " 75,\n",
              " 1528,\n",
              " 9944,\n",
              " 6,\n",
              " 11,\n",
              " 175,\n",
              " 912,\n",
              " 43,\n",
              " 3,\n",
              " 9,\n",
              " 307,\n",
              " 892,\n",
              " 6,\n",
              " 21,\n",
              " 677,\n",
              " 271,\n",
              " 5172,\n",
              " 16,\n",
              " 8,\n",
              " 3361,\n",
              " 138,\n",
              " 3,\n",
              " 1498,\n",
              " 13,\n",
              " 1979,\n",
              " 28740,\n",
              " 41,\n",
              " 17137,\n",
              " 2341,\n",
              " 13626,\n",
              " 3,\n",
              " 15,\n",
              " 17,\n",
              " 491,\n",
              " 5,\n",
              " 6,\n",
              " 12698,\n",
              " 3,\n",
              " 137,\n",
              " 3,\n",
              " 8212,\n",
              " 6,\n",
              " 8,\n",
              " 167,\n",
              " 1017,\n",
              " 4742,\n",
              " 13,\n",
              " 15574,\n",
              " 75,\n",
              " 1528,\n",
              " 9944,\n",
              " 894,\n",
              " 16,\n",
              " 2250,\n",
              " 24,\n",
              " 777,\n",
              " 14047,\n",
              " 9284,\n",
              " 19,\n",
              " 1009,\n",
              " 3,\n",
              " 9,\n",
              " 2945,\n",
              " 3375,\n",
              " 42,\n",
              " 26184,\n",
              " 6497,\n",
              " 13,\n",
              " 8,\n",
              " 1316,\n",
              " 220,\n",
              " 308,\n",
              " 1293,\n",
              " 3,\n",
              " 324,\n",
              " 7,\n",
              " 127,\n",
              " 5,\n",
              " 4908,\n",
              " 10435,\n",
              " 1528,\n",
              " 9944,\n",
              " 16,\n",
              " 8,\n",
              " 529,\n",
              " 18,\n",
              " 17899,\n",
              " 3375,\n",
              " 1254,\n",
              " 54,\n",
              " 92,\n",
              " 36,\n",
              " 816,\n",
              " 13,\n",
              " 38,\n",
              " 3,\n",
              " 9,\n",
              " 12103,\n",
              " 853,\n",
              " 13,\n",
              " 15889,\n",
              " 1582,\n",
              " 13631,\n",
              " 41,\n",
              " 566,\n",
              " 9,\n",
              " 3,\n",
              " 15,\n",
              " 17,\n",
              " 491,\n",
              " 5,\n",
              " 6,\n",
              " 1233,\n",
              " 61,\n",
              " 3,\n",
              " 10,\n",
              " 2250,\n",
              " 24,\n",
              " 3806,\n",
              " 8,\n",
              " 1293,\n",
              " 7,\n",
              " 13,\n",
              " 80,\n",
              " 1229,\n",
              " 45,\n",
              " 430,\n",
              " 5,\n",
              " 71,\n",
              " 1126,\n",
              " 1295,\n",
              " 65,\n",
              " 92,\n",
              " 118,\n",
              " 2930,\n",
              " 12,\n",
              " 3,\n",
              " 11600,\n",
              " 8755,\n",
              " 16,\n",
              " 975,\n",
              " 24817,\n",
              " 138,\n",
              " 3134,\n",
              " 7,\n",
              " 1009,\n",
              " 96,\n",
              " 16928,\n",
              " 975,\n",
              " 24817,\n",
              " 7,\n",
              " 121,\n",
              " 213,\n",
              " 8,\n",
              " 812,\n",
              " 13,\n",
              " 8,\n",
              " 6126,\n",
              " 8755,\n",
              " 19,\n",
              " 6478,\n",
              " 57,\n",
              " 3,\n",
              " 17,\n",
              " 8149,\n",
              " 10607,\n",
              " 41,\n",
              " 518,\n",
              " 76,\n",
              " 3,\n",
              " 15,\n",
              " 17,\n",
              " 491,\n",
              " 5,\n",
              " 6,\n",
              " 1360,\n",
              " 61,\n",
              " 3,\n",
              " 5,\n",
              " 282,\n",
              " 224,\n",
              " 6,\n",
              " 62,\n",
              " 169,\n",
              " 8,\n",
              " 337,\n",
              " 607,\n",
              " 756,\n",
              " 16,\n",
              " 66,\n",
              " 12341,\n",
              " 41,\n",
              " 3227,\n",
              " 7173,\n",
              " 2904,\n",
              " 61,\n",
              " 11,\n",
              " 610,\n",
              " 15577,\n",
              " 3476,\n",
              " 57,\n",
              " 14498,\n",
              " 8,\n",
              " 812,\n",
              " 13,\n",
              " 3,\n",
              " 172,\n",
              " 5,\n",
              " 597,\n",
              " 16501,\n",
              " 6,\n",
              " 14194,\n",
              " 56,\n",
              " 253,\n",
              " 24,\n",
              " 2491,\n",
              " 18,\n",
              " 9500,\n",
              " 23668,\n",
              " 13,\n",
              " 8,\n",
              " 756,\n",
              " 607,\n",
              " 429,\n",
              " 6339,\n",
              " 13289,\n",
              " 42,\n",
              " 394,\n",
              " 772,\n",
              " 28,\n",
              " 3,\n",
              " 10643,\n",
              " 8755,\n",
              " 6,\n",
              " 787,\n",
              " 8,\n",
              " 269,\n",
              " 1120,\n",
              " 12907,\n",
              " 603,\n",
              " 1628,\n",
              " 5,\n",
              " 101,\n",
              " 217,\n",
              " 44,\n",
              " 97,\n",
              " 7910,\n",
              " 8014,\n",
              " 66,\n",
              " 1293,\n",
              " 7,\n",
              " 33,\n",
              " 8,\n",
              " 337,\n",
              " 6,\n",
              " 11,\n",
              " 38,\n",
              " 8,\n",
              " 1425,\n",
              " 2188,\n",
              " 6,\n",
              " 62,\n",
              " 253,\n",
              " 24,\n",
              " 34,\n",
              " 54,\n",
              " 6212,\n",
              " 8432,\n",
              " 42,\n",
              " 9068,\n",
              " 2491,\n",
              " 251,\n",
              " 6,\n",
              " 2924,\n",
              " 24,\n",
              " 8,\n",
              " 825,\n",
              " 54,\n",
              " 11961,\n",
              " 8432,\n",
              " 8,\n",
              " 750,\n",
              " 2491,\n",
              " 686,\n",
              " 5,\n",
              " 282,\n",
              " 224,\n",
              " 6,\n",
              " 5777,\n",
              " 7754,\n",
              " 4602,\n",
              " 7,\n",
              " 46,\n",
              " 26429,\n",
              " 5307,\n",
              " 138,\n",
              " 682,\n",
              " 12,\n",
              " 8,\n",
              " 80,\n",
              " 1702,\n",
              " 270,\n",
              " 11,\n",
              " 62,\n",
              " 3,\n",
              " 19882,\n",
              " 17,\n",
              " 24,\n",
              " 175,\n",
              " 2254,\n",
              " 54,\n",
              " 36,\n",
              " 3334,\n",
              " 16,\n",
              " 647,\n",
              " 161,\n",
              " 5,\n",
              " 282,\n",
              " 8560,\n",
              " 12,\n",
              " 761,\n",
              " 3,\n",
              " 9,\n",
              " 712,\n",
              " 9689,\n",
              " 127,\n",
              " 30,\n",
              " 8,\n",
              " 41,\n",
              " 226,\n",
              " 3,\n",
              " 23,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 63,\n",
              " 3,\n",
              " 23,\n",
              " 3,\n",
              " 201,\n",
              " 1484,\n",
              " 9709,\n",
              " 10272,\n",
              " 15,\n",
              " 7,\n",
              " 669,\n",
              " 12,\n",
              " 16,\n",
              " 1010,\n",
              " 3,\n",
              " 9,\n",
              " 3438,\n",
              " 147,\n",
              " 3621,\n",
              " 24,\n",
              " 33,\n",
              " 4700,\n",
              " 28,\n",
              " 8,\n",
              " 14181,\n",
              " 4759,\n",
              " 78,\n",
              " 623,\n",
              " 5,\n",
              " 101,\n",
              " 22455,\n",
              " 15,\n",
              " 24,\n",
              " 8,\n",
              " 1418,\n",
              " 13,\n",
              " 224,\n",
              " 5275,\n",
              " 12,\n",
              " 394,\n",
              " 4221,\n",
              " 3,\n",
              " 9,\n",
              " 3,\n",
              " 13627,\n",
              " 620,\n",
              " 13,\n",
              " 12628,\n",
              " 447,\n",
              " 26322,\n",
              " 7,\n",
              " 41,\n",
              " 15,\n",
              " 5,\n",
              " 122,\n",
              " 5,\n",
              " 1706,\n",
              " 138,\n",
              " 18,\n",
              " 5540,\n",
              " 4128,\n",
              " 42,\n",
              " 4723,\n",
              " 494,\n",
              " 61,\n",
              " 1250,\n",
              " 135,\n",
              " 12,\n",
              " 394,\n",
              " 9162,\n",
              " 28131,\n",
              " 42,\n",
              " 2491,\n",
              " 18,\n",
              " 17558,\n",
              " 138,\n",
              " 251,\n",
              " 12,\n",
              " 17165,\n",
              " 1317,\n",
              " 6093,\n",
              " 13,\n",
              " 331,\n",
              " 5,\n",
              " 242,\n",
              " 3,\n",
              " 9,\n",
              " 1646,\n",
              " 283,\n",
              " 6892,\n",
              " 6,\n",
              " 752,\n",
              " 178,\n",
              " 166,\n",
              " 2103,\n",
              " 24,\n",
              " 62,\n",
              " 174,\n",
              " 44,\n",
              " 709,\n",
              " 80,\n",
              " 5697,\n",
              " 3760,\n",
              " 6,\n",
              " 38,\n",
              " 2904,\n",
              " 283,\n",
              " 6892,\n",
              " 19,\n",
              " 3,\n",
              " 9,\n",
              " 13080,\n",
              " 1681,\n",
              " 11,\n",
              " 3,\n",
              " 89,\n",
              " 19,\n",
              " 59,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][6:10]"
      ],
      "metadata": {
        "id": "xNqaRwbOzUVD",
        "outputId": "30d97d31-0d35-46c1-9f90-5dbd512b9527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates. A missing data imputation network to incorporate correlation, temporal relationships, and data uncertainty for the problem of data sparsity in EHRs, which yields higher AUC on mortality rate classification tasks. The work presented a method that combines VAE and uncertainty aware GRU for sequential missing data imputation and outcome prediction.',\n",
              " 'Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This paper studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This work explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.',\n",
              " 'We introduce MIST RNNs, which a) exhibit superior vanishing-gradient properties in comparison to LSTM; b) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies; and c) are much more efficient than previously-proposed NARX RNNs, with even fewer parameters and operations than LSTM.',\n",
              " 'Based on fuzzy set theory, we propose a model that given only the sizes of symmetric differences between pairs of multisets, learns representations of such multisets and their elements. This work proposes a new task of set learning, predicting the size of the symmetric difference between multisets, and gives a method to solve the task based on fuzzy set theory.']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.do_lower_case"
      ],
      "metadata": {
        "id": "6QMQKmjqFkHL",
        "outputId": "4ed990f5-4663-46a2-cebf-d851ddb2d147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'T5Config' object has no attribute 'do_lower_case'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1d79c395fff9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"attribute_map\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attribute_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'T5Config' object has no attribute 'do_lower_case'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('hello')\n"
      ],
      "metadata": {
        "id": "-MTiOKDZGJRk",
        "outputId": "b0bfdba3-3d3b-4840-ae0d-6c9550086d3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21820, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode('Hello')"
      ],
      "metadata": {
        "id": "Glz2s6NaHWTa",
        "outputId": "9847b24e-40cd-4d82-8889-69b9d5219511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8774, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmf68b93HqIB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxwoq2VsLWZ52R9tn5VGoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9100de41ec6946c495c032b05d90913d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40d035dada3347e2963c183d035d445c",
              "IPY_MODEL_ecd31a14240f408b82b5224d97694ab7",
              "IPY_MODEL_7afeab237ced4f01b70361e2a3ce7552",
              "IPY_MODEL_5b7408314641483980402784564e62e6",
              "IPY_MODEL_c4a0eef84a5e4061884e43d2a09da02a"
            ],
            "layout": "IPY_MODEL_b114a3fe857949408305dde0c9436974"
          }
        },
        "40d035dada3347e2963c183d035d445c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ddbf8c4aaf42fba6f967bf32d45ecc",
            "placeholder": "​",
            "style": "IPY_MODEL_52801cc2a7a646109c4a7141ef768900",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ecd31a14240f408b82b5224d97694ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_db74bb5ffc4f4900b17aaf842a618019",
            "placeholder": "​",
            "style": "IPY_MODEL_6deef8837ace4d94b5b803666ff4aa5a",
            "value": ""
          }
        },
        "7afeab237ced4f01b70361e2a3ce7552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2e00c125b4d942368e702682002f94ff",
            "style": "IPY_MODEL_571b4b08342141c798092b6b23ab410a",
            "value": true
          }
        },
        "5b7408314641483980402784564e62e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ca46452e63a34d99865718ab86726748",
            "style": "IPY_MODEL_493ec2745ddf4bf2bdce528219a1b309",
            "tooltip": ""
          }
        },
        "c4a0eef84a5e4061884e43d2a09da02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3c8673d4bac474d9c5799e3ee02bf6b",
            "placeholder": "​",
            "style": "IPY_MODEL_f44df68731c14e24914d1c8816c8edce",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "b114a3fe857949408305dde0c9436974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f2ddbf8c4aaf42fba6f967bf32d45ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52801cc2a7a646109c4a7141ef768900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db74bb5ffc4f4900b17aaf842a618019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6deef8837ace4d94b5b803666ff4aa5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e00c125b4d942368e702682002f94ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "571b4b08342141c798092b6b23ab410a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca46452e63a34d99865718ab86726748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "493ec2745ddf4bf2bdce528219a1b309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d3c8673d4bac474d9c5799e3ee02bf6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44df68731c14e24914d1c8816c8edce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4436655f7773484fafd1cff33ef78ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0c8fbe4c9cc47ef9de30e6f1d7b8a00",
              "IPY_MODEL_5ddedeae3c37411f809914dd40626747",
              "IPY_MODEL_2b9d07ace2bf4dd6864c4e7f70d8d184"
            ],
            "layout": "IPY_MODEL_365f866542054b8096f0425b740e3657"
          }
        },
        "d0c8fbe4c9cc47ef9de30e6f1d7b8a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88891a0283ed46ec815622f98ec2e88d",
            "placeholder": "​",
            "style": "IPY_MODEL_f0ae46782ed54cda804a8a992bff0092",
            "value": "vocab.json: 100%"
          }
        },
        "5ddedeae3c37411f809914dd40626747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9125306d57674cc79e44cbbc6fa73493",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7351b89d38c841699116d5aa21d2b8e6",
            "value": 898823
          }
        },
        "2b9d07ace2bf4dd6864c4e7f70d8d184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df3401496304e62b772d6b9dfa54491",
            "placeholder": "​",
            "style": "IPY_MODEL_34a039133bef4ba297da41e8e252412f",
            "value": " 899k/899k [00:00&lt;00:00, 9.01MB/s]"
          }
        },
        "365f866542054b8096f0425b740e3657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88891a0283ed46ec815622f98ec2e88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ae46782ed54cda804a8a992bff0092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9125306d57674cc79e44cbbc6fa73493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7351b89d38c841699116d5aa21d2b8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8df3401496304e62b772d6b9dfa54491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34a039133bef4ba297da41e8e252412f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae83c009d6af44cbb7235c317c8959d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4774ef2229d4eb0ab8335fecf443bfa",
              "IPY_MODEL_c2a1348eca4f40ea8f9976036ab98d2c",
              "IPY_MODEL_a2e1287ad5da41138aab3041744c2eb4"
            ],
            "layout": "IPY_MODEL_1c129f9f527e4511ab67540e3cf838e4"
          }
        },
        "e4774ef2229d4eb0ab8335fecf443bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ca1257e20946749a7a6b2dc2ea7194",
            "placeholder": "​",
            "style": "IPY_MODEL_8749f001e7a24926bffc01a5491e1e8a",
            "value": "merges.txt: 100%"
          }
        },
        "c2a1348eca4f40ea8f9976036ab98d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e97b25494a744f7bfbf01198bb4eab0",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d628a6203104fa696604ec4609eeffc",
            "value": 456318
          }
        },
        "a2e1287ad5da41138aab3041744c2eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba8eea1a6051482394bcd1176906e8f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5429d5d2ce5543bfaf691acd3abae273",
            "value": " 456k/456k [00:00&lt;00:00, 6.26MB/s]"
          }
        },
        "1c129f9f527e4511ab67540e3cf838e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ca1257e20946749a7a6b2dc2ea7194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8749f001e7a24926bffc01a5491e1e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e97b25494a744f7bfbf01198bb4eab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d628a6203104fa696604ec4609eeffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba8eea1a6051482394bcd1176906e8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5429d5d2ce5543bfaf691acd3abae273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66beca881e744a0d95d12bbcb071f9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e32b30837b684ea1a9369c97610084b5",
              "IPY_MODEL_9174c28f178e4720b081af53ace7648a",
              "IPY_MODEL_148c7002ce6a4dc491ee90bd3fd53e5f"
            ],
            "layout": "IPY_MODEL_a98ba1580ced44dfaf4573d89e4f20cb"
          }
        },
        "e32b30837b684ea1a9369c97610084b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f277c50799e447998d6091f3d671d8ef",
            "placeholder": "​",
            "style": "IPY_MODEL_77a6a099c602449cab53c4331133baf9",
            "value": "tokenizer.json: 100%"
          }
        },
        "9174c28f178e4720b081af53ace7648a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bdc6e8e939b4f04ace92239a8d59dc2",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a20f3e6076ee4028bae0076528f657c4",
            "value": 1355863
          }
        },
        "148c7002ce6a4dc491ee90bd3fd53e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a809ae1cbf4941e28a5ee6b8990f0d47",
            "placeholder": "​",
            "style": "IPY_MODEL_582775feaacc4557a9052e37fa54d1f1",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 33.7MB/s]"
          }
        },
        "a98ba1580ced44dfaf4573d89e4f20cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f277c50799e447998d6091f3d671d8ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a6a099c602449cab53c4331133baf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bdc6e8e939b4f04ace92239a8d59dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a20f3e6076ee4028bae0076528f657c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a809ae1cbf4941e28a5ee6b8990f0d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582775feaacc4557a9052e37fa54d1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79a88598be7e47e2b3df8f13fd34d4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb47d7607757459396206b5c29bd7584",
              "IPY_MODEL_fb62bc7f31ca4f3799a11d035c75a87d",
              "IPY_MODEL_7d7598e022994663a060abfaeebf8f50"
            ],
            "layout": "IPY_MODEL_8389228f026241feb945dd390466b2ba"
          }
        },
        "cb47d7607757459396206b5c29bd7584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ca2f7b5b2e424aa24ae42a85478c77",
            "placeholder": "​",
            "style": "IPY_MODEL_0d0586338ea242ac885a4167a824cfa3",
            "value": "config.json: 100%"
          }
        },
        "fb62bc7f31ca4f3799a11d035c75a87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_387c04fc5fbb421b87f9eaf0cc150e8b",
            "max": 1716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aa0091c91764f6ca2e21d65c45d18eb",
            "value": 1716
          }
        },
        "7d7598e022994663a060abfaeebf8f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bcfa13acbad4f1e9fb284a68f61d9a9",
            "placeholder": "​",
            "style": "IPY_MODEL_77f1d6f94bab409cbfadfd13c7597f3a",
            "value": " 1.72k/1.72k [00:00&lt;00:00, 135kB/s]"
          }
        },
        "8389228f026241feb945dd390466b2ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ca2f7b5b2e424aa24ae42a85478c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0586338ea242ac885a4167a824cfa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "387c04fc5fbb421b87f9eaf0cc150e8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa0091c91764f6ca2e21d65c45d18eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6bcfa13acbad4f1e9fb284a68f61d9a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f1d6f94bab409cbfadfd13c7597f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "972f7cc0237f488ab0a54befbfe85b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bac7c1c2a544f19b3bd5b3ec89b5cfa",
              "IPY_MODEL_81aff29d06114ac585119b7c0fe6dfe4",
              "IPY_MODEL_e1794a5fee8b4b3cb693689b7db95cdf"
            ],
            "layout": "IPY_MODEL_17f3736bee3647979f096f40113e0963"
          }
        },
        "5bac7c1c2a544f19b3bd5b3ec89b5cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56550977be954f68816b767163347f62",
            "placeholder": "​",
            "style": "IPY_MODEL_49d7da777634415da2235bf936ee4abe",
            "value": "Map: 100%"
          }
        },
        "81aff29d06114ac585119b7c0fe6dfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d06964dc33345948b552a48f26af805",
            "max": 192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_901d231559a646bf8ef44f11ad6a7d1f",
            "value": 192
          }
        },
        "e1794a5fee8b4b3cb693689b7db95cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e9f6cbeacb4e41a518ac76d93be103",
            "placeholder": "​",
            "style": "IPY_MODEL_ba259db36cbf4c16bfd297cfa6d563a9",
            "value": " 192/192 [00:01&lt;00:00, 135.28 examples/s]"
          }
        },
        "17f3736bee3647979f096f40113e0963": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56550977be954f68816b767163347f62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d7da777634415da2235bf936ee4abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d06964dc33345948b552a48f26af805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901d231559a646bf8ef44f11ad6a7d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27e9f6cbeacb4e41a518ac76d93be103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba259db36cbf4c16bfd297cfa6d563a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d2683ac2754716b0e0d9e3278cfe89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc2a494fbc2d4877ae995d6b628a20e9",
              "IPY_MODEL_702b85d5fab64c92ad5aceb22590dad4",
              "IPY_MODEL_f80148a45c1a4719a2be4a2dc2212532"
            ],
            "layout": "IPY_MODEL_5e092b22d4204ac2a5460189e74181d8"
          }
        },
        "fc2a494fbc2d4877ae995d6b628a20e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7488fa5a9a0421ba176d0ff94257498",
            "placeholder": "​",
            "style": "IPY_MODEL_503836257374431893318438c40c02f3",
            "value": "Map: 100%"
          }
        },
        "702b85d5fab64c92ad5aceb22590dad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b477dab0156d418b8628ce7eb0101f2b",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be4b80a5029443a687913fab23f85209",
            "value": 48
          }
        },
        "f80148a45c1a4719a2be4a2dc2212532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4300241ad43a426389660ba557b5041e",
            "placeholder": "​",
            "style": "IPY_MODEL_97cd20bbd1974fe49435f102ea74242d",
            "value": " 48/48 [00:00&lt;00:00, 107.22 examples/s]"
          }
        },
        "5e092b22d4204ac2a5460189e74181d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7488fa5a9a0421ba176d0ff94257498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "503836257374431893318438c40c02f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b477dab0156d418b8628ce7eb0101f2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4b80a5029443a687913fab23f85209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4300241ad43a426389660ba557b5041e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cd20bbd1974fe49435f102ea74242d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6e6414e83b5478e9bd21b830b0fb7b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f01fcdd5a3804589b6dd9b0353298e0d",
              "IPY_MODEL_2cb53e5c8fb9432795fa00dbd28a95ae",
              "IPY_MODEL_7f3636d640fe4670b623e492f8a336f7"
            ],
            "layout": "IPY_MODEL_01871aacaa8649f2ab1bdb072114f1c9"
          }
        },
        "f01fcdd5a3804589b6dd9b0353298e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f225c165ce93444a97f893b8ab1b4cc9",
            "placeholder": "​",
            "style": "IPY_MODEL_152a86e33f864838af4780a780a170b6",
            "value": "Map: 100%"
          }
        },
        "2cb53e5c8fb9432795fa00dbd28a95ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aa29606933f4bed8700a14e1596a938",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9a8843efae24b7a9e987ebc7d47e13c",
            "value": 60
          }
        },
        "7f3636d640fe4670b623e492f8a336f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715ef094f367439bbaf1128eb6d41f6b",
            "placeholder": "​",
            "style": "IPY_MODEL_cf68125bf1a14540825eab2fa30ce1fa",
            "value": " 60/60 [00:00&lt;00:00, 110.26 examples/s]"
          }
        },
        "01871aacaa8649f2ab1bdb072114f1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f225c165ce93444a97f893b8ab1b4cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152a86e33f864838af4780a780a170b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7aa29606933f4bed8700a14e1596a938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9a8843efae24b7a9e987ebc7d47e13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715ef094f367439bbaf1128eb6d41f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf68125bf1a14540825eab2fa30ce1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a947a76d796343fb8010d27f818fbf2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22ce83fed2d444919ba40dfb5526d90b",
              "IPY_MODEL_179503fdfa3f49e6a4e8f998af5d573d",
              "IPY_MODEL_c5198b7842714377acc4efeb347fcf6e"
            ],
            "layout": "IPY_MODEL_b952ae1ae73f4892aa7d8eef1904e9fe"
          }
        },
        "22ce83fed2d444919ba40dfb5526d90b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6394d8d637444b4490dadb9c0dcaf830",
            "placeholder": "​",
            "style": "IPY_MODEL_b07b49a5db434eff838e6dcec3cfc747",
            "value": "Map: 100%"
          }
        },
        "179503fdfa3f49e6a4e8f998af5d573d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee1e4766dcde41d7ba690f8fdca722fc",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbd28d54ee2e43f5a2e710515a782406",
            "value": 64
          }
        },
        "c5198b7842714377acc4efeb347fcf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44799cbdb0c248f8b14434970ccb551b",
            "placeholder": "​",
            "style": "IPY_MODEL_8214d98138f041079c83d5a9507178b6",
            "value": " 64/64 [00:00&lt;00:00, 188.96 examples/s]"
          }
        },
        "b952ae1ae73f4892aa7d8eef1904e9fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6394d8d637444b4490dadb9c0dcaf830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b07b49a5db434eff838e6dcec3cfc747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee1e4766dcde41d7ba690f8fdca722fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbd28d54ee2e43f5a2e710515a782406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44799cbdb0c248f8b14434970ccb551b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8214d98138f041079c83d5a9507178b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f25b823e67c84984b571042ef8376c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24d6aa12d60342a19399cf825b332e1c",
              "IPY_MODEL_8962a4e16c4b4fbb846656f35a70a3a9",
              "IPY_MODEL_7fe00692904040c38e5832f11b8975b9"
            ],
            "layout": "IPY_MODEL_ee17540ad8714e9b903d9f9a181af961"
          }
        },
        "24d6aa12d60342a19399cf825b332e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d14a957fe724702a82ada900361de55",
            "placeholder": "​",
            "style": "IPY_MODEL_22880aebf50e4f60b2e18fd8b00e88d3",
            "value": "Map: 100%"
          }
        },
        "8962a4e16c4b4fbb846656f35a70a3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a69a0d3006141c8a7a7f53c262294cb",
            "max": 16,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee8a801d8d03459fbc058b37cb528f21",
            "value": 16
          }
        },
        "7fe00692904040c38e5832f11b8975b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2001fdf69d5424cab0e321a1e2760ad",
            "placeholder": "​",
            "style": "IPY_MODEL_5724c8c3c435472da70b8bf114aa6829",
            "value": " 16/16 [00:00&lt;00:00, 159.87 examples/s]"
          }
        },
        "ee17540ad8714e9b903d9f9a181af961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d14a957fe724702a82ada900361de55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22880aebf50e4f60b2e18fd8b00e88d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a69a0d3006141c8a7a7f53c262294cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee8a801d8d03459fbc058b37cb528f21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2001fdf69d5424cab0e321a1e2760ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5724c8c3c435472da70b8bf114aa6829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "853aa86e41dc47d4ae9e4622941f1af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83579f36b9c44813b9533c37046b2d93",
              "IPY_MODEL_cd35a1f388034fe9929318cebca0e1aa",
              "IPY_MODEL_3adfde6716054b4eb3b968e10bb94264"
            ],
            "layout": "IPY_MODEL_ae25b03da35e465badd05b18e33c5a89"
          }
        },
        "83579f36b9c44813b9533c37046b2d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7393147bf8f497fb965e3f356a83b70",
            "placeholder": "​",
            "style": "IPY_MODEL_2489ce6eece4479ea965ddc4a59e95bc",
            "value": "Map: 100%"
          }
        },
        "cd35a1f388034fe9929318cebca0e1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f3fd7bc5cb042debc5576e5d33750d6",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18dfab3436384c5599b39fb00654deaa",
            "value": 20
          }
        },
        "3adfde6716054b4eb3b968e10bb94264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53557f686a0046caa7f97ce2da392007",
            "placeholder": "​",
            "style": "IPY_MODEL_5c8d845574a349cd9133d93d0c5c354b",
            "value": " 20/20 [00:00&lt;00:00, 173.39 examples/s]"
          }
        },
        "ae25b03da35e465badd05b18e33c5a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7393147bf8f497fb965e3f356a83b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2489ce6eece4479ea965ddc4a59e95bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f3fd7bc5cb042debc5576e5d33750d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18dfab3436384c5599b39fb00654deaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53557f686a0046caa7f97ce2da392007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c8d845574a349cd9133d93d0c5c354b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}