{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/main/TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "5bf9deef-a4ec-45b3-ddad-5adf51a4dbc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.17.1 dill-0.3.8 multiprocess-0.70.16\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=1d318f1b7323e86c221d4be5d2544e2ae544d8249b5d13e93fe83586c07ae29b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.17.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "ebb558e3-f14f-4ec4-b306-54991d7d2ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-1-8986b677eb66>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import TFBartForConditionalGeneration, BartTokenizer, T5Tokenizer, pipeline\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect w/ HuggingFace HUB\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "1dd943230c0e4cec83a5cdeed66b0db9",
            "ae92856842b74843bec347e5e8e8fab4",
            "596d925b1f1f4ef9a05a6b4b581dac1e",
            "dbf20a21c1f543f6a7794983db47e1db",
            "c454b3704a4144fc85d79cba426fc6ac",
            "cc0c7bbd1ab348889254a27ab4eb8381",
            "71847c1c39234053a4a2daf9dd17626f",
            "13e1b8f15e1c41a4b17ce7eefb9efdef",
            "709922f14d864c038ea4e03f0d8b6e2a",
            "69c20fa745cb4512875421cae0b8a24a",
            "45d54ce97632402b810dee7c750017eb",
            "43c292ffbe694379a7956360553efc1a",
            "ac2081230c2b49b7b5d62ffa2975f49d",
            "2da9fcd1e63b4dae98e7e23a0c4c82d2",
            "75006e2177d246a3b1c0e570f67ab0d3",
            "ddd57005a5944ef991a2d30d0a6c4a02",
            "06e3f0ba42884b59bce0815d982ac42a"
          ]
        },
        "id": "wpR8O7wbdMdI",
        "outputId": "edabda05-5d33-4f0d-aa16-24c1119fa04d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dd943230c0e4cec83a5cdeed66b0db9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "1e00e1b4-8404-45a5-8c41-101fbdc69e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "1I5H39lnQW9o",
        "outputId": "8c461040-603c-4e5f-c389-6a13e9dd3e54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              source    paper_id  \\\n",
              "0  Due to the success of deep learning to solving...   SysEexbRb   \n",
              "1  Generative Adversarial Networks (GANs) have ac...   ryj38zWRb   \n",
              "2  Dialogue systems require a great deal of diffe...  BJepraEFPr   \n",
              "3  Backdoor attacks aim to manipulate a subset of...  rkgyS0VFvr   \n",
              "4  The integration of a  Knowledge Base (KB) into...  SJl7tREFvr   \n",
              "\n",
              "                                              target title  \\\n",
              "0  We provide necessary and sufficient analytical...   NaN   \n",
              "1  Are GANs successful because of adversarial tra...   NaN   \n",
              "2  In this paper, we propose to learn a dialogue ...   NaN   \n",
              "3  We proposed a novel distributed backdoor attac...   NaN   \n",
              "4  Conventional memory networks generate many red...   NaN   \n",
              "\n",
              "   number_words_target                                 extractive_summary  \n",
              "0                   38  We are interested in the square loss function ...  \n",
              "1                   36  On the one hand, a generator plays to transfor...  \n",
              "2                   30  However, several problems arise with this impl...  \n",
              "3                   35  Compared to standard centralized backdoors, we...  \n",
              "4                   32  Given the large amount of dialogue data record...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94462a4f-89bf-4a1e-8355-af4639ea5bd2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Due to the success of deep learning to solving...</td>\n",
              "      <td>SysEexbRb</td>\n",
              "      <td>We provide necessary and sufficient analytical...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>38</td>\n",
              "      <td>We are interested in the square loss function ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Generative Adversarial Networks (GANs) have ac...</td>\n",
              "      <td>ryj38zWRb</td>\n",
              "      <td>Are GANs successful because of adversarial tra...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36</td>\n",
              "      <td>On the one hand, a generator plays to transfor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dialogue systems require a great deal of diffe...</td>\n",
              "      <td>BJepraEFPr</td>\n",
              "      <td>In this paper, we propose to learn a dialogue ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30</td>\n",
              "      <td>However, several problems arise with this impl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Backdoor attacks aim to manipulate a subset of...</td>\n",
              "      <td>rkgyS0VFvr</td>\n",
              "      <td>We proposed a novel distributed backdoor attac...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>Compared to standard centralized backdoors, we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The integration of a  Knowledge Base (KB) into...</td>\n",
              "      <td>SJl7tREFvr</td>\n",
              "      <td>Conventional memory networks generate many red...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32</td>\n",
              "      <td>Given the large amount of dialogue data record...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94462a4f-89bf-4a1e-8355-af4639ea5bd2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94462a4f-89bf-4a1e-8355-af4639ea5bd2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94462a4f-89bf-4a1e-8355-af4639ea5bd2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8561f21f-598a-4d09-8206-9eda8a837787\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8561f21f-598a-4d09-8206-9eda8a837787')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8561f21f-598a-4d09-8206-9eda8a837787 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1312,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1312,\n        \"samples\": [\n          \"Deep learning software demands reliability and performance. However, many of the existing deep learning frameworks are software libraries that act as an unsafe DSL in Python and a computation graph interpreter. We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM is more modular and more generic than existing deep learning compiler frameworks, and supports tensor DSLs with high expressivity. With our prototypical staged DSL embedded in Swift, we argue that the DLVM system enables a form of modular, safe and performant frameworks for deep learning. Within the deep learning community, most current approaches to neural networks make use of high-level frameworks with a tensor domain-specific language (DSL) such as Torch BID3 , TensorFlow BID0 , PyTorch (PyTorch Development Team, 2016) , and MXNet BID1 . Traditionally, developers would build a computation graph (or dynamically generate graph nodes) using a DSL and let the framework interpret the computation graph on parallel architectures such as NVIDIA GPUs. While using hand-tuned GPU subroutines usually yields the best performance for complex operators, advanced compiler techniques can be applied to simplify computation, merge high-level operators based on shaping conditions, and fuse compatible elementwise operators to a single kernel to minimize the latency between kernel launches. Recent projects, the TensorFlow XLA compiler and the NNVM compiler BID13 including TVM BID2 , have begun to apply compiler techniques to deep learning systems, targeting LLVM BID10 and various back-ends to achieve good performance. However, their design and implementation have not entirely followed established best practices in widely-used compiler frameworks in the industry. Moreover, some frameworks use operator-overloading algorithmic differentiation (AD) to compute gradients, leaving the gradient computation unoptimizable. The other approach to AD, source code transformation, can produce more efficient code. While frameworks such as TensorFlow already perform AD as a graph transformation and apply various optimizations, their AD transformation is not designed as a transformation pass in the pipeline of their compiler framework, but as part of the DSL library. Making AD part of the compiler framework would greatly simplify the development of DSLs, achieving separation of concerns. We introduce DLVM, a new compiler infrastructure for deep learning systems that addresses shortcomings of existing deep learning frameworks. Our solution includes (1) a domain-specific intermediate representation specifically designed for tensor computation, (2) principled use of modern compiler optimization techniques to substantially simplify neural network computation, including algebra simplification, AD checkpointing, compute kernel fusion, and various traditional compiler optimizations, (3) code generation through a mature compiler infrastructure that allows for transparent targeting of various hardware, and (4) an embedded DSL that supports static analysis, type safety, and natural expression of tensor computation, and has a just-in-time (JIT) compiler targeting DLVM for AD, optimizations, and code generation. Numerous existing projects provide specialized systems for machine learning but are not closely related to our work. These include Apache SystemML BID4 , a high-level language and framework for writing and executing machine learning problems targeting Apache Spark, and TACO BID9 ), a C++ library for compiling and optimizing kernels that is more similar to Halide BID15 than to our work. Our work treats the creation of neural networks as a compilers problem to be addressed using mature compiler techniques. SystemML does not consider this issue at all; TACO does use compiler optimization, but only at a very low level to generate individual kernels. Two projects closely related to this work are the TensorFlow XLA compiler and the NNVM compiler. The code representation in these frameworks is a \\\"sea of nodes\\\" representation, embedding control flow nodes and composite nodes in a data flow graph. To apply algorithmic differentiation on this IR requires non-standard processing. In contrast, our approach is designed from the start around the idea that a neural network (and its associated tensor computations) is itself a program, which is best optimized through robust application of mature techniques in a principled compilation pipeline. We represent tensor computation in static single assignment (SSA) form with control flow graph, and perform algorithmic differentiation, domain-specific optimizations, general-purpose optimizations, low-level optimizations, and code generation. XLA takes a similar approach to ours, transforming TensorFlow sub-graphs to XLA's HLO graph and performing optimizations. Our intermediate representation is much more expressive than XLA's by including modular IR components and general-purpose instructions; this enables our approach to support full-fledged DSLs including standalone compiled DSLs and perform more extensive optimizations such as inlining and interprocedual optimizations. Our approach also differs from XLA by representing composite functions such as min and max directly through primitive instructions such as compare and select, which enables us to apply generic AD, and by using SSA form with control flow graph, which allows for reusing battle-tested SSA optimization algorithms in the LLVM community. Importantly, our entire infrastructure was designed from the start around a robust compile-time framework for tensor DSLs, whereas XLA has been adapted around the existing TensorFlow infrastructure with a particular focus on hardware support for Google's Tensor Processing Units BID8 .Where TVM and NNVM are built as a DSL and a graph library in Python with a C++ implementation, DLVM's architecture is closer to LLVM and the Swift Intermediate Language BID7 , having an IR file format and a full-fledged command line toolchain. More specifically, our work differs from NNVM and XLA in the design and presence of an IR that has a textual parsable format, a module/function/basic block hierarchy, custom type declarations and memory semantics. The textual IR enables robust unit testing via FileCheck, which is used extensively in LLVM and most LLVM-based compilers. Moreover, DLVM and its associated DSLs are implemented entirely in Swift, a safe systems programming language, and thus have an elegantly compact codebase and type-safe APIs. Deep Learning Virtual Machine (DLVM) is a compiler infrastructure designed for modern deep learning systems.1 DLVM is designed to apply a multi-stage compiler optimization strategy to both high-level linear algebra and low-level parallelism, perform domain-specific transformations, relieve the overhead in front-end languages, and serve as the host for research and development of DSLs for deep learning. The complete DLVM software stack, including sample front-end deep learning DSLs, is shown in FIG0 on the preceding page. FIG1 illustrates the major stages in the DLVM compilation pipeline. The DLVM compilation stages address algorithmic differentiation, domain-specific optimizations, general-purpose optimizations, and static code generation targeting a variety of compute architectures. The raw and optimizable stages allow constructs for high-level tensor operations and various high-level optimizations. The compute and schedule stages allow constructs for low-level array operations lowered from tensor operations in high-level stages, borrowing the design from Halide (Ragan-Kelley et al., 2013).The DLVM Intermediate Representation (IR) is the core language of the system. It uses static single assignment (SSA) form, control flow graphs, high-level types including a first-class tensor type, and a set of linear algebra operators combined with a general-purpose instruction set (see TAB1 ). The system enables a wide variety of domain-specific analyses and transformations, such as reverse-mode AD, AD checkpointing, algebra simplification and linear algebra fusion. To demonstrate how DLVM helps the development of domain-specific languages (DSLs), in Section 3.4 we present one prototypical staged DSL: NNKit. NNKit features safe and natural expression of tensor computation alongside the host program, and targets DLVM for differentiation, optimizations and static code generation. Example Element-wise unary tanh %a: <10 x f32> Element-wise binary power %a: <10 x f32>, %b: 2: f32 Dot dot %a: <10 x 20 x f32>, %b: <20 x 2 x f32> Reduce reduce %a: <10 x 30 x f32> by add along 1 Transpose transpose %m: <2 x 3 x 4 x 5 x f32> Slice slice %a: <10 x 20 x i32> from 1 upto 5 Compare gt %a: <10 x 20 x bool>, %b: <1 x 20 x bool> Data type cast dataTypeCast %x: <10 x f32> to f64 Function application apply %foo(%x: f32, %y: f32): (f32, f32) -> f32 Branch branch 'block_name(%a: i32, %b: i32) Conditional branch conditional %cond: bool then 'bb0() else 'bb1() Shape cast shapeCast %a: <1 x 40 x f32> to 2 x 20 Inspired by the LLVM IR BID10 ) and the Swift Intermediate Language BID7 , DLVM IR is a graph-based, modular code representation, with both an in-memory format and a textual format. The code representation has a hierarchy of abstractions: module, function, basic block, and instruction. An instruction is the minimal unit of code that operates on values, which can be globals, function arguments or temporary virtual registers produced by instructions. Each module contains a collection of type definitions, global values and functions. Each function has a control flow graph formed by basic blocks and control flow edges. Each basic block contains an ordered list of instructions with data dependencies forming a directed acyclic graph. The DLVM IR has a high-level type system with tensor as a first-class type. The DLVM virtual instruction set includes domain-specific primitive math operators, as well as general-purpose instructions for memory management, control flow and function application. Domain-specific instructions include element-wise unary operators, such as tanh and negate, element-wise binary operators, such as add and power, and complex operators such as dot, transpose, and convolve. All element-wise binary operators support broadcasting. A sample of DLVM IR code is shown in FIG3 on the next page. The DLVM instruction set does not include composite math functions such as softmax, sigmoid, min or max. All of these functions can be composed of primitive math instructions and control flow constructs. This design allows for the standard AD algorithm to be applied to any differentiable program, with no need for special handling of composite cases. DLVM has a full-fledged pass infrastructure, performing various analyses and two kinds of transformations: differentiation and optimization. Differentiation constructs function definitions from gradient declarations using adjoint code generation (see Section 3.1.3 below). Optimization is then performed on the resulting IR, maximizing the code performance. Optimizations include domainspecific optimizations, such as algebra simplification, linear algebra fusion, matrix multiplication reordering, and AD checkpointing, and traditional compiler optimizations. Since DLVM IR is aware of mathematical operators such as tanh and power, the algebra simplification pass can find and simplify certain mathematical operations that are expensive or redundant. For example, x 2 can be simplified to x x ( stands for element-wise multiplication), and x 0 can be simplified to constant 1. Matrix multiplication reordering is another classic optimization that minimizes the number of sub-operations in a chain of matrix multiplications with different dimensionality, based on matrix multiplication's associativity. Since the DLVM optimizer is aware of linear algebra operations with static dimensionality, maximizing the performance by fusing verbose linear operations into a single matrix multiplication is beneficial as well. For example, it is very common to encounter expressions of the form Wx + b. When unoptimized, the matrix multiplication and the addition will be parallelized separately. Since launching compute kernels separately can be expensive, DLVM performs linear algebra fusion, which transforms subexpressions involving both matrix multiplication and element-wise operations into a single matrix multiplication instruction on padded tensors. Besides the simple pattern like an addition of matrix multiplication and a vector, we can apply the same approach to a polynomial of multiple matrix multiplications, turning the polynomial into a single matrix multiplication. For example, in a simple recurrent neural network (RNN), each cell of the recurrence is a feed forward neural network that takes two inputs: x t , the input local to the current timestep, and h t , the hidden state carried along the recurrence. The linear algebra fusion pass can simplify operations in h t = f (Wx t\\u22121 +Uh t\\u22121 +b) from two matrix multiplications and two additions into a single matrix multiplication. A more aggressive, interprocedural version of linear algebra fusion can optimize parameter passing and memory allocation, so that the entire concatenated matrix can be created and passed around in the first place without reallocation. Algorithmic differentiation (AD), also known as automatic differentiation, encompasses a family of a well-known techniques for algorithmically obtaining the derivatives of a function f : x \\u2208 R n \\u2192 y \\u2208 R m BID12 . The function f can be represented as a directed acyclic computation graph representing the composition of elementary computational operations for which the respective derivatives are well known. The partial derivative \\u2202yj \\u2202xi can be computed through recursive applications of the chain rule, either in the forward direction (corresponding to a bottom-up traversal of the computation graph) or in the backward direction (corresponding to a top-down traversal of the computation graph). The former approach, called forward-mode or tangent-mode AD, is used in some research communities, most commonly when m n . The latter approach, which includes the back-propagation algorithm BID17 ) as a special case, is called reverse-mode or adjoint-mode AD, and encompasses the techniques most commonly used for training the weights in neural networks. In DLVM, the differentiation pass is responsible for performing reverse-mode AD. This pass is responsible for generating DLVM IR code that calculates the derivative of a differentiable function. A function is marked as being automatically differentiable via gradient declarations. A gradient declaration is a function in a module that is declared with its mathematical relation with another function in the module and no function body. The function @foo_grad in FIG3 is an example of such a function. Gradient declarations are configurable, e.g. specifying arguments to differentiate with respect to, keeping original outputs, and toggling seedability to accept back-propagated gradients. The differentiation pass, when applied, canonicalizes every gradient declaration in the module to a normal function definition with basic blocks and instructions. The canonicalization process first copies basic blocks and instructions from the original function to the new function body, and then applies adjoint code generation to the function. Unlike many of the existing deep learning frameworks, AD in DLVM is a source code transformation, not interpretation (operator overloading) over the same program. This makes the compiler able to perform optimizations on the gradient computation separately and enables higher order differentiation. Given a differentiable function f (x 1 , x 2 , . . . , x n ), this pass creates a new function that computes the Jacobian J f . This approach to AD has several advantages with respect to AD performed by operator overloading / graph interpretation. Unlike operator overloading, the gradient function produced by AD is a standalone function that sits uniformly alongside other functions in an IR module, representationally unrelated to the original function. The generated function takes original inputs and produces a tuple of partial derivatives with respect to the inputs. In AD, not all values in the forward evaluation will necessarily be used to compute derivatives. In DLVM, unused operations can be easily eliminated by the aggressive dead code elimination pass in the compilation pipeline (see Section 3.1.4). In addition, an AD-specific optimization technique called checkpointing can further reduce memory consumption during gradient computation. AD in DLVM is configurable. The front-end can choose to differentiate a function with respect to selected arguments, to keep some of the outputs of the original function, to apply differentiation to a specific output when there are multiple return values, or to enable the function to accept backpropagated gradients (seeds) through function composition, all by gradient declarations. If the function returns multiple values in a tuple, the gradient declaration can also specify which tuple element to differentiate. Our approach to AD is implemented as a transformation from one function to another function. This approach also makes higher-order differentiation possible; this can be accomplished by declaring a higher-order gradient function that differentiates the original gradient function. General-purpose optimizations refer to traditional compiler optimizations applied to DLVM IR. These optimizations are important at the DLVM stage in the compilation pipeline, since linear algebra computation can be highly optimized or eliminated before they get lowered to LLVM IR which contain parallel execution and low-level information that prevent LLVM optimizations from identifying high-level patterns. Some of these optimizations are aggressive dead code elimination, common subexpression elimination, and sparse conditional constant propagation. Applying such optimizations on gradient computation is not feasible in other approaches to AD that use graph interpretation (operator overloading), because the forward pass and the backward pass are tied together in a single graph; mutating either evaluation pass will alter the semantics of the other. Two major design goals of DLVM are the ability to target multiple heterogenous parallel architectures from the same front-end DSL code (and corresponding DLVM IR), and the ability to perform aggressive optimizations on lowered programs. In order to attain these goals, DLVM code generation transforms DLVM IR into LLVM IR. LLVM is a robust and mature compiler infrastructure with multiple back-ends, including NVIDIA GPUs. Many high-level DLVM IR linear algebra instructions over tensors abstract lower-level operations. The DLVM compiler transforms the high-level DLVM IR into lower-level stages and ultimately into calls to BLAS and compute kernels in LLVM IR. Existing LLVM utilities are used to compile the generated LLVM IR to the final binary. In order to take full advantage of a variety of emerging heterogeneous parallel architectures, we plan for future versions of DLVM to target the IR of HPVM BID18 , a higherlevel heterogeneous compiler extension to LLVM IR that allows for transparent targeting of diverse architectures from a data flow graph. The front-end software associated with each DSL (see Section 3.4) is responsible for generating a DLVM IR for a given source language program to be compiled. The DLVM compiler infrastructure itself is a compiler from DLVM IR to LLVM IR, therefore having a command line toolchain is necessary for verifying, transforming and compiling batches of DLVM IR files ( * .dl). Unlike XLA and NNVM/TVM, which only provide a Python/C++ interface to their users, DLVM provides a command line interface like any industry-standard compiler. The DLVM optimizer utility, dlopt, accepts * .dl IR files and applies user-specified optimization passes on them. The DLVM compiler driver, dlc, accepts * .dl IR files and performs user-specified tasks, such as verification, differentiation, optimization passes, stage lowering passes, and code generation; the driver invokes the DLVM core library to achieve these tasks. Because of having a textual format of the IR, the DLVM framework can easily make use of the LLVM Integrated Tester (lit) and FileCheck to perform robust unit testing. In future development, we plan to introduce a DLVM bitcode format for compact storage and high-throughput processing of DLVM code. Most of existing deep learning DSLs are embedded in a dynamically typed scripting language such as Python and Lua. While these scripting languages provide flexibility and a large number of libraries for scientific computing, they can act as a barrier between lightweight prototyping code and systematic production code. This barrier significantly reduces the reliability of ML software, resulting in suboptimal programming experience and unnecessarily effortful development cycles. In software engineering, a proven approach to tackle this problem is language and compiler technologies, starting from a language that is amenable to static analysis. A well-designed deep learning DSL should support the needs of deep learning software developers by providing a safe environment for rapid prototyping, while simultaneously generating highly efficient code for training and inference. DSLs in a scripting language can easily achieve rapid prototyping, but they are generally incapable of providing a safe environment with optimal performance. We believe that the best solution is DSLs embedded in a type-safe, type-inferring programming language that is both fast and easy to learn. In our initial release of DLVM, we provide one such DSL, both as a proof-of-concept and as a reference implementation that showcases the capabilities of DLVM as a platform for deep learning DSL development. NNKit is a staged DSL embedded in Swift, featuring natural expression of tensor computation alongside the host program without losing static analyses and optimizations on the tensor program. Inspired by Lightweight Modular Staging BID16 , NNKit leverages the static type system of Swift to guarantee type safety of the DSL. Tensor types are wrapped in Rep<T>, meaning the representation of some computation that produces data of type T. to perform AD, optimizations, and low-level code generation. A sample of Swift code using NNKit is shown in FIG5 .The NNKit just-in-time compiler has four important phases: The first phase, expression staging, produces an unshaped graph IR of the tensor computation. The second phase, shape specialization, prepares to generate statically shaped DLVM IR for staged functions when they are applied to shaped tensors. The third phase, lowering, generates DLVM IR and passes it through DLVM, producing a dynamic library containing a function symbol. The final phase, function reification, loads the binary and wraps the low-level function to a Swift function. We anticipate other existing deep learning frameworks, such as TensorFlow, could be adapted to use DLVM as a back-end to their tensor math DSLs. The deep learning research community has a rich variety of available frameworks. While two existing projects have attempted a compilers approach to deep learning frameworks, and have respectively achieved good integration with existing systems (TensorFlow XLA) and good performance (NNVM + TVM), their design philosophies have not entirely followed established best practices in optimizing compiler design. While well intentioned, the remaining vast majority of other frameworks have failed to observe that the problem of front-end DSLs, algorithmic differentiation, and converting a neural network into efficient executable code is, at its core, a compilers problem. As a result, important issues of extensibility and optimization have been addressed in less than optimal fashion in such frameworks. Nevertheless, several such frameworks have achieved wide adoption. We believe that the principled application of optimizing compiler techniques will lead to substantial improvements in the tools available to deep learning researchers. DLVM and its associated front-end DSLs have a major role to play in this future. Our existing implementation supports reverse-mode AD in the core language, and utilizes LLVM to target NVIDIA GPUs. In our ongoing work, we plan to substantially increase the number of supported hardware architectures by utilizing HPVM as an additional back-end, and explore more advanced AD techniques such as mixing forward and reverse modes.\",\n          \"Self-supervision, in which a target task is improved without external supervision, has primarily been explored in settings that assume the availability of additional data. However, in many cases, particularly in healthcare, one may not have access to additional data (labeled or otherwise). In such settings, we hypothesize that self-supervision based solely on the structure of the data at-hand can help. We explore a novel self-supervision framework for time-series data, in which multiple auxiliary tasks (e.g., forecasting) are included to improve overall performance on a sequence-level target task without additional training data. We call this approach limited self-supervision, as we limit ourselves to only the data at-hand. We demonstrate the utility of limited self-supervision on three sequence-level classification tasks, two pertaining to real clinical data and one using synthetic data. Within this framework, we introduce novel forms of self-supervision and demonstrate their utility in improving performance on the target task. Our results indicate that limited self-supervision leads to a consistent improvement over a supervised baseline, across a range of domains. In particular, for the task of identifying atrial fibrillation from small amounts of electrocardiogram data, we observe a nearly 13% improvement in the area under the receiver operating characteristics curve (AUC-ROC) relative to the baseline (AUC-ROC=0.55 vs. AUC-ROC=0.62). Limited self-supervision applied to sequential data can aid in learning intermediate representations, making it particularly applicable in settings where data collection is difficult. Many problems involving sequential data, such as machine translation, sentiment analysis, and mortality prediction, are naturally framed as sequence-level tasks (Harutyunyan et al., 2017; Hassan et al., 2018; Radford et al., 2017) . Sequence-level tasks map a sequence of observations x 0:T to a single label y. Learning this mapping is often made challenging due to a high-D (dimension) low-N (number of samples) setting (Nasrabadi, 2007) . Such problems are particularly prevalent in healthcare tasks, which often involve limited quantities of labeled data captured at a high temporal resolution (e.g., electrocardiogram waveforms). In high-D low-N settings, researchers have had success with transfer learning techniques, by leveraging additional data to learn intermediate representations that are then used in the target task. When additional data are unavailable, it may be possible to improve the intermediate learned representation of the data with respect to the target task by considering additional tasks intrinsic to the data. In particular, we hypothesize that the structure of sequential data provides a rich source of innate supervision. For example, signal reconstruction or forecasting could improve the intermediate representation by capturing the underlying data-generating process. Such approaches are examples of self-supervision, where labels are derived from the input (as opposed to external sources). In this paper, we show that leveraging the sequential structure of the data at-hand can lead to improved performance on sequence-level tasks (i.e., the target task). More specifically, by considering self-supervised auxiliary tasks (e.g., signal reconstruction), in addition to the sequence-level task, one can learn useful intermediate representations of the data. Past work investigating self-supervision for sequential data has focused on full-signal reconstruction (Dai & Le, 2015) , and to a lesser extent forecasting (Ramachandran et al., 2016) . Building on past work, we examine the utility of self-supervision on sequential data when additional data are unavailable, and we propose new types of self-supervision tasks. We refer to this approach as 'limited self-supervision. ' We limit the self-supervision to the data at-hand, and focus on self-supervised auxiliary tasks relevant to sequential data ordered by time (i.e., time-series data). Our main contributions are as follows: \\u2022 We demonstrate the efficacy of the proposed limited self-supervision framework for improving performance across datasets/tasks with no additional data. \\u2022 We compare the utility of several different existing forms of self-supervision in our limiteddata setting, identify consistent trends across supervision types, and demonstrate the utility of combining multiple different forms of self-supervision. \\u2022 We propose a new form of self-supervision, piecewise-linear autoencoding, that trades off fine-grained signal modeling and long-term dependency propagation. We demonstrate that this is the best form of limited self-supervision across all tasks. Our work suggests that there is a wide range of time-series and sequence classification tasks where limited self-supervision could improve performance. It also shows the value of including multiple, simultaneous streams of auxiliary self-supervision. Our findings present a methodological contribution, in the form of a useful new type of self-supervision, piecewise-linear autoencoding. Further, our empirical findings on when and how auxiliary tasks help can inform future work in developing self-supervision techniques. Previous work has found value in self-supervised pretraining with large amounts of unlabeled data (Devlin et al., 2018; Ramachandran et al., 2016) . In our work, we focus on time-series data instead of language data. Several previous works have examined self-supervision for pretraining or feature extraction in the context of time series (Hyvarinen & Morioka, 2016; Oord et al., 2018) . We substitute the pretraining framework with a multitask learning framework, removing the requirement to train multiple individual models. However, in contrast to standard pretraining or multitask learning setups, we do not assume the availability of additional data for training. We posit that even in the absence of additional data, self-supervision can lead to improved performance on the target task. Multitask learning deals with training a single model to perform well on multiple tasks. By simultaneously training on multiple related tasks and sharing representations, multitask models can improve generalization (Caruana, 1998) . Multitask learning has been used successfully across a number of different clinical tasks (Wiens et al., 2016; Ahmed et al., 2016; Razavian et al., 2016; Harutyunyan et al., 2017) . In particular, the success of multitask learning in deep learning demonstrates its value for representation learning (Ruder, 2017) . Within the context of supervised learning, Schwab et al. considered a multitask framework for learning from sequential health data (Schwab et al., 2018) . Though they used self-supervision, they only considered a setting where large amounts of unlabeled data were available. Our work was inspired by the findings of Dai and Le (Dai & Le, 2015) . Dai and Le compared sequence-autoencoding and language modeling as auxiliary tasks for leveraging large pools of unlabeled data for natural language tasks (e.g., sentiment analysis). Their approach led to state-of-the-art performance on a range of problems. They found sequence-autoencoding led to larger improvements than language modeling. Interestingly, they found jointly training on the main and auxiliary task decreased performance relative to the baseline. In contrast, we focus on self-supervision for time series and do not assume access to additional data. Choi et al. (2018) examines self-supervision without additional data applied to hierarchical EHR data. They also demonstrate the benefit of adding auxiliary supervision. Our work differs from this by i) using sequential as opposed to hierarchical structure, ii) examining multiple streams of simultaneous supervision and iii) comparing a broad range of auxiliary tasks on general time series data. find that training a sequence classification model to simultaneously forecast the data as an imputation method improves overall performance, though they suggest this is due to better handling missing data. We examine the impact of auxiliary self-supervision more generally as a way to improve supervised representation learning. In this section, we present our proposed limited self-supervision framework. After describing our notation, we present our baseline encoder-decoder architecture and describe four self-supervised auxiliary tasks. We define a sequence as a set of observations {x t } T t=0 : x t \\u2208 R d ordered by the index t. We denote such a sequence as x 0:T . Each observation x t is a d-dimensional vector. We focus on univariate, evenly sampled time series. We categorize time-series tasks across three dimensions: i) target vs. auxiliary tasks, ii) external supervision vs. self-supervision, and iii) sequence-level vs. subsequence-level tasks. A target task is the task of interest, whereas auxiliary tasks are only useful insofar as they improve performance on the target task. External supervision occurs when the task labels are provided by an external source (e.g., object recognition). Self-supervision occurs when no additional supervision is required to generate the ground truth label (e.g., in autoencoding, the input itself serves as the supervision). A sequence-level task is one where the supervision pertains to the entire sequence (i.e., sequence classification). A subsequence-level task provides multiple instances of supervision across the signal. In our work, all of the target tasks are sequence-level tasks requiring external supervision. We denote the label of the target task as y. Our auxiliary tasks are all self-supervised and may be either sequence-or subsequence-level. In this work, we compare four different self-supervised auxiliary tasks. Throughout, we consider a fixed encoder architecture, focusing on the improvements offered by the auxiliary tasks. Specifically, we focus on recurrent neural networks with LSTM cells. We use a 1-layer LSTM with a number of hidden units determined on a task-by-task basis. These architectures have proven useful for sequential tasks in many domains (Oord et al., 2016; Ramachandran et al., 2016) , including health data (Harutyunyan et al., 2017) . We note, however, that nothing in our framework presupposes a particular architecture, these techniques could work with any representation-learning gradient-based approach. Figure 1a depicts our baseline architecture and its relation to auxiliary tasks. The encoder is implemented as a single-layer LSTM. The target decoder is a single fully connected layer mapping from hidden state z T to the output. This simple output layer purposely places a heavy burden on the encoder, since the representation learned by the encoder is shared by (and thus may improve from) all tasks. We train the model by minimizing the multi-class cross-entropy between our prediction\\u015d y (put through a softmax activation) and the one-hot distribution representing the correct label class (Eqn. 1). min Where \\u03b8 E and \\u03b8 D are parameters for the encoder and decoder respectively and i indexes class labels. We compare this baseline architecture trained with no auxiliary tasks to models trained with up to four auxiliary tasks. We consider four self-supervised auxiliary tasks: i) autoencoding (i.e. reconstruction), ii) forecasting, iii) partial-signal autoencoding, and iv) piecewise-linear autoencoding (shown in Figure 1b ). The auxiliary tasks take as input the output of the encoding network. Task X is implemented as a decoder D X , composed of a recurrent layer R X and a fully connected output layer O X . While our sequence-level target task requires only one prediction at the end of the sequence, some of our auxiliary tasks are subsequence-level, in which case the task takes as input the intermediate encoding representation z t . i) Autoencoding (AE). This is a standard method for unsupervised training in which we seek to, given the final hidden state produced by the encoder, output a reconstruction of the complete signal that minimizes error (Eqn. 2). This task requires that the hidden state encode compressed version of the input, compression encourages learning latent structure and discourages learning noise. The decoder, parameterized by a single-layer autoregressive LSTM, outputs a sequence: , and thus depends on \\u03b8 E . ( 2) ii) Forecasting. In this task, the decoder takes the hidden state produced at any time step by the decoder and attempts to predict the next h (for example, 6) elements in the sequence. This requires the hidden state to encode the dynamics of the data-generating process. The decoder outputs x t+1:t+h given past values x 0:t , minimizing (Eqn. 3). We use a single-layer LSTM similar to the AE decoder, though it is used at each time-step to decode the next h observation values. This auxiliary task is a variant of AE that differs in three ways: 1) instead of decoding the full signal x 0:T it decodes only the previous h (for example, 6) steps of the signal x t\\u2212h:t\\u22121 , 2) instead of one prediction being made at the end of the encoder, a prediction is made at every encoding step from x h onwards, and 3) the input to the decoding layer includes the current value, x t (Eqn. 4). It is implemented identically to the Forecast Decoder, the only difference is that it predicts the previous h observation values. This task allows us to examine the impact of signal reconstruction without requiring learning long-term dependencies, allowing for a more meaningful comparison with Forecasting. iv) Piecewise-Linear Autoencoding (PL-AE). A piecewise-linear approximation, or a combination of line segments, is capable of efficiently representing a wide class of signals (particularly non-periodic signals). It is a promising choice for an auxiliary task as it encourages a compact representation capturing the most important details of the signal. A piecewise-linear representation consists of two length n + 1 vectors (where n is the number of linear segments in the signal), a value vector v and a position vector p. The positions are defined as proportions of the original signal length, between 0 and 1. The decoder produces a series of points which define a piecewise-linear reconstruction of the complete input signal. The reconstruction is defined by linear interpolation between the series of points (p 0 , v 0 ) . . . (p n , v n ), where v i is the value of the signal at the point i, and p i is the relative position (or time) where the point occurs. Our decoder produces n + 1 such points using a single-layer autoregressive LSTM where the hidden state is fed to two output layers which map z we feed the previous point value v i\\u22121 and the sum of previous positions j=i\\u22121 j=0 p j to the decoder. After we have generated the target number of points, we normalize the position values to enforce n+1 j=0 p j = T and perform the interpolation. As this entire process is differentiable, we optimize the decoder directly on the interpolated reconstruction loss (Eqn. 5). Additional details on these tasks can be found in Appendix A.1. We optimize our model to minimize the loss: Where L target is a cross-entropy loss and is a weighted summation of auxiliary MSE losses. The weighting terms \\u03b1 X are defined as 0 if the auxiliary task X is not being used for training. If the task is being used, then \\u03b1 X = Ltarget L X , where the losses are calculated at the beginning of training using the newly-initialized network on the training data. This ensures that all tasks have losses of similar magnitude. To test our hypothesis, that auxiliary self-supervision improves performance on sequence-level tasks, we consider at a variety of sequence-level tasks across different types of synthetic and real data. We consider the following three tasks (two of which are based on publicly available real datasets): Piecewise-Linear Segment Prediction (PLA). We begin with simulated data, as it allows us to estimate the ability of self-supervised auxiliary tasks to identify long-term dependencies in the data. The dataset is composed of piecewise-linear signals, each of length 100. Point values are drawn from a uniform distribution and lie between -1 and 1. The number of line segments also varies uniformly between 1 and 6. The target task for this dataset is to estimate the number of distinct segments that occurred in the signal. 1,000 training, validation, and test sequences were generated independently. Patient Classification using Glucose Data (T1D). This task uses publicly available continuous glucose monitor (CGM) data collected from people with type 1 diabetes (T1D) (Fox et al., 2018) . Each signal x 0:T consists of 288 glucose measurements sampled every five minutes over the course of a day. This dataset contains 1,863 days of data from 40 patients. In this task, we aim to classify patients based on their data. Here, y \\u2208 {1, . . . , 40} represents the patient. Classifying patients is a proxy for the important problem of identifying signal dynamics. We preprocess the data by removing physiologically implausible glucose measurements, and linearly interpolating missing chunks of data. We exclude signals in which more than 20% of the measurements are missing and those that are missing a contiguous block longer than two hours. Data were collected by a series of multi-day sessions separated by three-month intervals. As our test set, we consider the final recording session from each patient. We select our validation set randomly from the remaining data. Atrial Fibrillation Detection (AF). Our final task uses electrocardiogram (ECG) data from the publicly available 2017 PhysioNet Challenge (Clifford et al., 2017) , in which the goal was to automatically diagnose atrial fibrillation (AF). This dataset contains four unevenly distributed classes: normal sinus rhythm, AF, other arrhythmia, and noise. We use the training data provided for the competition (the test data are not publicly available), resulting in 8,528 samples. 771 of those signals are labeled AF. We exclude signals with less than 30 seconds of data (967 signals total, 127 with AF) and truncate all signals to exactly 30 seconds. We also downsample the data, reducing signal size from 9,000 to 125. This speeds up training time and eases memory requirements. We use the validation set provided for the challenge as the test set (removing those examples from the training set), and randomly sample 10% of the training data for use as a validation. We implement all models in PyTorch (Paszke et al., 2017) , and optimize model parameters using Adam (Kingma & Ba, 2014) with an initial learning rate of 1e \\u2212 3 (the default PyTorch value). In practice we found altering the decoding horizon had little effect on performance, so we used h = 6 for all experiments. All encoding/decoding layers are composed of a single recurrent layer an identical number of hidden units, set on a per-task basis using performance on the validation set to balance training time, memory constraints, and target-task performance (evaluated using the earlystopping validation set). We used 128 hidden units for the PLA target task, 512 hidden units for the T1D target task, and 256 hidden units for the AF target task. The decoding layers also have 1 (or two for PL-AE) fully connected output layers. For the PL-AE, we set the number of line segments n = 6 as a reasonable size to approximate many signal types. We mitigate the risk of overfitting by using early stopping on a withheld validation set, training until we fail to improve performance for over 50 epochs and reporting test performance for the best performing model on the validation data. Some auxiliary tasks make predictions at multiple points in the signal. This helps prevent the vanishing gradient problem, which can impede learning with large sequences. To avoid conflating these sorts of improvements with those caused by learning better representations, we use label propagation with our target task (sequence classification), linearly annealing contributions to the loss function over the length of the signal (Dai & Le, 2015) . The propagated losses are combined using a weighted average, with the weights linearly annealed from 0 to 1 over the length of the signal. Our code and synthetic data will be made publicly available to allow for replication and extension. For the purposes of double-blind peer review, we have released the code and PLA data on an anonymous Google Drive account 1 . We begin by establishing that the proposed self-supervised auxiliary task framework improves target task performance. We then look more carefully at the effects of different types of auxiliary tasks. We conclude by looking at the relationship between auxiliary task and target task performance, which sheds light on the mechanism by which auxiliary tasks improve performance. To evaluate the results of these experiments, we measure the macro-averaged AUC-ROC on the target task, and the mean absolute percent error (MAPE) for the auxiliary tasks. We repeat all experiments using three random initializations and average the results. The Benefit of Limited Self-Supervision. We begin by examining our main hypothesis, that limited self-supervision improves sequence-level task performance without additional data. In we plot target-task performance across our three tasks with a varying number of self-supervised auxiliary tasks. We estimate performance for a given number of auxiliary tasks by averaging the performance of all possible combinations. For all three sequence-level tasks, the inclusion of four auxiliary tasks improves performance relative to no auxiliary tasks. Moreover, we observe that performance tends to increase with the number of auxiliary tasks. The one exception is in the T1D task, where improvement peaks at three auxiliary tasks. Relative Contribution of Different Auxiliary Tasks. We now examine how different auxiliary tasks contribute differently to performance. To measure the impact of individual auxiliary tasks, we average performance across all models that include the auxiliary task versus all models that do not. This allows us to observe an auxiliary task's individual contribution, and its ability to usefully combine with other streams of self-supervision. The averaged change in AUC-ROC indicates the marginal improvement offered by the task (Figure 3) . PL-AE outperforms all other auxiliary tasks, including AE, on all three datasets. Since PL-AE limits the temporal granularity of the output, this suggests that modeling fine temporal granularity does not help, and may even hurt performance. The forecasting task underperforms all other auxiliary tasks. This finding is in line with the findings of (Dai & Le, 2015) . However, the explanation Dai and Le provide (that the AE encourages long-term dependency modeling) is inconsistent with the performance of PS-AE, which also does not model long-term dependencies. The fact that the PS-AE outperforms the forecasting task, but generally underperforms AE, suggests that while modeling long-term dependencies may improve performance, Table 1 : The performance of particular combinations of auxiliary tasks. All methods include the dataset-specific target task. AE and Forecast refers to Autoencoding and Forecasting respectively, the auxiliary tasks explored in (Dai & Le, 2015) . PLAE and PSAE refer to Piecewise-Linear Autoencoder and Partial-Signal Autoencoder, our novel forms of self-supervision. We see that our newly proposed forms of self-supervision outperform the other approaches on all datasets. AUC-ROC * 100 Auxiliary Tasks  PLA  T1D A) The effect training data size has on auxiliary tasks. As the amount of data increases, we tend to see an increase in the improvement afforded by the auxiliary tasks. B) The relationship between auxiliary task performance (in MAPE, lower is better) and target task performance (in AUC-ROC, higher is better) on the T1D data. Calculated over all training data sizes, there is a somewhat weak relationship (Pearson R = -0.53, line not shown). However, when we condition on the amount of training data, we find that different relationships emerge (Pearson R = 0.34 when training size = 1,000 vs. -0.72 when training size = 1,500). there is likely some other reason that AE works well. We also examine the performance of particular combinations of self-supervised auxiliary tasks in Table 2 . We see that our proposed auxiliary tasks outperform and are complementary to standard auxiliary task combinations. Why do Self-Supervised Auxiliary Tasks Help? To investigate the underlying mechanism by which auxiliary tasks improve performance, we examine model performance as we vary the number of auxiliary tasks and the amount of training data (results shown in Figure 4a ). As the amount of training data increases, the added value from the auxiliary tasks increases on average. This suggests that auxiliary tasks are not simply acting as a form of regularization, since otherwise we would expect to see larger improvements on smaller training sets. If the auxiliary tasks result in better representations (not just regularized representations), their impact on performance should correlate with the auxiliary task performance. A decrease in auxiliary task error should lead to a better intermediate representation and an increase in target task performance (i.e., AE MAPE and AUC-ROC should be negatively correlated). Meanwhile, if the auxiliary tasks work as regularizers, exhausting representation capacity, we would expect to see little effect or a positive correlation (if the regularization effect is too strong, and auxiliary performance comes at the cost of target performance). We explore this, specifically for AE with the T1D data in Figure 4b . This relationship is highly dependent on the amount of data. When training data are limited, there is a weak positive correlation between auxiliary task error and target performance, suggesting a regularizing effect. However, with the full amount of training data, we see a strong negative correlation. Auxiliary tasks give sizable improvements at 1,000 and 1,500 training examples (averaged improvement of 0.016 and 0.010 respectively). This suggests that 1) auxiliary tasks act both to regularize and to enhance intermediate representations, depending on the amount of data, and 2) there is an amount of training data where they are effective in either role. These findings suggest that auxiliary self-supervised tasks may be useful across a wide range of training set sizes. In this paper, we introduced a limited self-supervised framework, in which we sought to improve sequence-level task performance without additional data. By jointly training our target task with auxiliary self-supervised tasks, we demonstrated small but consistent improvements across three different sequence classification tasks. Our novel piecewise-linear autoencoding task emerged as the most useful auxiliary task across all datasets. In contrast, forecasting, which presents an intuitively appealing form of self-supervision, led to the smallest improvements. , continuing untilx 0:T is fully generated. To provide a shorter path for gradient flow, we decode in reverse order, generatin\\u011d x T :0 instead ofx 0:T (Goodfellow et al., 2016) . Motivation. Forecasting encourages E to encode the dynamics of the data-generating process. Without information about the underlying dynamics, future value prediction is either challenging or trivial (if the signal does not change). Such dynamics may carry valuable information for a range of sequence-level tasks, though the aspects of the dynamics most relevant to the target task may differ from those that predict future values. Details. We focus on a multi-output forecasting architecture, predicting several future values simultaneously (Taieb et al., 2010) . This is done using a recurrent decoder, similar to the autoencoder, but applied at each encoding step and expanded only h steps, where h is a hyperparameter. Motivation. Previous work has found advantages to reconstruction over prediction (Dai & Le, 2015) . Dai and Le hypothesize the observed superiority of sequence autoencoding over forecasting may result from the short-term nature of the language modeling task (only predicting the next word). To investigate the effect of short-term dependency auxiliary tasks, we use a multi-step forecasting system, where we can examine the effect of varying the prediction horizon. Analogously, we use PS-AE to examine the effect of producing short-term reconstructions. Details. PS-AE is implemented using a setup identical to our forecasting approach, except we estimate the previous h values instead of the subsequent values (Figure 1 a, D P S ) . Motivation. AE encourages fine-grained modeling of the signal over long periods, whereas PS-AE encourages fine-grained modeling over a short period. While PS-AE can control the range of temporal dependencies modeled using the decoding horizon h, it does not vary the granularity at which the output is modeled. What level of signal granularity is required for reasonable target-task performance? To explore this question, we introduce Piecewise-Linear (PL) Autoencoding. Details. To generate a PL representation of a signal with n distinct pieces, we take the encoded representation of the signal, z T , and feed it into a recurrent layer R P L . \\u2192 v j , p j respectively. Since we know p 0 = 0, at the first step we generate only an initial value. We continue the decoding process, feeding v j , j k=0 p k to the decoder to generate z P L j+1 , until we have generated n + 1 points. We then use linear interpolation to map (v 0 , p 0 ) . . . (v n , p n ) \\u2192x 0:T . We normalize the position vector and use the cumulative summation to determine segment positions. Figure 5 we present plots showing average AUC-ROC for all possible combinations of auxiliary tasks, for each data set. Here the contribution of each task can be examined. In order to evaluate the use of self-supervised auxiliary tasks in an easily replicable setting, and in order to facilitate comparisons with state-of-the are methods, we performed classification on 7 data sets from the UCR Time Series Archive https://www.cs.ucr.edu/\\u02dceamonn/time_ series_data_2018/. We included tasks that had fixed length sequences and a sample to label ratio greater than 100. We only included the subset of these tasks for which our baseline architecture performed better than random chance. These selections were to ensure that the tasks were reasonably well suited to our baseline architecture. For each dataset, the last 20% of the training sample was used as a validation set, with the testing sample used as a test set. Table 2 shows baseline results for each task (accuracy when our model was run with no auxiliary tasks), the highest accuracy achieved with any combination of auxiliary tasks, and state-of-the-art performance according to timeseriesclassification.com. Although the baseline architecture is routinely and drastically outperformed by state-of-the-art methods, the addition of auxiliary tasks improved performance to be significantly closer to the state-of-the-art level in most cases, and our model achieved at-state-of-the-art accuracy for the Two Patterns task. Although our model was unable to achieve state-of-the-art performance on most tasks, the improvement over baseline indicates that the addition of self-supervised auxiliary tasks could enhance better suited baseline architectures on these and other tasks.\",\n          \"Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against a well-defined class of adversaries. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest robustness against a first-order adversary as a natural security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Recent breakthroughs in computer vision and speech recognition are bringing trained classifiers into the center of security-critical systems. Important examples include vision for autonomous cars, face recognition, and malware detection. These developments make security aspects of machine learning increasingly important. In particular, resistance to adversarially chosen inputs is becoming a crucial design goal. While trained models tend to be very effective in classifying benign inputs, recent work BID6 BID24 BID8 BID16 BID22 shows that an adversary is often able to manipulate the input so that the model produces an incorrect output. This phenomenon has received particular attention in the context of deep neural networks, and there is now a quickly growing body of work on this topic BID7 BID13 BID20 BID25 BID23 BID27 . Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability BID24 BID8 BID16 BID22 BID15 . This holds even when the benign example was classified correctly, and the change is imperceptible to a human. Apart from the security implications, this phenomenon also demonstrates that our current models are not learning the underlying concepts in a robust manner. All these findings raise a fundamental question:How can we learn models robust to adversarial inputs?There are now many proposed defense mechanisms for the adversarial setting. Examples include defensive distillation BID18 , feature squeezing BID31 , and several detection approaches for adversarial inputs (see BID4 for references). While these works constitute important first steps in exploring the realm of possibilities, they do not offer a good understanding of the guarantees they provide. We can never be certain that a particular defense mechanism prevents the existence of some well-defined class of adversarial attacks. This makes it difficult to navigate the landscape of adversarial robustness or to fully evaluate the possible security implications. Moreover, subsequent work BID2 BID11 has shown that most of these defenses can be bypassed by stronger, adaptive adversaries. In this paper, we study the adversarial robustness of neural networks through the lens of robust optimization. We use a natural saddle point (min-max) formulation to capture the notion of security against adversarial attacks in a principled manner. This formulation allows us to be precise about the type of security guarantee we would like to achieve, i.e., the broad class of attacks we want to be resistant to (in contrast to defending only against specific known attacks). The formulation also enables us to cast both attacks and defenses into a common theoretical framework. Most prior work on adversarial examples naturally fits into this framework. In particular, adversarial training directly corresponds to optimizing this saddle point problem. Similarly, prior methods for attacking neural networks correspond to specific algorithms for solving the underlying optimization problem. Equipped with this perspective, we make the following contributions.1. We conduct a careful experimental study of the optimization landscape corresponding to this saddle point formulation. Despite the non-convexity and non-concavity of its constituent parts, we find that the underlying optimization problem is tractable after all. In particular, we provide strong evidence that first-order methods can reliably solve this problem and motivate projected gradient descent (PGD) as a universal \\\"first-order adversary\\\", i.e., the strongest attack utilizing the local first order information about the network. We supplement these insights with ideas from real analysis to further motivate adversarial training against a PGD adversary as a strong and natural defense.2. We explore the impact of network architecture on adversarial robustness and find that model capacity plays an important role. To reliably withstand strong adversarial attacks, networks require a significantly larger capacity than for correctly classifying benign examples only. This shows that a robust decision boundary of the saddle point problem can be significantly more complicated than a decision boundary that simply separates the benign data points.3. Building on the above insights, we train networks on MNIST and CIFAR10 that are robust to a wide range of adversarial attacks against adversaries bounded by 0.3 and 8 in \\u221e norm respectively. Our approach is based on optimizing the aforementioned saddle point formulation and uses our optimal \\\"first-order adversary\\\". Our best MNIST model achieves an accuracy of more than 89% against the strongest adversaries in our test suite. In particular, our MNIST network is even robust against white box attacks of an iterative adversary. Our CIFAR10 model achieves an accuracy of 46% against the same adversary. Furthermore, in case of the weaker black box (transfer) attacks, our MNIST and CIFAR10 networks achieve an accuracy of more than 95% and 64%, respectively (a more detailed overview can be found in TAB0 ). To the best of our knowledge, we are the first to achieve these levels of robustness on MNIST and CIFAR10 against a broad set of attacks. Overall, these findings suggest that secure neural networks are within reach. In order to further support this claim, we have invited the community to attempt attacks against our MNIST and CIFAR10 networks in the form of an open challenge 1,2 . At the time of writing, we received about fifteen submissions to the MNIST challenge and the best submission achieved roughly 93% accuracy in a black box attack. We received no submissions for the CIFAR10 challenge that went beyond the 64% accuracy of our attack. Considering that other proposed defenses were often quickly broken BID4 , we believe that our robust models are significant progress on the defense side. Furthermore, recent work on verifiable adversarial examples showed that our proposed defense reliably increased the robustness to any \\u221e -bounded attack. Much of our discussion will revolve around an optimization view of adversarial robustness. This perspective not only captures the phenomena we want to study in a precise manner, but will also 1 https://github.com/MadryLab/mnist_challenge 2 https://github.com/MadryLab/cifar10_challenge inform our investigations. To this end, let us consider a standard classification task with an underlying data distribution D over pairs of examples x \\u2208 R d and corresponding labels y \\u2208 [k]. We also assume that we are given a suitable loss function L(\\u03b8, x, y), for instance the cross-entropy loss for a neural network. As usual, \\u03b8 \\u2208 R p is the set of model parameters. Our goal then is to find model parameters \\u03b8 that minimize the risk DISPLAYFORM0 Empirical risk minimization (ERM) has been tremendously successful as a recipe for finding classifiers with small population risk. Unfortunately, ERM often does not yield models that are robust to adversarially crafted examples BID8 BID13 BID15 BID27 . Formally, there are efficient algorithms (\\\"adversaries\\\") that take an example x belonging to class c 1 as input and find examples x adv such that x adv is very close to x but the model incorrectly classifies x adv as belonging to class c 2 = c 1 .In order to reliably train models that are robust to adversarial attacks, it is necessary to augment the ERM paradigm. Instead of resorting to methods that directly focus on improving the robustness to specific attacks, our approach is to first propose a concrete guarantee that an adversarially robust model should satisfy. We then adapt our training methods towards achieving this guarantee. The first step towards such a guarantee is to specify an threat model, i.e., a precise definition of the attacks our models should be resistant to. For each data point x, we introduce a set of allowed perturbations S \\u2286 R d that formalizes the manipulative power of the adversary. In image classification, we choose S so that it captures perceptual similarity between images. For instance, the \\u221e -ball around x has recently been studied as a natural notion for adversarial perturbations BID8 . While we focus on robustness against \\u221e -bounded attacks in this paper, we remark that more comprehensive notions of perceptual similarity are an important direction for future research. Next, we modify the definition of population risk E D [L] by incorporating the above adversary. Instead of computing the loss L directly on samples from the distribution D, we allow the adversary to perturb the input first. This gives rise to the following saddle point problem, which is our central object of study: DISPLAYFORM1 Formulations of this type (and their finite-sample counterparts) have a long history in robust optimization, going back to Wald BID28 BID29 BID30 . It turns out that this formulation is also particularly useful in our context. We will refer to the quantity \\u03c1(\\u03b8) as the adversarial loss of the network with parameters \\u03b8. First, this formulation gives us a unifying perspective that encompasses much prior work on adversarial robustness. Our perspective stems from viewing the saddle point problem as the composition of an inner maximization problem and an outer minimization problem. Both of these problems have a natural interpretation in our context. The inner maximization problem aims to find an adversarial version of a given data point x that achieves a high loss. This is precisely the problem of attacking a given neural network. On the other hand, the goal of the outer minimization problem is to find model parameters so that the adversarial loss given by the inner attack problem is minimized. This is precisely the problem of training a robust classifier using adversarial training techniques. Second, the saddle point problem specifies a clear goal that a robust classifier should achieve, as well as a quantitative measure of its robustness. In particular, when the parameters \\u03b8 yield a (nearly) vanishing risk, the corresponding model is perfectly robust to attacks specified by our threat model. Our paper investigates the structure of this saddle point problem in the context of deep neural networks. This formulation will be the main drive of our investigations that will lead us to training techniques that produce models with high resistance to a wide range of adversarial attacks. Current work on adversarial examples usually focuses on specific defensive mechanisms, or on attacks against such defenses. An important feature of formulation (2.1) is that attaining small adversarial loss gives a guarantee that no allowed attack will fool the network. By definition, no adversarial perturbations are possible because the loss is small for all perturbations allowed by our threat model. This perspective allows us to reduce the task of finding truly robust models to an optimization problem. Hence, we can now focus our attention solely on obtaining a good solution to Problem (2.1).Gradients from attacks. Since Stochastic Gradient Descent (SGD) and its variants are by far the most successful algorithms for training neural networks, we also want to apply SGD to Problem (2.1). This raises the question how we can compute gradients \\u2207 \\u03b8 \\u03c1(\\u03b8) for the outer minimization problem. Since the adversarial loss function \\u03c1(\\u03b8) corresponds to a maximization problem, we cannot simply apply the usual backpropagation algorithm. Instead, a natural approach is to compute the gradient at the maximizer of the inner maximization problem. A priori, it is not clear that this is a valid descent direction for the saddle point problem. However, for the case of continuously differentiable functions, Danskin's theorem -a classic theorem in optimization -states that this is indeed true and gradients at maximizers of the inner problem correspond to descent directions for the saddle point problem (see Appendix C for details).Leveraging this connection, our goal now is to find a reliable algorithm for solving the inner maximization problem, i.e., to evaluate \\u03c1(\\u03b8). When instantiated for a batch of examples (instead of the expectation over the entire distribution D), finding a maximizer \\u03b4 \\u2208 S of \\u03c1(\\u03b8) corresponds exactly to finding an attack on the neural network. This allows us to employ known attacks as inner maximization algorithms. Prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) and multiple variations of it BID8 . FGSM is an attack for an \\u221e -bounded adversary and computes an adversarial example as DISPLAYFORM0 One can interpret this attack as a simple one-step scheme for maximizing the inner part of the saddle point formulation. A more powerful adversary is the multi-step variant FGSM k , which is essentially projected gradient descent (PGD) on the negative loss function BID13 : DISPLAYFORM1 Loss landscape. While PGD is a well-motivated approach for the inner maximization problem, it is not clear whether we can actually find a good solution in a reasonable amount of time. The problem is non-concave, so a priori we have no guarantees on the solution quality of PGD. One of our contributions is demonstrating that, in practice, the inner maximization problem is indeed well-behaved. In particular, we experimentally explore the structure given by the non-concave inner problem and find that its loss landscape has a surprisingly tractable structure of local maxima (see Appendix A). This structure also points towards projected gradient descent as the \\\"ultimate\\\" first-order adversary (see Section 5).Despite the fact that the exact assumptions of Danskin's theorem do not hold for our problem (the function is not continuously differentiable due to ReLU activations, and we only compute approximate maximizers of the inner problem), our experiments suggest that we can still use these gradients to optimize our problem. By applying SGD using the gradient of the loss at adversarial examples, we can consistently reduce the loss of the saddle point problem during training (e.g., see FIG0 in Section 4). These observations suggest that we reliably optimize the saddle point formulation (2.1) and thus train robust classifiers. Model capacity. Before we proceed to our main experiment results in the next section, we briefly mention another important insight from our robust optimization perspective. Solving the problem from Equation (2.1) successfully is not sufficient to guarantee robust and accurate classification. We also require that the value of the problem (i.e., the final loss we achieve against adversarial examples) is small, which then provides guarantees for the performance of our classifier. In particular, achieving a very small value corresponds to a perfect classifier, which is robust to adversarial inputs. In Appendix B, we show experimentally that network capacity plays a crucial role in enabling robustness. In particular, training a robust classifier requires a significantly larger network than only achieving high accuracy on natural examples. Following our understanding developed in the previous section, we can now apply our proposed approach to train robust classifiers. For both MNIST and CIFAR10, our adversary of choice will be projected gradient descent starting from a random perturbation around the natural example. As our experiments suggest (Appendix A) this algorithm is very efficient at reliably producing examples of (near) maximal loss. In a sense, it seems to correspond to a \\\"ultimate\\\" f irst order adversary. Since we are training the model for multiple epochs, we did not see any benefit in restarting PGD multiple times per batch -a new start is chosen each time the same example is encountered. During the training procedure against the PGD adversary, we observe a steady decrease in the training loss of adversarial examples, illustrated in FIG0 . This behavior indicates that we are consistently decreasing the adversarial loss and indeed successfully solving our original optimization problem. The sharp drops in the CIFAR10 plot correspond to decreases in training learning rate. These plots illustrate that we can consistently reduce the value of the inner problem of the saddle point formulation (2.1), thus producing an increasingly robust classifier. We evaluate the trained models against a range of adversaries. We illustrate our results in TAB0 for CIFAR10. The adversaries we consider are:\\u2022 White-box attacks with PGD for a different number of of iterations and restarts, denoted by source A.\\u2022 White-box attacks from BID3 . We use their suggested loss function and minimize it using PGD. This is denoted as CW, where the corresponding attack with a high confidence parameter (\\u03ba = 50) is denoted as CW+.\\u2022 Black-box attacks from an independently trained copy of the network, denoted A'.\\u2022 Black-box attacks from a version of the same network trained only on natural examples, denoted A nat .\\u2022 Black-box attacks from a different convolution architecture, denoted B, described in BID26 .MNIST. We run 40 iterations of projected gradient descent as our adversary, with a step size of 0.01 (we choose to take gradient steps in the \\u221e norm, i.e. adding the sign of the gradient, since this makes the choice of the step size simpler). We train and evaluate against perturbations of size \\u03b5 = 0.3. We use a network consisting of two convolutional layers with 32 and 64 filters respectively, each followed by 2 \\u00d7 2 max-pooling, and a fully connected layer of size 1024. When trained with natural examples, this network reaches 99.2% accuracy on the evaluation set. However, when evaluating on examples perturbed with FGSM the accuracy drops to 6.4%. Given that the resulting MNIST model is very robust, we investigated the learned parameters in order to understand how they affect adversarial robustness. The results of the investigation are presented in Appendix E. For the CIFAR10 dataset, we use the two architectures described in B (the original Resnet and its 10\\u00d7 wider variant). We trained the network against a PGD adversary with \\u221e projected gradient descent again, this time using 7 steps of size 2, and a total \\u03b5 = 8. For our hardest adversary we chose 20 steps with the same settings, since other hyperparameter choices didn't offer a significant decrease in accuracy. The results of our experiments appear in TAB1 . The adversarial robustness of our network is significant, given the power of iterative adversaries, but still far from satisfactory. We believe that further progress is possible along these lines by understanding how adversarial training works and what techniques can complement it leading to robust models. Resistance for different values of \\u03b5 and 2 -bounded attacks. In order to perform a broader evaluation of the adversarial robustness of our models, we run two kinds of additional experiments. On one hand, we investigate the resistance to \\u221e -bounded attacks for different values of \\u03b5. On the other hand, we examine the resistance of our model to attacks that are bounded in 2 as opposed to \\u221e norm. The results appear in Figure 2 . We emphasize that the models we are examining here correspond to training against \\u221e -bounded attacks with the original value of \\u03b5 = 0.3, for MNIST, and \\u03b5 = 8 for CIFAR10. In particular, our MNIST model retains significant resistance to 2 -norm-bounded perturbations too -it has good accuracy even for \\u03b5 = 4.5.We provide a sample of corresponding adversarial examples in FIG0 of Appendix F. One can observe that some of the underlying perturbations are large enough that even a human could be confused. Training Accuracy. It is worth noting our MNIST and (wide) CIFAR10 networks reached 100% adversarial accuracy on the training set. That is we can fit the training set even against a PGD adversary of \\u03b5 = 0.3 and \\u03b5 = 8 respectively. This shows that the landscape of the underlying optimization problem is tractable and does not present a significant barrier to our techniques. DISPLAYFORM0 Figure 2: Performance of our adversarially trained networks against PGD adversaries of different strength. The MNIST and CIFAR10 networks were trained against \\u03b5 = 0.3 and \\u03b5 = 8 PGD \\u221e adversaries respectively (the training \\u03b5 is denoted with a red dashed lines in the \\u221e plots). We notice that for \\u03b5 less or equal to the value used during training, the performance is equal or better. Running Time. Unfortunately, solving the robust version of the problem instead of the standard one imposes a significant computational overhead. Standard training requires one forward and one backward pass through the network for each training batch. Instead, adversarial training with a k-step PGD adversary, requires additionally k forward and k backward passes through the network to compute the adversarial version of the training batch. This implies an increase in running time of a factor of (k + 1). We hope that future research will propose ways to mitigate this drawback. Our exploration of the loss landscape (Appendix A) shows that the local maxima found by PGD all have similar loss values, both for normally trained networks and adversarially trained networks. This concentration phenomenon suggests an intriguing view on the problem in which robustness against the PGD adversary yields robustness against all first-order adversaries, i.e., attacks that rely only on first-order information. As long as the adversary only uses gradients of the loss function with respect to the input, we conjecture that it will not find significantly better local maxima than PGD. This hypothesis is validated by the experimental evidence provided in Section 4: if we train a network to be robust against PGD adversaries, it becomes robust against a wide range of other attacks as well. Of course, our exploration with PGD does not preclude the existence of some isolated maxima with much larger function value. However, our experiments suggest that such better local maxima are hard to find with first order methods: even a large number of random restarts did not find function values with significantly different loss values (see Appendix A). Incorporating the computational power of the adversary into the threat model should be reminiscent of the notion of polynomially bounded adversary that is a cornerstone of modern cryptography. There, this classic threat model allows the adversary to only solve problems that require at most polynomial computation time. Here, we employ an optimization-based view on the power of the adversary as it is more suitable in the context of machine learning. After all, we have not yet developed a thorough understanding of the computational complexity of many recent machine learning problems. However, the vast majority of optimization problems in ML is solved with first-order methods, and variants of SGD are the most effective way of training deep learning models in particular. Hence we believe that the class of attacks relying on first-order information is, in some sense, universal for the current practice of deep learning. Put together, these two ideas chart the way towards machine learning models with guaranteed robustness. If we train the network to be robust against PGD adversaries, it will be robust against a wide range of attacks that encompasses all current approaches. In fact, this robustness guarantee would become even stronger in the context of transfer attacks, i.e., attacks in which the adversary does not have a direct access to the target network. Instead, the adversary only has less specific information such as the (rough) model architecture and the training data set. One can view this threat model as an example of \\\"zero order\\\" attacks, i.e., attacks in which the adversary has no direct access to the classifier and is only able to evaluate it on chosen examples without gradient feedback. Still, even for the case of zero-order attacks, the gradient of the network can be estimated using a finite differences method, rendering first-order attacks also relevant in this context. We discuss transferability in Appendix D. We observe that increasing network capacity and strengthening the adversary we train against (FGSM or PGD training, rather than natural training) improves resistance against transfer attacks. Also, as expected, the resistance of our best models to such attacks tends to be significantly larger than to the (strongest) first order attacks. Due to the growing body of work on adversarial examples in the context of deep learning networks BID9 BID7 BID25 BID19 BID2 BID27 BID8 BID13 , we focus only on the most related papers here. Before we compare our contributions, we remark that robust optimization has been studied outside deep learning for multiple decades. We refer the reader to BID1 for an overview of this field. To the best of our knowledge, in the context of adversarial examples, an explicit formulation of the min-max optimization first appeared in , BID21 , and BID14 . All of these works, however, consider very weak adversaries/methods for solving the maximization problem, mainly relying on linearizing the loss and performing a single step, similar to FGSM. These adversaries do not capture the full range of possible attacks and thus training only against them leaves the resulting classifier vulnerable to more powerful, iterative attacks. Recent work on adversarial training on ImageNet also observed that the model capacity is important for adversarial training BID13 . However, their work was focused on FGSM attacks, since they report the iterative attacks are too expensive computationally and don't provide any significant benefits. In contrast to that, we discover that for the datasets we considered training against iterative adversaries does result in a model that is robust against such adversaries. A more recent paper BID27 ) also explores the transferability phenomenon. This exploration focuses mostly on the region around natural examples where the loss is (close to) linear. When large perturbations are allowed, this region does not give a complete picture of the adversarial landscape. This is confirmed by our experiments, as well as pointed out by BID26 .Another recent paper BID26 , considers adversarial training using black-box attacks from similar networks in order to increase the robustness of the network against such adversaries. However, this is not an effective defense against the white-box setting we consider, since a PGD adversary can reliably produce adversarial examples for such networks. Our findings provide evidence that deep neural networks can be made resistant to adversarial attacks. As our theory and experiments indicate, we can design reliable adversarial training methods. One of the key insights behind this is the unexpectedly regular structure of the underlying optimization task: even though the relevant problem corresponds to the maximization of a highly non-concave function with many distinct local maxima, their values are highly concentrated. Overall, our findings give us hope that adversarially robust deep learning models may be within current reach. For the MNIST dataset, our networks are very robust, achieving high accuracy for a wide range of powerful adversaries and large perturbations. Our experiments on CIFAR10 have not reached the same level of performance yet. However, our results already show that our techniques lead to significant increase in the robustness of the network. We believe that further exploring this direction will lead to adversarially robust networks for this dataset. We thank Wojciech Matusik for kindly providing us with computing resources to perform this work. The inner problem of the saddle point formulation (2.1) corresponds to finding an adversarial example for a given network and data point (subject to our attack model). As this problem requires us to maximize a highly non-concave function, one would expect it to be intractable. Indeed, this is the conclusion reached by prior work which then resorted to linearizing the inner maximization problem BID21 . As pointed out above, this linearization approach yields well-known methods such as FGSM. While training against FGSM adversaries has shown some successes, recent work also highlights important shortcomings of this one-step approach BID26 .To understand the inner problem in more detail, we investigate the landscape of local maxima for multiple models on MNIST and CIFAR10. The main tool in our experiments is projected gradient descent (PGD), since it is the standard method for large-scale constrained optimization. In order to explore a large part of the loss landscape, we re-start PGD from many points in the \\u221e balls around data points from the respective evaluation sets. Surprisingly, our experiments show that the inner problem is tractable after all, at least from the perspective of first-order methods. While there are many local maxima spread widely apart within x i + S, they tend to have very well-concentrated loss values. This echoes the folklore belief that training neural networks is possible because the loss (as a function of model parameters) typically has many local minima with very similar values. Specifically, in our experiments we found the following phenomena:\\u2022 We observe that the loss achieved by the adversary increases in a fairly consistent way and plateaus rapidly when performing projected \\u221e gradient descent for randomly chosen starting points inside x + S (see FIG2 ). FIG0 ). The adversarial loss plateaus after a small number of iterations. The optimization trajectories and final loss values are also fairly clustered, especially on CIFAR10. Moreover, the final loss values on adversarially trained networks are significantly smaller than on their naturally trained counterparts.\\u2022 Investigating the concentration of maxima further, we observe that over a large number of random restarts, the loss of the final iterate follows a well-concentrated distribution without extreme outliers (see FIG3 ; we verified this concentration based on 10 5 restarts).\\u2022 To demonstrate that maxima are noticeably distinct, we also measured the 2 distance and angles between all pairs of them and observed that distances are distributed close to the expected distance between two random points in the \\u221e ball, and angles are close to 90\\u2022 . Along the line segment between local maxima, the loss is convex, attaining its maximum at the endpoints and is reduced by a constant factor in the middle. Nevertheless, for the entire segment, the loss is considerably higher than that of a random point.\\u2022 Finally, we observe that the distribution of maxima suggests that the recently developed subspace view of adversarial examples is not fully capturing the richness of attacks BID27 of the example, and deteriorating overall correlation with the gradient direction as the scale of perturbation increases. For a fixed set S of possible perturbations, the value of the problem (2.1) is entirely dependent on the architecture of the classifier we are learning. Consequently, the architectural capacity of the model becomes a major factor affecting its overall performance. At a high level, classifying examples in a robust way requires a stronger classifier, since the presence of adversarial examples changes the decision boundary of the problem to a more complicated one (see Figure 5 for an illustration).Figure 5: A conceptual illustration of \\\"natural\\\" vs. \\\"adversarial\\\" decision boundaries. Left: A set of points that can be easily separated with a simple (in this case, linear) decision boundary. Middle: The simple decision boundary does not separate the \\u221e -balls (here, squares) around the data points. Hence there are adversarial examples (the red stars) that will be misclassified. Right: Separating the \\u221e -balls requires a significantly more complicated decision boundary. The resulting classifier is robust to adversarial examples with bounded \\u221e -norm perturbations. Our experiments verify that capacity is crucial for robustness, as well as for the ability to successfully train against strong adversaries. For the MNIST dataset, we consider a simple convolutional network and study how its behavior changes against different adversaries as we keep doubling the size of network (i.e. double the number of convolutional filters and the size of the fully connected layer). The initial network has a convolutional layer with 2 filters, followed by another convolutional layer with 4 filters, and a fully connected hidden layer with 64 units. Convolutional layers are followed by 2 \\u00d7 2 max-pooling layers and adversarial examples are constructed with \\u03b5 = 0.3. The results are in FIG4 .For the CIFAR10 dataset, we used the Resnet model BID10 TFM (2017) . We performed data augmentation using random crops and flips, as well as per image standarization. To increase the capacity, we modified the network incorporating wider layers by a factor of 10. This results in a network with 5 residual units with (16, 160, 320, 640) filters each. This network can achieve an accuracy of 95.2% when trained with natural examples. Adversarial examples were constructed with \\u03b5 = 8. Results on capacity experiments appear in FIG4 .We observe the following phenomena:Capacity alone helps. We observe that increasing the capacity of the network when training using only natural examples (apart from increasing accuracy on these examples) increases the robustness against one-step perturbations. This effect is greater when considering adversarial examples with smaller \\u03b5. FGSM adversaries don't increase robustness (for large \\u03b5). When training the network using adversarial examples generated with the FGSM, we observe that the network overfits to these adversarial examples. This behavior is known as label leaking BID13 and stems from the fact that the adversary produces a very restricted set of adversarial examples that the network can overfit to. These networks have poor performance on natural examples and don't exhibit any kind of robustness against PGD adversaries. For the case of smaller \\u03b5 the loss is ofter linear enough in the \\u221e ball around natural examples, that FGSM finds adversarial examples close to those found by PGD thus being a reasonable adversary to train against. Weak models may fail to learn non-trivial classifiers. In the case of small capacity networks, attempting to train against a strong adversary (PGD) prevents the network from learning anything meaningful. The network converges to always predicting a fixed class, even though it could converge to an accurate classifier through natural training. The small capacity of the network forces the training procedure to sacrifice performance on natural examples in order to provide any kind of robustness against adversarial inputs. The value of the saddle point problem decreases as we increase the capacity. Fixing an adversary model, and training against it, the value of (2.1) drops as capacity increases, indicating the the model can fit the adversarial examples increasingly well. More capacity and stronger adversaries decrease transferability. Either increasing the capacity of the network, or using a stronger method for the inner optimization problem reduces the effectiveness of transferred adversarial inputs. We validate this experimentally by observing that the correlation between gradients from the source and the transfer network, becomes less significant as capacity increases. We describe our experiments in Appendix D. Recall that our goal is to minimize the value of the saddle point problem DISPLAYFORM0 In practice, we don't have access to the distribution D so both the gradients and the value of \\u03c1(\\u03b8) will be computed using sampled input points. Therefore we can consider -without loss of generality-the case of a single random example x with label y, in which case the problem becomes DISPLAYFORM1 If we assume that the loss L is continuously differentiable in \\u03b8, we can compute a descent direction for \\u03b8 by utilizing the classical theorem of Danskin. In the first three plots/tables of each dataset, we show how the natural and adversarial accuracy changes with respect to capacity for each training regime. In the final plot/table, we show the value of the cross-entropy loss on the adversarial examples the networks were trained on. This corresponds to the value of our saddle point formulation (2.1) for different sets of allowed perturbations. Theorem C.1 (Danskin). Let S be nonempty compact topological space and g : R n \\u00d7 S \\u2192 R be such that g(\\u00b7, \\u03b4) is differentiable for every \\u03b4 \\u2208 S and \\u2207 \\u03b8 g(\\u03b8, \\u03b4) is continuous on R n \\u00d7 S. Also, let \\u03b4 * (\\u03b8) = {\\u03b4 \\u2208 arg max \\u03b4\\u2208S g(\\u03b8, \\u03b4)}.Then the corresponding max-function DISPLAYFORM2 is locally Lipschitz continuous, directionally differentiable, and its directional derivatives satisfy DISPLAYFORM3 In particular, if for some \\u03b8 \\u2208 R n the set \\u03b4 * (\\u03b8) = {\\u03b4 * \\u03b8 } is a singleton, the the max-function is differentiable at \\u03b8 and DISPLAYFORM4 The intution behind the theorem is that since gradients are local objects, and the function \\u03c6(\\u03b8) is locally the same as g(\\u03b8, \\u03b4 measure zero, we can assume that this will not be an issue in practice, as we will never encounter the problematic points. Another technical issue is that, due to the not concavity of the inner problem, we are not able to compute global maximizers, since PGD will converge to local maxima. In such cases, we can consider a subset S of S such that the local maximum is a global maximum in the region S . Applying the theorem for S gives us that the gradient corresponds to a descent direction for the saddle point problem when the adversary is constrained in S . Therefore if the inner maximum is a true adversarial example for the network, then SGD using the gradient at that point will decrease the loss value at this particular adversarial examples, thus making progress towards a robust model. These arguments suggest that the conclusions of the theorem are still valid in our saddle point problem, and -as our experiments confirm-we can solve it reliably. A lot of recent literature on adversarial training discusses the phenomenon of transferability Goodfellow et al. (2014) ; BID13 ; BID27 , i.e. adversarial examples transfer between differently trained networks. This raises concerns for practical applications, since it suggests that deep networks are extremely vulnerable to attacks, even when there is no direct access to the target network. This phenomenon is further confirmed by our current experiments. 4 Moreover, we notice that the extent to which adversarial examples transfer decreases as we increase either network capacity or the power of the adversary used for training the network. This serves as evidence for the fact that the transferability phenomenon can be alleviated by using high capacity networks in conjunction with strong oracles for the inner optimization problem. MNIST. In an attempt to understand these phenomena we inspect the loss functions corresponding to the trained models we used for testing transferability. More precisely, we compute angles between gradients of the loss functions evaluated over a large set of input examples, and plot their distribution. Similarly, we plot the value of the loss functions between clean and perturbed examples for both the source and transfer networks. In Figure 8 we plot our experimental findings on the MNIST dataset for \\u03b5 = 0.3. We consider a naturally trained large network (two convolutional layers of sizes 32 and 64, and a fully connected layer of size 1024), which we train twice starting with different initializations. We plot the distribution of angles between gradients for the same test image in the two resulting networks (orange histograms), noting that they are somewhat correlated. As opposed to this, we see that pairs of gradients for random pairs of inputs for one architecture are as uncorrelated as they can be (blue histograms), since the distribution of their angles looks Gaussian. Next, we run the same experiment on a naturally trained very large network (two convolutional layers of sizes 64 and 128, and a fully connected layer of size 1024). We notice a mild increase in classification accuracy for transferred examples. Finally, we repeat the same set of experiments, after training the large and very large networks against the FGSM adversary. We notice that gradients between the two architectures become significantly less correlated. Also, the classification accuracy for transferred examples increases significantly compared to the naturally trained networks. We further plot how the value of the loss function changes when moving from the natural input towards the adversarially perturbed input (in Figure 8 we show these plots for four images in the MNIST test dataset), for each pair of networks we considered. We observe that, while for the naturally trained networks, when moving towards the perturbed point, the value of the loss function on the transfer architecture tends to start increasing soon after it starts increasing on the source architecture. In contrast, for the stronger models, the loss function on the transfer network tends to start increasing later, and less aggressively. For the CIFAR10 dataset, we investigate the transferability of the FGSM and PGD adversaries between our simple and wide architectures, each trained on natural, FGSM and PGD examples. Transfer accuracies for the FGSM adversary and PGD adversary between all pairs of such configurations (model + training method) with independently random weight initialization are given in tables 3 and 4 respectively. The results exhibit the following trends:\\u2022 Stronger adversaries decrease transferability: In particular, transfer attacks between two PGD-trained models are less successful than transfer attacks between their naturally-trained counterparts. Moreover, adding PGD training helps with transferability from all adversarial datasets, except for those with source a PGD-trained model themselves. This applies to both FGSM attacks and PGD attacks.\\u2022 Capacity decreases transferability: In particular, transfer attacks between two PGDtrained wide networks are less successful than transfer attacks between their simple PGDtrained counterparts. Moreover, with few close exceptions, changing the architecture from simple to wide (and keeping the training method the same) helps with transferability from all adversarial datasets. We additionally plotted how the loss of a network behaves in the direction of FGSM and PGD examples obtained from itself and an independently trained copy; results for the simple naturally trained network and the wide PGD trained network are given in Table 7 . As expected, we observe the following phenomena:\\u2022 sometimes, the FGSM adversary manages to increase loss faster near the natural example, but as we move towards the boundary of the \\u221e box of radius \\u03b5, the PGD attack always achieves higher loss.\\u2022 the transferred attacks do worse than their white-box counterparts in terms of increasing the loss;\\u2022 and yet, the transferred PGD attacks dominate the white-box FGSM attacks for the naturally trained network (and sometimes for the PGD-trained one too). Tables on the left show the accuracy of the networks against three types of input (clean, perturbed with FGSM, perturbed with PGD ran for 40 steps); the first column shows the resilience of the first network against examples produced using its own gradients, the second column shows resilience of the second network against examples transferred from the former network. The histograms reflect angles between pairs of gradients corresponding to the same inputs versus the baseline consisting of angles between gradients from random pairs of points. Images on the right hand side reflect how the loss functions of the native and the transfer network change when moving in the direction of the perturbation; the perturbation is at 1 on the horizontal axis. Plots in the top row are for FGSM perturbations, plots in the bottom row are for PGD perturbations produced over 40 iterations. The robust MNIST model described so far is small enough that we can visually inspect most of its parameters. Doing so will allow us to understand how it is different from a naturally trained variant and what are the general characteristics of a network that is robust against \\u221e adversaries. We will compare three different networks: a naturally trained model, and two adversarially trained ones. The latter two models are identical, modulo the random weight initialization, and were used as the public and secret models used for our robustness challenge. Initially, we examine the first convolutional layer of each network. We observe that the robust models only utilize 3 out of the total 32 filters, and for each of these filters only one weight is non-zero. By doing so, the convolution degrades into a scaling of the original image. Combined with the bias and the ReLU that follows, this results in a thresholding filter, or equivalently ReLU(\\u03b1x \\u2212 \\u03b2) for some constants \\u03b1, \\u03b2. From the perspective of adversarial robustness, thresholding filters are immune to any perturbations on pixels with value less than \\u03b2 \\u2212 \\u03b5. We visualize a sample of the filters in Figure 9 (plots a, c, and e).Having observed that the first layer of the network essentially maps the original image to three copies thresholded at different values, we examine the second convolutional layer of the classifier. Again, the filter weights are relatively sparse and have a significantly wider value range than the naturally trained version. Since only three channels coming out of the first layer matter, is follows (and is verified) that the only relevant convolutional filters are those that interact with these three channels. We visualize a sample of the filters in Figure 9 (plots b, d, and f).Finally, we examine the softmax/output layer of the network. While the weights seem to be roughly similar between all three version of the network, we notice a significant difference in the class biases. The adversarially trained networks heavily utilize class biases (far from uniform), and do so in a way very similar to each other. A plausible explanation is that certain classes tend to be very vulnerable to adversarial perturbations, and the network learns to be more conservative in predicting them. The plots can be found in FIG0 .All of the \\\"tricks\\\" described so far seem intuitive to a human and would seem reasonable directions when trying to increase the adversarial robustness of a classifier. We emphasize the none of these modifications were hard-coded in any way and they were all learned solely through adversarial training. We attempted to manually introduce these modifications ourselves, aiming to achieve adversarial robustness without adversarial training, but with no success. A simple PGD adversary could fool the resulting models on all the test set examples. Figure 9 : Visualizing a sample of the convolutional filters. For the natural model (a,b) we visualize random filters, since there is no observable difference in any of them. For the first layer of robust networks we make sure to include the 3 non-zero filters. For the second layer, the first three columns represent convolutional filters that utilize the 3 non-zero channels, and we choose the most interesting ones (larger range of values). We observe that adversarially trained networks have significantly more concentrated weights. Moreover, the first convolutional layer degrades into a few thresholding filters.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1312,\n        \"samples\": [\n          \"ryG6xZ-RZ\",\n          \"rJl5MeHKvB\",\n          \"rJzIBfZAb\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1312,\n        \"samples\": [\n          \"We introduce a novel compiler infrastructure that addresses shortcomings of existing deep learning frameworks. Proposal to move from ad-hoc code generation in deep learning engines to compiler and languages best practices. This paper presents a compiler framework that allows definition of domain-specific languages for deep learning systems, and defines compilation stages that can take advantage of standard optimizations and specialized optimizations for neural networks. This paper introduces a DLVM to take advantage of the compiler aspects of a tensor compiler\",\n          \"We show that extra unlabeled data is not required for self-supervised auxiliary tasks to be useful for time series classification, and present new and effective auxiliary tasks. This paper proposes a self-supervised method for learning from time series data in healthcare settings via designing auxilliary tasks based on data's internal structure to create more labeled auxilliary training tasks. This paper propose an approach for self-supervised learning on time series.\",\n          \"We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries. Investigates a minimax formulation of deep network learning to increase their robustness, using projected gradient descent as the main adversary.  This paper proposes to look at making neural networks resistant to adversarial loss through the framework of saddle-point problems. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1114,\n        \"samples\": [\n          \"L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data\",\n          \"Deep Graph Matching Consensus\",\n          \"Toward learning better metrics for sequence generation training with policy gradient\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 30,\n        \"max\": 149,\n        \"num_unique_values\": 91,\n        \"samples\": [\n          79,\n          49,\n          67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1312,\n        \"samples\": [\n          \"We present DLVM, a design and implementation of a compiler infrastructure with a linear algebra intermediate representation, algorithmic differentiation by adjoint code generation, domain- specific optimizations and a code generator targeting GPU via LLVM. Within the deep learning community, most current approaches to neural networks make use of high-level frameworks with a tensor domain-specific language (DSL) such as Torch BID3 , TensorFlow BID0 , PyTorch (PyTorch Development Team, 2016) , and MXNet BID1 . Recent projects, the TensorFlow XLA compiler and the NNVM compiler BID13 including TVM BID2 , have begun to apply compiler techniques to deep learning systems, targeting LLVM BID10 and various back-ends to achieve good performance. Our solution includes (1) a domain-specific intermediate representation specifically designed for tensor computation, (2) principled use of modern compiler optimization techniques to substantially simplify neural network computation, including algebra simplification, AD checkpointing, compute kernel fusion, and various traditional compiler optimizations, (3) code generation through a mature compiler infrastructure that allows for transparent targeting of various hardware, and (4) an embedded DSL that supports static analysis, type safety, and natural expression of tensor computation, and has a just-in-time (JIT) compiler targeting DLVM for AD, optimizations, and code generation. Importantly, our entire infrastructure was designed from the start around a robust compile-time framework for tensor DSLs, whereas XLA has been adapted around the existing TensorFlow infrastructure with a particular focus on hardware support for Google's Tensor Processing Units BID8 .Where TVM and NNVM are built as a DSL and a graph library in Python with a C++ implementation, DLVM's architecture is closer to LLVM and the Swift Intermediate Language BID7 , having an IR file format and a full-fledged command line toolchain. It uses static single assignment (SSA) form, control flow graphs, high-level types including a first-class tensor type, and a set of linear algebra operators combined with a general-purpose instruction set (see TAB1 ). The system enables a wide variety of domain-specific analyses and transformations, such as reverse-mode AD, AD checkpointing, algebra simplification and linear algebra fusion. Since DLVM IR is aware of mathematical operators such as tanh and power, the algebra simplification pass can find and simplify certain mathematical operations that are expensive or redundant. Since the DLVM optimizer is aware of linear algebra operations with static dimensionality, maximizing the performance by fusing verbose linear operations into a single matrix multiplication is beneficial as well. A more aggressive, interprocedural version of linear algebra fusion can optimize parameter passing and memory allocation, so that the entire concatenated matrix can be created and passed around in the first place without reallocation. The front-end can choose to differentiate a function with respect to selected arguments, to keep some of the outputs of the original function, to apply differentiation to a specific output when there are multiple return values, or to enable the function to accept backpropagated gradients (seeds) through function composition, all by gradient declarations. In software engineering, a proven approach to tackle this problem is language and compiler technologies, starting from a language that is amenable to static analysis. DSLs in a scripting language can easily achieve rapid prototyping, but they are generally incapable of providing a safe environment with optimal performance. In our initial release of DLVM, we provide one such DSL, both as a proof-of-concept and as a reference implementation that showcases the capabilities of DLVM as a platform for deep learning DSL development. We anticipate other existing deep learning frameworks, such as TensorFlow, could be adapted to use DLVM as a back-end to their tensor math DSLs. In our ongoing work, we plan to substantially increase the number of supported hardware architectures by utilizing HPVM as an additional back-end, and explore more advanced AD techniques such as mixing forward and reverse modes.\",\n          \"We explore a novel self-supervision framework for time-series data, in which multiple auxiliary tasks (e.g., forecasting) are included to improve overall performance on a sequence-level target task without additional training data. Our results indicate that limited self-supervision leads to a consistent improvement over a supervised baseline, across a range of domains. In this paper, we show that leveraging the sequential structure of the data at-hand can lead to improved performance on sequence-level tasks (i.e., the target task). We posit that even in the absence of additional data, self-supervision can lead to improved performance on the target task. Within the context of supervised learning, Schwab et al. considered a multitask framework for learning from sequential health data (Schwab et al., 2018) . Self-supervision occurs when no additional supervision is required to generate the ground truth label (e.g., in autoencoding, the input itself serves as the supervision). A piecewise-linear representation consists of two length n + 1 vectors (where n is the number of linear segments in the signal), a value vector v and a position vector p. We select our validation set randomly from the remaining data. In practice we found altering the decoding horizon had little effect on performance, so we used h = 6 for all experiments. To avoid conflating these sorts of improvements with those caused by learning better representations, we use label propagation with our target task (sequence classification), linearly annealing contributions to the loss function over the length of the signal (Dai & Le, 2015) . For the purposes of double-blind peer review, we have released the code and PLA data on an anonymous Google Drive account 1 . AE and Forecast refers to Autoencoding and Forecasting respectively, the auxiliary tasks explored in (Dai & Le, 2015) . To investigate the underlying mechanism by which auxiliary tasks improve performance, we examine model performance as we vary the number of auxiliary tasks and the amount of training data (results shown in Figure 4a ). As the amount of training data increases, the added value from the auxiliary tasks increases on average. By jointly training our target task with auxiliary self-supervised tasks, we demonstrated small but consistent improvements across three different sequence classification tasks. Dai and Le hypothesize the observed superiority of sequence autoencoding over forecasting may result from the short-term nature of the language modeling task (only predicting the next word).\",\n          \"These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. In particular, when the parameters \\u03b8 yield a (nearly) vanishing risk, the corresponding model is perfectly robust to attacks specified by our threat model. However, for the case of continuously differentiable functions, Danskin's theorem -a classic theorem in optimization -states that this is indeed true and gradients at maximizers of the inner problem correspond to descent directions for the saddle point problem (see Appendix C for details).Leveraging this connection, our goal now is to find a reliable algorithm for solving the inner maximization problem, i.e., to evaluate \\u03c1(\\u03b8). This structure also points towards projected gradient descent as the \\\"ultimate\\\" first-order adversary (see Section 5).Despite the fact that the exact assumptions of Danskin's theorem do not hold for our problem (the function is not continuously differentiable due to ReLU activations, and we only compute approximate maximizers of the inner problem), our experiments suggest that we can still use these gradients to optimize our problem. In particular, our MNIST model retains significant resistance to 2 -norm-bounded perturbations too -it has good accuracy even for \\u03b5 = 4.5.We provide a sample of corresponding adversarial examples in FIG0 of Appendix F. One can observe that some of the underlying perturbations are large enough that even a human could be confused. However, our experiments suggest that such better local maxima are hard to find with first order methods: even a large number of random restarts did not find function values with significantly different loss values (see Appendix A). While training against FGSM adversaries has shown some successes, recent work also highlights important shortcomings of this one-step approach BID26 .To understand the inner problem in more detail, we investigate the landscape of local maxima for multiple models on MNIST and CIFAR10. Specifically, in our experiments we found the following phenomena:\\u2022 We observe that the loss achieved by the adversary increases in a fairly consistent way and plateaus rapidly when performing projected \\u221e gradient descent for randomly chosen starting points inside x + S (see FIG2 ). Moreover, the final loss values on adversarially trained networks are significantly smaller than on their naturally trained counterparts.\\u2022 Investigating the concentration of maxima further, we observe that over a large number of random restarts, the loss of the final iterate follows a well-concentrated distribution without extreme outliers (see FIG3 ; we verified this concentration based on 10 5 restarts).\\u2022 To demonstrate that maxima are noticeably distinct, we also measured the 2 distance and angles between all pairs of them and observed that distances are distributed close to the expected distance between two random points in the \\u221e ball, and angles are close to 90\\u2022 . Nevertheless, for the entire segment, the loss is considerably higher than that of a random point.\\u2022 Finally, we observe that the distribution of maxima suggests that the recently developed subspace view of adversarial examples is not fully capturing the richness of attacks BID27 of the example, and deteriorating overall correlation with the gradient direction as the scale of perturbation increases. At a high level, classifying examples in a robust way requires a stronger classifier, since the presence of adversarial examples changes the decision boundary of the problem to a more complicated one (see Figure 5 for an illustration).Figure 5: A conceptual illustration of \\\"natural\\\" vs. \\\"adversarial\\\" decision boundaries. For the MNIST dataset, we consider a simple convolutional network and study how its behavior changes against different adversaries as we keep doubling the size of network (i.e. double the number of convolutional filters and the size of the fully connected layer). These arguments suggest that the conclusions of the theorem are still valid in our saddle point problem, and -as our experiments confirm-we can solve it reliably. Transfer accuracies for the FGSM adversary and PGD adversary between all pairs of such configurations (model + training method) with independently random weight initialization are given in tables 3 and 4 respectively. Tables on the left show the accuracy of the networks against three types of input (clean, perturbed with FGSM, perturbed with PGD ran for 40 steps); the first column shows the resilience of the first network against examples produced using its own gradients, the second column shows resilience of the second network against examples transferred from the former network. The plots can be found in FIG0 .All of the \\\"tricks\\\" described so far seem intuitive to a human and would seem reasonable directions when trying to increase the adversarial robustness of a classifier.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "8aaaba95-31ec-4171-91b1-a934c3f97e51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "fa0e02fa-9185-4a63-c970-7579acbde991"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "7c5a3d25-6730-4cd5-bcef-d7dccd0797f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "47848a12-ece9-42b2-9256-d3dd5bd52bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(647, 8) (162, 8) (809, 8) (203, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenize data\n",
        "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# # Function in order to tokenize source and target\n",
        "# max_input_length = 1024\n",
        "\n",
        "# def tokenize_function(data):\n",
        "#   model_inputs = tokenizer(text=data['extractive_summary'], text_target=data['target'], max_length=max_input_length, truncation=True)\n",
        "#   return model_inputs\n",
        "\n",
        "# tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "VyuuPBgIsJim"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "f19ec48d-4a8d-416a-e1f6-3408c643e676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "aeefc57837934324a7e1ef4bdcfd9447",
            "2739d969fde144a7b42389a1aadb0b2c",
            "4c06f600e9b94125b4bf3a81b691fa2a",
            "a1547cd289c34b8ca8cb109f9f1c49df",
            "17495c6333df4ecd8d1a1ad3a31cd0c4",
            "939bf63cd3184afd82415fb94c82fe33",
            "4e79aead0c27484b864866b369c94e97",
            "10e23144400444e7bf2e6131ecf02f1a",
            "7ea020348df3411090914db144e6e189",
            "2e49a0f9f88742388f2d650ad7f6cf70",
            "2af5f9fe9351421f8eae3f1272b7a9da",
            "5509c550924742f6a0e73d85a0b286e1",
            "6d93b7ce4ecf4c42b89df06a9ddf6479",
            "e3491e6e0ab946e19aed2301c6fada8b",
            "3ad0c1d4110e4520b9c8441e8fe3babb",
            "1cf718e47fc74362af84ab898623d0c4",
            "9f783173c2e34c56ad41327b19ef54e6",
            "87f252b1aa44495d866c4e3962940b78",
            "99ab7978f2094594ac1237c3defbe7a4",
            "7a642428e21a46cbb7f34a5751fbcef6",
            "d084dbacc0054c51bb9a4f5799e75ea0",
            "0931a4fb76c14959833ddd5f9463b51c",
            "a333681a89e34956815a2d7e4f336e7d",
            "197c655fe01f4ea4adce823ad30cdbc5",
            "363bc915c2664871bf0d026be9fb6daf",
            "ebcd58a9af324d5eb01f492e780b21a9",
            "0842201c877c42419a22463902cdd0ba",
            "56d147e0cca14cfa9f892e4af327bd34",
            "e768301caab644eaa32e5bcb3553798a",
            "dcdc90474fcf4dc1a027f8ce9ffb169c",
            "0ca8cc5db8a541b7b26cb57f60d9070d",
            "03fffc49a95d4f138309e6ec47ebc4ee",
            "830477f58cc442889f0daef213088a1d"
          ]
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeefc57837934324a7e1ef4bdcfd9447"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5509c550924742f6a0e73d85a0b286e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a333681a89e34956815a2d7e4f336e7d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('facebook/bart-base')\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = True\n",
        "model.generation_config.temperature = 0.5\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "# model.generation_config.renormalize_logits = True\n",
        "\n",
        "model.config.attention_dropout = 0.1\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "name_model = 'sampling-norep-v3/'\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "2160793f-5100-4075-8d55-b3b4c91ed2fa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXfD8z7vdqC",
        "outputId": "90204fd9-098e-49f2-905d-a0c38187493b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartConfig {\n",
              "  \"_name_or_path\": \"facebook/bart-base\",\n",
              "  \"activation_dropout\": 0.1,\n",
              "  \"activation_function\": \"gelu\",\n",
              "  \"add_bias_logits\": false,\n",
              "  \"add_final_layer_norm\": false,\n",
              "  \"architectures\": [\n",
              "    \"BartModel\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"classif_dropout\": 0.1,\n",
              "  \"classifier_dropout\": 0.0,\n",
              "  \"d_model\": 768,\n",
              "  \"decoder_attention_heads\": 12,\n",
              "  \"decoder_ffn_dim\": 3072,\n",
              "  \"decoder_layerdrop\": 0.0,\n",
              "  \"decoder_layers\": 6,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"dropout\": 0.1,\n",
              "  \"early_stopping\": true,\n",
              "  \"encoder_attention_heads\": 12,\n",
              "  \"encoder_ffn_dim\": 3072,\n",
              "  \"encoder_layerdrop\": 0.0,\n",
              "  \"encoder_layers\": 6,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"id2label\": {\n",
              "    \"0\": \"LABEL_0\",\n",
              "    \"1\": \"LABEL_1\",\n",
              "    \"2\": \"LABEL_2\"\n",
              "  },\n",
              "  \"init_std\": 0.02,\n",
              "  \"is_encoder_decoder\": true,\n",
              "  \"label2id\": {\n",
              "    \"LABEL_0\": 0,\n",
              "    \"LABEL_1\": 1,\n",
              "    \"LABEL_2\": 2\n",
              "  },\n",
              "  \"max_position_embeddings\": 1024,\n",
              "  \"model_type\": \"bart\",\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"normalize_before\": false,\n",
              "  \"normalize_embedding\": true,\n",
              "  \"num_beams\": 4,\n",
              "  \"num_hidden_layers\": 6,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"scale_embedding\": false,\n",
              "  \"task_specific_params\": {\n",
              "    \"summarization\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 128,\n",
              "      \"min_length\": 12,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_cnn\": {\n",
              "      \"length_penalty\": 2.0,\n",
              "      \"max_length\": 142,\n",
              "      \"min_length\": 56,\n",
              "      \"num_beams\": 4\n",
              "    },\n",
              "    \"summarization_xsum\": {\n",
              "      \"length_penalty\": 1.0,\n",
              "      \"max_length\": 62,\n",
              "      \"min_length\": 11,\n",
              "      \"num_beams\": 6\n",
              "    }\n",
              "  },\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.35.2\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50265\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the dataset to inspect the batches\n",
        "for batch in train_dataset.take(100):  # Take the first batch for inspection\n",
        "    print(batch[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CsKTRNhvqCQ",
        "outputId": "38adf07c-d213-4243-e990-5af2b6feae17"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3376,  5969, ...,     1,     1,     1],\n",
            "       [    0, 44891,     7, ...,     1,     1,     1],\n",
            "       [    0,     6,   992, ...,    81, 14307,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 39936, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4195, ..., 28695,     5,     2],\n",
            "       [    0, 13863,    89, ...,     1,     1,     1],\n",
            "       [    0, 46797,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16991,     9, ...,     1,     1,     1],\n",
            "       [    0,  9690, 16894, ...,  5342,  2222,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 26039, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 15243,   484, ...,     1,     1,     1],\n",
            "       [    0, 21119,  4945, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  6680, ...,     1,     1,     1],\n",
            "       [    0,   170,  3608, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 29235, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 16215, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 47380, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1106,   215, ...,     8,  1850,     2],\n",
            "       [    0,  3972, 22016, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,   170,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 47302, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 43043, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 33731,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,  4340, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,  6448, ...,     1,     1,     1],\n",
            "       [    0,   387, 35948, ...,     1,     1,     1],\n",
            "       [    0,  1121,   937, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 39231, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   717, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   846,    12, ...,     1,     1,     1],\n",
            "       [    0, 10105,     9, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    28, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 17629, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 47032, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6373, ...,     1,     1,     1],\n",
            "       [    0, 46874,  2088, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 43123, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,  3854,     9,     2],\n",
            "       [    0,   170,    67, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13360, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 13033, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  5709, ...,   230,     6,     2],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,  8269, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709, 25342, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0,  2522,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0, 3684, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  4528,   426, ...,     1,     1,     1],\n",
            "       [    0,   250,   864, ...,     1,     1,     1],\n",
            "       [    0,   170,  2807, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  9437, ...,     1,     1,     1],\n",
            "       [    0, 40450,  9097, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3084, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46498, ...,     1,     1,     1],\n",
            "       [    2,     0, 17105, ...,     1,     1,     1],\n",
            "       [    2,     0, 46444, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2709,    41, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     5, 14612,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 5320, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9355, ...,     1,     1,     1],\n",
            "       [    0,  5771,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42158, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  6243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 3762,    9, ...,    1,    1,    1],\n",
            "       [   0, 3762,  169, ...,    1,    1,    1],\n",
            "       [   0,  713,   34, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  170, 1455, ...,    1,    1,    1],\n",
            "       [   0,  170,  109, ...,    1,    1,    1],\n",
            "       [   0, 5975,  272, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 42578, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   534, ..., 37357,     5, 23341],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 18377,     5, ...,     1,     1,     1],\n",
            "       [    0, 39936,  1364, ...,     1,     1,     1],\n",
            "       [    0,   133,  4472, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   713,   892, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1966, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 14563, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 30597, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133, 30673, ...,     1,     1,     1],\n",
            "       [    0,   170, 33461, ...,     1,     1,     1],\n",
            "       [    0, 49111,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   243,    16, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 45395, 40955, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42274, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 46692, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   216, ...,     1,     1,     1],\n",
            "       [    0,  9058,  1537, ...,  3854,  6533,     2],\n",
            "       [    0,  2522, 15491, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   510,  8631, ...,     1,     1,     1],\n",
            "       [    0, 45461,  6448, ...,     1,     1,     1],\n",
            "       [    0, 27728,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 46011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 41084, ...,     1,     1,     1],\n",
            "       [    2,     0,   113, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   386, ...,     1,     1,     1],\n",
            "       [    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,   713,  1639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    64, ...,     1,     1,     1],\n",
            "       [    0,   565, 26582, ...,     1,     1,     1],\n",
            "       [    0,  4528,  6448, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1],\n",
            "       [    2,     0,  8532, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0,   133,  2731, ...,   141,  1365,     2],\n",
            "       [    0,  5771,   258, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 14246, ...,     1,     1,     1],\n",
            "       [    0, 35703,     6, ...,     1,     1,     1],\n",
            "       [    0,   170,   492, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 34447, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 48293,  1836, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,  5428, 22098,     2],\n",
            "       [    0,   133,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1524, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       [    2,     0, 47744, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12592, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,    12,   170, ...,     1,     1,     1],\n",
            "       [    0,   713,   173, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 16713, 25376, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,  1779,    89, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 40089, 25373, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   448,  7629, ...,     1,     1,     1],\n",
            "       [    0,  4148,     5, ...,     1,     1,     1],\n",
            "       [    0,  1106,    52, ...,    33,  4163,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 25077, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  2765, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1106,    52, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 45288,    29, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133,   434, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     7,  1807,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  133, ...,    1,    1,    1],\n",
            "       [   2,    0, 9167, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  1197, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   717,  6486, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 33837, 10518, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 18522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 16040, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,     5, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  7939,  1423, ...,  3278,    63,     2],\n",
            "       [    0,   133,   335, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42489, ...,     1,     1,     1],\n",
            "       [    2,     0, 48269, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 28062, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   713,   173, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,  1296,   114,     2],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2847,     6, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11321, 20237, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17245, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 34647, ...,     1,     1,     1],\n",
            "       [    2,     0,  2522, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,    52, ...,     1,     1,     1],\n",
            "       [    0, 21461,    11, ...,     1,     1,     1],\n",
            "       [    0,  2765,  4655, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  4442, ...,     1,     1,     1],\n",
            "       [    0, 45408, 19047, ...,     1,     1,     1],\n",
            "       [    0,   713,  1548, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 6179, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0, 2709, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 38386,    10, ...,     1,     1,     1],\n",
            "       [    0,  4528, 15716, ...,     1,     1,     1],\n",
            "       [    0,   448,    36, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1850, ...,     1,     1,     1],\n",
            "       [    0, 35416,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 14484, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  9344, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3813,    12, ...,     1,     1,     1],\n",
            "       [    0,  1620,     5, ...,     1,     1,     1],\n",
            "       [    0, 44863,  1319, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,    84, ...,     1,     1,     1],\n",
            "       [    0,   133,    78, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36984, ...,     1,     1,     1],\n",
            "       [    2,     0, 20930, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,   819, 21154,     2],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1106,   854, ...,     1,     1,     1],\n",
            "       [    0,  1213,    67, ...,  4091, 48981,     2],\n",
            "       [    0,   170,   694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   102, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 34788, ...,     1,     1,     1],\n",
            "       [    2,     0, 35660, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0, 1121,  171, ...,  347,   12,    2],\n",
            "       [   0, 1121, 1285, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  713,   16, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170, 9637, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 33020, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42713,     6, ...,     1,     1,     1],\n",
            "       [    0,  4771,  3109, ...,     1,     1,     1],\n",
            "       [    0, 44908,  4843, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35716,    87, ...,    11, 37365,     2],\n",
            "       [    0,   133, 39135, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0, 13755, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  5393, ...,     1,     1,     1],\n",
            "       [    0,  1121,  6477, ...,     1,     1,     1],\n",
            "       [    0,   243,    64, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 30872,   724, ...,     1,     1,     1],\n",
            "       [    0, 12444,   857, ...,     1,     1,     1],\n",
            "       [    0,  9690,  4010, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  5448, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1],\n",
            "       [    0, 18377,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,  9157, 16771, ...,     1,     1,     1],\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,  6647, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   102, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43253, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 30770,     6, ...,     1,     1,     1],\n",
            "       [    0,   170, 17013, ...,     1,     1,     1],\n",
            "       [    0,   133,  1850, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  5848, ...,     1,     1,     1],\n",
            "       [    0,   133, 13477, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 23996, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4286, ...,     1,     1,     1],\n",
            "       [    0, 44311,     5, ...,     1,     1,     1],\n",
            "       [    0,   713,   817, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5383, 38416, ...,     1,     1,     1],\n",
            "       [    0,  1779,  3563, ...,     9,   230,     2],\n",
            "       [    0, 13863,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   243, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,  1109, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2765, 24097, ...,     1,     1,     1],\n",
            "       [    0, 23055,  8738, ...,     1,     1,     1],\n",
            "       [    0,  2522,  5694, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 18776, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40103, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 14721,  9179, ...,     1,     1,     1],\n",
            "       [    0,   170,  6053, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   713,    16, ...,     1,     1,     1],\n",
            "       [    0, 13863,  3326, ...,    16,   888,     2],\n",
            "       [    0, 43872,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42124, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[   0,  250, 4819, ...,    1,    1,    1],\n",
            "       [   0, 2709, 4327, ...,    1,    1,    1],\n",
            "       [   0,  713,  173, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   0,  133, 5849, ...,    1,    1,    1],\n",
            "       [   0, 1121,   42, ...,    1,    1,    1],\n",
            "       [   0,  170,  311, ...,    1,    1,    1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 24514, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 42489, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,  2655, 20992,     2],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 43195,  7651, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,  1236,    15,     2],\n",
            "       [    0,  1121,  1524, ..., 45371,    15,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44188, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38416, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0, 13863,    51, ...,     1,     1,     1],\n",
            "       [    0,  3762,  1860, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 40846, ...,     1,     1,     1],\n",
            "       [    0,  1620,    52, ...,     1,     1,     1],\n",
            "       [    0,  5771,   171, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 13360,    12, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 27477, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43780, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0, 39936,   775, ...,    52,    33,     2],\n",
            "       ...,\n",
            "       [    0,  1342,  4458, ...,     1,     1,     1],\n",
            "       [    0,  3908,     5, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ..., 19282,     6,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 36949,    41, ...,     1,     1,     1],\n",
            "       [    0,  4688,   419, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  3972,    42, ...,   775,    36,     2],\n",
            "       [    0,  9344,  1938, ...,     1,     1,     1],\n",
            "       [    0,   170,  7015, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 4688, ...,    1,    1,    1],\n",
            "       [   2,    0,  133, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0, 1694, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  5203, ...,     1,     1,     1],\n",
            "       [    0, 39531,  4400, ...,     1,     1,     1],\n",
            "       [    0, 45297,    15, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   133, 15491, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       [    0,  1620,    10, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 36542, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0,  3908,  2284, ...,   922,  4791,     2],\n",
            "       ...,\n",
            "       [    0, 40025,  3693, ...,     1,     1,     1],\n",
            "       [    0, 30597, 10244, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   133, ...,     1,     1,     1],\n",
            "       [    2,     0, 48313, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   173, ...,     1,     1,     1],\n",
            "       [    0, 40566,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,  1548, ...,     1,     1,     1],\n",
            "       [    0, 23271,     9, ..., 42472, 26070,     2],\n",
            "       [    0, 48454,    12, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41933, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   170,   240, ...,     1,     1,     1],\n",
            "       [    0, 43714,    40, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0,   133,   485, ...,     1,     1,     1],\n",
            "       [    0, 32703,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  9685, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45336, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   486, ...,     1,     1,     1],\n",
            "       [    0,   133, 28894, ...,     1,     1,     1],\n",
            "       [    0, 38386,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   970,    16, ...,  3364,     5,     2],\n",
            "       [    0,   713, 12360, ...,     1,     1,     1],\n",
            "       [    0,  1121,   485, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,  1034, ...,     1,     1,     1],\n",
            "       [    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0, 29182,   215, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 32730,     6, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0, 18377,    14, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,   170,  2883, ...,     1,     1,     1],\n",
            "       [    0,   170,  2639, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,  5848, ...,    36, 13424,     2],\n",
            "       [    0, 44863,    31, ...,     1,     1,     1],\n",
            "       [    0, 48684,   680, ..., 20145,  4007,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 49360,    11, ...,  6068,   600,     2],\n",
            "       [    0, 45942,  6448, ...,     1,     1,     1],\n",
            "       [    0, 30383, 26713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 41542,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1285, ...,     1,     1,     1],\n",
            "       [    0,  9089, 15528, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45356, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 40884, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1213,  1157, ..., 20910,    73,     2],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       [    0,  1779,  1058, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 39972,    52, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 28588, ...,     1,     1,     1],\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   465, ...,     1,     1,     1],\n",
            "       [    0, 42702,     6, ...,     1,     1,     1],\n",
            "       [    0, 20319,  2408, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   800, ...,     1,     1,     1],\n",
            "       [    0,  2709,    55, ...,     1,     1,     1],\n",
            "       [    0, 10653,   428, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 243, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    97, ...,     1,     1,     1],\n",
            "       [    0,  4528, 41885, ...,     1,     1,     1],\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ...,     1,     1,     1],\n",
            "       [    0,   170,  1455, ...,     1,     1,     1],\n",
            "       [    0,  1620,    41, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10127, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   243, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250,  1353, ...,     1,     1,     1],\n",
            "       [    0,   713,  3315, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  1989, ..., 14612, 26070,     2],\n",
            "       [    0,  3972,  1100, ...,     1,     1,     1],\n",
            "       [    0,  5320, 10074, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 44466, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 46101, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44863, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,   133, 32809, ...,     1,     1,     1],\n",
            "       [    0,     6,  3023, ...,  1558, 15421,     2],\n",
            "       ...,\n",
            "       [    0, 10653,   428, ...,     1,     1,     1],\n",
            "       [    0, 20861, 44871, ...,     1,     1,     1],\n",
            "       [    0,   713,   839, ...,     8,    63,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   347, ...,     1,     1,     1],\n",
            "       [    2,     0, 41640, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 48455, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 14693,   352, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,     1,     1,     1],\n",
            "       [    0, 21438,   520, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522, 11909, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,   133, 16681, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 43170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,   936, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  4528,  8369, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 27331,   937, ...,     1,     1,     1],\n",
            "       [    0,  2709,  4327, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 21680, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  6209,    42, ...,     1,     1,     1],\n",
            "       [    0,  2522,   762, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170, 15393, ...,     1,     1,     1],\n",
            "       [    0,  2522,  1421, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0, 20086, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  3972,     5, ...,     1,     1,     1],\n",
            "       [    0,   170, 24934, ...,     1,     1,     1],\n",
            "       [    0,  2522, 39030, ...,  3907,     4,     2],\n",
            "       ...,\n",
            "       [    0,  1121,    10, ...,     1,     1,     1],\n",
            "       [    0,   170,   892, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     6,   549,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   176, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0, 47515, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45566, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   574,  3439, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   163, ...,     1,     1,     1],\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,    43,   396,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 713, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133, 15306, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,   645, ..., 12612,   534,     2],\n",
            "       [    0,  3972,  1306, ...,     1,     1,     1],\n",
            "       [    0, 19847,  1239, ...,  6315, 36173,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,  4993, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,  1121, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 46159, 43141, ...,     1,     1,     1],\n",
            "       [    0, 10777,     5, ...,     1,     1,     1],\n",
            "       [    0,  1121, 14117, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 13863,  1337, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 42200,     6, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 565, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 28084,     7, ...,     1,     1,     1],\n",
            "       [    0,  1121,   144, ...,     1,     1,     1],\n",
            "       [    0, 45875,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 38416,    29, ...,     1,     1,     1],\n",
            "       [    0,  2709,  1246, ...,     1,     1,     1],\n",
            "       [    0,  1121,  5709, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 25382, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 40025, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,  3972, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170,   311, ...,   511, 22772,     2],\n",
            "       [    0,  3762,     9, ...,     1,     1,     1],\n",
            "       [    0,   133,   986, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   170,   172, ...,     1,     1,     1],\n",
            "       [    0,  3972, 33942, ...,   892,  2939,     2],\n",
            "       [    0, 45875,     6, ...,    31,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 1121, ...,    1,    1,    1],\n",
            "       [   2,    0,  250, ...,    1,    1,    1],\n",
            "       [   2,    0,  717, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 15393, ...,    11,   130,     2],\n",
            "       [    0, 20867,  7316, ...,     1,     1,     1],\n",
            "       [    0,   713,  5665, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 11773,     6, ...,     1,     1,     1],\n",
            "       [    0,  9058, 24454, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44426, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771, 10364, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  5771,   209, ...,     1,     1,     1],\n",
            "       [    0,   133,  8611, ...,     1,     1,     1],\n",
            "       [    0,  3972, 19893, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 44908, ...,     1,     1,     1],\n",
            "       [    2,     0, 34002, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 19186, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23803,  2655, ...,     1,     1,     1],\n",
            "       [    0,   133,   538, ...,     1,     1,     1],\n",
            "       [    0, 37666,     6, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 40022,  1253, ...,     1,     1,     1],\n",
            "       [    0,  5771,   144, ...,     1,     1,     1],\n",
            "       [    0,  2765,  2623, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 19163, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 42850,     6, ...,     1,     1,     1],\n",
            "       [    0,  3972,    42, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,  3034, ...,     1,     1,     1],\n",
            "       [    0,  1121,  2171, ...,     1,     1,     1],\n",
            "       [    0,  3972,     5, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 42606, ...,     1,     1,     1],\n",
            "       [    2,     0, 17425, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4688, ...,     1,     1,     1],\n",
            "       [    2,     0,   448, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 39936,   173, ...,     1,     1,     1],\n",
            "       [    0,  1121,   209, ...,     1,     1,     1],\n",
            "       [    0,  3972,   892, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   387,   293, ...,    16,   505,     2],\n",
            "       [    0, 21518,  1537, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 45628, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 20930,    15, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,  2409,   114, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288,  8150, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,    45,   946,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 41895, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,   211, ...,   683,    36,     2],\n",
            "       [    0,   250,   194, ...,     1,     1,     1],\n",
            "       [    0,   170,   311, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  1620,   251, ...,     1,     1,     1],\n",
            "       [    0, 10993,     6, ...,   740,     6,     2],\n",
            "       [    0,  1121,   103, ...,   255,     8,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 28062, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 17312, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 38741, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 35166, 37700, ...,     1,     1,     1],\n",
            "       [    0,   170,  3364, ...,     1,     1,     1],\n",
            "       [    0,   250,   367, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 24989,  7373, ...,     1,     1,     1],\n",
            "       [    0,  9157, 37794, ...,     1,     1,     1],\n",
            "       [    0,   717,  4182, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 10928, ...,     1,     1,     1],\n",
            "       [    2,     0, 15622, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4897, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   387,   293, ...,     1,     1,     1],\n",
            "       [    0, 42395,     6, ...,     1,     1,     1],\n",
            "       [    0,  5771,   419, ...,   468,   321,     2],\n",
            "       ...,\n",
            "       [    0,   530,   495, ...,     1,     1,     1],\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,  2522, 40150, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[   2,    0, 9685, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       ...,\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1],\n",
            "       [   2,    0,  170, ...,    1,    1,    1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   133,  4554, ...,     1,     1,     1],\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 11913, 26739, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 35490,     5, ...,     1,     1,     1],\n",
            "       [    0,   387,  2688, ...,     1,     1,     1],\n",
            "       [    0,   713,  5044, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   338, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,  5771,  1365, ...,     1,     1,     1],\n",
            "       [    0,   133,   211, ...,     1,     1,     1],\n",
            "       [    0,  1121,   171, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 48812,  2577, ...,    14, 20070,     2],\n",
            "       [    0,  5771,   608, ...,     1,     1,     1],\n",
            "       [    0, 46375,     6, ...,    14,     5,     2]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  4030, ...,     1,     1,     1],\n",
            "       [    2,     0, 43597, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   495, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   713, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   170, 10516, ...,     1,     1,     1],\n",
            "       [    0,   713,  1421, ...,   163,  2688,     2],\n",
            "       [    0,  3762,    16, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,   717,  6195, ...,     1,     1,     1],\n",
            "       [    0,   133,  2270, ...,     1,     1,     1],\n",
            "       [    0,  2895,     9, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0, 12444, ...,     1,     1,     1],\n",
            "       [    2,     0, 22011, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0,   250, 17309, ...,     1,     1,     1],\n",
            "       [    0,  1121,  9322, ...,     1,     1,     1],\n",
            "       [    0,  3762,   169, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0, 45288, 15380, ...,     1,     1,     1],\n",
            "       [    0,   133,  7626, ...,     1,     1,     1],\n",
            "       [    0,   170,   304, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       [    2,     0, 26412, ...,     1,     1,     1],\n",
            "       [    2,     0, 46101, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 27331,    10, ...,   204,     6,     2],\n",
            "       [    0,  9690,  1202, ...,     1,     1,     1],\n",
            "       [    0,  7605,   209, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2571,  6018, ...,     1,     1,     1],\n",
            "       [    0,   170,  6581, ...,     1,     1,     1],\n",
            "       [    0,   250, 31809, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 1, 1, 1],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1],\n",
            "       ...,\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 170, ...,   1,   1,   1],\n",
            "       [  2,   0, 250, ...,   1,   1,   1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[    0, 23295, 37465, ...,     1,     1,     1],\n",
            "       [    0,  1121,    42, ...,     1,     1,     1],\n",
            "       [    0,   170,    67, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    0,  2522,   775, ...,     1,     1,     1],\n",
            "       [    0,  1121,  1989, ...,     1,     1,     1],\n",
            "       [    0, 20930,    15, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(8, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(8, 182), dtype=int64, numpy=\n",
            "array([[    2,     0, 36949, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,  1121, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  3762, ...,     1,     1,     1]])>}\n",
            "{'input_ids': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[    0,  1620,    10, ...,     1,     1,     1],\n",
            "       [    0, 40683,   112, ...,     1,     1,     1],\n",
            "       [    0,   170,    40, ..., 13956,  1916,     2],\n",
            "       ...,\n",
            "       [    0, 10462,     6, ...,     1,     1,     1],\n",
            "       [    0, 19192,    52, ...,     1,     1,     1],\n",
            "       [    0, 13863,  1844, ...,     1,     1,     1]])>, 'attention_mask': <tf.Tensor: shape=(7, 1024), dtype=int64, numpy=\n",
            "array([[1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 1, 1, 1],\n",
            "       ...,\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0],\n",
            "       [1, 1, 1, ..., 0, 0, 0]])>, 'decoder_input_ids': <tf.Tensor: shape=(7, 182), dtype=int64, numpy=\n",
            "array([[    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0,  1000, ...,     1,     1,     1],\n",
            "       [    2,     0,   250, ...,     1,     1,     1],\n",
            "       ...,\n",
            "       [    2,     0,   170, ...,     1,     1,     1],\n",
            "       [    2,     0, 45288, ...,     1,     1,     1],\n",
            "       [    2,     0,   170, ...,     1,     1,     1]])>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "96a37571-a85b-46d0-822c-15337ad2a4f0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_model"
      ],
      "metadata": {
        "id": "dyGROt7TwXn6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 10\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "fec0f7cd-d17d-4c56-fb64-c523f8dc25b4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n",
            "81/81 [==============================] - 3557s 44s/step - loss: 3.8539 - val_loss: 3.3430 - rouge1: 38.3698 - rouge2: 10.1688 - rougeL: 22.2593 - rougeLsum: 31.8182 - gen_len: 88.4506\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3040s 38s/step - loss: 3.4917 - val_loss: 3.2807 - rouge1: 40.0500 - rouge2: 10.8711 - rougeL: 22.9560 - rougeLsum: 33.0063 - gen_len: 82.0000\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3153s 39s/step - loss: 3.3288 - val_loss: 3.2417 - rouge1: 39.2427 - rouge2: 10.4373 - rougeL: 22.8725 - rougeLsum: 32.7607 - gen_len: 82.4815\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3104s 39s/step - loss: 3.1818 - val_loss: 3.2276 - rouge1: 40.0325 - rouge2: 11.1820 - rougeL: 23.2123 - rougeLsum: 33.2422 - gen_len: 84.0309\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3291s 41s/step - loss: 3.0478 - val_loss: 3.2148 - rouge1: 40.4019 - rouge2: 10.9217 - rougeL: 23.2547 - rougeLsum: 33.4422 - gen_len: 85.5432\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3194s 40s/step - loss: 2.9362 - val_loss: 3.2262 - rouge1: 39.8779 - rouge2: 10.3569 - rougeL: 22.9050 - rougeLsum: 32.9253 - gen_len: 81.9753\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3124s 39s/step - loss: 2.8306 - val_loss: 3.2327 - rouge1: 40.3397 - rouge2: 10.9453 - rougeL: 23.1704 - rougeLsum: 33.6539 - gen_len: 83.5309\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3146s 39s/step - loss: 2.7277 - val_loss: 3.2288 - rouge1: 39.9447 - rouge2: 10.5412 - rougeL: 22.8769 - rougeLsum: 33.1718 - gen_len: 85.0062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "f09db19d-85fd-44b8-d76e-3ee5168e9405"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c25434ec-c808-43da-b3df-579bfce352b0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "699df823-a6bf-4496-f7a2-78909e82cba1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {'sampling-norep-v0' : 'sampling-norep-v0/',\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              #  'greedy-norep-v3' : 'greedy-norep-v3/',\n",
        "               'sampling-norep-v3' : 'sampling-norep-v3'}\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/BART/model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Tjj2cjL2mE",
        "outputId": "184d644e-2892-4f36-e503-667824846e20"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v0/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2,\n",
            "  \"max_length\": 150,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.2,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v3.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'\n",
        "save_path = save_paths[name]\n",
        "\n",
        "with open(save_path + '/training_history.json', 'r') as file:\n",
        "    loaded_history = json.load(file)\n",
        "\n",
        "H = History()\n",
        "H.history = loaded_history\n",
        "\n",
        "\n",
        "def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()\n",
        "\n",
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/figure.png')"
      ],
      "metadata": {
        "id": "CEUKdl4cdUPx",
        "outputId": "dd6c46b3-440c-4144-9a18-40cbf4278f64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v3'"
      ],
      "metadata": {
        "id": "AQseeydBMTlf"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:5],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = model.generate(tokenized_data['test']['input_ids'][6:8],\n",
        "                                        generation_config=model.generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "DXuVco4oyC_7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42j6DcwmJShR",
        "outputId": "f366e627-2a4f-4cae-9161-273836d3ef9d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet framework for in-and out-of-distribution classification. This paper proposes a variational approach to solve the uncertainty estimation problem in deep neural networks by considering the label-level distribution of image input and output labels. The authors propose a new uncertainty metric for deep neural network classification that is more robust than existing uncertainty measures. This article proposes a novel variational method for solving uncertainty estimation on deep neural nets.',\n",
              " 'An unsupervised method for analyzing the contribution of individual neurons to NMT models. This paper proposes a novel approach to interpret language pairs using neural networks, and proposes a new method for measuring the contributions of each neuron to the model. The authors propose a novel method for learning languages from neural networks that can be used to control the translation performance of language pairs.',\n",
              " 'A deep diagonal-circulant ReLU network that can be decomposed into products of diagonal and circulant matrices This paper proposes to replace the weight matrix of a fully connected layer with a new type of matrix. The authors propose a method for building deep ReLU networks based on a combination of diagonal/circular matrices with low rank approximators in order to improve performance.',\n",
              " 'Explicit cognitive theory or analogy-like computation in neural networks This paper proposes a novel approach to solving complex examples of visual and symbolic representations. The authors propose an approach to the problem of visual analogy by proposing a new model that learns to contrast abstract relational structures with visual representations. This work proposes a method for learning to compare different representations of objects, which can be used to solve complex analogical problems such as visual analogy.',\n",
              " 'A novel concept annotation task for medical time series data. This paper proposes a novel method of predicting and localizing medical concepts by modeling the medical context data as input. The authors propose a novel approach to the problem of identifying medical concepts in medical time-series data, which can be used to predict and localize medical concepts. This article introduces a novel framework for understanding medical concepts using medical context information.']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data['test']['target'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMr_neO6pJNK",
        "outputId": "25d85c44-b7e6-4820-927c-54279cb4c854"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The study proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This study proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The article investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.',\n",
              " 'We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TReOxk1fk1cx",
        "outputId": "eea11b79-2cc4-44e3-9121-a817e0d65fe8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.',\n",
              " 'Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.',\n",
              " 'We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The article proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This article proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.',\n",
              " 'The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "model.generation_config.do_sample = False\n",
        "model.generation_config.temperature = 1\n",
        "model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "model.generation_config.suppress_tokens = [\n",
        "    # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "                                          tokenizer.convert_tokens_to_ids('ƒ†propose'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†proposes'),\n",
        "                                          # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "                                           tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "                                          #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "\n",
        "model.generation_config.num_beam_groups = 4\n",
        "model.generation_config.diversity_penalty = 0.7\n",
        "\n",
        "model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "model.generation_config.repetition_penalty = 1.3\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r4mAa8_-ql",
        "outputId": "a4abf904-55d4-42b6-9cbe-7b5095a3ea61"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"diversity_penalty\": 0.7,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.3,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    15393,\n",
            "    21037,\n",
            "    32687\n",
            "  ]\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('they proposes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EysouFNL26dN",
        "outputId": "decc56fd-e82f-48df-89e2-9b621383fc4d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['they', 'ƒ†proposes']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisXx8nuVDQ4",
        "outputId": "f70b8744-cbe0-4319-fc58-4bc80c0ff137"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: 0/202\n",
            "[INFO]: 10/202\n",
            "[INFO]: 20/202\n",
            "[INFO]: 30/202\n",
            "[INFO]: 40/202\n",
            "[INFO]: 50/202\n",
            "[INFO]: 60/202\n",
            "[INFO]: 70/202\n",
            "[INFO]: 80/202\n",
            "[INFO]: 90/202\n",
            "[INFO]: 100/202\n",
            "[INFO]: 110/202\n",
            "[INFO]: 120/202\n",
            "[INFO]: 130/202\n",
            "[INFO]: 140/202\n",
            "[INFO]: 150/202\n",
            "[INFO]: 160/202\n",
            "[INFO]: 170/202\n",
            "[INFO]: 180/202\n",
            "[INFO]: 190/202\n",
            "[INFO]: 200/202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models[name]\n",
        "model.generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFgiii-sbTK4",
        "outputId": "41684db4-b656-4232-d3b5-51f1c72b8369"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"begin_suppress_tokens\": [\n",
              "    170\n",
              "  ],\n",
              "  \"bos_token_id\": 0,\n",
              "  \"decoder_start_token_id\": 2,\n",
              "  \"diversity_penalty\": 0.5,\n",
              "  \"early_stopping\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"forced_bos_token_id\": 0,\n",
              "  \"forced_eos_token_id\": 2,\n",
              "  \"length_penalty\": 2.0,\n",
              "  \"max_length\": 150,\n",
              "  \"min_length\": 80,\n",
              "  \"no_repeat_ngram_size\": 3,\n",
              "  \"num_beam_groups\": 4,\n",
              "  \"num_beams\": 4,\n",
              "  \"pad_token_id\": 1,\n",
              "  \"repetition_penalty\": 1.8,\n",
              "  \"suppress_tokens\": [\n",
              "    1698,\n",
              "    32687\n",
              "  ]\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bNZCUqSvN7NV",
        "outputId": "bc3472f2-1429-4206-dcf8-b0d504c53db9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/greedy-norep-v5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "beb7244c-9a96-46aa-e5a8-66046cfd17bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       1.420792\n",
              "std       18.729447\n",
              "min      -39.000000\n",
              "25%      -12.000000\n",
              "50%        0.500000\n",
              "75%       14.000000\n",
              "max       53.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 6\n",
        "print('ORIGINAL:' + tokenized_data['test']['target'][i])\n",
        "print('FINE TUNED MODEL:' + tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[i])\n",
        "print('PRETRAINED MODEL:' + tokenizer.batch_decode(pretrained_generated_ids, skip_special_tokens=True)[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzU40R-ALuzq",
        "outputId": "7e6ad700-beac-4a0d-f100-174353d2d63e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL:OE teaches anomaly detectors to learn heuristics for detecting unseen anomalies; experiments are in classification, density estimation, and calibration in NLP and vision settings; we do not tune on test distribution samples, unlike previous work\n",
            "FINE TUNED MODEL:Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. We show that Outlier Exposure can improve calibration performance in this realistic setting.\n",
            "PRETRAINED MODEL:However, when there is a distribution mismatch, deep neural network classifiers tend to give\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = 28\n",
        "print(summaries.iloc[m, 0])\n",
        "print(summaries.iloc[m, 1])"
      ],
      "metadata": {
        "id": "lcWyZXvGLKcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1428815a-8c86-41c0-b4c1-7c39c492acdc"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space.  This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.\n",
            "We extend the reinforcement learning paradigm to a d-dimensional hypercube and show that quantile regression is capable of training orders of magnitudes faster in high dimensional metric spaces. This paper proposes a method to train a deep neural network to approximate the quantile function of the optimal action distribution. The authors propose a new reinforcement learning algorithm to train convolutional neural networks with quantile functions, showing that it can be used to train orders of magnitude faster on vector rewards.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "-_h7Z8KBp09t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small', errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 1024\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "MwmAPTxGp28R",
        "outputId": "53a6f292-38a3-42a4-ba45-28d8bcc6ec67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "dbc0add65ab945569b8847ec3ab6c58c",
            "b446919236c647208707eaaaf768c703",
            "49706c3c871f48db8e33d94097aa6437",
            "f78a4cb866a743a6bb3dd5f7da818e32",
            "600eb04aa9414e0eb8eeb58f7241dd9a",
            "e12007eea4c540aea7d7ab971790bc19",
            "67cd88b9dbd34d6ea18ad767df22f64e",
            "bbe5baa6a13a4eb186753670f0123bbb",
            "44431cb5081a4faf9ddf9575149b3d5f",
            "9b90d8b0ab234566ac9bbc3566008952",
            "338619e648134b6197a2fb02419e786f",
            "cf1197632cfc486a9cd3228d484a0bd8",
            "33da498708ef445e89a3ff23fa13c821",
            "88c38d46c97c4544a620814103470a35",
            "3550089feb9842ca9a5d2909fea20275",
            "50b3b849032d4fc7891b3832384c6c1c",
            "c7ae27ca9a644f4385184d0c375cbaf9",
            "8ed9bd847bb34c79a701a3178ddbd425",
            "b24656e594fc4971b4f4adf7c51c5b20",
            "0747e02447294413b73c4c8db0a7b0c4",
            "387a79a629254ece9ff7663d95a609f8",
            "c1a9184423d74db3be92daabc6cbde97",
            "43ca706d9d0a48e28a7458486d67a3cb",
            "9a1b2a4600de49798b0302ea4ccb49c0",
            "2ab4d0f2f03241829c410252e8cd7048",
            "47fc3920753649d28d5446fceba7feb5",
            "a674198f9eac495abdae39b339ceebdc",
            "e07c4f7df939456ca4ceb51fed27656d",
            "64207229408346f8958056df101e499f",
            "bc133a65258741b9a190d6460040baed",
            "219014837f39433a99bcc3d57eb1239b",
            "955fa99ec0e74f19a70345682dd7382c",
            "b56bec11a8334b6f9b4a1fc2a23d20e0"
          ]
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbc0add65ab945569b8847ec3ab6c58c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf1197632cfc486a9cd3228d484a0bd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43ca706d9d0a48e28a7458486d67a3cb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained('google-t5/t5-small')\n",
        "\n",
        "model.generation_config.max_length = 150\n",
        "model.generation_config.min_length = 60\n",
        "model.generation_config.length_penalty = 2.0\n",
        "model.generation_config.num_beams = 4\n",
        "\n",
        "# model.generation_config.do_sample = True\n",
        "# model.generation_config.temperature = 0.5\n",
        "# model.generation_config.begin_suppress_tokens  = [tokenizer.convert_tokens_to_ids('We')]\n",
        "# model.generation_config.suppress_tokens = [\n",
        "#     # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "#                                           # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "#                                            tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "#                                           #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "# if not model.generation_config.do_sample:\n",
        "#   model.generation_config.num_beam_groups = 4\n",
        "#   model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "# model.generation_config.no_repeat_ngram_size = 3\n",
        "\n",
        "# model.generation_config.repetition_penalty = 1.8\n",
        "\n",
        "name_model = 'v0/'\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "_mad6Hc1ttRX",
        "outputId": "3ab80b26-a4f1-458a-8eab-005a1419fe44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "6XdKgWLNwgRD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "6PGwmRZ2yS78"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yvPZ6vubyYl1",
        "outputId": "3777b4a4-be36-48a9-ef21-f9e137e89709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  16449536  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  35330816  \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  41625344  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60506624 (230.81 MB)\n",
            "Trainable params: 60506624 (230.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + name_model"
      ],
      "metadata": {
        "id": "HceVCymny0eR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "p_wTT5KMyi86"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = False\n",
        "epochs = 1\n",
        "\n",
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "VeSspMbVyp_q",
        "outputId": "8bb3c50e-4784-4023-90d7-b046e26d1edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "81/81 [==============================] - 1515s 19s/step - loss: 4.1625 - val_loss: 2.3214 - rouge1: 15.7087 - rouge2: 3.0173 - rougeL: 9.2573 - rougeLsum: 12.8335 - gen_len: 60.0926\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/spiece.model',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/v0/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNqaRwbOzUVD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8/kQx9D3gahLob2vhL66V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1dd943230c0e4cec83a5cdeed66b0db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae92856842b74843bec347e5e8e8fab4",
              "IPY_MODEL_596d925b1f1f4ef9a05a6b4b581dac1e",
              "IPY_MODEL_dbf20a21c1f543f6a7794983db47e1db",
              "IPY_MODEL_c454b3704a4144fc85d79cba426fc6ac",
              "IPY_MODEL_cc0c7bbd1ab348889254a27ab4eb8381"
            ],
            "layout": "IPY_MODEL_71847c1c39234053a4a2daf9dd17626f"
          }
        },
        "ae92856842b74843bec347e5e8e8fab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13e1b8f15e1c41a4b17ce7eefb9efdef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_709922f14d864c038ea4e03f0d8b6e2a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "596d925b1f1f4ef9a05a6b4b581dac1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_69c20fa745cb4512875421cae0b8a24a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_45d54ce97632402b810dee7c750017eb",
            "value": ""
          }
        },
        "dbf20a21c1f543f6a7794983db47e1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_43c292ffbe694379a7956360553efc1a",
            "style": "IPY_MODEL_ac2081230c2b49b7b5d62ffa2975f49d",
            "value": true
          }
        },
        "c454b3704a4144fc85d79cba426fc6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_2da9fcd1e63b4dae98e7e23a0c4c82d2",
            "style": "IPY_MODEL_75006e2177d246a3b1c0e570f67ab0d3",
            "tooltip": ""
          }
        },
        "cc0c7bbd1ab348889254a27ab4eb8381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddd57005a5944ef991a2d30d0a6c4a02",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_06e3f0ba42884b59bce0815d982ac42a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "71847c1c39234053a4a2daf9dd17626f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "13e1b8f15e1c41a4b17ce7eefb9efdef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709922f14d864c038ea4e03f0d8b6e2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69c20fa745cb4512875421cae0b8a24a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d54ce97632402b810dee7c750017eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c292ffbe694379a7956360553efc1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac2081230c2b49b7b5d62ffa2975f49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2da9fcd1e63b4dae98e7e23a0c4c82d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75006e2177d246a3b1c0e570f67ab0d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ddd57005a5944ef991a2d30d0a6c4a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06e3f0ba42884b59bce0815d982ac42a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeefc57837934324a7e1ef4bdcfd9447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2739d969fde144a7b42389a1aadb0b2c",
              "IPY_MODEL_4c06f600e9b94125b4bf3a81b691fa2a",
              "IPY_MODEL_a1547cd289c34b8ca8cb109f9f1c49df"
            ],
            "layout": "IPY_MODEL_17495c6333df4ecd8d1a1ad3a31cd0c4"
          }
        },
        "2739d969fde144a7b42389a1aadb0b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_939bf63cd3184afd82415fb94c82fe33",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e79aead0c27484b864866b369c94e97",
            "value": "Map:‚Äá100%"
          }
        },
        "4c06f600e9b94125b4bf3a81b691fa2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e23144400444e7bf2e6131ecf02f1a",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ea020348df3411090914db144e6e189",
            "value": 647
          }
        },
        "a1547cd289c34b8ca8cb109f9f1c49df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e49a0f9f88742388f2d650ad7f6cf70",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2af5f9fe9351421f8eae3f1272b7a9da",
            "value": "‚Äá647/647‚Äá[00:04&lt;00:00,‚Äá154.82‚Äáexamples/s]"
          }
        },
        "17495c6333df4ecd8d1a1ad3a31cd0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "939bf63cd3184afd82415fb94c82fe33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e79aead0c27484b864866b369c94e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e23144400444e7bf2e6131ecf02f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea020348df3411090914db144e6e189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e49a0f9f88742388f2d650ad7f6cf70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af5f9fe9351421f8eae3f1272b7a9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5509c550924742f6a0e73d85a0b286e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d93b7ce4ecf4c42b89df06a9ddf6479",
              "IPY_MODEL_e3491e6e0ab946e19aed2301c6fada8b",
              "IPY_MODEL_3ad0c1d4110e4520b9c8441e8fe3babb"
            ],
            "layout": "IPY_MODEL_1cf718e47fc74362af84ab898623d0c4"
          }
        },
        "6d93b7ce4ecf4c42b89df06a9ddf6479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f783173c2e34c56ad41327b19ef54e6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_87f252b1aa44495d866c4e3962940b78",
            "value": "Map:‚Äá100%"
          }
        },
        "e3491e6e0ab946e19aed2301c6fada8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99ab7978f2094594ac1237c3defbe7a4",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a642428e21a46cbb7f34a5751fbcef6",
            "value": 162
          }
        },
        "3ad0c1d4110e4520b9c8441e8fe3babb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d084dbacc0054c51bb9a4f5799e75ea0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0931a4fb76c14959833ddd5f9463b51c",
            "value": "‚Äá162/162‚Äá[00:01&lt;00:00,‚Äá136.31‚Äáexamples/s]"
          }
        },
        "1cf718e47fc74362af84ab898623d0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f783173c2e34c56ad41327b19ef54e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f252b1aa44495d866c4e3962940b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99ab7978f2094594ac1237c3defbe7a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a642428e21a46cbb7f34a5751fbcef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d084dbacc0054c51bb9a4f5799e75ea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0931a4fb76c14959833ddd5f9463b51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a333681a89e34956815a2d7e4f336e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_197c655fe01f4ea4adce823ad30cdbc5",
              "IPY_MODEL_363bc915c2664871bf0d026be9fb6daf",
              "IPY_MODEL_ebcd58a9af324d5eb01f492e780b21a9"
            ],
            "layout": "IPY_MODEL_0842201c877c42419a22463902cdd0ba"
          }
        },
        "197c655fe01f4ea4adce823ad30cdbc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56d147e0cca14cfa9f892e4af327bd34",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e768301caab644eaa32e5bcb3553798a",
            "value": "Map:‚Äá100%"
          }
        },
        "363bc915c2664871bf0d026be9fb6daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcdc90474fcf4dc1a027f8ce9ffb169c",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ca8cc5db8a541b7b26cb57f60d9070d",
            "value": 203
          }
        },
        "ebcd58a9af324d5eb01f492e780b21a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03fffc49a95d4f138309e6ec47ebc4ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_830477f58cc442889f0daef213088a1d",
            "value": "‚Äá203/203‚Äá[00:01&lt;00:00,‚Äá138.12‚Äáexamples/s]"
          }
        },
        "0842201c877c42419a22463902cdd0ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d147e0cca14cfa9f892e4af327bd34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e768301caab644eaa32e5bcb3553798a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcdc90474fcf4dc1a027f8ce9ffb169c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca8cc5db8a541b7b26cb57f60d9070d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03fffc49a95d4f138309e6ec47ebc4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830477f58cc442889f0daef213088a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbc0add65ab945569b8847ec3ab6c58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b446919236c647208707eaaaf768c703",
              "IPY_MODEL_49706c3c871f48db8e33d94097aa6437",
              "IPY_MODEL_f78a4cb866a743a6bb3dd5f7da818e32"
            ],
            "layout": "IPY_MODEL_600eb04aa9414e0eb8eeb58f7241dd9a"
          }
        },
        "b446919236c647208707eaaaf768c703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e12007eea4c540aea7d7ab971790bc19",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_67cd88b9dbd34d6ea18ad767df22f64e",
            "value": "Map:‚Äá100%"
          }
        },
        "49706c3c871f48db8e33d94097aa6437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbe5baa6a13a4eb186753670f0123bbb",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44431cb5081a4faf9ddf9575149b3d5f",
            "value": 647
          }
        },
        "f78a4cb866a743a6bb3dd5f7da818e32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b90d8b0ab234566ac9bbc3566008952",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_338619e648134b6197a2fb02419e786f",
            "value": "‚Äá647/647‚Äá[00:02&lt;00:00,‚Äá216.83‚Äáexamples/s]"
          }
        },
        "600eb04aa9414e0eb8eeb58f7241dd9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e12007eea4c540aea7d7ab971790bc19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67cd88b9dbd34d6ea18ad767df22f64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbe5baa6a13a4eb186753670f0123bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44431cb5081a4faf9ddf9575149b3d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b90d8b0ab234566ac9bbc3566008952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "338619e648134b6197a2fb02419e786f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf1197632cfc486a9cd3228d484a0bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33da498708ef445e89a3ff23fa13c821",
              "IPY_MODEL_88c38d46c97c4544a620814103470a35",
              "IPY_MODEL_3550089feb9842ca9a5d2909fea20275"
            ],
            "layout": "IPY_MODEL_50b3b849032d4fc7891b3832384c6c1c"
          }
        },
        "33da498708ef445e89a3ff23fa13c821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ae27ca9a644f4385184d0c375cbaf9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8ed9bd847bb34c79a701a3178ddbd425",
            "value": "Map:‚Äá100%"
          }
        },
        "88c38d46c97c4544a620814103470a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b24656e594fc4971b4f4adf7c51c5b20",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0747e02447294413b73c4c8db0a7b0c4",
            "value": 162
          }
        },
        "3550089feb9842ca9a5d2909fea20275": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_387a79a629254ece9ff7663d95a609f8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c1a9184423d74db3be92daabc6cbde97",
            "value": "‚Äá162/162‚Äá[00:00&lt;00:00,‚Äá216.03‚Äáexamples/s]"
          }
        },
        "50b3b849032d4fc7891b3832384c6c1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7ae27ca9a644f4385184d0c375cbaf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ed9bd847bb34c79a701a3178ddbd425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b24656e594fc4971b4f4adf7c51c5b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0747e02447294413b73c4c8db0a7b0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "387a79a629254ece9ff7663d95a609f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1a9184423d74db3be92daabc6cbde97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43ca706d9d0a48e28a7458486d67a3cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a1b2a4600de49798b0302ea4ccb49c0",
              "IPY_MODEL_2ab4d0f2f03241829c410252e8cd7048",
              "IPY_MODEL_47fc3920753649d28d5446fceba7feb5"
            ],
            "layout": "IPY_MODEL_a674198f9eac495abdae39b339ceebdc"
          }
        },
        "9a1b2a4600de49798b0302ea4ccb49c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e07c4f7df939456ca4ceb51fed27656d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_64207229408346f8958056df101e499f",
            "value": "Map:‚Äá100%"
          }
        },
        "2ab4d0f2f03241829c410252e8cd7048": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc133a65258741b9a190d6460040baed",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_219014837f39433a99bcc3d57eb1239b",
            "value": 203
          }
        },
        "47fc3920753649d28d5446fceba7feb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_955fa99ec0e74f19a70345682dd7382c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b56bec11a8334b6f9b4a1fc2a23d20e0",
            "value": "‚Äá203/203‚Äá[00:00&lt;00:00,‚Äá207.77‚Äáexamples/s]"
          }
        },
        "a674198f9eac495abdae39b339ceebdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07c4f7df939456ca4ceb51fed27656d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64207229408346f8958056df101e499f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc133a65258741b9a190d6460040baed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "219014837f39433a99bcc3d57eb1239b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "955fa99ec0e74f19a70345682dd7382c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56bec11a8334b6f9b4a1fc2a23d20e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}