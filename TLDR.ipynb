{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/main/TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viFR7xG_ILqB",
        "outputId": "9c1f6fad-f6be-43bc-e780-924520ec2000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0opTsk9TzaZ",
        "outputId": "326ad594-bad0-4cd5-aa5e-1c418e16e372"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<PAPER>\n",
            "  <S sid=\"0\">Cascaded Grammatical Relation Assignment</S>\n",
            "  <ABSTRACT>\n",
            "    <S sid=\"1\" ssid=\"1\">In this paper we discuss cascaded Memory- Based grammatical relations assignment.</S>\n",
            "    <S sid=\"2\" ssid=\"2\">In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).</S>\n",
            "    <S sid=\"3\" ssid=\"3\">In the last stage, we assign grammatical relations to pairs of chunks.</S>\n",
            "    <S sid=\"4\" ssid=\"4\">We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.</S>\n",
            "  </ABSTRACT>\n",
            "  <SECTION title=\"1 Introduction\" number=\"1\">\n",
            "    <S sid=\"5\" ssid=\"1\">When dealing with large amounts of text, finding structure in sentences is often a useful preprocessing step.</S>\n",
            "    <S sid=\"6\" ssid=\"2\">Traditionally, full parsing is used to find structure in sentences.</S>\n",
            "    <S sid=\"7\" ssid=\"3\">However, full parsing is a complex task and often provides us with more information then we need.</S>\n",
            "    <S sid=\"8\" ssid=\"4\">For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred over full parsing.</S>\n",
            "    <S sid=\"9\" ssid=\"5\">For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.</S>\n",
            "    <S sid=\"10\" ssid=\"6\">In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence.</S>\n",
            "    <S sid=\"11\" ssid=\"7\">Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998).</S>\n",
            "    <S sid=\"12\" ssid=\"8\">The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers?</S>\n",
            "    <S sid=\"13\" ssid=\"9\">What is the effect of cascading?</S>\n",
            "    <S sid=\"14\" ssid=\"10\">Will errors at a lower level percolate to higher modules?</S>\n",
            "    <S sid=\"15\" ssid=\"11\">Recently, many people have looked at cascaded and/or shallow parsing and OR assignment.</S>\n",
            "    <S sid=\"16\" ssid=\"12\">Abney (1991) is one of the first who proposed to split up parsing into several cascades.</S>\n",
            "    <S sid=\"17\" ssid=\"13\">He suggests to first find the chunks and then the dependecies between these chunks.</S>\n",
            "    <S sid=\"18\" ssid=\"14\">Crefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions.</S>\n",
            "    <S sid=\"19\" ssid=\"15\">Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree.</S>\n",
            "    <S sid=\"20\" ssid=\"16\">(Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results.</S>\n",
            "    <S sid=\"21\" ssid=\"17\">Argamon et at.</S>\n",
            "    <S sid=\"22\" ssid=\"18\">(1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification.</S>\n",
            "    <S sid=\"23\" ssid=\"19\">However, their subject and object finders are independent of their chunker (i.e. not cascaded).</S>\n",
            "    <S sid=\"24\" ssid=\"20\">Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade.</S>\n",
            "    <S sid=\"25\" ssid=\"21\">Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier.</S>\n",
            "    <S sid=\"26\" ssid=\"22\">We illustrate this claim on the task of finding grammatical relations (e.g. subject, object, locative) to verbs in text.</S>\n",
            "    <S sid=\"27\" ssid=\"23\">The CR assigner uses several sources of information step by step such as several types of XP chunks (NP, VP, PP, ADJP and ADVP), and adverbial functions assigned to these chunks (e.g. temporal, local).</S>\n",
            "    <S sid=\"28\" ssid=\"24\">Since not all of these entities are predicted reliably, it is the question whether each source leads to an improvement of the overall GR assignment.</S>\n",
            "    <S sid=\"29\" ssid=\"25\">In the rest of this paper we will first briefly describe Memory-Based Learning in Section 2.</S>\n",
            "    <S sid=\"30\" ssid=\"26\">In Section 3.1, we discuss the chunking classifiers that we later use as steps in the cascade.</S>\n",
            "    <S sid=\"31\" ssid=\"27\">Section 3.2 describes the basic GR classifier.</S>\n",
            "    <S sid=\"32\" ssid=\"28\">Section 3.3 presents the architecture and results of the cascaded GR assignment experiments.</S>\n",
            "    <S sid=\"33\" ssid=\"29\">We discuss the results in Section 4 and conclude with Section 5.</S>\n",
            "  </SECTION>\n",
            "  <SECTION title=\"2 Memory-Based Learning\" number=\"2\">\n",
            "    <S sid=\"34\" ssid=\"1\">Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory.</S>\n",
            "    <S sid=\"35\" ssid=\"2\">In recent work Daelemans et at.</S>\n",
            "    <S sid=\"36\" ssid=\"3\">(1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also &amp;quot;remembers&amp;quot; exceptional, low-frequency cases which are useful to extrapolate from.</S>\n",
            "    <S sid=\"37\" ssid=\"4\">Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997).</S>\n",
            "    <S sid=\"38\" ssid=\"5\">We have used the following MBL algorithms': test item and each memory item is defined as the number of features for which they have a different value (overlap metric).</S>\n",
            "    <S sid=\"39\" ssid=\"6\">IB1-IG : IB1 with information gain (an information-theoretic notion measuring the reduction of uncertainty about the class to be predicted when knowing the value of a feature) to weight the cost of a feature value mismatch during comparison.</S>\n",
            "    <S sid=\"40\" ssid=\"7\">IGTree : In this variant, a decision tree is created with features as tests, and ordered according to the information gain of the features, as a heuristic approximation of the computationally more expensive IB1 variants.</S>\n",
            "    <S sid=\"41\" ssid=\"8\">For more references and information about these algorithms we refer to (Daelemans et al., 1998; Daelemans et al., 1999b).</S>\n",
            "    <S sid=\"42\" ssid=\"9\">For other memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998).</S>\n",
            "  </SECTION>\n",
            "  <SECTION title=\"3 Methods and Results\" number=\"3\">\n",
            "    <S sid=\"43\" ssid=\"1\">In this section we describe the stages of the cascade.</S>\n",
            "    <S sid=\"44\" ssid=\"2\">The very first stage consists of a MemoryBased Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996).</S>\n",
            "    <S sid=\"45\" ssid=\"3\">The next three stages involve determining boundaries and labels of chunks.</S>\n",
            "    <S sid=\"46\" ssid=\"4\">Chunks are nonrecursive, non-overlapping constituent parts of sentences (see (Abney, 1991)).</S>\n",
            "    <S sid=\"47\" ssid=\"5\">First, we simultaneously chunk sentences into: NP-, VP: Prep-, ADJP- and APVP-chunks.</S>\n",
            "    <S sid=\"48\" ssid=\"6\">As these chunks are non-overlapping, no words can belong to more than one chunk, and thus no conflicts can arise.</S>\n",
            "    <S sid=\"49\" ssid=\"7\">Prep-chunks are the prepositional part of PPs, thus excluding the nominal part.</S>\n",
            "    <S sid=\"50\" ssid=\"8\">Then we join a Prep-chunk and one — or more coordinated — NP-chunks into a PPchunk.</S>\n",
            "    <S sid=\"51\" ssid=\"9\">Finally, we assign adverbial function (ADVFUNC) labels (e.g. locative or temporal) to all chunks.</S>\n",
            "    <S sid=\"52\" ssid=\"10\">In the last stage of the cascade, we label several types of grammatical relations between pairs of words in the sentence.</S>\n",
            "    <S sid=\"53\" ssid=\"11\">The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal (WSJ) corpus (Marcus et al., 1993).</S>\n",
            "    <S sid=\"54\" ssid=\"12\">For all experiments, we used sections 00-19 as training material and 20-24 as test material.</S>\n",
            "    <S sid=\"55\" ssid=\"13\">See Section 4 for results on other train/test set splittings.</S>\n",
            "    <S sid=\"56\" ssid=\"14\">For evaluation of our results we use the precision and recall measures.</S>\n",
            "    <S sid=\"57\" ssid=\"15\">Precision is the percentage of predicted chunks/relations that are actually correct, recall is the percentage of correct chunks/relations that are actually found.</S>\n",
            "    <S sid=\"58\" ssid=\"16\">For convenient comparisons of only one value, we also list the Fo---1 value (C.J.van Rijsbergen, 1979).</S>\n",
            "    <S sid=\"59\" ssid=\"17\">(i32+1)pree.rec with /3 1 In the first experiment described in this section, the task is to segment the sentence into chunks and to assign labels to these chunks.</S>\n",
            "    <S sid=\"60\" ssid=\"18\">This process of chunking and labeling is carried out by assigning a tag to each word in a sentence leftto-right.</S>\n",
            "    <S sid=\"61\" ssid=\"19\">Ramshaw and Marcus (1995) first assigned a chunk tag to each word in the sentence: I for inside a chunk, 0 for outside a chunk, and B for inside a chunk, but the preceding word is in another chunk.</S>\n",
            "    <S sid=\"62\" ssid=\"20\">As we want to find more than one kind of chunk, we have to further differentiate the JOB tags as to which kind of chunk (NP, VP, Prep, ADJP or ADVP) the word is in.</S>\n",
            "    <S sid=\"63\" ssid=\"21\">With the extended JOB tag set at hand we can tag the sentence: After having found Prep-, NP- and other chunks, we collapse Preps and NPs to PPs in a second step.</S>\n",
            "    <S sid=\"64\" ssid=\"22\">While the GR assigner finds relations between VPs and other chunks (cf.</S>\n",
            "    <S sid=\"65\" ssid=\"23\">Section 3.2), the PP chunker finds relations between prepositions and NPs 2 in a way similar to OR. assignment (see Section 3.2).</S>\n",
            "    <S sid=\"66\" ssid=\"24\">In the last chunking/labeling step, we assign adverbial functions to chunks.</S>\n",
            "    <S sid=\"67\" ssid=\"25\">The classes are the adverbial function labels from the treebank: LOC (locative), TMP (temporal), DIR.</S>\n",
            "    <S sid=\"68\" ssid=\"26\">(directional), PRP (purpose and reason), MNR (manner), EXT (extension) or &amp;quot;2 for none of the former.</S>\n",
            "    <S sid=\"69\" ssid=\"27\">Table 1 gives an overview of the results of the chunking-labeling experiments, using the following algorithms, determined by validation on the train set: IBI-IG for XP-chunking and IGTree for PP-chunking and ADVFUNCs assignment.</S>\n",
            "    <S sid=\"70\" ssid=\"28\">In grammatical relation assignment we assign a GR to pairs of words in a sentence.</S>\n",
            "    <S sid=\"71\" ssid=\"29\">In our 2PPs containing anything else than NPs (e.g.</S>\n",
            "    <S sid=\"72\" ssid=\"30\">'without bringing his wife) are not searched for. ments.</S>\n",
            "    <S sid=\"73\" ssid=\"31\">NP-,VP-, ADJP-, ADVP- and Prepchunks are found simultaneously, but for convenience, precision and recall values are given separately for each type of chunk. experiments, one of these words is always a verb, since this yields the most important GRs.</S>\n",
            "    <S sid=\"74\" ssid=\"32\">The other word is the head of the phrase which is annotated with this grammatical relation in the treebank.</S>\n",
            "    <S sid=\"75\" ssid=\"33\">A preposition is the head of a PP, a noun of an NP and so on.</S>\n",
            "    <S sid=\"76\" ssid=\"34\">Defining relations to hold between heads means that the algorithm can, for example, find a subject relation between a noun and a verb without necessarily having to make decisions about the precise boundaries of the subject NP.</S>\n",
            "    <S sid=\"77\" ssid=\"35\">Suppose we had the POS-tagged sentence shown in Figure 1 and we wanted the algorithm to decide whether, and if so how, Miller (henceforth: the focus) is related to the first verb organized.</S>\n",
            "    <S sid=\"78\" ssid=\"36\">We then construct an instance for this pair of words by extracting a set of feature values from the sentence.</S>\n",
            "    <S sid=\"79\" ssid=\"37\">The instance contains information about the verb and the focus: a feature for the word form and a feature for the POS of both.</S>\n",
            "    <S sid=\"80\" ssid=\"38\">It also has similar features for the local context of the focus.</S>\n",
            "    <S sid=\"81\" ssid=\"39\">Experiments on the training data suggest an optimal context width of two elements to the left and one to the right.</S>\n",
            "    <S sid=\"82\" ssid=\"40\">In the present case, elements are words or punctuation signs.</S>\n",
            "    <S sid=\"83\" ssid=\"41\">In addition to the lexical and the local context information, we include superficial information about clause structure: The first feature indicates the distance from the verb to the focus, counted in elements.</S>\n",
            "    <S sid=\"84\" ssid=\"42\">A negative distance means that the focus is to the left of the verb.</S>\n",
            "    <S sid=\"85\" ssid=\"43\">The second feature contains the number of other verbs between the verb and the focus.</S>\n",
            "    <S sid=\"86\" ssid=\"44\">The third feature is the number of intervening commas.</S>\n",
            "    <S sid=\"87\" ssid=\"45\">The features were chosen by manual 6-7, 8-9 and 12-13 describe the context words, Features 10-11 the focus word.</S>\n",
            "    <S sid=\"88\" ssid=\"46\">Empty contexts are indicated by the value &amp;quot;-&amp;quot; for all features.</S>\n",
            "    <S sid=\"89\" ssid=\"47\">&amp;quot;feature engineering&amp;quot;.</S>\n",
            "    <S sid=\"90\" ssid=\"48\">Table 2 shows the complete instance for Miller-organized in row 5, together with the other first four instances for the sentence.</S>\n",
            "    <S sid=\"91\" ssid=\"49\">The class is mostly &amp;quot;-&amp;quot;, to indicate that the word does not have a direct grammatical relation to organized.</S>\n",
            "    <S sid=\"92\" ssid=\"50\">Other possible classes are those from a list of more than 100 different labels found in the treebank.</S>\n",
            "    <S sid=\"93\" ssid=\"51\">These are combinations of a syntactic category and zero, one or more functions, e.g.</S>\n",
            "    <S sid=\"94\" ssid=\"52\">NP-SBJ for subject, NP-PRD for predicative object, NP for (in)direct object3, PP-LOC for locative PP adjunct, PP-LOC-CLR for subcategorised locative PP, etcetera.</S>\n",
            "    <S sid=\"95\" ssid=\"53\">According to their information gain values, features are ordered with decreasing importance as follows: 11,13, 10, 1, 2, 8, 12, 9, 6 , 4 , 7 , 3 , 5.</S>\n",
            "    <S sid=\"96\" ssid=\"54\">Intuitively,. this ordering makes sense.</S>\n",
            "    <S sid=\"97\" ssid=\"55\">The most important feature is the POS of the focus, because this determines whether it can have a GR to a verb at all (fninctuation cannot) and what kind of relation is possible.</S>\n",
            "    <S sid=\"98\" ssid=\"56\">The POS of the following word is important, because e.g. a noun followed by a noun is probably not the head of an NP and will therefore not have a direct GR to the verb.</S>\n",
            "    <S sid=\"99\" ssid=\"57\">The word itself may be important if it is e.g. a preposition, a pronoun or a clearly temporal/local adverb.</S>\n",
            "    <S sid=\"100\" ssid=\"58\">Features 1 and 2 give some indication of the complexity of the structure intervening between the focus and the verb.</S>\n",
            "    <S sid=\"101\" ssid=\"59\">The more complex this structure, the lower the probability that the focus and the verb are related.</S>\n",
            "    <S sid=\"102\" ssid=\"60\">Context further away is less important than near context.</S>\n",
            "    <S sid=\"103\" ssid=\"61\">To test the effects of the chunking steps from Section 3.1 on this task, we will now construct instances based on more structured input text, like that in Figure 2.</S>\n",
            "    <S sid=\"104\" ssid=\"62\">This time, the focus is described by five features instead of two, for the additional information: which type of chunk it is in, what, the preposition is if it is in a PP chunk, and what the adverbial function is, if any.</S>\n",
            "    <S sid=\"105\" ssid=\"63\">We still have a context of two elements left, one right, but elements are now defined to be either chunks, or words outside any chunk, or punctuation.</S>\n",
            "    <S sid=\"106\" ssid=\"64\">Each chunk in the context is represented by its last word (which is the semantically most important word in most cases), by the POS of the last word, and by the type of chunk.</S>\n",
            "    <S sid=\"107\" ssid=\"65\">The distance feature is adapted to the new definition of element, too, and instead of counting intervening verbs, we now count intervening VP chunks.</S>\n",
            "    <S sid=\"108\" ssid=\"66\">Figure 3 shows the first five instances for the sentence in Figure 2.</S>\n",
            "    <S sid=\"109\" ssid=\"67\">Class value&amp;quot; &amp;quot; again means the focus is not directly related to the verb&amp;quot; (but to some other verb or a non-verbal element).</S>\n",
            "    <S sid=\"110\" ssid=\"68\">According to their information gain values, features are ordered in decreasing importance as follows: 16, 15, 12, 14, 11, 2, 1, 19, 10, 9, 13, 18, 6, 17, 8, 4, 7, 3, 5.</S>\n",
            "    <S sid=\"111\" ssid=\"69\">Comparing this to the earlier feature ordering, we see that most of the new features are distance and intervening VPs and commas.</S>\n",
            "    <S sid=\"112\" ssid=\"70\">Features 4 and 5 show the verb and its POS.</S>\n",
            "    <S sid=\"113\" ssid=\"71\">Features 6-8, 9-11 and 17-19 describe the context words/chunks, Features 12-16 the focus chunk.</S>\n",
            "    <S sid=\"114\" ssid=\"72\">Empty contexts are indicated by the &amp;quot;-&amp;quot; for all features. very important, thereby justifying their introduction.</S>\n",
            "    <S sid=\"115\" ssid=\"73\">Relative to the other &amp;quot;old&amp;quot; features, the structural features 1 and 2 have gained importance, probably because more structure is available in the input to represent.</S>\n",
            "    <S sid=\"116\" ssid=\"74\">In principle, we would have to construct one instance for each possible pair of a verb and a focus word in the sentence.</S>\n",
            "    <S sid=\"117\" ssid=\"75\">However, we restrict instances to those where there is at most one other verb/VP chunk between the verb and the focus, in case the focus precedes the verb, and no other verb in case the verb precedes the focus.</S>\n",
            "    <S sid=\"118\" ssid=\"76\">This restriction allows, for example, for a relative clause on the subject (as in our example sentence).</S>\n",
            "    <S sid=\"119\" ssid=\"77\">In the training data, 97.9% of the related pairs fulfill this condition (when counting VP chunks).</S>\n",
            "    <S sid=\"120\" ssid=\"78\">Experiments on the training data showed that increasing the admitted number of intervening VP chunks slightly increases recall, at the cost of precision.</S>\n",
            "    <S sid=\"121\" ssid=\"79\">Having constructed all instances from the test data and from a training set with the same level of partial structure, we first train the IGTree algorithm, and then let it classify the test instances.</S>\n",
            "    <S sid=\"122\" ssid=\"80\">Then, for each test instance that was classified with a grammatical relation, we check whether the same verb-focuspair appears with the same relation in the GR list extracted directly from the treebank.</S>\n",
            "    <S sid=\"123\" ssid=\"81\">This gives us the precision of the classifier.</S>\n",
            "    <S sid=\"124\" ssid=\"82\">Checking the treebank list versus the classified list yields We have already seen from the example that the level of structure in the input text can influence the composition of the instances.</S>\n",
            "    <S sid=\"125\" ssid=\"83\">We are interested in the effects of different sorts of partial structure in the input data on the classification performance of the final classifier.</S>\n",
            "    <S sid=\"126\" ssid=\"84\">Therefore, we ran a series of experiments.</S>\n",
            "    <S sid=\"127\" ssid=\"85\">The classification task was always that of finding grammatical relations to verbs and performance was always measured by precision and recall on those relations (the test set contained 45825 relations).</S>\n",
            "    <S sid=\"128\" ssid=\"86\">The amount of structure in the input data varied.</S>\n",
            "    <S sid=\"129\" ssid=\"87\">Table 4 shows the results of the experiments.</S>\n",
            "    <S sid=\"130\" ssid=\"88\">In the first experiment, only POS tagged input is used.</S>\n",
            "    <S sid=\"131\" ssid=\"89\">Then, NP chunks are added.</S>\n",
            "    <S sid=\"132\" ssid=\"90\">Other sorts of chunks are inserted at each subsequent step.</S>\n",
            "    <S sid=\"133\" ssid=\"91\">Finally, the adverbial function labels are added.</S>\n",
            "    <S sid=\"134\" ssid=\"92\">We can see that the more structure we add, the better precision and recall of the grammatical relations get: precision increases from 60.7% to 74.8%, recall from 41.3% to 67.9%.</S>\n",
            "    <S sid=\"135\" ssid=\"93\">This in spite of the fact that the added information is not always correct, because it was predicted for the test material on the basis of the training material by the classifiers described in Section 3.1.</S>\n",
            "    <S sid=\"136\" ssid=\"94\">As we have seen in Table 1, especially ADJP and ADVP chunks and adverbial function labels did not have very high precision and recall.</S>\n",
            "  </SECTION>\n",
            "  <SECTION title=\"4 Discussion\" number=\"4\">\n",
            "    <S sid=\"137\" ssid=\"1\">There are three ways how two cascaded modules can interact.</S>\n",
            "    <S sid=\"138\" ssid=\"2\">• The first module can add information on which the later module can (partially) base its decisions.</S>\n",
            "    <S sid=\"139\" ssid=\"3\">This is the case between the adverbial functions finder and the relations finder.</S>\n",
            "    <S sid=\"140\" ssid=\"4\">The former adds an extra informative feature to the instances of the latter (Feature 16 in Table 3).</S>\n",
            "    <S sid=\"141\" ssid=\"5\">Cf. column two of Table 4.</S>\n",
            "    <S sid=\"142\" ssid=\"6\">• The first module can restrict the number of decisions to be made by the second one.</S>\n",
            "    <S sid=\"143\" ssid=\"7\">This is the case in the combination of the chunking steps and the relations finder.</S>\n",
            "    <S sid=\"144\" ssid=\"8\">Without the chunker, the relations finder would have to decide for every word, whether it is the head of a constituent that bears a relation to the verb.</S>\n",
            "    <S sid=\"145\" ssid=\"9\">With the churlker., the relations finder has to make this decision for fewer words, namely only for those which are the last word in a chunk resp. the preposition of a PP chunk.</S>\n",
            "    <S sid=\"146\" ssid=\"10\">Practically, this reduction of the number of decisions (which translates into a reduction of instances) as can be seen in the third column of Table 4.</S>\n",
            "    <S sid=\"147\" ssid=\"11\">• The first module can reduce the number of elements used for the instances by counting one chunk as just one context element.</S>\n",
            "    <S sid=\"148\" ssid=\"12\">We can see the effect in the feature that indicates the distance in elements between the focus and the verb.</S>\n",
            "    <S sid=\"149\" ssid=\"13\">The more chunks are used, the smaller the average absolute distance (see column four Table 4).</S>\n",
            "    <S sid=\"150\" ssid=\"14\">All three effects interact in the cascade we describe.</S>\n",
            "    <S sid=\"151\" ssid=\"15\">The PP chunker reduces the number of decisions for the relations finder (instead of one instance for the preposition and one for the NP chunk, we get only one instance for the PP chunk-). introduces an extra feature (Feature 12 in Table 3), and changes the context (instead of a preposition and an NP, context may now be one PP).</S>\n",
            "    <S sid=\"152\" ssid=\"16\">As we already noted above, precision and recall are monotonically increasing when adding more structure.</S>\n",
            "    <S sid=\"153\" ssid=\"17\">However, we note large differences, such as NP chunks which increase Fs_i by more than 10%, and VP chunks which add another 6.8%, whereas ADVPs and ADJPs yield hardly any improvement.</S>\n",
            "    <S sid=\"154\" ssid=\"18\">This may partially be explained by the fact that these chunks are less frequent than the former two.</S>\n",
            "    <S sid=\"155\" ssid=\"19\">Preps, on the other hand, while hardly reducing the average distance or the number of instances, improve 1,3-1 by nearly 1%.</S>\n",
            "    <S sid=\"156\" ssid=\"20\">PPs yield another 1.1%.</S>\n",
            "    <S sid=\"157\" ssid=\"21\">What may come as a surprise is that adverbial functions again increase F,3,1 by nearly 2%, despite the fact that F1 for this ADVFTJNC assignment step was not very high.</S>\n",
            "    <S sid=\"158\" ssid=\"22\">This result shows that cascaded modules need not be perfect to be useful.</S>\n",
            "    <S sid=\"159\" ssid=\"23\">Up to now, we only looked at the overall results.</S>\n",
            "    <S sid=\"160\" ssid=\"24\">Table 4 also shows individual Fp_1 values for four selected common grammatical relations: subject NP, (in)direct object NP, locative PP adjunct and temporal PP adjunct.</S>\n",
            "    <S sid=\"161\" ssid=\"25\">Note that the steps have different effects on the different relations: Adding NPs increases Fp=i by 11.3% for subjects resp.</S>\n",
            "    <S sid=\"162\" ssid=\"26\">16.2% for objects, but only 3.9% resp.</S>\n",
            "    <S sid=\"163\" ssid=\"27\">3.7% for locatives and temporals.</S>\n",
            "    <S sid=\"164\" ssid=\"28\">Adverbial functions are more important for the two adjuncts (+6.3% resp.</S>\n",
            "    <S sid=\"165\" ssid=\"29\">+15%) than for the two complements (+0.2% resp.</S>\n",
            "    <S sid=\"166\" ssid=\"30\">+0.7%).</S>\n",
            "    <S sid=\"167\" ssid=\"31\">Argamon et al. (1998) report F13=1 for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper.</S>\n",
            "    <S sid=\"168\" ssid=\"32\">Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier.</S>\n",
            "    <S sid=\"169\" ssid=\"33\">For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a).</S>\n",
            "    <S sid=\"170\" ssid=\"34\">That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%.</S>\n",
            "    <S sid=\"171\" ssid=\"35\">To study the effect of the errors in the lower modules other than the tagger, we used &amp;quot;perfect&amp;quot; test data in a last experiment, i.e. data annotated with partial information taken directly from the treebank.</S>\n",
            "    <S sid=\"172\" ssid=\"36\">The results are shown in Table 5.</S>\n",
            "    <S sid=\"173\" ssid=\"37\">We see that later modules suffer from errors of earlier modules (as could be expected): Fp_1 of PP chunking is 92% but could have previous modules in the cascade) vs. on &amp;quot;perfect&amp;quot; input (enriched with partial treebank annotation).</S>\n",
            "    <S sid=\"174\" ssid=\"38\">For PPs, this means perfect POS tags and chunk labels/boundaries, for ADVFUNC additionally perfect PP chunks, for GR assignment also perfect ADVFUNC labels. been 97.9% if all previous chunks would have been correct (+5.9%).</S>\n",
            "    <S sid=\"175\" ssid=\"39\">For adverbial functions, the difference is 3.5%.</S>\n",
            "    <S sid=\"176\" ssid=\"40\">For grammatical relation assignment, the last module in the cascade, the difference is, not surprisingly, the largest: 7.9% for chunks only, 12.3% for chunks and ADVFUNCs.</S>\n",
            "    <S sid=\"177\" ssid=\"41\">The latter percentage shows what could maximally be gained by further improving the chunker and ADVFUNCs finder.</S>\n",
            "    <S sid=\"178\" ssid=\"42\">On realistic data, a realistic ADVFUNCs finder improves CR assigment by 1.9%.</S>\n",
            "    <S sid=\"179\" ssid=\"43\">On perfect data, a perfect ADVFUNCs finder increases performance by 6.3%.</S>\n",
            "  </SECTION>\n",
            "  <SECTION title=\"5 Conclusion and Future Research\" number=\"5\">\n",
            "    <S sid=\"180\" ssid=\"1\">In this paper we studied cascaded grammatical relations assignment.</S>\n",
            "    <S sid=\"181\" ssid=\"2\">We showed that even the use of imperfect modules improves the overall result of the cascade.</S>\n",
            "    <S sid=\"182\" ssid=\"3\">In future research we plan to also train our classifiers on imperfectly chunked material.</S>\n",
            "    <S sid=\"183\" ssid=\"4\">This enables the classifier to better cope with systematic errors in train and test material.</S>\n",
            "    <S sid=\"184\" ssid=\"5\">We expect that especially an improvement of the adverbial function assignment will lead to better OR assignment.</S>\n",
            "    <S sid=\"185\" ssid=\"6\">Finally, since cascading proved effective for GR. assignment we intend to study the effect of cascading different types of XP chunkers on chunking performance.</S>\n",
            "    <S sid=\"186\" ssid=\"7\">We might e.g. first find ADJP chunks, then use that chunker's output as additional input for the NP chunker, then use the combined output as input to the VP chunker and so on.</S>\n",
            "    <S sid=\"187\" ssid=\"8\">Other chunker orderings are possible, too.</S>\n",
            "    <S sid=\"188\" ssid=\"9\">Likewise, it might be better to find different grammatical relations subsequently, instead of simultaneously.</S>\n",
            "  </SECTION>\n",
            "</PAPER>\n",
            "{\n",
            "  \"PAPER\": {\n",
            "    \"S\": {\n",
            "      \"@sid\": \"0\",\n",
            "      \"#text\": \"Cascaded Grammatical Relation Assignment\"\n",
            "    },\n",
            "    \"ABSTRACT\": {\n",
            "      \"S\": [\n",
            "        {\n",
            "          \"@sid\": \"1\",\n",
            "          \"@ssid\": \"1\",\n",
            "          \"#text\": \"In this paper we discuss cascaded Memory- Based grammatical relations assignment.\"\n",
            "        },\n",
            "        {\n",
            "          \"@sid\": \"2\",\n",
            "          \"@ssid\": \"2\",\n",
            "          \"#text\": \"In the first stages of the cascade, we find chunks of several types (NP,VP,ADJP,ADVP,PP) and label them with their adverbial function (e.g. local, temporal).\"\n",
            "        },\n",
            "        {\n",
            "          \"@sid\": \"3\",\n",
            "          \"@ssid\": \"3\",\n",
            "          \"#text\": \"In the last stage, we assign grammatical relations to pairs of chunks.\"\n",
            "        },\n",
            "        {\n",
            "          \"@sid\": \"4\",\n",
            "          \"@ssid\": \"4\",\n",
            "          \"#text\": \"We studied the effect of adding several levels to this cascaded classifier and we found that even the less performing chunkers enhanced the performance of the relation finder.\"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    \"SECTION\": [\n",
            "      {\n",
            "        \"@title\": \"1 Introduction\",\n",
            "        \"@number\": \"1\",\n",
            "        \"S\": [\n",
            "          {\n",
            "            \"@sid\": \"5\",\n",
            "            \"@ssid\": \"1\",\n",
            "            \"#text\": \"When dealing with large amounts of text, finding structure in sentences is often a useful preprocessing step.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"6\",\n",
            "            \"@ssid\": \"2\",\n",
            "            \"#text\": \"Traditionally, full parsing is used to find structure in sentences.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"7\",\n",
            "            \"@ssid\": \"3\",\n",
            "            \"#text\": \"However, full parsing is a complex task and often provides us with more information then we need.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"8\",\n",
            "            \"@ssid\": \"4\",\n",
            "            \"#text\": \"For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred over full parsing.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"9\",\n",
            "            \"@ssid\": \"5\",\n",
            "            \"#text\": \"For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"10\",\n",
            "            \"@ssid\": \"6\",\n",
            "            \"#text\": \"In this paper we discuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"11\",\n",
            "            \"@ssid\": \"7\",\n",
            "            \"#text\": \"Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"12\",\n",
            "            \"@ssid\": \"8\",\n",
            "            \"#text\": \"The questions we will answer in this paper are: Can we reuse these modules in a cascade of classifiers?\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"13\",\n",
            "            \"@ssid\": \"9\",\n",
            "            \"#text\": \"What is the effect of cascading?\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"14\",\n",
            "            \"@ssid\": \"10\",\n",
            "            \"#text\": \"Will errors at a lower level percolate to higher modules?\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"15\",\n",
            "            \"@ssid\": \"11\",\n",
            "            \"#text\": \"Recently, many people have looked at cascaded and/or shallow parsing and OR assignment.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"16\",\n",
            "            \"@ssid\": \"12\",\n",
            "            \"#text\": \"Abney (1991) is one of the first who proposed to split up parsing into several cascades.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"17\",\n",
            "            \"@ssid\": \"13\",\n",
            "            \"#text\": \"He suggests to first find the chunks and then the dependecies between these chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"18\",\n",
            "            \"@ssid\": \"14\",\n",
            "            \"#text\": \"Crefenstette (1996) describes a cascade of finite-state transducers, which first finds noun and verb groups, then their heads, and finally syntactic functions.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"19\",\n",
            "            \"@ssid\": \"15\",\n",
            "            \"#text\": \"Brants and Skut (1998) describe a partially automated annotation tool which constructs a complete parse of a sentence by recursively adding levels to the tree.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"20\",\n",
            "            \"@ssid\": \"16\",\n",
            "            \"#text\": \"(Collins, 1997; Ratnaparkhi, 1997) use cascaded processing for full parsing with good results.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"21\",\n",
            "            \"@ssid\": \"17\",\n",
            "            \"#text\": \"Argamon et at.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"22\",\n",
            "            \"@ssid\": \"18\",\n",
            "            \"#text\": \"(1998) applied Memory-Based Sequence Learning (MBSL) to NP chunking and subject/object identification.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"23\",\n",
            "            \"@ssid\": \"19\",\n",
            "            \"#text\": \"However, their subject and object finders are independent of their chunker (i.e. not cascaded).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"24\",\n",
            "            \"@ssid\": \"20\",\n",
            "            \"#text\": \"Drawing from this previous work we will explicitly study the effect of adding steps to the grammatical relations assignment cascade.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"25\",\n",
            "            \"@ssid\": \"21\",\n",
            "            \"#text\": \"Through experiments with cascading several classifiers, we will show that even using imperfect classifiers can improve overall performance of the cascaded classifier.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"26\",\n",
            "            \"@ssid\": \"22\",\n",
            "            \"#text\": \"We illustrate this claim on the task of finding grammatical relations (e.g. subject, object, locative) to verbs in text.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"27\",\n",
            "            \"@ssid\": \"23\",\n",
            "            \"#text\": \"The CR assigner uses several sources of information step by step such as several types of XP chunks (NP, VP, PP, ADJP and ADVP), and adverbial functions assigned to these chunks (e.g. temporal, local).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"28\",\n",
            "            \"@ssid\": \"24\",\n",
            "            \"#text\": \"Since not all of these entities are predicted reliably, it is the question whether each source leads to an improvement of the overall GR assignment.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"29\",\n",
            "            \"@ssid\": \"25\",\n",
            "            \"#text\": \"In the rest of this paper we will first briefly describe Memory-Based Learning in Section 2.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"30\",\n",
            "            \"@ssid\": \"26\",\n",
            "            \"#text\": \"In Section 3.1, we discuss the chunking classifiers that we later use as steps in the cascade.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"31\",\n",
            "            \"@ssid\": \"27\",\n",
            "            \"#text\": \"Section 3.2 describes the basic GR classifier.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"32\",\n",
            "            \"@ssid\": \"28\",\n",
            "            \"#text\": \"Section 3.3 presents the architecture and results of the cascaded GR assignment experiments.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"33\",\n",
            "            \"@ssid\": \"29\",\n",
            "            \"#text\": \"We discuss the results in Section 4 and conclude with Section 5.\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@title\": \"2 Memory-Based Learning\",\n",
            "        \"@number\": \"2\",\n",
            "        \"S\": [\n",
            "          {\n",
            "            \"@sid\": \"34\",\n",
            "            \"@ssid\": \"1\",\n",
            "            \"#text\": \"Memory-Based Learning (MBL) keeps all training data in memory and only abstracts at classification time by extrapolating a class from the most similar item(s) in memory.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"35\",\n",
            "            \"@ssid\": \"2\",\n",
            "            \"#text\": \"In recent work Daelemans et at.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"36\",\n",
            "            \"@ssid\": \"3\",\n",
            "            \"#text\": \"(1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also &quot;remembers&quot; exceptional, low-frequency cases which are useful to extrapolate from.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"37\",\n",
            "            \"@ssid\": \"4\",\n",
            "            \"#text\": \"Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"38\",\n",
            "            \"@ssid\": \"5\",\n",
            "            \"#text\": \"We have used the following MBL algorithms': test item and each memory item is defined as the number of features for which they have a different value (overlap metric).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"39\",\n",
            "            \"@ssid\": \"6\",\n",
            "            \"#text\": \"IB1-IG : IB1 with information gain (an information-theoretic notion measuring the reduction of uncertainty about the class to be predicted when knowing the value of a feature) to weight the cost of a feature value mismatch during comparison.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"40\",\n",
            "            \"@ssid\": \"7\",\n",
            "            \"#text\": \"IGTree : In this variant, a decision tree is created with features as tests, and ordered according to the information gain of the features, as a heuristic approximation of the computationally more expensive IB1 variants.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"41\",\n",
            "            \"@ssid\": \"8\",\n",
            "            \"#text\": \"For more references and information about these algorithms we refer to (Daelemans et al., 1998; Daelemans et al., 1999b).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"42\",\n",
            "            \"@ssid\": \"9\",\n",
            "            \"#text\": \"For other memory-based approaches to parsing, see (Bod, 1992) and (Sekine, 1998).\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@title\": \"3 Methods and Results\",\n",
            "        \"@number\": \"3\",\n",
            "        \"S\": [\n",
            "          {\n",
            "            \"@sid\": \"43\",\n",
            "            \"@ssid\": \"1\",\n",
            "            \"#text\": \"In this section we describe the stages of the cascade.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"44\",\n",
            "            \"@ssid\": \"2\",\n",
            "            \"#text\": \"The very first stage consists of a MemoryBased Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"45\",\n",
            "            \"@ssid\": \"3\",\n",
            "            \"#text\": \"The next three stages involve determining boundaries and labels of chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"46\",\n",
            "            \"@ssid\": \"4\",\n",
            "            \"#text\": \"Chunks are nonrecursive, non-overlapping constituent parts of sentences (see (Abney, 1991)).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"47\",\n",
            "            \"@ssid\": \"5\",\n",
            "            \"#text\": \"First, we simultaneously chunk sentences into: NP-, VP: Prep-, ADJP- and APVP-chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"48\",\n",
            "            \"@ssid\": \"6\",\n",
            "            \"#text\": \"As these chunks are non-overlapping, no words can belong to more than one chunk, and thus no conflicts can arise.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"49\",\n",
            "            \"@ssid\": \"7\",\n",
            "            \"#text\": \"Prep-chunks are the prepositional part of PPs, thus excluding the nominal part.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"50\",\n",
            "            \"@ssid\": \"8\",\n",
            "            \"#text\": \"Then we join a Prep-chunk and one \\u2014 or more coordinated \\u2014 NP-chunks into a PPchunk.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"51\",\n",
            "            \"@ssid\": \"9\",\n",
            "            \"#text\": \"Finally, we assign adverbial function (ADVFUNC) labels (e.g. locative or temporal) to all chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"52\",\n",
            "            \"@ssid\": \"10\",\n",
            "            \"#text\": \"In the last stage of the cascade, we label several types of grammatical relations between pairs of words in the sentence.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"53\",\n",
            "            \"@ssid\": \"11\",\n",
            "            \"#text\": \"The data for all our experiments was extracted from the Penn Treebank II Wall Street Journal (WSJ) corpus (Marcus et al., 1993).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"54\",\n",
            "            \"@ssid\": \"12\",\n",
            "            \"#text\": \"For all experiments, we used sections 00-19 as training material and 20-24 as test material.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"55\",\n",
            "            \"@ssid\": \"13\",\n",
            "            \"#text\": \"See Section 4 for results on other train/test set splittings.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"56\",\n",
            "            \"@ssid\": \"14\",\n",
            "            \"#text\": \"For evaluation of our results we use the precision and recall measures.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"57\",\n",
            "            \"@ssid\": \"15\",\n",
            "            \"#text\": \"Precision is the percentage of predicted chunks/relations that are actually correct, recall is the percentage of correct chunks/relations that are actually found.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"58\",\n",
            "            \"@ssid\": \"16\",\n",
            "            \"#text\": \"For convenient comparisons of only one value, we also list the Fo---1 value (C.J.van Rijsbergen, 1979).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"59\",\n",
            "            \"@ssid\": \"17\",\n",
            "            \"#text\": \"(i32+1)pree.rec with /3 1 In the first experiment described in this section, the task is to segment the sentence into chunks and to assign labels to these chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"60\",\n",
            "            \"@ssid\": \"18\",\n",
            "            \"#text\": \"This process of chunking and labeling is carried out by assigning a tag to each word in a sentence leftto-right.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"61\",\n",
            "            \"@ssid\": \"19\",\n",
            "            \"#text\": \"Ramshaw and Marcus (1995) first assigned a chunk tag to each word in the sentence: I for inside a chunk, 0 for outside a chunk, and B for inside a chunk, but the preceding word is in another chunk.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"62\",\n",
            "            \"@ssid\": \"20\",\n",
            "            \"#text\": \"As we want to find more than one kind of chunk, we have to further differentiate the JOB tags as to which kind of chunk (NP, VP, Prep, ADJP or ADVP) the word is in.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"63\",\n",
            "            \"@ssid\": \"21\",\n",
            "            \"#text\": \"With the extended JOB tag set at hand we can tag the sentence: After having found Prep-, NP- and other chunks, we collapse Preps and NPs to PPs in a second step.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"64\",\n",
            "            \"@ssid\": \"22\",\n",
            "            \"#text\": \"While the GR assigner finds relations between VPs and other chunks (cf.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"65\",\n",
            "            \"@ssid\": \"23\",\n",
            "            \"#text\": \"Section 3.2), the PP chunker finds relations between prepositions and NPs 2 in a way similar to OR. assignment (see Section 3.2).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"66\",\n",
            "            \"@ssid\": \"24\",\n",
            "            \"#text\": \"In the last chunking/labeling step, we assign adverbial functions to chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"67\",\n",
            "            \"@ssid\": \"25\",\n",
            "            \"#text\": \"The classes are the adverbial function labels from the treebank: LOC (locative), TMP (temporal), DIR.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"68\",\n",
            "            \"@ssid\": \"26\",\n",
            "            \"#text\": \"(directional), PRP (purpose and reason), MNR (manner), EXT (extension) or &quot;2 for none of the former.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"69\",\n",
            "            \"@ssid\": \"27\",\n",
            "            \"#text\": \"Table 1 gives an overview of the results of the chunking-labeling experiments, using the following algorithms, determined by validation on the train set: IBI-IG for XP-chunking and IGTree for PP-chunking and ADVFUNCs assignment.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"70\",\n",
            "            \"@ssid\": \"28\",\n",
            "            \"#text\": \"In grammatical relation assignment we assign a GR to pairs of words in a sentence.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"71\",\n",
            "            \"@ssid\": \"29\",\n",
            "            \"#text\": \"In our 2PPs containing anything else than NPs (e.g.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"72\",\n",
            "            \"@ssid\": \"30\",\n",
            "            \"#text\": \"'without bringing his wife) are not searched for. ments.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"73\",\n",
            "            \"@ssid\": \"31\",\n",
            "            \"#text\": \"NP-,VP-, ADJP-, ADVP- and Prepchunks are found simultaneously, but for convenience, precision and recall values are given separately for each type of chunk. experiments, one of these words is always a verb, since this yields the most important GRs.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"74\",\n",
            "            \"@ssid\": \"32\",\n",
            "            \"#text\": \"The other word is the head of the phrase which is annotated with this grammatical relation in the treebank.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"75\",\n",
            "            \"@ssid\": \"33\",\n",
            "            \"#text\": \"A preposition is the head of a PP, a noun of an NP and so on.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"76\",\n",
            "            \"@ssid\": \"34\",\n",
            "            \"#text\": \"Defining relations to hold between heads means that the algorithm can, for example, find a subject relation between a noun and a verb without necessarily having to make decisions about the precise boundaries of the subject NP.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"77\",\n",
            "            \"@ssid\": \"35\",\n",
            "            \"#text\": \"Suppose we had the POS-tagged sentence shown in Figure 1 and we wanted the algorithm to decide whether, and if so how, Miller (henceforth: the focus) is related to the first verb organized.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"78\",\n",
            "            \"@ssid\": \"36\",\n",
            "            \"#text\": \"We then construct an instance for this pair of words by extracting a set of feature values from the sentence.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"79\",\n",
            "            \"@ssid\": \"37\",\n",
            "            \"#text\": \"The instance contains information about the verb and the focus: a feature for the word form and a feature for the POS of both.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"80\",\n",
            "            \"@ssid\": \"38\",\n",
            "            \"#text\": \"It also has similar features for the local context of the focus.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"81\",\n",
            "            \"@ssid\": \"39\",\n",
            "            \"#text\": \"Experiments on the training data suggest an optimal context width of two elements to the left and one to the right.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"82\",\n",
            "            \"@ssid\": \"40\",\n",
            "            \"#text\": \"In the present case, elements are words or punctuation signs.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"83\",\n",
            "            \"@ssid\": \"41\",\n",
            "            \"#text\": \"In addition to the lexical and the local context information, we include superficial information about clause structure: The first feature indicates the distance from the verb to the focus, counted in elements.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"84\",\n",
            "            \"@ssid\": \"42\",\n",
            "            \"#text\": \"A negative distance means that the focus is to the left of the verb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"85\",\n",
            "            \"@ssid\": \"43\",\n",
            "            \"#text\": \"The second feature contains the number of other verbs between the verb and the focus.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"86\",\n",
            "            \"@ssid\": \"44\",\n",
            "            \"#text\": \"The third feature is the number of intervening commas.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"87\",\n",
            "            \"@ssid\": \"45\",\n",
            "            \"#text\": \"The features were chosen by manual 6-7, 8-9 and 12-13 describe the context words, Features 10-11 the focus word.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"88\",\n",
            "            \"@ssid\": \"46\",\n",
            "            \"#text\": \"Empty contexts are indicated by the value &quot;-&quot; for all features.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"89\",\n",
            "            \"@ssid\": \"47\",\n",
            "            \"#text\": \"&quot;feature engineering&quot;.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"90\",\n",
            "            \"@ssid\": \"48\",\n",
            "            \"#text\": \"Table 2 shows the complete instance for Miller-organized in row 5, together with the other first four instances for the sentence.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"91\",\n",
            "            \"@ssid\": \"49\",\n",
            "            \"#text\": \"The class is mostly &quot;-&quot;, to indicate that the word does not have a direct grammatical relation to organized.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"92\",\n",
            "            \"@ssid\": \"50\",\n",
            "            \"#text\": \"Other possible classes are those from a list of more than 100 different labels found in the treebank.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"93\",\n",
            "            \"@ssid\": \"51\",\n",
            "            \"#text\": \"These are combinations of a syntactic category and zero, one or more functions, e.g.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"94\",\n",
            "            \"@ssid\": \"52\",\n",
            "            \"#text\": \"NP-SBJ for subject, NP-PRD for predicative object, NP for (in)direct object3, PP-LOC for locative PP adjunct, PP-LOC-CLR for subcategorised locative PP, etcetera.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"95\",\n",
            "            \"@ssid\": \"53\",\n",
            "            \"#text\": \"According to their information gain values, features are ordered with decreasing importance as follows: 11,13, 10, 1, 2, 8, 12, 9, 6 , 4 , 7 , 3 , 5.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"96\",\n",
            "            \"@ssid\": \"54\",\n",
            "            \"#text\": \"Intuitively,. this ordering makes sense.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"97\",\n",
            "            \"@ssid\": \"55\",\n",
            "            \"#text\": \"The most important feature is the POS of the focus, because this determines whether it can have a GR to a verb at all (fninctuation cannot) and what kind of relation is possible.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"98\",\n",
            "            \"@ssid\": \"56\",\n",
            "            \"#text\": \"The POS of the following word is important, because e.g. a noun followed by a noun is probably not the head of an NP and will therefore not have a direct GR to the verb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"99\",\n",
            "            \"@ssid\": \"57\",\n",
            "            \"#text\": \"The word itself may be important if it is e.g. a preposition, a pronoun or a clearly temporal/local adverb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"100\",\n",
            "            \"@ssid\": \"58\",\n",
            "            \"#text\": \"Features 1 and 2 give some indication of the complexity of the structure intervening between the focus and the verb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"101\",\n",
            "            \"@ssid\": \"59\",\n",
            "            \"#text\": \"The more complex this structure, the lower the probability that the focus and the verb are related.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"102\",\n",
            "            \"@ssid\": \"60\",\n",
            "            \"#text\": \"Context further away is less important than near context.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"103\",\n",
            "            \"@ssid\": \"61\",\n",
            "            \"#text\": \"To test the effects of the chunking steps from Section 3.1 on this task, we will now construct instances based on more structured input text, like that in Figure 2.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"104\",\n",
            "            \"@ssid\": \"62\",\n",
            "            \"#text\": \"This time, the focus is described by five features instead of two, for the additional information: which type of chunk it is in, what, the preposition is if it is in a PP chunk, and what the adverbial function is, if any.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"105\",\n",
            "            \"@ssid\": \"63\",\n",
            "            \"#text\": \"We still have a context of two elements left, one right, but elements are now defined to be either chunks, or words outside any chunk, or punctuation.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"106\",\n",
            "            \"@ssid\": \"64\",\n",
            "            \"#text\": \"Each chunk in the context is represented by its last word (which is the semantically most important word in most cases), by the POS of the last word, and by the type of chunk.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"107\",\n",
            "            \"@ssid\": \"65\",\n",
            "            \"#text\": \"The distance feature is adapted to the new definition of element, too, and instead of counting intervening verbs, we now count intervening VP chunks.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"108\",\n",
            "            \"@ssid\": \"66\",\n",
            "            \"#text\": \"Figure 3 shows the first five instances for the sentence in Figure 2.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"109\",\n",
            "            \"@ssid\": \"67\",\n",
            "            \"#text\": \"Class value&quot; &quot; again means the focus is not directly related to the verb&quot; (but to some other verb or a non-verbal element).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"110\",\n",
            "            \"@ssid\": \"68\",\n",
            "            \"#text\": \"According to their information gain values, features are ordered in decreasing importance as follows: 16, 15, 12, 14, 11, 2, 1, 19, 10, 9, 13, 18, 6, 17, 8, 4, 7, 3, 5.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"111\",\n",
            "            \"@ssid\": \"69\",\n",
            "            \"#text\": \"Comparing this to the earlier feature ordering, we see that most of the new features are distance and intervening VPs and commas.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"112\",\n",
            "            \"@ssid\": \"70\",\n",
            "            \"#text\": \"Features 4 and 5 show the verb and its POS.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"113\",\n",
            "            \"@ssid\": \"71\",\n",
            "            \"#text\": \"Features 6-8, 9-11 and 17-19 describe the context words/chunks, Features 12-16 the focus chunk.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"114\",\n",
            "            \"@ssid\": \"72\",\n",
            "            \"#text\": \"Empty contexts are indicated by the &quot;-&quot; for all features. very important, thereby justifying their introduction.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"115\",\n",
            "            \"@ssid\": \"73\",\n",
            "            \"#text\": \"Relative to the other &quot;old&quot; features, the structural features 1 and 2 have gained importance, probably because more structure is available in the input to represent.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"116\",\n",
            "            \"@ssid\": \"74\",\n",
            "            \"#text\": \"In principle, we would have to construct one instance for each possible pair of a verb and a focus word in the sentence.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"117\",\n",
            "            \"@ssid\": \"75\",\n",
            "            \"#text\": \"However, we restrict instances to those where there is at most one other verb/VP chunk between the verb and the focus, in case the focus precedes the verb, and no other verb in case the verb precedes the focus.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"118\",\n",
            "            \"@ssid\": \"76\",\n",
            "            \"#text\": \"This restriction allows, for example, for a relative clause on the subject (as in our example sentence).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"119\",\n",
            "            \"@ssid\": \"77\",\n",
            "            \"#text\": \"In the training data, 97.9% of the related pairs fulfill this condition (when counting VP chunks).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"120\",\n",
            "            \"@ssid\": \"78\",\n",
            "            \"#text\": \"Experiments on the training data showed that increasing the admitted number of intervening VP chunks slightly increases recall, at the cost of precision.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"121\",\n",
            "            \"@ssid\": \"79\",\n",
            "            \"#text\": \"Having constructed all instances from the test data and from a training set with the same level of partial structure, we first train the IGTree algorithm, and then let it classify the test instances.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"122\",\n",
            "            \"@ssid\": \"80\",\n",
            "            \"#text\": \"Then, for each test instance that was classified with a grammatical relation, we check whether the same verb-focuspair appears with the same relation in the GR list extracted directly from the treebank.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"123\",\n",
            "            \"@ssid\": \"81\",\n",
            "            \"#text\": \"This gives us the precision of the classifier.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"124\",\n",
            "            \"@ssid\": \"82\",\n",
            "            \"#text\": \"Checking the treebank list versus the classified list yields We have already seen from the example that the level of structure in the input text can influence the composition of the instances.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"125\",\n",
            "            \"@ssid\": \"83\",\n",
            "            \"#text\": \"We are interested in the effects of different sorts of partial structure in the input data on the classification performance of the final classifier.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"126\",\n",
            "            \"@ssid\": \"84\",\n",
            "            \"#text\": \"Therefore, we ran a series of experiments.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"127\",\n",
            "            \"@ssid\": \"85\",\n",
            "            \"#text\": \"The classification task was always that of finding grammatical relations to verbs and performance was always measured by precision and recall on those relations (the test set contained 45825 relations).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"128\",\n",
            "            \"@ssid\": \"86\",\n",
            "            \"#text\": \"The amount of structure in the input data varied.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"129\",\n",
            "            \"@ssid\": \"87\",\n",
            "            \"#text\": \"Table 4 shows the results of the experiments.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"130\",\n",
            "            \"@ssid\": \"88\",\n",
            "            \"#text\": \"In the first experiment, only POS tagged input is used.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"131\",\n",
            "            \"@ssid\": \"89\",\n",
            "            \"#text\": \"Then, NP chunks are added.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"132\",\n",
            "            \"@ssid\": \"90\",\n",
            "            \"#text\": \"Other sorts of chunks are inserted at each subsequent step.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"133\",\n",
            "            \"@ssid\": \"91\",\n",
            "            \"#text\": \"Finally, the adverbial function labels are added.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"134\",\n",
            "            \"@ssid\": \"92\",\n",
            "            \"#text\": \"We can see that the more structure we add, the better precision and recall of the grammatical relations get: precision increases from 60.7% to 74.8%, recall from 41.3% to 67.9%.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"135\",\n",
            "            \"@ssid\": \"93\",\n",
            "            \"#text\": \"This in spite of the fact that the added information is not always correct, because it was predicted for the test material on the basis of the training material by the classifiers described in Section 3.1.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"136\",\n",
            "            \"@ssid\": \"94\",\n",
            "            \"#text\": \"As we have seen in Table 1, especially ADJP and ADVP chunks and adverbial function labels did not have very high precision and recall.\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@title\": \"4 Discussion\",\n",
            "        \"@number\": \"4\",\n",
            "        \"S\": [\n",
            "          {\n",
            "            \"@sid\": \"137\",\n",
            "            \"@ssid\": \"1\",\n",
            "            \"#text\": \"There are three ways how two cascaded modules can interact.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"138\",\n",
            "            \"@ssid\": \"2\",\n",
            "            \"#text\": \"\\u2022 The first module can add information on which the later module can (partially) base its decisions.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"139\",\n",
            "            \"@ssid\": \"3\",\n",
            "            \"#text\": \"This is the case between the adverbial functions finder and the relations finder.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"140\",\n",
            "            \"@ssid\": \"4\",\n",
            "            \"#text\": \"The former adds an extra informative feature to the instances of the latter (Feature 16 in Table 3).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"141\",\n",
            "            \"@ssid\": \"5\",\n",
            "            \"#text\": \"Cf. column two of Table 4.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"142\",\n",
            "            \"@ssid\": \"6\",\n",
            "            \"#text\": \"\\u2022 The first module can restrict the number of decisions to be made by the second one.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"143\",\n",
            "            \"@ssid\": \"7\",\n",
            "            \"#text\": \"This is the case in the combination of the chunking steps and the relations finder.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"144\",\n",
            "            \"@ssid\": \"8\",\n",
            "            \"#text\": \"Without the chunker, the relations finder would have to decide for every word, whether it is the head of a constituent that bears a relation to the verb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"145\",\n",
            "            \"@ssid\": \"9\",\n",
            "            \"#text\": \"With the churlker., the relations finder has to make this decision for fewer words, namely only for those which are the last word in a chunk resp. the preposition of a PP chunk.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"146\",\n",
            "            \"@ssid\": \"10\",\n",
            "            \"#text\": \"Practically, this reduction of the number of decisions (which translates into a reduction of instances) as can be seen in the third column of Table 4.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"147\",\n",
            "            \"@ssid\": \"11\",\n",
            "            \"#text\": \"\\u2022 The first module can reduce the number of elements used for the instances by counting one chunk as just one context element.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"148\",\n",
            "            \"@ssid\": \"12\",\n",
            "            \"#text\": \"We can see the effect in the feature that indicates the distance in elements between the focus and the verb.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"149\",\n",
            "            \"@ssid\": \"13\",\n",
            "            \"#text\": \"The more chunks are used, the smaller the average absolute distance (see column four Table 4).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"150\",\n",
            "            \"@ssid\": \"14\",\n",
            "            \"#text\": \"All three effects interact in the cascade we describe.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"151\",\n",
            "            \"@ssid\": \"15\",\n",
            "            \"#text\": \"The PP chunker reduces the number of decisions for the relations finder (instead of one instance for the preposition and one for the NP chunk, we get only one instance for the PP chunk-). introduces an extra feature (Feature 12 in Table 3), and changes the context (instead of a preposition and an NP, context may now be one PP).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"152\",\n",
            "            \"@ssid\": \"16\",\n",
            "            \"#text\": \"As we already noted above, precision and recall are monotonically increasing when adding more structure.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"153\",\n",
            "            \"@ssid\": \"17\",\n",
            "            \"#text\": \"However, we note large differences, such as NP chunks which increase Fs_i by more than 10%, and VP chunks which add another 6.8%, whereas ADVPs and ADJPs yield hardly any improvement.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"154\",\n",
            "            \"@ssid\": \"18\",\n",
            "            \"#text\": \"This may partially be explained by the fact that these chunks are less frequent than the former two.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"155\",\n",
            "            \"@ssid\": \"19\",\n",
            "            \"#text\": \"Preps, on the other hand, while hardly reducing the average distance or the number of instances, improve 1,3-1 by nearly 1%.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"156\",\n",
            "            \"@ssid\": \"20\",\n",
            "            \"#text\": \"PPs yield another 1.1%.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"157\",\n",
            "            \"@ssid\": \"21\",\n",
            "            \"#text\": \"What may come as a surprise is that adverbial functions again increase F,3,1 by nearly 2%, despite the fact that F1 for this ADVFTJNC assignment step was not very high.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"158\",\n",
            "            \"@ssid\": \"22\",\n",
            "            \"#text\": \"This result shows that cascaded modules need not be perfect to be useful.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"159\",\n",
            "            \"@ssid\": \"23\",\n",
            "            \"#text\": \"Up to now, we only looked at the overall results.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"160\",\n",
            "            \"@ssid\": \"24\",\n",
            "            \"#text\": \"Table 4 also shows individual Fp_1 values for four selected common grammatical relations: subject NP, (in)direct object NP, locative PP adjunct and temporal PP adjunct.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"161\",\n",
            "            \"@ssid\": \"25\",\n",
            "            \"#text\": \"Note that the steps have different effects on the different relations: Adding NPs increases Fp=i by 11.3% for subjects resp.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"162\",\n",
            "            \"@ssid\": \"26\",\n",
            "            \"#text\": \"16.2% for objects, but only 3.9% resp.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"163\",\n",
            "            \"@ssid\": \"27\",\n",
            "            \"#text\": \"3.7% for locatives and temporals.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"164\",\n",
            "            \"@ssid\": \"28\",\n",
            "            \"#text\": \"Adverbial functions are more important for the two adjuncts (+6.3% resp.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"165\",\n",
            "            \"@ssid\": \"29\",\n",
            "            \"#text\": \"+15%) than for the two complements (+0.2% resp.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"166\",\n",
            "            \"@ssid\": \"30\",\n",
            "            \"#text\": \"+0.7%).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"167\",\n",
            "            \"@ssid\": \"31\",\n",
            "            \"#text\": \"Argamon et al. (1998) report F13=1 for subject and object identification of respectively 86.5% and 83.0%, compared to 81.8% and 81.0% in this paper.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"168\",\n",
            "            \"@ssid\": \"32\",\n",
            "            \"#text\": \"Note however that Argamon et al. (1998) do not identify the head of subjects, subjects in embedded clauses, or subjects and objects related to the verb only through a trace, which makes their task easier.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"169\",\n",
            "            \"@ssid\": \"33\",\n",
            "            \"#text\": \"For a detailed comparison of the two methods on the same task see (Daelemans et al., 1999a).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"170\",\n",
            "            \"@ssid\": \"34\",\n",
            "            \"#text\": \"That paper also shows that the chunking method proposed here performs about as well as other methods, and that the influence of tagging errors on (NP) chunking is less than 1%.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"171\",\n",
            "            \"@ssid\": \"35\",\n",
            "            \"#text\": \"To study the effect of the errors in the lower modules other than the tagger, we used &quot;perfect&quot; test data in a last experiment, i.e. data annotated with partial information taken directly from the treebank.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"172\",\n",
            "            \"@ssid\": \"36\",\n",
            "            \"#text\": \"The results are shown in Table 5.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"173\",\n",
            "            \"@ssid\": \"37\",\n",
            "            \"#text\": \"We see that later modules suffer from errors of earlier modules (as could be expected): Fp_1 of PP chunking is 92% but could have previous modules in the cascade) vs. on &quot;perfect&quot; input (enriched with partial treebank annotation).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"174\",\n",
            "            \"@ssid\": \"38\",\n",
            "            \"#text\": \"For PPs, this means perfect POS tags and chunk labels/boundaries, for ADVFUNC additionally perfect PP chunks, for GR assignment also perfect ADVFUNC labels. been 97.9% if all previous chunks would have been correct (+5.9%).\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"175\",\n",
            "            \"@ssid\": \"39\",\n",
            "            \"#text\": \"For adverbial functions, the difference is 3.5%.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"176\",\n",
            "            \"@ssid\": \"40\",\n",
            "            \"#text\": \"For grammatical relation assignment, the last module in the cascade, the difference is, not surprisingly, the largest: 7.9% for chunks only, 12.3% for chunks and ADVFUNCs.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"177\",\n",
            "            \"@ssid\": \"41\",\n",
            "            \"#text\": \"The latter percentage shows what could maximally be gained by further improving the chunker and ADVFUNCs finder.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"178\",\n",
            "            \"@ssid\": \"42\",\n",
            "            \"#text\": \"On realistic data, a realistic ADVFUNCs finder improves CR assigment by 1.9%.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"179\",\n",
            "            \"@ssid\": \"43\",\n",
            "            \"#text\": \"On perfect data, a perfect ADVFUNCs finder increases performance by 6.3%.\"\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      {\n",
            "        \"@title\": \"5 Conclusion and Future Research\",\n",
            "        \"@number\": \"5\",\n",
            "        \"S\": [\n",
            "          {\n",
            "            \"@sid\": \"180\",\n",
            "            \"@ssid\": \"1\",\n",
            "            \"#text\": \"In this paper we studied cascaded grammatical relations assignment.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"181\",\n",
            "            \"@ssid\": \"2\",\n",
            "            \"#text\": \"We showed that even the use of imperfect modules improves the overall result of the cascade.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"182\",\n",
            "            \"@ssid\": \"3\",\n",
            "            \"#text\": \"In future research we plan to also train our classifiers on imperfectly chunked material.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"183\",\n",
            "            \"@ssid\": \"4\",\n",
            "            \"#text\": \"This enables the classifier to better cope with systematic errors in train and test material.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"184\",\n",
            "            \"@ssid\": \"5\",\n",
            "            \"#text\": \"We expect that especially an improvement of the adverbial function assignment will lead to better OR assignment.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"185\",\n",
            "            \"@ssid\": \"6\",\n",
            "            \"#text\": \"Finally, since cascading proved effective for GR. assignment we intend to study the effect of cascading different types of XP chunkers on chunking performance.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"186\",\n",
            "            \"@ssid\": \"7\",\n",
            "            \"#text\": \"We might e.g. first find ADJP chunks, then use that chunker's output as additional input for the NP chunker, then use the combined output as input to the VP chunker and so on.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"187\",\n",
            "            \"@ssid\": \"8\",\n",
            "            \"#text\": \"Other chunker orderings are possible, too.\"\n",
            "          },\n",
            "          {\n",
            "            \"@sid\": \"188\",\n",
            "            \"@ssid\": \"9\",\n",
            "            \"#text\": \"Likewise, it might be better to find different grammatical relations subsequently, instead of simultaneously.\"\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import xmltodict\n",
        "import json\n",
        "\n",
        "# Specify the path to your XML file\n",
        "xml_file_path = path + 'W99-0629.xml'\n",
        "\n",
        "# Parse the XML file\n",
        "tree = ET.parse(xml_file_path)\n",
        "\n",
        "# Get the root element\n",
        "root = tree.getroot()\n",
        "\n",
        "xml_string = ET.tostring(root, encoding=\"utf-8\").decode(\"utf-8\")\n",
        "\n",
        "print(xml_string)\n",
        "\n",
        "data_dict = xmltodict.parse(xml_string)\n",
        "\n",
        "# Convert the dictionary to a JSON string\n",
        "json_string = json.dumps(data_dict, indent=2)\n",
        "\n",
        "# Print or use the resulting JSON string\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IENFFy_cS1BQ",
        "outputId": "55ba344c-6c6e-4b91-aa84-9dd488342ad0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<xml.etree.ElementTree.ElementTree at 0x7e39079e3d60>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "166e361e-0840-4cbd-e43d-ceee452b8292"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "f4b49baf-22d5-4a35-eccb-1a8b761432f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr_wV5WgL3ao"
      },
      "outputs": [],
      "source": [
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/Data/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9m7SGAHRxdM",
        "outputId": "998861e0-2bf6-4604-9d75-18e222665565"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['top1000_complete', 'TLDR']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ6P7DbMSEXW"
      },
      "outputs": [],
      "source": [
        "path = BASE_PATH + 'W99-0629/'+ 'Documents_xml/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDQucciXSWgQ"
      },
      "outputs": [],
      "source": [
        "###########################33"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "rOo2flUzaIGG",
        "outputId": "78c8addf-f515-4dd3-f268-d21fa3b1601a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              source  \\\n",
              "0  [Due to the success of deep learning to solvin...   \n",
              "1  [The backpropagation (BP) algorithm is often t...   \n",
              "2  [We introduce the 2-simplicial Transformer, an...   \n",
              "3  [We present Tensor-Train RNN (TT-RNN), a novel...   \n",
              "4  [Recent efforts on combining deep models with ...   \n",
              "\n",
              "                                       source_labels  \\\n",
              "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "2  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
              "\n",
              "                                        rouge_scores    paper_id  \\\n",
              "0  [0.3018867874688502, 0.37209301838831804, 0.60...   SysEexbRb   \n",
              "1  [0.0, 0.0, 0.13043477920604923, 0.142857139229...  SygvZ209F7   \n",
              "2  [0.33333332839506175, 0.8888888839111112, 0.11...  rkecJ6VFvr   \n",
              "3  [0.06666666222222252, 0.06451612466181092, 0.0...   HJJ0w--0W   \n",
              "4  [0.2777777727932099, 0.5714285666581633, 0.095...   HyH9lbZAW   \n",
              "\n",
              "                                              target  \n",
              "0  [We provide necessary and sufficient analytica...  \n",
              "1  [Biologically plausible learning algorithms, p...  \n",
              "2  [We introduce the 2-simplicial Transformer and...  \n",
              "3  [Accurate forecasting over very long time hori...  \n",
              "4  [We propose a variational message-passing algo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d81c54f-285e-4b7d-84d6-491556ff79ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>source_labels</th>\n",
              "      <th>rouge_scores</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Due to the success of deep learning to solvin...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.3018867874688502, 0.37209301838831804, 0.60...</td>\n",
              "      <td>SysEexbRb</td>\n",
              "      <td>[We provide necessary and sufficient analytica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[The backpropagation (BP) algorithm is often t...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.0, 0.0, 0.13043477920604923, 0.142857139229...</td>\n",
              "      <td>SygvZ209F7</td>\n",
              "      <td>[Biologically plausible learning algorithms, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[We introduce the 2-simplicial Transformer, an...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.33333332839506175, 0.8888888839111112, 0.11...</td>\n",
              "      <td>rkecJ6VFvr</td>\n",
              "      <td>[We introduce the 2-simplicial Transformer and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[We present Tensor-Train RNN (TT-RNN), a novel...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.06666666222222252, 0.06451612466181092, 0.0...</td>\n",
              "      <td>HJJ0w--0W</td>\n",
              "      <td>[Accurate forecasting over very long time hori...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Recent efforts on combining deep models with ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[0.2777777727932099, 0.5714285666581633, 0.095...</td>\n",
              "      <td>HyH9lbZAW</td>\n",
              "      <td>[We propose a variational message-passing algo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d81c54f-285e-4b7d-84d6-491556ff79ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7d81c54f-285e-4b7d-84d6-491556ff79ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7d81c54f-285e-4b7d-84d6-491556ff79ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3f204270-6ab1-4e92-acae-214ad026ec4c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f204270-6ab1-4e92-acae-214ad026ec4c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3f204270-6ab1-4e92-acae-214ad026ec4c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Due to the success of deep learning to solving a variety of challenging machine learning tasks, there is a rising interest in understanding loss functions for training neural networks from a theoretical aspect.', 'Particularly, the properties of critical points and the landscape around them are of importance to determine the convergence performance of optimization algorithms.', 'In this paper, we provide a necessary and sufficient characterization of the analytical forms for the critical points (as well as global minimizers) of the square loss functions for linear neural networks.', 'We show that the analytical forms of the critical points characterize the values of the corresponding loss functions as well as the necessary and sufficient conditions to achieve global minimum.', 'Furthermore, we exploit the analytical forms of the critical points to characterize the landscape properties for the loss functions of linear neural networks and shallow ReLU networks.', 'One particular conclusion is that: While the loss function of linear networks has no spurious local minimum, the loss function of one-hidden-layer nonlinear networks with ReLU activation function does have local minimum that is not global minimum.', 'In the past decade, deep neural networks BID8 have become a popular tool that has successfully solved many challenging tasks in a variety of areas such as machine learning, artificial intelligence, computer vision, and natural language processing, etc.', 'As the understandings of deep neural networks from different aspects are mostly based on empirical studies, there is a rising need and interest to develop understandings of neural networks from theoretical aspects such as generalization error, representation power, and landscape (also referred to as geometry) properties, etc.', 'In particular, the landscape properties of loss functions (that are typically nonconex for neural networks) play a central role to determine the iteration path and convergence performance of optimization algorithms.', 'One major landscape property is the nature of critical points, which can possibly be global minima, local minima, saddle points.', 'There have been intensive efforts in the past into understanding such an issue for various neural networks.', 'For example, it has been shown that every local minimum of the loss function is also a global minimum for shallow linear networks under the autoencoder setting and invertibility assumptions BID1 and for deep linear networks BID11 ; BID14 ; Yun et al. (2017) respectively under different assumptions.', 'The conditions on the equivalence between local minimum or critical point and global minimum has also been established for various nonlinear neural networks Yu & Chen (1995) ; BID9 ; BID15 ; BID17 ; BID6 under respective assumptions.', 'However, most previous studies did not provide characterization of analytical forms for critical points of loss functions for neural networks with only very few exceptions.', 'In BID1 , the authors provided an analytical form for the critical points of the square loss function of shallow linear networks under certain conditions.', 'Such an analytical form further helps to establish the landscape properties around the critical points.', 'Further in BID13 , the authors characterized certain sufficient form of critical points for the square loss function of matrix factorization problems and deep linear networks.', 'The focus of this paper is on characterizing the sufficient and necessary forms of critical points for broader scenarios, i.e., shallow and deep linear networks with no assumptions on data matrices and network dimensions, and shallow ReLU networks over certain parameter space.', 'In particular, such analytical forms of critical points capture the corresponding loss function values and the necessary and sufficient conditions to achieve global minimum.', 'This further enables us to establish new landscape properties around these critical points for the loss function of these networks under general settings, and provides alternative (yet simpler and more intuitive) proofs for existing understanding of the landscape properties.', 'OUR CONTRIBUTION 1) For the square loss function of linear networks with one hidden layer, we provide a full (necessary and sufficient) characterization of the analytical forms for its critical points and global minimizers.', 'These results generalize the characterization in BID1 to arbitrary network parameter dimensions and any data matrices.', 'Such a generalization further enables us to establish the landscape property, i.e., every local minimum is also a global minimum and all other critical points are saddle points, under no assumptions on parameter dimensions and data matrices.', 'From a technical standpoint, we exploit the analytical forms of critical points to provide a new proof for characterizing the landscape around the critical points under full relaxation of assumptions, where the corresponding approaches in BID1 are not applicable.', 'As a special case of linear networks, the matrix factorization problem satisfies all these landscape properties.2) For the square loss function of deep linear networks, we establish a full (necessary and sufficient) characterization of the analytical forms for its critical points and global minimizers.', 'Such characterizations are new and have not been established in the existing art.', 'Furthermore, such analytical form divides the set of non-global-minimum critical points into different categories.', 'We identify the directions along which the loss function value decreases for two categories of the critical points, for which our result directly implies the equivalence between the local minimum and the global minimum.', 'For these cases, our proof generalizes the result in BID11 under no assumptions on the network parameter dimensions and data matrices.3) For the square loss function of one-hidden-layer nonlinear neural networks with ReLU activation function, we provide a full characterization of both the existence and the analytical forms of the critical points in certain types of regions in the parameter space.', 'Particularly, in the case where there is one hidden unit, our results fully characterize the existence and the analytical forms of the critical points in the entire parameter space.', 'Such characterization were not provided in previous work on nonlinear neural networks.', 'Moreover, we apply our results to a concrete example to demonstrate that both local minimum that is not a global minimum and local maximum do exist in such a case.', 'Analytical forms of critical points: Characterizing the analytical form of critical points for loss functions of neural networks dates back to BID1 , where the authors provided an analytical form of the critical points for the square loss function of linear networks with one hidden layer.', 'In BID13 , the authors provided a sufficient condition of critical points of a generic function, i.e., the fixed point of invariant groups.', 'They then characterized certain sufficient forms of critical points for the square loss function of matrix factorization problems and deep linear networks, whereas our results provide sufficient and necessary forms of critical points for deep linear networks via a different approach.', 'Properties of critical points: BID1 ; BID0 studied the linear autoencoder with one hidden layer and showed the equivalence between the local minimum and the global minimum.', 'Moreover, BID2 generalized these results to the complex-valued autoencoder setting.', 'The deep linear networks were studied by some recent work BID11 ; BID14 Yun et al. (2018) , in which the equivalence between the local minimum and the global minimum was established respectively under different assumptions.', 'Particularly, Yun et al. (2017) established a necessary and sufficient condition for a critical point of the deep linear network to be a global minimum.', 'A similar result was established in BID7 for deep linear networks under the setting that the widths of intermediate layers are larger than those of the input and output layers.', 'The effect of regularization on the critical points for a two-layer linear network was studied in Taghvaei et al. (2017) .For nonlinear neural networks, Yu & Chen (1995) studied a nonlinear neural network with one hidden layer and sigmoid activation function, and showed that every local minimum is also a global minimum provided that the number of input units equals the number of data samples.', 'BID9 considered a class of multi-layer nonlinear networks with a pyramidal structure, and showed that all critical points of full column rank achieve the zero loss when the sample size is less than the input dimension.', 'These results were further generalized to a larger class of nonlinear networks in BID15 , in which they also showed that critical points with non-degenerate Hessian are global minimum.', 'BID3 b) connected the loss surface of deep nonlinear networks with the Hamiltonian of the spin-glass model under certain assumptions and characterized the distribution of the local minimum.', 'BID11 further eliminated some of the assumptions in BID3 , and established the equivalence between the local minimum and the global minimum by reducing the loss function of the deep nonlinear network to that of the deep linear network.', 'BID17 showed that a two-layer nonlinear network has no bad differentiable local minimum.', 'BID6 studied a one-hidden-layer nonlinear neural network with the parameters restricted in a set of directions of lines, and showed that most local minima are global minima.', 'Tian (2017) considered a two-layer ReLU network with Gaussian input data, and showed that critical points in certain region are non-isolated and characterized the critical-point-free regions.', 'Geometric curvature BID10 established the gradient dominance condition of deep linear residual networks, and Zhou & Liang (2017) further established the gradient dominance condition and regularity condition around the global minimizers for deep linear, deep linear residual and shallow nonlinear networks.', 'BID12 studied the property of the Hessian matrix for deep linear residual networks.', 'The local strong convexity property was established in BID16 for overparameterized nonlinear networks with one hidden layer and quadratic activation functions, and was established in Zhong et al. (2017) for a class of nonlinear networks with one hidden layer and Gaussian input data.', 'Zhong et al. (2017) further established the local linear convergence of gradient descent method with tensor initialization.', 'BID18 studied a one-hidden-layer nonlinear network with a single output, and showed that the volume of sub-optimal differentiable local minima is exponentially vanishing in comparison with the volume of global minima.', 'BID5 investigated the saddle points in deep neural networks using the results from statistical physics and random matrix theory.', 'Notation: The pseudoinverse, column space and null space of a matrix M are denoted by M † , col(M ) and ker(M ), respectively.', 'For any index sets I, J ⊂ N, M I,J denotes the submatrix of M formed by the entries with the row indices in I and the column indices in J. For positive integers i ≤ j, we define i : j = {i, i + 1, . . . , j − 1, j}. The projection operator onto a linear subspace V is denoted by P V .', 'In this section, we study linear neural networks with one hidden layer.', 'Suppose we have an input data matrix X ∈ R d0×m and a corresponding output data matrix Y ∈ R d2×m , where there are in total m data samples.', 'We are interested in learning a model that maps from X to Y via a linear network with one hidden layer.', 'Specifically, we denote the weight parameters between the output layer and the hidden layer of the network as A 2 ∈ R d2×d1 , and denote the weight parameters between the hidden layer and the input layer of the network as A 1 ∈ R d1×d0 .', 'We are interested in the square loss function of this linear network, which is given by DISPLAYFORM0 Note that in a special case where X = I, L reduces to a loss function for the matrix factorization problem, to which all our results apply.', 'The loss function L has been studied in BID1 under the assumptions that d 2 = d 0 ≥ d 1 and the matrices XX , Y X (XX ) −1 XY are invertible.', 'In our study, no assumption is made on either the parameter dimensions or the invertibility of the data matrices.', 'Such full generalization of the results in BID1 turns out to be critical for our study of nonlinear shallow neural networks in Section 4.We further define Σ := Y X † XY and denote its full singular value decomposition as U ΛU .', 'Suppose that Σ has r distinct positive singular values σ 1 > · · · > σ r > 0 with multiplicities m 1 , . . .', ', m r , respectively, and hasm zero singular values.', 'Recall that DISPLAYFORM1 Our first result provides a full characterization of all critical points of L. Theorem 1 (Characterization of critical points).', 'All critical points of L are necessarily and sufficiently characterized by a matrix L 1 ∈ R d1×d0 , a block matrix V ∈ R d2×d1 and an invertible matrix C ∈ R d1×d1 via DISPLAYFORM2 (2) DISPLAYFORM3 , where both V i ∈ R mi×pi and V ∈ Rm ×p consist of orthonormal columns with the number of columns DISPLAYFORM4 Theorem 1 characterizes the necessary and sufficient forms for all critical points of L. Intuitively, the matrix C captures the invariance of the product A 2 A 1 under an invertible transform, and L 1 captures the degree of freedom of the solution set for linear systems.', 'In general, the set of critical points is uncountable and cannot be fully listed out.', 'However, the analytical forms in eqs. (1) and (2) do allow one to construct some critical points of L by specifying choices of L 1 , V , C that fulfill the condition in eq. (3).', 'For example, choosing L 1 = 0 guarantees eq. (3), in which case eqs. (1) and (2) yield a critical point (C −1 V U Y X † , U V C) for any invertible matrix C and any block matrix V that takes the form specified in Theorem 1.', 'For nonzero L 1 , one can fix a proper V and solve the linear equation on C in eq. (3).', 'If a solution exists, we then obtain the form of a corresponding critical point.', 'We further note that the analytical structures of the critical points are more important, which have direct implications on the global optimality conditions and landscape properties as we show in the remaining part of the section.', 'Remark 1.', 'We note that the block pattern parameters {p i } r i=1 andp denote the number of columns of {V i } r i=1 and V , respectively, and their sum equals the rank of A 2 , i.e., DISPLAYFORM5 The parameters p i , i = 1, . . .', ', r,p of V contain all useful information of the critical points that determine the function value of L as presented in the following proposition.', 'DISPLAYFORM6 Proposition 1 evaluates the function value L at a critical point using the parameters {p i } r i=1 .', 'To explain further, recall that the data matrix Σ has each singular value σ i with multiplicity m i .', 'For each i, the critical point captures p i out of m i singular values σ i .', 'Hence, for a σ i with larger value (i.e., a smaller index i), it is desirable that a critical point captures a larger number p i of them.', 'In this way, the critical point captures more important principle components of the data so that the value of the loss function is further reduced as suggested by Proposition 1.', 'In summary, the parameters {p i } r i=1 characterize how well the learned model fits the data in terms of the value of the loss function.', 'Moreover, the parameters {p i } r i=1 also determine a full characterization of the global minimizers as given below.', 'Proposition 2 (Characterization of global minimizers).', 'A critical point (A 1 , A 2 ) of L is a global minimizer if and only if it falls into the following two cases.', 'DISPLAYFORM7 The analytical form of any global minimizer can be obtained from Theorem 1 with further specification to the above two cases.', 'Proposition 2 establishes the neccessary and sufficient conditions for any critical point to be a global minimizer.', 'If the data matrix Σ has a large number of nonzero singular values, i.e., the first case, one needs to exhaust the representation budget (i.e., rank) of A 2 and capture as many large singular values as the rank allows to achieve the global minimum; Otherwise, A 2 of a global minimizer can be non-full rank and still captures all nonzero singular values.', 'Note that A 2 must be full rank in the case 1, and so is A 1 if we further adopt the assumptions on the network size and data matrices in BID1 .', 'Furthermore, the parameters {p i } r i=1 naturally divide all non-global-minimum critical points (A 1 , A 2 ) of L into the following two categories.• (Non-optimal order): The matrix V specified in Theorem 1 satisfies that there exists 1 ≤ i < j ≤ r such that p i < m i and p j > 0.• (Optimal order): rank(A 2 ) < min{d 2 , d 1 } and the matrix V specified in Theorem 1 satisfies that DISPLAYFORM8 To understand the above two categories, note that a critical point of L with non-optimal order captures a smaller singular value σ j (since p j > 0) while skipping a larger singular value σ i with a lower index i < j (since p i < m i ), and hence cannot be a global minimizer.', 'On the other hand, although a critical point of L with optimal order captures the singular values in the optimal (i.e., decreasing) order, it does not fully utilize the representation budget of A 2 (because A 2 is non-full rank) to further capture nonzero singular values and reduce the function value, and hence cannot be a global minimizer either.', 'Next, we show that these two types of non-global-minimum critical points have different landscape properties around them.', 'Throughout, a matrix M is called the perturbation of M if it lies in an arbitrarily small neighborhood of M .Proposition 3 (Landscape around critical points).', 'The critical points of L have the following landscape properties.1.', 'A non-optimal-order critical point (A 1 , A 2 ) has a perturbation ( A 1 , A 2 ) with rank( A 2 ) = rank(A 2 ), which achieves a lower function value; 2.', 'An optimal-order critical point (A 1 , A 2 ) has a perturbation ( A 1 , A 2 ) with rank( A 2 ) = rank(A 2 ) + 1, which achieves a lower function value; 3.', 'Any point in X := {(A 1 , A 2 ) : A 2 A 1 X = 0} has a perturbation (A 1 , A 2 ), which achieves a higher function value;As a consequence, items 1 and 2 imply that any non-global-minimum critical point has a descent direction, and hence cannot be a local minimizer.', 'Thus, any local minimizer must be a global minimizer.', 'Item 3 implies that any point has an ascent direction whenever the output is nonzero.', 'Hence, there does not exist any local/global maximizer in X .', 'Furthermore, item 3 together with items 1 and 2 implies that any non-global-minimum critical point in X has both descent and ascent directions, and hence must be a saddle point.', 'We summarize these facts in the following theorem.', 'Theorem 2 (Landscape of L).', 'The loss function L satisfies: 1) every local minimum is also a global minimum; 2) every non-global-minimum critical point in X is a saddle point.', 'We note that the saddle points in Theorem 2 can be non-strict when the data matrices are singular.', 'As an illustrative example, consider the following loss function of a shallow linear network L(a 2 , a 1 ) = 1 2 (a 2 a 1 x − y) 2 , where a 1 , a 2 , x and y are all scalars.', 'Consider the case y = 0.', 'Then, the Hessian at the saddle point a 1 = 0, a 2 = 1 is [x 2 , 0; 0, 0], which does not have any negative eigenvalue.', 'From a technical point of view, the proof of item 1 of Proposition 3 applies that in BID0 and generalizes it to the setting where Σ can have repeated singular values and may not be invertible.', 'To further understand the perturbation scheme from a high level perspective, note that non-optimalorder critical points capture a smaller singular value σ j instead of a larger one σ i with i < j. Thus, one naturally perturbs the singular vector corresponding to σ j along the direction of the singular vector corresponding to σ i .', 'Such a perturbation scheme preserves the rank of A 2 and reduces the value of the loss function.', 'More importantly, the proof of item 2 of Proposition 3 introduces a new technique.', 'As a comparison, BID1 proves a similar result as item 2 using the strict convexity of the function, which requires the parameter dimensions to satisfy d 2 = d 0 ≥ d 1 and the data matrices to be invertible.', 'In contrast, our proof completely removes these restrictions by introducing a new perturbation direction and exploiting the analytical forms of critical points in eqs. (1) and (2) and the condition in eq. (3).', 'The accomplishment of the proof further requires careful choices of perturbation parameters as well as judicious manipulations of matrices.', 'We refer the reader to the supplemental materials for more details.', 'As a high level understanding, since optimal-order critical points capture the singular values in an optimal (i.e., decreasing) order, the previous perturbation scheme for non-optimal-order critical points does not apply.', 'Instead, we increase the rank of A 2 by one in a way that the perturbed matrix captures the next singular value beyond the ones that have already been captured so that the value of the loss function can be further reduced.', 'In this section, we study deep linear networks with ≥ 2 layers.', 'We denote the weight parameters between the layers as A k ∈ R d k ×d k−1 for k = 1, . . .', ', , respectively.', 'The input and output data are denoted by X ∈ R d0×m , Y ∈ R d ×m , respectively.', 'We are interested in the square loss function of deep linear networks, which is given by DISPLAYFORM0 , respectively, andm(k) zero singular values.', 'Our first result provides a full characterization of all critical points of L D , where we denote DISPLAYFORM1 Theorem 3 (Characterization of critical points).', 'All critical points of L D are necessarily and sufficiently characterized by matrices DISPLAYFORM2 . .', ', A can be individually expressed out recursively via the following two equations: DISPLAYFORM3 DISPLAYFORM4 Note that the forms of the individual parameters A 1 , . . .', ', A can be obtained as follows by recursively applying eqs. (4) and (5).', 'First, eq. (5) with k = 0 yields the form of A ( ,2) .', 'Then, eq. (4) with k = 0 and the form of A ( ,2) yield the form of A 1 .', 'Next, eq. (5) with k = 1 yields the form of A ( ,3) , and then, eq. (4) with k = 1 and the forms of A ( ,3) , A 1 further yield the form of A 2 .', 'Inductively, one obtains the expressions of all individual parameter matrices.', 'Furthermore, the first condition in eq. FORMULA13 is a consistency condition that guarantees that the analytical form for the entire product of parameter matrices factorizes into the forms of individual parameter matrices.', 'Similarly to shallow linear networks, while the set of critical points here is also uncountable, Theorem 3 suggests ways to obtain some critical points.', 'For example, if we set L k = 0 for all k (i.e., eq. (6) is satisfied), we can obtain the form of critical points for any invertible C k and proper V k with the structure specified in Theorem 3.', 'For nonzero L k , eq. (6) needs to be verified for given C k and V k to determine a critical point.', 'Similarly to shallow linear networks, the parameters {p i (0)} r (0) i=1 ,p(0) determine the value of the loss function at the critical points and further specify the analytical form for the global minimizers, as we present in the following two propositions.', 'DISPLAYFORM5 DISPLAYFORM6 In particular, A ( ,2) can be non-full rank with rank(A ( ,2) ) = DISPLAYFORM7 The analytical form of any global minimizer can be obtained from Theorem 3 with further specification to the above two cases.', 'In particular for case 1, if we further adopt the invertibility assumptions on data matrices as in BID1 and assume that all parameter matrices are square, then all global minima must correspond to full rank parameter matrices.', 'We next exploit the analytical forms of the critical points to further understand the landscape of the loss function L D .', 'It has been shown in BID11 that every local minimum of L D is also a global minimum, under certain conditions on the parameter dimensions and the invertibility of the data matrices.', 'Here, our characterization of the analytical forms for the critical points allow us to understand such a result from an alternative viewpoint.', 'The proofs for certain cases (that we discuss below) are simpler and more intuitive, and no assumption is made on the data matrices and dimensions of the network.', 'Similarly to shallow linear networks, we want to understand the local landscape around the critical points.', 'However, due to the effect of depth, the critical points of L D are more complicated than those of L. Among them, we identify the following subsets of the non-global-minimum critical DISPLAYFORM8 • (Deep-non-optimal order): There exist 0 ≤ k ≤ − 2 such that the matrix V k specified in Theorem 3 satisfies that there exist 1 ≤ i < j ≤ r(k) such that p i (k) < m i (k) and p j (k) > 0.• (Deep-optimal order): (A , A −1 ) is not a global minimizer of L D with A ( −2,1) being fixed, rank(A ) < min{d , d −1 }, and the matrix V −2 specified in Theorem 3 satisfies that DISPLAYFORM9 The following result summarizes the landscape of L D around the above two types of critical points.', 'The loss function L D has the following landscape properties.', 'deep-non-optimal-order critical point (A 1 , . . . , A ) has a perturbation (A 1 , . . .', ', A k+1 , . . .', ', A ) with rank( A ) = rank(A ), which achieves a lower function value.', '2.', 'A deep-optimal-order critical point (A 1 , . . . , A ) has a perturbation (A 1 , . . .', ', A −1 , A ) with rank( A ) = rank(A ) + 1, which achieves a lower function value.', '3.', 'Any point in X D := {(A 1 , . . .', ', A ) : A ( ,1) X = 0} has a perturbation (A 1 , . . .', ', A ) that achieves a higher function value.', 'Consequently, 1) every local minimum of L D is also a global minimum for the above two types of critical points; and 2) every critical point of these two types in X D is a saddle point.', 'Theorem 4 implies that the landscape of L D for deep linear networks is similar to that of L for shallow linear networks, i.e., the pattern of the parameters {p i (k)} r(k) i=1 implies different descent directions of the function value around the critical points.', 'Our approach does not handle the remaining set of non-global minimizers, i.e., there exists q ≤ −1 such that (A , . . .', ', A q ) is a global minimum point of L D with A (q−1,1) being fixed, and A ( ,q) is of optimal order.', 'It is unclear how to perturb the intermediate weight parameters using their analytical forms for deep networks , and we leave this as an open problem for the future work.', 'In this section, we study nonlinear neural networks with one hidden layer.', 'In particular, we consider nonlinear networks with ReLU activation function σ : R → R that is defined as σ(x) := max{x, 0}. Our study focuses on the set of differentiable critical points.', 'The weight parameters between the layers are denoted by A 2 ∈ R d2×d1 , A 1 ∈ R d1×d0 , respectively, and the input and output data are denoted by X ∈ R d0×m , Y ∈ R d2×m , respectively.', 'We are interested in the square loss function which is given by DISPLAYFORM0 where σ acts on A 1 X entrywise.', 'Existing studies on nonlinear networks characterized the sufficient conditions for critical points being global minimum BID9 Since the activation function σ is piecewise linear, the entire parameter space can be partitioned into disjoint cones.', 'In particular, we consider the set of cones K I×J where I ⊂ {1, . . .', ', d 1 }, J ⊂ {1, . . .', ', m} that satisfy DISPLAYFORM1 where \"≥\" and \"<\" represent entrywise comparisons.', 'Within K I×J , the term σ(A 1 X) activates only the entries σ(A 1 X) I:J , and the corresponding loss function L N is equivalent to DISPLAYFORM2 Hence, within K I×J , L N reduces to the loss of a shallow linear network with parameters ((A 2 ) :,I , (A 1 ) I,: ) and input & output data pair (X :,J , Y :,J ).', 'Note that our results on shallow linear networks in Section 2 are applicable to all parameter dimensions and data matrices.', 'Thus, Theorem 1 fully characterizes the forms of critical points of L N in K I×J .', 'Moreover, the existence of such critical points can be analytically examined by substituting their forms into eq. (8).', 'In summary, we obtain the following result, where we denote Σ J := Y :,J X † :,J X :,J Y :,J with the full singular value decomposition U J Λ J U J , and suppose that Σ J has r(J) distinct positive singular values σ 1 (J) > · · · > σ r(J) (J) with multiplicities m 1 , . . .', ', m r(J) , respectively, andm(J) zero singular values.', 'Proposition 6 (Characterization of critical points).', 'All critical points of L N in K I×J for any I ⊂ {1, . . .', ', d 1 }, J ⊂ {1, . . .', ', m} are necessarily and sufficiently characterized by an L 1 ∈ R |I|×d0 , a block matrix V ∈ R d2×|I| and an invertible matrix C ∈ R |I|×|I| such that DISPLAYFORM3 DISPLAYFORM4 ×p consist of orthonormal columns with p i ≤ m i for i = 1, . . .', ', r(J),p ≤m such that DISPLAYFORM5 Moreover, a critical point in K I×J exists if and only if there exists such C, V , L 1 that DISPLAYFORM6 Other entries of A 1 X < 0.To further illustrate, we consider a special case where the nonlinear network has one unit in the hidden layer, i.e., d 1 = 1, in which case A 1 and A 2 are row and column vectors, respectively.', 'Then, the entire parameter space can be partitioned into disjoint cones taking the form of K I×J , and I = {1} is the only nontrivial choice.', 'We obtain the following result from Proposition 6.Proposition 7 (Characterization of critical points).', 'Consider L N with d 1 = 1 and any J ⊂ {1, . . .', ', m}. Then, any nonzero critical point of L N within K {1}×J can be necessarily and sufficiently characterized by an 1 ∈ R 1×d0 , a block unit vector v ∈ R d2×1 and a scalar c ∈ R such that DISPLAYFORM7 Specifically, v is a unit vector that is supported on the entries corresponding to the same singular value of Σ J .', 'Moreover, a nonzero critical point in K {1}×J exists if and only if there exist such c, v, 1 that satisfy DISPLAYFORM8 DISPLAYFORM9 We note that Proposition 7 characterizes both the existence and the forms of critical points of L N over the entire parameter space for nonlinear networks with a single hidden unit.', 'The condition in eq. FORMULA24 is guaranteed because P ker(v) = 0 for v = 0.To further understand Proposition 7, suppose that there exists a critical point in K {1}×J with v being supported on the entries that correspond to the i-th singular value of Σ J .', 'Then, Proposition 1 implies that DISPLAYFORM10 In particular, the critical point achieves the local minimum DISPLAYFORM11 .', 'This is because in this case the critical point is full rank with an optimal order, and hence corresponds to the global minimum of the linear network in eq. (9).', 'Since the singular values of Σ J may vary with the choice of J, L N may achieve different local minima in different cones.', 'Thus, local minimum that is not global minimum can exist for L N .', 'The following proposition concludes this fact by considering a concrete example.', 'Proposition 8.', 'For one-hidden-layer nonlinear neural networks with ReLU activation function, there exists local minimum that is not global minimum, and there also exists local maximum.', 'FORMULA13 and FORMULA19 hold if c −1 (v) 1,: ≥ 0, ( 1 ) 1,: < 0.', 'Similarly to the previous case, choosing c = 1, v = (1, 0) , 1 = (−1, 0) yields a local minimum that achieves the function value L n = 2.', 'Hence, local minimum that is not global minimum does exist.', 'Moreover, in the cone K I×J with I = {1}, J = ∅, the function L N remains to be the constant 5 2 , and all points in this cone are local minimum or local maximum.', 'Thus, the landscape of the loss function of nonlinear networks is very different from that of the loss function of linear networks.', 'In this paper, we provide full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks, namely, shallow linear networks, deep linear networks, and shallow ReLU nonlinear networks.', 'We show that such analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points.', 'As a consequence, the loss function for linear networks has no spurious local minimum, while such point does exist for nonlinear networks with ReLU activation.', 'In the future, it is interesting to further explore nonlinear neural networks.', 'In particular, we wish to characterize the analytical form of critical points for deep nonlinear networks and over the full parameter space.', 'Such results will further facilitate the understanding of the landscape properties around these critical points.', 'Notations: For any matrix M , denote vec(M ) as the column vector formed by stacking its columns.', 'Denote the Kronecker product as \"⊗\".', 'Then, the following useful relationships hold for any dimension compatible matrices M , U , V , W : DISPLAYFORM0 DISPLAYFORM1 DISPLAYFORM2 DISPLAYFORM3 Recall that a point DISPLAYFORM4 DISPLAYFORM5 We first prove eqs. (1) and (2) .', 'DISPLAYFORM6 Next, we derive the form of A 2 .', 'Recall the full singular value decomposition Σ = U ΛU , where Λ is a diagonal matrix with distinct singular values σ 1 > . . .', '> σ r > 0 and multiplicities m 1 , . . .', ', m r , respectively.', 'We also assume that there arem number of zero singular values in Λ. Using the fact that P col(A2) = U P col(U A2) U , the last equality in eq. (26) reduces to DISPLAYFORM7 By the multiplicity pattern of the singular values in Λ, P col(U A2) must be block diagonal.', 'Specifically, we can write P col(U A2) = diag( P 1 , . . .', ', P r , P), where P i ∈ R mi×mi and P ∈ Rm ×m .Also, since P col(U A2) is a projection, P 1 , . . . , P r , P must all be projections.', 'Note that P col(U A2) has rank rank(A 2 ), and suppose that P 1 , . . .', ', P r , P have ranks p 1 , . . .', ', p r ,p, respectively.', 'Then, we must have p i ≤ m i for i = 1, . . .', ', r,p ≤m and r i=1 p i +p = rank(A 2 ).', 'Also, note that each projection can be expressed as P i = V i V i with V i ∈ R mi×pi , V ∈ Rm ×p consisting of orthonormal columns.', 'Hence, we can write P col(U A2) = V V where V = diag(V 1 , . . .', ', V r , V ).', 'We then conclude that P col(A2) = U P col(U A2) U = U V V U .', 'Thus, A 2 has the same column space as U V , and there must exist an invertible matrix DISPLAYFORM8 Then, plugging A † 2 = C −1 V U into eq. (25) yields the desired form of A 1 .We now prove eq. (3).', 'Note that the above proof is based on the equations DISPLAYFORM9 Hence, the forms of A 1 , A 2 in eqs. (1) and (2) need to further satisfy ∇ A2 L = 0.', 'By eq. FORMULA19 and the form of A 2 , we obtain that DISPLAYFORM10 This expression, together with the form of A 1 in eq. (1), implies that DISPLAYFORM11 where (i) uses the fact that X † XX = X , (ii) uses the fact that the block pattern of V is compatible with the multiplicity pattern of the singular values in Λ, and hence V V ΛV = ΛV .', 'On the other hand, we also obtain that DISPLAYFORM12 Thus, to satisfy ∇ A2 L = 0 in eq. FORMULA12 , we require that DISPLAYFORM13 which is equivalent to DISPLAYFORM14 Lastly, note that (I − U V (U V ) ) = P col(U V ) ⊥ , and (I − V V ) = P ker(V ) , which concludes the proof.', 'By expansion we obtain that L = DISPLAYFORM0 .', 'Consider any (A 1 , A 2 ) that satisfies eq. FORMULA4 , we have shown that such a point also satisfies eq. (27), which further yields that DISPLAYFORM1 where (i) follows from the fact that Tr( P col(A2) Σ P col(A2) ) = Tr( P col(A2) Σ), and (ii) uses the fact that P col(A2) = U P col(U A2) U .', 'In particular, a critical point (A 1 , A 2 ) satisfies eq. (28).', 'Moreover, using the form of the critical point A 2 = U V C, eq. FORMULA20 further becomes DISPLAYFORM2 where (i) is due to P col(V C) = P col(V ) = V V , and (ii) utilizes the block pattern of V and the multiplicity pattern of Λ that are specified in Theorem 1.', '(1): Consider a critical point (A 1 , A 2 ) with the forms given by Theorem 1.', 'By choosing L 1 = 0, the condition in eq. FORMULA4 is guaranteed.', 'Then, we can specify a critical point with any V that satisfies the block pattern specified in Theorem 1, i.e., we can choose any p i , i = 1, . . .', ', r,p such that p i ≤ m i for i = 1, . . .', ', r,p ≤m and DISPLAYFORM0 m i , the global minimum value is achieved by a full rank A 2 with rank(A 2 ) = min{d 2 , d 1 } and DISPLAYFORM1 That is, the singular values are selected in a decreasing order to minimize the function value.(2): If (A 2 , A 1 ) is a global minimizer and min{d y , d} > r i=1 m i , the global minimum can be achieved by choosing p i = m i for all i = 1, . . .', ', r andp ≥ 0.', 'In particular, we do not need a full rank A 2 to achieve the global minimum.', 'For example, we can choose rank(A 2 ) = r i=1 m i < min{d y , d} with p i = m i for all i = 1, . . .', ', r andp = 0.', 'We first prove item 1.', 'Consider a non-optimal-order critical point (A 1 , A 2 ).', 'By Theorem 1, we can write A 2 = U V C where V = [diag(V 1 , . . .', ', V r , V ), 0] and V i , i = 1, . . .', ', r, V consist of orthonormal columns.', 'Define the orthonormal block diagonal matrix Since (A 1 , A 2 ) is a non-optimal-order critical point, there exists 1 ≤ i < j ≤ r such that p i < m i and p j > 0.', 'Then, consider the following perturbation of U S for some > 0.', 'DISPLAYFORM0 DISPLAYFORM1 with which we further define the perturbation matrix A 2 = M S V C. Also, let the perturbation matrix A 1 be generated by eq. (1) with U ← M and V ← S V .', 'Note that with this construction, ( A 1 , A 2 ) satisfies eq. (25), which further implies eq. (27) for ( A 1 , A 2 ), i.e., A 2 A 1 X = P col( A2) Y X † X. Thus, eq. (28) holds for the point ( A 1 , A 2 ), and we obtain that DISPLAYFORM2 where the last equality uses the fact that S ΛS = Λ, as can be observed from the block pattern of S and the multiplicity pattern of Λ. Also, by the construction of M and the form of S V , a careful calculation shows that only the i, j-th diagonal elements of P col(S U M S V ) have changed, i.e., DISPLAYFORM3 As the index i, j correspond to the singular values σ i , σ j , respectively, and σ i > σ j , one obtain that DISPLAYFORM4 Thus, the construction of the point ( A 2 , A 1 ) achieves a lower function value for any > 0.', 'Letting → 0 and noticing that M is a perturbation of U S, the point ( A 2 , A 1 ) can be in an arbitrary neighborhood of (A 2 , A 1 ).', 'Lastly, note that rank( A 2 ) = rank(A 2 ).', 'This completes the proof of item 1.Next, we prove item 2.', 'Consider an optimal-order critical point (A 1 , A 2 ).', 'Then, A 2 must be non-full rank, since otherwise a full rank A 2 with optimal order corresponds to a global minimizer by Proposition 2.', 'Since there exists some k ≤ r such that 0] .', 'Using this expression, eq. (1) yields that DISPLAYFORM5 DISPLAYFORM6 We now specify our perturbation scheme.', 'Recalling the orthonormal matrix S defined in eq. (29).', 'Then, we consider the following matrices for some 1 , 2 > 0 DISPLAYFORM7 For this purpose, we need to utilize the condition of critical points in eq. (3), which can be equivalently expressed as DISPLAYFORM8 (ii) ⇔ (CL 1 ) (rank(A2)+1):d1,: XY (I − U S :,1:(q−1) (U S :,1:(q−1) ) ) = 0where (i) follows by taking the transpose and then simplifying, and (ii) uses the fact that V = SS V = S :,1:(q−1) in the case of optimal-order critical point.', 'Calculating the function value at ( A 1 , A 2 ), we obtain that DISPLAYFORM9 .', 'We next simplify the above three trace terms using eq. (31).', 'For the first trace term, observe that DISPLAYFORM10 2 Tr(S :,q ΛS :,q ) where (i) follows from eq. (31) as S :,q is orthogonal to the columns of S :,1:(q−1) .', 'For the second trace term, we obtain that DISPLAYFORM11 = 2Tr( 2 U S :,q (CL 1 ) (rank(A2)+1),: XY U V diag (U V diag ) ) + 2Tr( 1 2 U S :,q S :,q ΛSS V diag (U V diag ) ) (i) = 2Tr( 2 U S :,q (CL 1 ) (rank(A2)+1),: XY U V diag (U V diag ) ) + 2Tr( 1 2 σ k U S :,q e q S V diag (U V diag ) )(ii) = 2Tr( 2 U S :,q (CL 1 ) (rank(A2)+1),: XY U V diag (U V diag ) ), where (i) follows from S :,q ΛS = σ k e q , and (ii) follows from e q S V diag = 0.', 'For the third trace term, we obtain that 2Tr(P Y ) = 2Tr( 2 U S :,q (CL 1 ) (rank(A2)+1),: XY ) + 2Tr( 1 2 U S :,q (U S :,q ) Σ) = 2Tr( 2 U S :,q (CL 1 ) (rank(A2)+1),: XY ) + 2Tr( 1 2 S :,q ΛS :,q ).Combining the expressions for the three trace terms above, we conclude that Consider a critical point (A 1 , . . .', ', A ) so that eq. FORMULA4', 'Observe that the product matrix A ( ,2) is equivalent to the class of matrices B 2 ∈ R min{d ,...,d2}×d1 .Consider a critical point (B 2 , A 1 ) of the shallow linear network L :=', 'The proof is similar to that for shallow linear networks.', 'Consider a deep-non-optimal-order critical point (A 1 , . . .', ', A ), and define the orthonormal block matrix S k using the blocks of V k in a similar way as eq. (29).', 'Then, A (l,k+2) takes the form A (l,k+2) = U k S k S k V k C k .', 'Since A (l,k+2) is of non-optimal order, there exists i < j < r(k) such that p i (k) < m i (k) and p j (k) > 0.', 'Thus, we perturb the j-th column of U k S k to be , and denote the resulting matrix as M k .Then, we perturb A to be A = M k (U k S k ) A so that A A ( −1,k+2) = M k S k V k C k .', 'Moreover, we generate A k+1 by eq. (4) with U k ← M k , V k ← S k V k .', 'Note that such construction satisfies eq. (32), and hence also satisfies eq. (34), which further yields that DISPLAYFORM0 With the above equation, the function value at this perturbed point is evaluated as DISPLAYFORM1 Then, a careful calculation shows that only the i, j-th diagonal elements of DISPLAYFORM2 have changed, and are Now consider a deep-optimal-order critical point (A 1 , . . .', ', A ).', 'Note that with A ( −2,1) fixed to be a constant, the deep linear network reduces to a shallow linear network with parameters (A , A −1 ).', 'Since (A , A −1 ) is not a non-global minimum critical point of this shallow linear network and A is of optimal-order, we can apply the perturbation scheme in the proof of Proposition 3 to identify a perturbation ( A , A −1 ) with rank( A ) = rank(A ) + 1 that achieves a lower function value.', 'Consider any point in X D .', 'Since A ( ,1) X = 0, we can scale the nonzero row, say, the i-th row (A ) i,: A ( −1,1) X properly in the same way as that in the proof of Proposition 3 to increase the function value.', 'Lastly, item 1 and item 2 imply that every local minimum is a global minimum for these two types of critical points.', 'Moreover, combining items 1,2 and 3, we conclude that every critical point of these two types in X D is a saddle point.']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "jsonl_file_path = BASE_PATH + 'TLDR/train.jsonl'\n",
        "\n",
        "# Read the JSON Lines file into a list of dictionaries\n",
        "data_list = []\n",
        "with open(jsonl_file_path, 'r') as jsonl_file:\n",
        "    for line in jsonl_file:\n",
        "        data_dict = json.loads(line)\n",
        "        data_list.append(data_dict)\n",
        "\n",
        "# Convert the list of dictionaries to a Pandas DataFrame\n",
        "df = pd.DataFrame(data_list)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df.head())\n",
        "print(df['source'][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['source'] = df['source'].apply(lambda x : ' '.join(x))\n",
        "df['target'] = df['target'].apply(lambda x : ' '.join(x))\n",
        "\n",
        "no_need_columns = ['source_labels', 'rouge_scores']\n",
        "df.drop(columns=no_need_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PvhhDOSFsRsb",
        "outputId": "49319f6d-b810-434c-bdf9-23b17dc9b040"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source    paper_id  \\\n",
              "0     Due to the success of deep learning to solving...   SysEexbRb   \n",
              "1     The backpropagation (BP) algorithm is often th...  SygvZ209F7   \n",
              "2     We introduce the 2-simplicial Transformer, an ...  rkecJ6VFvr   \n",
              "3     We present Tensor-Train RNN (TT-RNN), a novel ...   HJJ0w--0W   \n",
              "4     Recent efforts on combining deep models with p...   HyH9lbZAW   \n",
              "...                                                 ...         ...   \n",
              "1987  Semi-supervised learning, i.e. jointly learnin...  rJel41BtDH   \n",
              "1988  Model-free reinforcement learning (RL) has bee...   Skw0n-W0Z   \n",
              "1989  We introduce a neural architecture to perform ...  rJgFtkhEtr   \n",
              "1990  Machine learned large-scale retrieval systems ...  SJxPVcSonN   \n",
              "1991  The ability to autonomously explore and naviga...  BJgMFxrYPB   \n",
              "\n",
              "                                                 target  \n",
              "0     We provide necessary and sufficient analytical...  \n",
              "1     Biologically plausible learning algorithms, pa...  \n",
              "2     We introduce the 2-simplicial Transformer and ...  \n",
              "3     Accurate forecasting over very long time horiz...  \n",
              "4     We propose a variational message-passing algor...  \n",
              "...                                                 ...  \n",
              "1987  Pseudo-labeling has shown to be a weak alterna...  \n",
              "1988  We show that a special goal-condition value fu...  \n",
              "1989  A novel neural architecture for efficient amor...  \n",
              "1990  We propose a novel two-tower shared-bottom mod...  \n",
              "1991  We address the task of autonomous exploration ...  \n",
              "\n",
              "[1992 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9fc0df21-8967-4ec4-b372-d3c2ca823552\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Due to the success of deep learning to solving...</td>\n",
              "      <td>SysEexbRb</td>\n",
              "      <td>We provide necessary and sufficient analytical...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The backpropagation (BP) algorithm is often th...</td>\n",
              "      <td>SygvZ209F7</td>\n",
              "      <td>Biologically plausible learning algorithms, pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We introduce the 2-simplicial Transformer, an ...</td>\n",
              "      <td>rkecJ6VFvr</td>\n",
              "      <td>We introduce the 2-simplicial Transformer and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We present Tensor-Train RNN (TT-RNN), a novel ...</td>\n",
              "      <td>HJJ0w--0W</td>\n",
              "      <td>Accurate forecasting over very long time horiz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Recent efforts on combining deep models with p...</td>\n",
              "      <td>HyH9lbZAW</td>\n",
              "      <td>We propose a variational message-passing algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>Semi-supervised learning, i.e. jointly learnin...</td>\n",
              "      <td>rJel41BtDH</td>\n",
              "      <td>Pseudo-labeling has shown to be a weak alterna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>Model-free reinforcement learning (RL) has bee...</td>\n",
              "      <td>Skw0n-W0Z</td>\n",
              "      <td>We show that a special goal-condition value fu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>We introduce a neural architecture to perform ...</td>\n",
              "      <td>rJgFtkhEtr</td>\n",
              "      <td>A novel neural architecture for efficient amor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>Machine learned large-scale retrieval systems ...</td>\n",
              "      <td>SJxPVcSonN</td>\n",
              "      <td>We propose a novel two-tower shared-bottom mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>The ability to autonomously explore and naviga...</td>\n",
              "      <td>BJgMFxrYPB</td>\n",
              "      <td>We address the task of autonomous exploration ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1992 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9fc0df21-8967-4ec4-b372-d3c2ca823552')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9fc0df21-8967-4ec4-b372-d3c2ca823552 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9fc0df21-8967-4ec4-b372-d3c2ca823552');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0e90d781-bb18-4a59-a9b1-00a45bfd5e54\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0e90d781-bb18-4a59-a9b1-00a45bfd5e54')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0e90d781-bb18-4a59-a9b1-00a45bfd5e54 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['count_target'] = df['target'].apply(lambda x : len(x.split()))\n",
        "df['count_source'] = df['source'].apply(lambda x : len(x.split()))"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df[df['count_target']>=15]\n",
        "aux.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGXcUp0zqxwS",
        "outputId": "4ed96f74-ed56-467f-e0e9-3df69a18e97f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1433, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['count_target'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPi1WBg6r7at",
        "outputId": "ba8dd36f-18af-4f6a-92c3-78e077d1502b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51.78836833602585"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bix9GOPvvYx1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuh/lat9qTLshUfnaUR3ee",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}