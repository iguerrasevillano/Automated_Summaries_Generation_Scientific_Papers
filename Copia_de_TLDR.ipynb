{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/clean_version/Copia_de_TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "2cb27249-3ca1-4229-d96a-021e6537acc7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=6eed7773d7501e3b9c01a31c15754bdd44c6c887e499868a31e3a90ffbfb1a8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.62.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "2c23c62724134ade82fe013c6484da03",
            "ca9091b173674a11b509e568b4b3d132",
            "9f0238f70c174f79bc00581e9a7d8c5f",
            "f3d75633840c45be84b00fb9a8a9648f",
            "7eab4d7b2def43e9857e757f4f37b101",
            "7c2e80b91f3c4eceafbddd8999520720",
            "fcdfcaf0e592434181ed5b671891a82e",
            "09eff1aeffeb41f1bcaea0b370c8afd9",
            "9553d2575f8544069c68e5fe2ac878d9",
            "49783bb6665941d6b38faf0ee01c9ca2",
            "4cd66c6c840642b08397fce4698dcdf2"
          ]
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "75dba026-30bf-494b-dfad-6c0960e8e566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<ipython-input-1-d54cd7d9c1fe>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c23c62724134ade82fe013c6484da03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import BartTokenizer, T5Tokenizer, pipeline, TFT5ForConditionalGeneration\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "ccdff799-b4f7-4777-c1df-1431e8738c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "  def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "1I5H39lnQW9o",
        "outputId": "6b5069bb-79bb-4b59-bd8c-8f3ff0f0caf3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source    paper_id  \\\n",
              "256   Knowledge Bases (KBs) are becoming increasingl...   HkyI-5667   \n",
              "904   RMSProp and ADAM continue to be extremely popu...  rkgd0iA9FQ   \n",
              "1208  Principal Filter Analysis (PFA) is an easy to ...  rkl42iA5t7   \n",
              "522   While Bayesian optimization (BO) has achieved ...   SknC0bW0-   \n",
              "429   We study the problem of representation learnin...  H1emus0qF7   \n",
              "\n",
              "                                                 target  \\\n",
              "256   Probabilistic Rule Learning system using Lifte...   \n",
              "904   In this paper we prove convergence to critical...   \n",
              "1208  We propose an easy to implement, yet effective...   \n",
              "522   We propose a Bayes-optimal Bayesian optimizati...   \n",
              "429   We translate a bound on sub-optimality of repr...   \n",
              "\n",
              "                                                  title  number_words_target  \\\n",
              "256   Scalable Rule Learning in Probabilistic Knowle...                   30   \n",
              "904   Convergence Guarantees for RMSProp and ADAM in...                   48   \n",
              "1208  NETWORK COMPRESSION USING CORRELATION ANALYSIS...                   90   \n",
              "522   Continuous-fidelity Bayesian Optimization with...                   34   \n",
              "429   Near-Optimal Representation Learning for Hiera...                   79   \n",
              "\n",
              "                                     extractive_summary  \n",
              "256   However, the ProbFOIL approach of De Raedt et ...  \n",
              "904   We show that at very high values of the moment...  \n",
              "1208  We propose two closed-form algorithms based on...  \n",
              "522   While Bayesian optimization (BO) has achieved ...  \n",
              "429   To study this problem, we develop a notion of ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89dca7de-d3d6-4d81-8e7d-f3e5f630ff6f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>Knowledge Bases (KBs) are becoming increasingl...</td>\n",
              "      <td>HkyI-5667</td>\n",
              "      <td>Probabilistic Rule Learning system using Lifte...</td>\n",
              "      <td>Scalable Rule Learning in Probabilistic Knowle...</td>\n",
              "      <td>30</td>\n",
              "      <td>However, the ProbFOIL approach of De Raedt et ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>RMSProp and ADAM continue to be extremely popu...</td>\n",
              "      <td>rkgd0iA9FQ</td>\n",
              "      <td>In this paper we prove convergence to critical...</td>\n",
              "      <td>Convergence Guarantees for RMSProp and ADAM in...</td>\n",
              "      <td>48</td>\n",
              "      <td>We show that at very high values of the moment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208</th>\n",
              "      <td>Principal Filter Analysis (PFA) is an easy to ...</td>\n",
              "      <td>rkl42iA5t7</td>\n",
              "      <td>We propose an easy to implement, yet effective...</td>\n",
              "      <td>NETWORK COMPRESSION USING CORRELATION ANALYSIS...</td>\n",
              "      <td>90</td>\n",
              "      <td>We propose two closed-form algorithms based on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>While Bayesian optimization (BO) has achieved ...</td>\n",
              "      <td>SknC0bW0-</td>\n",
              "      <td>We propose a Bayes-optimal Bayesian optimizati...</td>\n",
              "      <td>Continuous-fidelity Bayesian Optimization with...</td>\n",
              "      <td>34</td>\n",
              "      <td>While Bayesian optimization (BO) has achieved ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>We study the problem of representation learnin...</td>\n",
              "      <td>H1emus0qF7</td>\n",
              "      <td>We translate a bound on sub-optimality of repr...</td>\n",
              "      <td>Near-Optimal Representation Learning for Hiera...</td>\n",
              "      <td>79</td>\n",
              "      <td>To study this problem, we develop a notion of ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89dca7de-d3d6-4d81-8e7d-f3e5f630ff6f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-89dca7de-d3d6-4d81-8e7d-f3e5f630ff6f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-89dca7de-d3d6-4d81-8e7d-f3e5f630ff6f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8ba59a5f-c1d5-41e7-8abe-5de7ed231658\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ba59a5f-c1d5-41e7-8abe-5de7ed231658')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8ba59a5f-c1d5-41e7-8abe-5de7ed231658 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear. Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants. In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways. First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time. \\n\\n Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10. Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \\\\beta_1. We show that at very high values of the momentum parameter (\\\\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses. On the other hand, NAG can sometimes do better when ADAM's \\\\beta_1 is set to the most commonly used value: \\\\beta_1 = 0.9, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance. \\n\\n We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates. Many optimization questions arising in machine learning can be cast as a finite sum optimization problem of the form: min x f (x) where f (x) = 1 k k i=1 f i (x). Most neural network problems also fall under a similar structure where each function f i is typically non-convex. A well-studied algorithm to solve such problems is Stochastic Gradient Descent (SGD), which uses updates of the form: x t+1 := x t \\u2212 \\u03b1\\u2207f it (x t ), where \\u03b1 is a step size, andf it is a function chosen randomly from {f 1 , f 2 , . . . , f k } at time t. Often in neural networks, \\\"momentum\\\" is added to the SGD update to yield a two-step update process given as: v t+1 = \\u00b5v t \\u2212 \\u03b1\\u2207f it (x t ) followed by x t+1 = x t + v t+1 . This algorithm is typically called the Heavy-Ball (HB) method (or sometimes classical momentum), with \\u00b5 > 0 called the momentum parameter (Polyak, 1987) . In the context of neural nets, another variant of SGD that is popular is Nesterov's Accelerated Gradient (NAG), which can also be thought of as a momentum method (Sutskever et al., 2013) , and has updates of the form v t+1 = \\u00b5v t \\u2212 \\u03b1\\u2207f it (x t + \\u00b5v t ) followed by x t+1 = x t + v t+1 (see Algorithm 1 for more details).Momentum methods like HB and NAG have been shown to have superior convergence properties compared to gradient descent in the deterministic setting both for convex and non-convex functions (Nesterov, 1983; Polyak, 1987; Zavriev & Kostyuk, 1993; Ochs, 2016; O'Neill & Wright, 2017; Jin et al., 2017) . While (to the best of our knowledge) there is no clear theoretical justification in the stochastic case of the benefits of NAG and HB over regular SGD in general (Yuan et al., 2016; Kidambi et al., 2018; Wiegerinck et al., 1994; Orr & Leen, 1994; Yang et al., 2016; Gadat et al., 2018) , unless considering specialized function classes (Loizou & Richt\\u00e1rik, 2017) ; in practice, these momentum methods, and in particular NAG, have been repeatedly shown to have good convergence and generalization on a range of neural net problems (Sutskever et al., 2013; Lucas et al., 2018; Kidambi et al., 2018) .The performance of NAG (as well as HB and SGD), however, are typically quite sensitive to the selection of its hyper-parameters: step size, momentum and batch size (Sutskever et al., 2013) . Thus, \\\"adaptive gradient\\\" algorithms such as RMSProp (Algorithm 2) (Tieleman & Hinton, 2012) and ADAM (Algorithm 3) (Kingma & Ba, 2014) have become very popular for optimizing deep neural networks (Melis et al., 2017; Xu et al., 2015; Denkowski & Neubig, 2017; Gregor et al., 2015; Radford et al., 2015; Bahar et al., 2017; Kiros et al., 2015) . The reason for their widespread popularity seems to be the fact that they are believed to be easier to tune than SGD, NAG or HB. Adaptive gradient methods use as their update direction a vector which is the image under a linear transformation (often called the \\\"diagonal pre-conditioner\\\") constructed out of the history of the gradients, of a linear combination of all the gradients seen till now. It is generally believed that this \\\"pre-conditioning\\\" makes these algorithms much less sensitive to the selection of its hyperparameters. A precursor to these RMSProp and ADAM was outlined in Duchi et al. (2011) .Despite their widespread use in neural net problems, adaptive gradients methods like RMSProp and ADAM lack theoretical justifications in the non-convex setting -even with exact/deterministic gradients (Bernstein et al., 2018) . Further, there are also important motivations to study the behavior of these algorithms in the deterministic setting because of usecases where the amount of noise is controlled during optimization, either by using larger batches (Martens & Grosse, 2015; De et al., 2017; Babanezhad et al., 2015) or by employing variance-reducing techniques (Johnson & Zhang, 2013; Defazio et al., 2014) .Further, works like Wilson et al. (2017) and Keskar & Socher (2017) have shown cases where SGD (no momentum) and HB (classical momentum) generalize much better than RMSProp and ADAM with stochastic gradients. Wilson et al. (2017) also show that ADAM generalizes poorly for large enough nets and that RMSProp generalizes better than ADAM on a couple of neural network tasks (most notably in the character-level language modeling task). But in general it's not clear and no heuristics are known to the best of our knowledge to decide whether these insights about relative performances (generalization or training) between algorithms hold for other models or carry over to the full-batch setting. A summary of our contributions In this work we try to shed some light on the above described open questions about adaptive gradient methods in the following two ways.\\u2022 To the best of our knowledge, this work gives the first convergence guarantees for adaptive gradient based standard neural-net training heuristics. Specifically we show run-time bounds for deterministic RMSProp and ADAM to reach approximate criticality on smooth non-convex functions, as well as for stochastic RMSProp under an additional assumption. Recently, Reddi et al. (2018) have shown in the setting of online convex optimization that there are certain sequences of convex functions where ADAM and RMSprop fail to converge to asymptotically zero average regret. We contrast our findings with Theorem 3 in Reddi et al. (2018) . Their counterexample for ADAM is constructed in the stochastic optimization framework and is incomparable to our result about deterministic ADAM. Our proof of convergence to approximate critical points establishes a key conceptual point that for adaptive gradient algorithms one cannot transfer intuitions about convergence from online setups to their more common use case in offline setups.\\u2022 Our second contribution is empirical investigation into adaptive gradient methods, where our goals are different from what our theoretical results are probing. We test the convergence and generalization properties of RMSProp and ADAM and we compare their performance against NAG on a variety of autoencoder experiments on MNIST data, in both full and mini-batch settings. In the full-batch setting, we demonstrate that ADAM with very high values of the momentum parameter (\\u03b2 1 = 0.99) matches or outperforms carefully tuned NAG and RMSProp, in terms of getting lower training and test losses. We show that as the autoencoder size keeps increasing, RMSProp fails to generalize pretty soon. In the mini-batch experiments we see exactly the same behaviour for large enough nets. We further validate this behavior on an image classification task on CIFAR-10 using a VGG-9 convolutional neural network, the results to which we present in the Appendix E. We note that recently it has been shown by Lucas et al. (2018) , that there are problems where NAG generalizes better than ADAM even after tuning \\u03b2 1 . In contrast our experiments reveal controlled setups where tuning ADAM's \\u03b2 1 closer to 1 than usual practice helps close the generalization gap with NAG and HB which exists at standard values of \\u03b2 1 .Remark. Much after this work was completed we came to know of a related paper (Li & Orabona, 2018 ) which analyzes convergence rates of a modification of AdaGrad (not RMSPRop or ADAM).After the initial version of our work was made public, a few other analysis of adaptive gradient methods have also appeared like Chen et al. (2018) , Zhou et al. (2018) and Zaheer et al. (2018) . Firstly we define the smoothness property that we assume in our proofs for all our non-convex objectives. This is a standard assumption used in the optimization literature. Definition 1. L\\u2212smoothness If f : R d \\u2192 R is at least once differentiable then we call it L\\u2212smooth for some L > 0 if for all x, y \\u2208 R d the following inequality holds, DISPLAYFORM0 We need one more definition that of square-root of diagonal matrices, Definition 2. Square root of the Penrose inverse DISPLAYFORM1 , where {e i } {i=1,...,d} is the standard basis of R d Now we list out the pseudocodes used for NAG, RMSProp and ADAM in theory and experiments, Nesterov's Accelerated Gradient (NAG) Algorithm Algorithm 1 NAG 1: Input : A step size \\u03b1, momentum \\u00b5 \\u2208 [0, 1), and an initial starting point x 1 \\u2208 R d , and we are given query access to a (possibly noisy) oracle for gradients of f : DISPLAYFORM2 Initialize : v 1 = 0 4: DISPLAYFORM3 6: DISPLAYFORM4 end for 8: end function DISPLAYFORM5 , and we are given query access to a (possibly noisy) oracle for gradients of f : DISPLAYFORM6 Initialize : v 0 = 0 4: DISPLAYFORM7 8: end for 10: end function DISPLAYFORM8 DISPLAYFORM9 and we are given oracle access to the gradients of f : DISPLAYFORM10 Initialize : m 0 = 0, v 0 = 0 4: DISPLAYFORM11 9: DISPLAYFORM12 end for 11: end function Previously it has been shown in Rangamani et al. (2017) that mini-batch RMSProp can off-the-shelf do autoencoding on depth 2 autoencoders trained on MNIST data while similar results using nonadaptive gradient descent methods requires much tuning of the step-size schedule. Here we give the first result about convergence to criticality for stochastic RMSProp albeit under a certain technical assumption about the training set (and hence on the first order oracle). Towards that we need the following definition, Definition 3. The sign function We define the function sign : DISPLAYFORM0 DISPLAYFORM1 c) \\u03c3 f < \\u221e is an upperbound on the norm of the gradients of f i and (d) f has a minimizer, i.e., there exists x * such that f (x * ) = min x\\u2208R d f (x). Let the gradient oracle be s.t when invoked at some x t \\u2208 R d it uniformly at random picks i t \\u223c {1, 2, .., k} and returns, \\u2207f it (x t ) = g t . Then corresponding to any , \\u03be > 0 and a starting point x 1 for Algorithm 2, we can define, DISPLAYFORM2 (1\\u2212\\u03b22)\\u03be s.t. we are guaranteed that the iterates of Algorithm 2 using a constant step-length of, \\u03b1 = DISPLAYFORM3 will find an \\u2212critical point in at most T steps in the sense that, min t=1,2.. DISPLAYFORM4 Remark. We note that the theorem above continues to hold even if the constraint (b) that we have about the signs of the gradients of the {f i } i=1,...,k only holds on the points in R d that the stochastic RMSProp visits and its not necessary for the constraint to be true everywhere in the domain. Further we can say in otherwords that this constraint ensures all the options for the gradient that this stochastic oracle has at any point, to lie in the same orthant of R d though this orthant itself can change from one iterate of the next. A related result was concurrently shown by Zaheer et al. (2018) .Next we see that such sign conditions are not necessary to guarantee convergence of the deterministic RMSProp which corresponds to the full-batch RMSProp experiments in Section 5.3. Theorem 3.2. Convergence of deterministic RMSProp -the version with standard speeds (Proof in subsection A.2) Let f : R d \\u2192 R be L\\u2212smooth and let \\u03c3 < \\u221e be an upperbound on the norm of the gradient of f . Assume also that f has a minimizer, i.e., there exists x * such that f (x * ) = min x\\u2208R d f (x). Then the following holds for Algorithm 2 with a deterministic gradient oracle:For any , \\u03be > 0, using a constant step length of DISPLAYFORM5 , where x 1 is the first iterate of the algorithm. One might wonder if the \\u03be parameter introduced in the algorithms above is necessary to get convergence guarantees for RMSProp. Towards that in the following theorem we show convergence of another variant of deterministic RMSProp which does not use the \\u03be parameter and instead uses other assumptions on the objective function and step size modulation. But these tweaks to eliminate the need of \\u03be come at the cost of the convergence rates getting weaker. Theorem 3.3. Convergence of deterministic RMSProp -the version with no \\u03be shift (Proof in subsection A.3) Let f : R d \\u2192 R be L\\u2212smooth and let \\u03c3 < \\u221e be an upperbound on the norm of the gradient of f . Assume also that f has a minimizer, i.e., there exists x * such that f (x * ) = min x\\u2208R d f (x), and the function f be bounded from above and below by constants B and DISPLAYFORM6 the Algorithm 2 with a deterministic gradient oracle and \\u03be = 0 is guaranteed to reach a t-th iterate s.t. 1 \\u2264 t \\u2264 T and \\u2207f (x t ) \\u2264 .In Section 5.3 we show results of our experiments with full-batch ADAM. Towards that, we analyze deterministic ADAM albeit in the small \\u03b2 1 regime. We note that a small \\u03b2 1 does not cut-off contributions to the update direction from gradients in the arbitrarily far past (which are typically significantly large), and neither does it affect the non-triviality of the pre-conditioner which does not depend on \\u03b2 1 at all. Theorem 3.4. Deterministic ADAM converges to criticality (Proof in subsection A.4) Let f : R d \\u2192 R be L\\u2212smooth and let \\u03c3 < \\u221e be an upperbound on the norm of the gradient of f . Assume also that f has a minimizer, i.e., there exists x * such that f (x * ) = min x\\u2208R d f (x). Then the following holds for Algorithm 3: DISPLAYFORM7 , there exist step sizes \\u03b1 t > 0, t = 1, 2, . . . and a natural number T (depending on \\u03b2 1 , \\u03be) such that \\u2207f (x t ) \\u2264 for some t \\u2264 T .In particular if one sets \\u03b2 1 = +2\\u03c3 , \\u03be = 2\\u03c3, and DISPLAYFORM8 3( +2\\u03c3) 2 where g t is the gradient of the objective at the t th iterate, then T can be taken to be DISPLAYFORM9 , where x 2 is the second iterate of the algorithm. Our motivations towards the above theorem were primarily rooted in trying to understand the situations where ADAM can converge at all (given the negative results about ADAM as in Reddi et al. (2018) ). But we point out that it remains open to tighten the analysis of deterministic ADAM and obtain faster rates than what we have shown in the theorem above. Remark. It is sometimes believed that ADAM gains over RMSProp because of its \\\"bias correction term\\\" which refers to the step length of ADAM having an iteration dependence of the following form, 1 \\u2212 \\u03b2 t 2 /(1 \\u2212 \\u03b2 t 1 ). In the above theorem, we note that the 1/(1 \\u2212 \\u03b2 t 1 ) term of this \\\"bias correction term\\\" naturally comes out from theory! 4 EXPERIMENTAL SETUP For testing the empirical performance of ADAM and RMSProp, we perform experiments on fully connected autoencoders using ReLU activations and shared weights and on CIFAR-10 using VGG-9, a convolutional neural network. Let z \\u2208 R d be the input vector to the autoencoder, {W i } i=1,.., denote the weight matrices of the net and {b i } i=1,..,2 be the bias vectors. Then the output\\u1e91 \\u2208 R d of the autoencoder is defined as\\u1e91 DISPLAYFORM10 This defines an autoencoder with 2 \\u2212 1 hidden layers using weight matrices and 2 bias vectors. Thus, the parameters of this model are given by DISPLAYFORM11 (where we imagine all vectors to be column vectors by default). The loss function, for an input z is then given by: f (z; x) = z \\u2212\\u1e91 2 .Such autoencoders are a fairly standard setup that have been used in previous work (Arpit et al., 2015; Baldi, 2012; Kuchaiev & Ginsburg, 2017; Vincent et al., 2010) . There have been relatively fewer comparisons of ADAM and RMSProp with other methods on a regression setting. We were motivated by Rangamani et al. (2017) who had undertaken a theoretical analysis of autoencoders and in their experiments had found RMSProp to have good reconstruction error for MNIST when used on even just 2 layer ReLU autoencoders. To keep our experiments as controlled as possible, we make all layers in a network have the same width (which we denote as h). Thus, given a size d for the input image, the weight matrices (as defined above) are given by: DISPLAYFORM12 . . , . This allowed us to study the effect of increasing depth or width h without having to deal with added confounding factors. For all experiments, we use the standard \\\"Glorot initialization\\\" for the weights (Glorot & Bengio, 2010) , where each element in the weight matrix is initialized by sampling from a uniform distribution with [\\u2212limit, limit], limit = 6/(fan in + fan out ), where fan in denotes the number of input units in the weight matrix, and fan out denotes the number of output units in the weight matrix. All bias vectors were initialized to zero. No regularization was used. We performed autoencoder experiments on the MNIST dataset for various network sizes (i.e., different values of and h). We implemented all experiments using TensorFlow (Abadi et al., 2016) using an NVIDIA GeForce GTX 1080 Ti graphics card. We compared the performance of ADAM and RMSProp with Nesterov's Accelerated Gradient (NAG). All experiments were run for 10 5 iterations. We tune over the hyper-parameters for each optimization algorithm using a grid search as described in the Appendix (Section B). To pick the best set of hyper-parameters, we choose the ones corresponding to the lowest loss on the training set at the end of 10 5 iterations. Further, to cut down on the computation time so that we can test a number of different neural net architectures, we crop the MNIST image from 28 \\u00d7 28 down to a 22 \\u00d7 22 image by removing 3 pixels from each side (almost all of which is whitespace). We are interested in first comparing these algorithms in the full-batch setting. To do this in a computationally feasible way, we consider a subset of the MNIST dataset (we call this: mini-MNIST), which we build by extracting the first 5500 images in the training set and first 1000 images in the test set in MNIST. Thus, the training and testing datasets in mini-MNIST is 10% of the size of the MNIST dataset. Thus the training set in mini-MNIST contains 5500 images, while the test set contains 1000 images. This subset of the dataset is a fairly reasonable approximation of the full MNIST dataset (i.e., contains roughly the same distribution of labels as in the full MNIST dataset), and thus a legitimate dataset to optimize on. To test if our conclusions on the full-batch case extend to the mini-batch case, we then perform the same experiments in a mini-batch setup where we fix the mini-batch size at 100. For the mini-batch experiment, we consider the full training set of MNIST, instead of the mini-MNIST dataset considered for the full-batch experiments and we also test on CIFAR-10 using VGG-9, a convolutional neural network. The \\u03be parameter is a feature of the default implementations of RMSProp and ADAM such as in TensorFlow. Most interestingly this strictly positive parameter is crucial for our proofs. In this section we present experimental evidence that attempts to clarify that this isn't merely a theoretical artefact but its value indeed has visible effect on the behaviours of these algorithms. We see in FIG1 that on increasing the value of this fixed shift parameter \\u03be, ADAM in particular, is strongly helped towards getting lower gradient norms and lower test losses though it can hurt its ability to get lower training losses. The plots are shown for optimally tuned values for the other hyper-parameters. To check whether NAG, ADAM or RMSProp is capable of consistently moving from a \\\"bad\\\" saddle point to a \\\"good\\\" saddle point region, we track the most negative eigenvalue of the Hessian \\u03bb min (Hessian). Even for a very small neural network with around 10 5 parameters, it is still intractable to store the full Hessian matrix in memory to compute the eigenvalues. Instead, we use the Scipy library function scipy.sparse.linalg.eigsh that can use a function that computes the matrix-vector products to compute the eigenvalues of the matrix (Lehoucq et al., 1998) . Thus, for finding the eigenvalues of the Hessian, it is sufficient to be able to do Hessian-vector products. This can be done exactly in a fairly efficient way (Townsend, 2008) .We display a representative plot in FIG3 which shows that NAG in particular has a distinct ability to gradually, but consistently, keep increasing the minimum eigenvalue of the Hessian while continuing to decrease the gradient norm. However unlike as in deeper autoencoders in this case the gradient norms are consistently bigger for NAG, compared to RMSProp and ADAM. In contrast, RSMProp and ADAM quickly get to a high value of the minimum eigenvalue and a small gradient norm, but somewhat stagnate there. In short, the trend looks better for NAG, but in actual numbers RMSProp and ADAM do better. In Figure 3 , we show how the training loss, test loss and gradient norms vary through the iterations for RMSProp, ADAM (at \\u03b2 1 = 0.9 and 0.99) and NAG (at \\u00b5 = 0.9 and 0.99) on a 3 hidden layer autoencoder with 1000 nodes in each hidden layer trained on mini-MNIST. Appendix D.1 and D.2 have more such comparisons for various neural net architectures with varying depth and width and input image sizes, where the following qualitative results also extend. Conclusions from the full-batch experiments of training autoencoders on mini-MNIST:\\u2022 Pushing \\u03b2 1 closer to 1 significantly helps ADAM in getting lower training and test losses and at these values of \\u03b2 1 , it has better performance on these metrics than all the other algorithms. One sees cases like the one displayed in Figure 3 where ADAM at \\u03b2 1 = 0.9 was getting comparable or slightly worse test and training errors than NAG. But once \\u03b2 1 gets closer to 1, ADAM's performance sharply improves and gets better than other algorithms.\\u2022 Increasing momentum helps NAG get lower gradient norms though on larger nets it might hurt its training or test performance. NAG does seem to get the lowest gradient norms compared to the other algorithms, except for single hidden layer networks like in FIG3 . In Figure 4 , we show how training loss, test loss and gradient norms vary when using mini-batches of size 100, on a 5 hidden layer autoencoder with 1000 nodes in each hidden layer trained on the full MNIST dataset. The same phenomenon as here has been demonstrated in more such mini-batch comparisons on autoencoder architectures with varying depths and widths in Appendix D.3 and on VGG-9 with CIFAR-10 in Appendix E.Conclusions from the mini-batch experiments of training autoencoders on full MNIST dataset:\\u2022 Mini-batching does seem to help NAG do better than ADAM on small nets. However, for larger nets, the full-batch behavior continues, i.e., when ADAM's momentum parameter \\u03b2 1 is pushed closer to 1, it gets better generalization (significantly lower test losses) than NAG at any momentum tested.\\u2022 In general, for all metrics (test loss, training loss and gradient norm reduction) both ADAM as well as NAG seem to improve in performance when their momentum parameter (\\u00b5 for NAG and \\u03b2 1 for ADAM) is pushed closer to 1. This effect, which was present in the full-batch setting, seems to get more pronounced here.\\u2022 As in the full-batch experiments, NAG continues to have the best ability to reduce gradient norms while for larger enough nets, ADAM at large momentum continues to have the best training error. To the best of our knowledge, we present the first theoretical guarantees of convergence to criticality for the immensely popular algorithms RMSProp and ADAM in their most commonly used setting of optimizing a non-convex objective. By our experiments, we have sought to shed light on the important topic of the interplay between adaptivity and momentum in training nets. By choosing to study textbook autoencoder architectures where various parameters of the net can be changed controllably we highlight the following two aspects that (a) the value of the gradient shifting hyperparameter \\u03be has a significant influence on the performance of ADAM and RMSProp and (b) ADAM seems to perform particularly well (often supersedes Nesterov accelerated gradient method) when its momentum parameter \\u03b2 1 is very close to 1. On VGG-9 with CIFAR-10 and for the task of training autoencoders on MNIST we have verified these conclusions across different widths and depths of nets as well as in the full-batch and the mini-batch setting (with large nets) and under compression of the input/out image size. Curiously enough, this regime of \\u03b2 1 being close to 1 is currently not within the reach of our proof techniques of showing convergence for ADAM. Our experiments give strong reasons to try to advance theory in this direction in future work. Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. Now we give the proof of Theorem 3.1.Proof. We define \\u03c3 t := max k=1,..,t \\u2207f i k (x k ) and we solve the recursion for v t as, DISPLAYFORM0 . This lets us write the following bounds, DISPLAYFORM1 i and this lets us get the following bounds, DISPLAYFORM2 Now we invoke the bounded gradient assumption about the f i functions and replace in the above equation the eigenvalue bounds of the pre-conditioner by worst-case estimates \\u00b5 max and \\u00b5 min defined as, DISPLAYFORM3 Using the L-smoothness of f between consecutive iterates x t and x t+1 we have, DISPLAYFORM4 We note that the update step of stochastic RMSProp is x t+1 = x t \\u2212 \\u03b1(V t ) \\u2212 1 2 g t where g t is the stochastic gradient at iterate x t . Let H t = {x 1 , x 2 , .., x t } be the set of random variables corresponding to the first t iterates. The assumptions we have about the stochastic oracle give us the following relations, DISPLAYFORM5 f . Now we can invoke these stochastic oracle's properties and take a conditional (on H t ) expectation over g t of the L\\u2212smoothness in equation to get, DISPLAYFORM6 We now separately analyze the middle term in the RHS above in Lemma A.1 below and we get, DISPLAYFORM7 We substitute the above into equation 1 and take expectations over H t to get, DISPLAYFORM8 Doing the above replacements to upperbound the RHS of equation 2 and summing the inequation over t = 1 to t = T and taking the average and replacing the LHS by a lowerbound of it, we get, DISPLAYFORM9 Replacing into the RHS above the optimal choice of, DISPLAYFORM10 Thus stochastic RMSProp with the above step-length is guaranteed is reach \\u2212criticality in number of iterations given by, T \\u2264 DISPLAYFORM11 Lemma A.1. At any time t, the following holds, DISPLAYFORM12 Proof. DISPLAYFORM13 Now we introduce some new variables to make the analysis easier to present. Let a pi := [\\u2207f p (x t )] i where p indexes the training data set, p \\u2208 {1, . . . , k}. (conditioned on H t , a pi s are constants) This implies, DISPLAYFORM14 where the expectation is taken over the oracle call at the t th update step. Further our instantiation of the oracle is equivalent to doing the uniformly at random sampling, (g t ) i \\u223c {a pi } p=1,...,k .Given that we have, DISPLAYFORM15 i +di where we have defined DISPLAYFORM16 This leads to an explicit form of the needed expectation over the t th \\u2212oracle call as, DISPLAYFORM17 Substituting the above (and the definition of the constants a pi ) back into equation 3 we have, DISPLAYFORM18 Substituting this, the above expression can be written as, DISPLAYFORM19 Note that with this substitution, the RHS of the claimed lemma becomes, DISPLAYFORM20 Therefore our claim is proved if we show that for all i, DISPLAYFORM21 To further simplify, we define DISPLAYFORM22 \\u2212\\u00b5 min . We therefore need to show, DISPLAYFORM23 We first bound d i by recalling the definition of \\u03c3 f (from which it follows that a 2 pi \\u2264 \\u03c3 2 f ), DISPLAYFORM24 The inequality follows since \\u03b2 2 \\u2208 (0, 1]Putting this all together, we get, DISPLAYFORM25 Now our assumption that for all x, sign(\\u2207f p (x)) = sign(\\u2207f q (x)) for all p, q \\u2208 {1, . . . , k} leads to the conclusion that the term a pi a qi \\u2265 0. And we had already shown in equation 5 that DISPLAYFORM26 Thus we have shown that (a i 1 k )(q i a i ) \\u2265 0 and this finishes the proof. Proof. By the L\\u2212smoothness condition and the update rule in Algorithm 2 we have, DISPLAYFORM0 For 0 < \\u03b4 DISPLAYFORM1 we now show a strictly positive lowerbound on the following function, DISPLAYFORM2 We define \\u03c3 t := max i=1,..,t \\u2207f (x i ) and we solve the recursion for v t as, DISPLAYFORM3 . This lets us write the following bounds, DISPLAYFORM4 Now we define, t := min k=1,..,t,i=1,..,d (\\u2207f (x k )) 2 i and this lets us get the following sequence of inequalities, DISPLAYFORM5 So combining equations 9 and 8 into equation 7 and from the exit line in the loop we are assured that \\u2207f (x t ) 2 = 0 and combining these we have, DISPLAYFORM6 Now our definition of \\u03b4 2 t allows us to define a parameter 0 < \\u03b2 t := DISPLAYFORM7 t and rewrite the above equation as, DISPLAYFORM8 We can as well satisfy the conditions needed on the variables, \\u03b2 t and \\u03b4 t by choosing, DISPLAYFORM9 Then the worst-case lowerbound in equation 10 becomes, DISPLAYFORM10 This now allows us to see that a constant step length \\u03b1 t = \\u03b1 > 0 can be defined as, DISPLAYFORM11 and this is such that the above equation can be written as, DISPLAYFORM12 2 . This when substituted back into equation 6 we have, DISPLAYFORM13 This gives us, DISPLAYFORM14 Thus for any given > 0, T satisfying, DISPLAYFORM15 2 is a sufficient condition to ensure that the algorithm finds a point x result := arg min t=1,,.T \\u2207f (x t ) 2 with \\u2207f (x result ) 2 \\u2264 2 .Thus we have shown that using a constant step length of \\u03b1 = DISPLAYFORM16 Proof. From the L\\u2212smoothness condition on f we have between consecutive iterates of the above algorithm, DISPLAYFORM17 Now the recursion for v t can be solved to get, DISPLAYFORM18 . Substituting this in a lowerbound on the LHS of equation 13 we get, DISPLAYFORM19 Summing the above we get, DISPLAYFORM20 Now we substitute \\u03b1 t = \\u03b1 \\u221a t and invoke the definition of B and B u to write the first term on the RHS of equation 15 as, DISPLAYFORM21 Now we bound the second term in the RHS of equation 15 as follows. Lets first define a function P (T ) as follows, DISPLAYFORM22 2 and that gives us, DISPLAYFORM23 So substituting the above two bounds back into the RHS of the above inequality 15and removing the factor of 1 \\u2212 \\u03b2 T 2 < 1 from the numerator, we can define a point x result as follows, DISPLAYFORM24 Thus it follows that for T = O( 1 4 ) the algorithm 2 is guaranteed to have found at least one point DISPLAYFORM25 Proof. Let us assume to the contrary that g t > for all t = 1, 2, 3. . . .. We will show that this assumption will lead to a contradiction. By L\\u2212smoothness of the objective we have the following relationship between the values at consecutive updates, DISPLAYFORM26 Substituting the update rule using a dummy step length \\u03b7 t > 0 we have, DISPLAYFORM27 The RHS in equation 16 above is a quadratic in \\u03b7 t with two roots: 0 and DISPLAYFORM28 So the quadratic's minimum value is at the midpoint of this interval, which gives us a candidate t th \\u2212step length i.e \\u03b1 * DISPLAYFORM29 2 and the value of the quadratic at this point DISPLAYFORM30 That is with step lengths being this \\u03b1 * t we have the following guarantee of decrease of function value between consecutive steps, DISPLAYFORM31 Now we separately lower bound the numerator and upper bound the denominator of the RHS above. DISPLAYFORM32 Further we note that the recursion of v t can be solved as, DISPLAYFORM33 k . Now we define, t := min k=1,..,t,i=1,..,d (g 2 k ) i and this gives us, DISPLAYFORM34 We solve the recursion for m t to get, DISPLAYFORM35 Then by triangle inequality and defining \\u03c3 t := max i=1,..,t \\u2207f (x i ) we have, m t \\u2264 (1 \\u2212 \\u03b2 t 1 )\\u03c3 t . Thus combining this estimate of m t with equation 18 we have, DISPLAYFORM36 m t To analyze this we define the following sequence of functions for each i = 0, 1, 2.., t DISPLAYFORM37 This gives us the following on substituting the update rule for m t , DISPLAYFORM38 Lets define, \\u03c3 t\\u22121 := max i=1,.. ,t\\u22121 \\u2207f (x i ) and this gives us for i \\u2208 {1, .., t \\u2212 1}, DISPLAYFORM39 We note the following identity, DISPLAYFORM40 Now we use the lowerbounds proven on Q i \\u2212 \\u03b2 1 Q i\\u22121 for i \\u2208 {1, .., t \\u2212 1} and Q t \\u2212 \\u03b2 1 Q t\\u22121 to lowerbound the above sum as, DISPLAYFORM41 We can evaluate the following lowerbound, DISPLAYFORM42 Next we remember that the recursion of v t can be solved as, v t = (1 \\u2212 \\u03b2 2 ) t k=1 \\u03b2 t\\u2212k 2 g 2 k and we define, \\u03c3 t := max i=1,..,t \\u2207f (x i ) to get, DISPLAYFORM43 Now we combine the above and equation 18 and the known value of Q 0 = 0 (from definition and initial conditions) to get from the equation 20, DISPLAYFORM44 In the above inequalities we have set t = 0 and we have set, \\u03c3 t = \\u03c3 t\\u22121 = \\u03c3. Now we examine the following part of the lowerbound proven above, DISPLAYFORM45 Now we remember the assumption that we are working under i.e g t > . Also by definition 0 < \\u03b2 1 < 1 and hence we have 0 < \\u03b2 1 \\u2212 \\u03b2 t 1 < \\u03b2 1 . This implies, DISPLAYFORM46 > 1 where the last inequality follows because of our choice of as stated in the theorem statement. This allows us to define a constant, DISPLAYFORM47 Similarly our definition of \\u03be allows us to define a constant \\u03b8 2 > 0 to get, DISPLAYFORM48 Putting the above back into the lowerbound for Q t in equation 22 we have, DISPLAYFORM49 Now we substitute the above and equation 19 into equation 17 to get, DISPLAYFORM50 In the theorem statement we choose to call as the final \\u03b1 t the lowerbound proven above. We check below that this smaller value of \\u03b1 t still guarantees a decrease in the function value that is sufficient for the statement of the theorem to hold. A consistency check! Let us substitute the above final value of the step length DISPLAYFORM51 DISPLAYFORM52 The RHS above can be simplified to be shown to be equal to the RHS in equation 24 at the same values of \\u03b8 1 and \\u03b8 2 as used above. And we remember that the bound on the running time was derived from this equation 24. Here we describe how we tune the hyper-parameters of each optimization algorithm. NAG has two hyper-parameters, the step size \\u03b1 and the momentum \\u00b5. The main hyper-parameters for RMSProp are the step size \\u03b1, the decay parameter \\u03b2 2 and the perturbation \\u03be. ADAM, in addition to the ones in RMSProp, also has a momentum parameter \\u03b2 1 . We vary the step-sizes of ADAM in the conventional way of \\u03b1 t = \\u03b1 1 \\u2212 \\u03b2 t 2 /(1 \\u2212 \\u03b2 t 1 ). For tuning the step size, we follow the same method used in Wilson et al. (2017) . We start out with a logarithmically-spaced grid of five step sizes. If the best performing parameter was at one of the extremes of the grid, we tried new grid points so that the best performing parameters were at one of the middle points in the grid. While it is computationally infeasible even with substantial resources to follow a similarly rigorous tuning process for all other hyper-parameters, we do tune over them somewhat as described below. NAG The initial set of step sizes used for NAG were: {3e\\u22123, 1e\\u22123, 3e\\u22124, 1e\\u22124, 3e\\u22125}. We tune the momentum parameter over values \\u00b5 \\u2208 {0.9, 0.99}. The initial set of step sizes used were: {3e\\u22124, 1e\\u22124, 3e\\u22125, 1e\\u22125, 3e\\u22126}. We tune over \\u03b2 2 \\u2208 {0.9, 0.99}. We set the perturbation value \\u03be = 10 \\u221210 , following the default values in TensorFlow, except for the experiments in Section 5.1. In Section 5.1, we show the effect on convergence and generalization properties of ADAM and RMSProp when changing this parameter \\u03be. Note that ADAM and RMSProp uses an accumulator for keeping track of decayed squared gradient v t . For ADAM this is recommended to be initialized at v 0 = 0. However, we found in the TensorFlow implementation of RMSProp that it sets v 0 = 1 d . Instead of using this version of the algorithm, we used a modified version where we set v 0 = 0. We typically found setting v 0 = 0 to lead to faster convergence in our experiments. ADAM The initial set of step sizes used were: {3e\\u22124, 1e\\u22124, 3e\\u22125, 1e\\u22125, 3e\\u22126}. For ADAM, we tune over \\u03b2 1 values of {0.9, 0.99}. For ADAM, We set \\u03b2 2 = 0.999 for all our experiments as is set as the default in TensorFlow. Unless otherwise specified we use for the perturbation value \\u03be = 10 \\u22128 for ADAM, following the default values in TensorFlow. Contrary to what is the often used values of \\u03b2 1 for ADAM (usually set to 0.9), we found that we often got better results on the autoencoder problem when setting \\u03b2 1 = 0.99. In Figure 5 , we show the same effect of changing \\u03be as in Section 5.1 on a 1 hidden layer network of 1000 nodes, while keeping all other hyper-parameters fixed (such as learning rate, \\u03b2 1 , \\u03b2 2 ). These other hyper-parameter values were fixed at the best values of these parameters for the default values of \\u03be, i.e., \\u03be = 10 \\u221210 for RMSProp and \\u03be = 10 \\u22128 for ADAM. To test whether our conclusions are consistent across different input dimensions, we do two experiments where we resize the 22 \\u00d7 22 MNIST image to 17 \\u00d7 17 and to 12 \\u00d7 12. Resizing is done using TensorFlow's tf.image.resize images method, which uses bilinear interpolation. 17 \\u00d7 17 Figure 9 shows results on input images of size 17 \\u00d7 17 on a 3 layer network with 1000 hidden nodes in each layer. Our main results extend to this input dimension, where we see ADAM with \\u03b2 1 = 0.99 both converging the fastest as well as generalizing the best, while NAG does better than ADAM with \\u03b2 1 = 0.9. In FIG1 , we present results on additional neural net architectures on mini-batches of size 100 with an input dimension of 22 \\u00d7 22. We see that most of our full-batch results extend to the minibatch case.(a) 1 hidden layer; 1000 nodes (b) 3 hidden layers; 1000 nodes each (c) 9 hidden layers; 1000 nodes each (d) 1 hidden layer; 1000 nodes (e) 3 hidden layers; 1000 nodes each (f) 9 hidden layers; 1000 nodes each (g) 1 hidden layer; 1000 nodes (h) 3 hidden layers; 1000 nodes each (i) 9 hidden layers; 1000 nodes each FIG1 : Experiments on various networks with mini-batch size 100 on full MNIST dataset with input image size 22 \\u00d7 22. First row shows the loss on the full training set, middle row shows the loss on the test set, and bottom row shows the norm of the gradient on the training set.\",\n          \"We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods. Hierarchical reinforcement learning has long held the promise of extending the successes of existing reinforcement learning (RL) methods BID9 BID22 BID16 to more complex, difficult, and temporally extended tasks BID18 BID25 BID3 . Recently, goal-conditioned hierarchical designs, in which higher-level policies communicate goals to lower-levels and lower-level policies are rewarded for reaching states (i.e. observations) which are close to these desired goals, have emerged as an effective paradigm for hierarchical RL BID17 ; BID14 ; Vezhnevets et al. (2017) , inspired by earlier work BID6 ; BID21 ). In this hierarchical design, representation learning is crucial; that is, a representation function must be chosen mapping state observations to an abstract space. Goals (desired states) are then specified by the choice of a point in this abstract space. Previous works have largely studied two ways to choose the representation: learning the representation end-to-end together with the higher-and lower-level policies (Vezhnevets et al., 2017) , or using the state space as-is for the goal space (i.e., the goal space is a subspace of the state space) BID17 BID14 . The former approach is appealing, but in practice often produces poor results (see BID17 and our own experiments), since the resulting representation is under-defined; i.e., not all possible sub-tasks are expressible as goals in the space. On the other hand, fixing the representation to be the full state means that no information is lost, but this choice is difficult to scale to higher dimensions. For example, if the state observations are entire images, the higher-level must output target images for the lower-level, which can be very difficult. We instead study how unsupervised objectives can be used to train a representation that is more concise than the full state, but also not as under-determined as in the end-to-end approach. In order to do so in a principled manner, we propose a measure of sub-optimality of a given representation. This measure aims to answer the question: How much does using the learned representation in place of the full representation cause us to lose, in terms of expected reward, against the optimal policy? This question is important, because a useful representation will compress the state, hopefully making the learning problem easier. At the same time, the compression might cause the representation to lose information, making the optimal policy impossible to express. It is therefore critical to understand how lossy a learned representation is, not in terms of reconstruction, but in terms of the ability to represent near-optimal policies on top of this representation. Our main theoretical result shows that, for a particular choice of representation learning objective, we can learn representations for which the return of the hierarchical policy approaches the return of the optimal policy within a bounded error. This suggests that, if the representation is learned with a principled objective, the 'lossy-ness' in the resulting representation should not cause a decrease in overall task performance. We then formulate a representation learning approach that optimizes this bound. We further extend our result to the case of temporal abstraction, where the higher-level controller only chooses new goals at fixed time intervals. To our knowledge, this is the first result showing that hierarchical goal-setting policies with learned representations and temporal abstraction can achieve bounded sub-optimality against the optimal policy. We further observe that the representation learning objective suggested by our theoretical result closely resembles several other recently proposed objectives based on mutual information (van den Oord et al., 2018; BID12 , suggesting an intriguing connection between mutual information and goal representations for hierarchical RL. Results on a number of difficult continuous-control navigation tasks show that our principled representation learning objective yields good qualitative and quantitative performance compared to existing methods. Following previous work BID17 , we consider a two-level hierarchical policy on an MDP M = (S, A, R, T ), in which the higher-level policy modulates the behavior of a lowerlevel policy by choosing a desired goal state and rewarding the lower-level policy for reaching this state. While prior work has used a sub-space of the state space as goals BID17 , in more general settings, some type of state representation is necessary. That is, consider a state representation function f : S \\u2192 R d . A two-level hierarchical policy on M is composed of a higher-level policy \\u03c0 hi (g|s), where g \\u2208 G = R d is the goal space, that samples a high-level action (or goal) g t \\u223c \\u03c0 hi (g|s t ) every c steps, for fixed c. A non-stationary, goal-conditioned, lower-level policy \\u03c0 lo (a|s t , g t , s t+k , k) then translates these high-level actions into low-level actions a t+k \\u2208 A for k \\u2208 [0, c \\u2212 1]. The process is then repeated, beginning with the higher-level policy selecting another goal according to s t+c . The policy \\u03c0 lo is trained using a goal-conditioned reward; e.g. the reward of a transition g, s, s is \\u2212D(f (s ), g), where D is a distance function. In this work we adopt a slightly different interpretation of the lower-level policy and its relation to \\u03c0 hi . Every c steps, the higher-level policy chooses a goal g t based on a state s t . We interpret this state-goal pair as being mapped to a nonstationary policy \\u03c0(a|s t+k , k), \\u03c0 \\u2208 \\u03a0, where \\u03a0 denotes the set of all possible c-step policies acting on M. We use \\u03a8 to denote this mapping from S \\u00d7 G to \\u03a0. In other words, on every c th step, we encounter some state s t \\u2208 S. We use the higher-level policy to sample a goal g t \\u223c \\u03c0 hi (g|s t ) and translate this to a policy \\u03c0 t = \\u03a8(s t , g t ). We then use \\u03c0 t to sample actions a t+k \\u223c \\u03c0 t (a|s t+k , k) for k \\u2208 [0, c \\u2212 1]. The process is then repeated from s t+c .Although the difference in this interpretation is subtle, the introduction of \\u03a8 is crucial for our subsequent analysis. The communication of g t is no longer as a goal which \\u03c0 hi desires to reach, but rather more precisely, as an identifier to a low-level behavior which \\u03c0 hi desires to induce or activate. The mapping \\u03a8 is usually expressed as the result of an RL optimization over \\u03a0; e.g., DISPLAYFORM0 where we use P \\u03c0 (s t+k |s t ) to denote the probability of being in state s t+k after following \\u03c0 for k steps starting from s t . We will consider variations on this low-level objective in later sections. From Equation 1 it is clear how the choice of representation f affects \\u03a8 (albeit indirectly).We will restrict the environment reward function R to be defined only on states. We use R max to denote the maximal absolute reward: R max = sup S |R(s)|. In the previous section, we introduced two-level policies where a higher-level policy \\u03c0 hi chooses goals g, which are translated to lower-level behaviors via \\u03a8. The introduction of this hierarchy leads to a natural question: How much do we lose by learning \\u03c0 hi which is only able to act on M via \\u03a8? The choice of \\u03a8 restricts the type and number of lower-level behaviors that the higher-level policy can induce. Thus, the optimal policy on M is potentially not expressible by \\u03c0 hi . Despite the potential lossy-ness of \\u03a8, can one still learn a hierarchical policy which is near-optimal?To approach this question, we introduce a notion of sub-optimality with respect to the form of \\u03a8: Let \\u03c0 * hi (g|s, \\u03a8) be the optimal higher-level policy acting on G and using \\u03a8 as the mapping from G to low-level behaviors. Let \\u03c0 * hier be the corresponding full hierarchical policy on M. We will compare \\u03c0 * hier to an optimal hierarchical policy \\u03c0 * agnostic to \\u03a8. To define \\u03c0 * we begin by introducing an optimal higher-level policy \\u03c0 * * hi (\\u03c0|s) agnostic to \\u03a8; i.e. every c steps, \\u03c0 * * hi samples a low-level behavior \\u03c0 \\u2208 \\u03a0 which is applied to M for the following c steps. In this way, \\u03c0 * * hi may express all possible low-level behaviors. We then denote \\u03c0 * as the full hierarchical policy resulting from \\u03c0 * * hi . We would like to compare \\u03c0 * hier to \\u03c0 * . A natural and common way to do so is in terms of state values. Let V \\u03c0 (s) be the future value achieved by a policy \\u03c0 starting at state s. We define the sub-optimality of \\u03a8 as DISPLAYFORM0 The state values V \\u03c0 * hier (s) are determined by the form of \\u03a8, which is in turn determined by the choice of representation f . However, none of these relationships are direct. It is unclear how a change in f will result in a change to the sub-optimality. In the following section, we derive a series of bounds which establish a more direct relationship between SubOpt(\\u03a8) and f . Our main result will show that if one defines \\u03a8 as a slight modification of the traditional objective given in Equation 1, then one may translate sub-optimality of \\u03a8 to a practical representation learning objective for f . In this section, we provide proxy expressions that bound the sub-optimality induced by a specific choice of \\u03a8. Our main result is Claim 4, which connects the sub-optimality of \\u03a8 to both goalconditioned policy objectives (i.e., the objective in 1) and representation learning (i.e., an objective for the function f ). For ease of presentation, we begin by presenting our results in the restricted case of c = 1 and deterministic lower-level policies. In this setting, the class of low-level policies \\u03a0 may be taken to be simply A, where a \\u2208 \\u03a0 corresponds to a policy which always chooses action a. There is no temporal abstraction: The higher-level policy chooses a high-level action g \\u2208 G at every step, which is translated via \\u03a8 to a low-level action a \\u2208 A. Our claims are based on quantifying how many of the possible low-level behaviors (i.e., all possible state to state transitions) can be produced by \\u03a8 for different choices of g. To quantify this, we make use of an auxiliary inverse goal model \\u03d5(s, a), which aims to predict which goal g will cause \\u03a8 to yield an action\\u00e3 = \\u03a8(s, g) that induces a next state distribution P (s |s,\\u00e3) similar to P (s |s, a).3 We have the following theorem, which bounds the sub-optimality in terms of total variation divergences between P (s |s, a) and P (s |s,\\u00e3): DISPLAYFORM0 DISPLAYFORM1 Proof. See Appendices A and B for all proofs. Theorem 1 allows us to bound the sub-optimality of \\u03a8 in terms of how recoverable the effect of any action in A is, in terms of transition to the next state. One way to ensure that effects of actions in A are recoverable is to have an invertible \\u03a8. That is, if there exists \\u03d5 : S \\u00d7 A \\u2192 G such that \\u03a8(s, \\u03d5(s, a)) = a for all s, a, then the sub-optimality of \\u03a8 is 0.However, in many cases it may not be desirable or feasible to have an invertible \\u03a8. Looking back at Theorem 1, we emphasize that its statement requires only the effect of any action to be recoverable. That is, for any s, \\u2208 S, a \\u2208 A, we require only that there exist some g \\u2208 G (given by \\u03d5(s, a)) which yields a similar next-state distribution. To this end, we have the following claim, which connects the sub-optimality of \\u03a8 to both representation learning and the form of the low-level objective. Claim 2. Let \\u03c1(s) be a prior and f, \\u03d5 be so that, for DISPLAYFORM2 If the low-level objective is defined as DISPLAYFORM3 then the sub-optimality of \\u03a8 is bounded by C .We provide an intuitive explanation of the statement of Claim 2. First, consider that the distribution K(s |s, a) appearing in Equation 4 may be interpreted as a dynamics model determined by f and \\u03d5. By bounding the difference between the true dynamics P (s |s, a) and the dynamics K(s |s, a) implied by f and \\u03d5, Equation FORMULA4 states that the representation f should be chosen in such a way that dynamics in representation space are roughly given by \\u03d5(s, a). This is essentially a representation learning objective for choosing f , and in Section 5 we describe how to optimize it in practice. Moving on to Equation 5, we note that the form of \\u03a8 here is only slightly different than the onestep form of the standard goal-conditioned objective in Equation FORMULA0 . 5 Therefore, all together Claim 2 establishes a deep connection between representation learning (Equation 4), goal-conditioned policy learning (Equation 5), and sub-optimality. We now move on to presenting the same results in the fully general, temporally abstracted setting, in which the higher-level policy chooses a high-level action g \\u2208 G every c steps, which is transformed via \\u03a8 to a c-step lower-level behavior policy \\u03c0 \\u2208 \\u03a0. In this setting, the auxiliary inverse goal model \\u03d5(s, \\u03c0) is a mapping from S \\u00d7 \\u03a0 to G and aims to predict which goal g will cause \\u03a8 to yield a policy\\u03c0 = \\u03a8(s, g) that induces future state distributions P\\u03c0(s t+k |s t ) similar to P \\u03c0 (s t+k |s t ), for k \\u2208 [1, c]. We weight the divergences between the distributions by weights w k = 1 for k < c and DISPLAYFORM0 The analogue to Theorem 1 is as follows: DISPLAYFORM1 then SubOpt(\\u03a8) \\u2264 C , where C = 2\\u03b3 1\\u2212\\u03b3 c R max w. For the analogue to Claim 2, we simply replace the single-step KL divergences and low-level rewards with a discounted weighted sum thereof:Claim 4. Let \\u03c1(s) be a prior over S. Let f, \\u03d5 be such that, DISPLAYFORM2 DISPLAYFORM3 If the low-level objective is defined as DISPLAYFORM4 then the sub-optimality of \\u03a8 is bounded by C .Claim 4 is the main theoretical contribution of our work. As in the previous claim, we have a strong statement, saying that if the low-level objective is defined as in Equation 9, then minimizing the sub-optimality may be done by optimizing a representation learning objective based on Equation 8. We emphasize that Claim 4 applies to any class of low-level policies \\u03a0, including either closed-loop or open-loop policies. We now have the mathematical foundations necessary to learn representations that are provably good for use in hierarchical RL. We begin by elaborating on how we translate Equation 8 into a practical training objective for f and auxiliary \\u03d5 (as well as a practical parameterization of policies \\u03c0 as input to \\u03d5). We then continue to describe how one may train a lower-level policy to match the objective presented in Equation FORMULA10 . In this way, we may learn f and lower-level policy to directly optimize a bound on the sub-optimality of \\u03a8. A pseudocode of the full algorithm is presented in the Appendix (see Algorithm 1). Consider a representation function f \\u03b8 : S \\u2192 R d and an auxiliary function \\u03d5 \\u03b8 : S \\u00d7 \\u03a0 \\u2192 R d , parameterized by vector \\u03b8. In practice, these are separate neural networks: DISPLAYFORM0 While the form of Equation 8 suggests to optimize a supremum over all s t and \\u03c0, in practice we only have access to a replay buffer which stores experience s 0 , a 0 , s 1 , a 1 , . . . sampled from our hierarchical behavior policy. Therefore, we propose to choose s t sampled uniformly from the replay buffer and use the subsequent c actions a t:t+c\\u22121 as a representation of the policy \\u03c0, where we use a t:t+c\\u22121 to denote the sequence a t , . . . , a t+c\\u22121 . Note that this is equivalent to setting the set of candidate policies \\u03a0 to A c (i.e., \\u03a0 is the set of c-step, deterministic, open-loop policies). This choice additionally simplifies the possible structure of the function approximator used for \\u03d5 \\u03b8 (a standard neural net which takes in s t and a t:t+c\\u22121 ). Our proposed representation learning objective is thus, DISPLAYFORM1 where J(\\u03b8, s t , a t:t+c\\u22121 ) will correspond to the inner part of the supremum in Equation 8.We now define the inner objective J(\\u03b8, s t , a t:t+c\\u22121 ). To simplify notation, we use DISPLAYFORM2 suggests the following learning objective on each s t , \\u03c0 \\u2261 a t:t+c\\u22121 : DISPLAYFORM3 DISPLAYFORM4 where B is a constant. The gradient with respect to \\u03b8 is then, DISPLAYFORM5 Published as a conference paper at ICLR 2019The first term of Equation 14 is straightforward to estimate using experienced s t+1:t+k . We set \\u03c1 to be the replay buffer distribution, so that the numerator of the second term is also straightforward. We approximate the denominator of the second term using a mini-batch S of states independently sampled from the replay buffer: DISPLAYFORM6 This completes the description of our representation learning algorithm. Connection to Mutual Information Estimators. The form of the objective we optimize (i.e. Equation 13) is very similar to mutual information estimators, mostly CPC (van den Oord et al., 2018) . Indeed, one may interpret our objective as maximizing a mutual information M I(s t+k ; s t , \\u03c0) via an energy function given by E \\u03b8 (s t+k , s t , \\u03c0). The main differences between our approach and these previous proposals are as follows: (1) Previous approaches maximize a mutual information M I(s t+k ; s t ) agnostic to actions or policy. (2) Previous approaches suggest to define the energy function as exp(f (s t+k ) T M k f (s t )) for some matrix M k , whereas our energy function is based on the distance D used for low-level reward. (3) Our approach is provably good for use in hierarchical RL, and hence our theoretical results may justify some of the good performance observed by others using mutual information estimators for representation learning. Different approaches to translating our theoretical findings to practical implementations may yield objectives more or less similar to CPC, some of which perform better than others (see Appendix D). Equation 9 suggests to optimize a policy \\u03c0 st,g (a|s t+k , k) for every s t , g. This is equivalent to the parameterization \\u03c0 lo (a|s t , g, s t+k , k), which is standard in goal-conditioned hierarchical designs. Standard RL algorithms may be employed to maximize the low-level reward implied by Equation 9: DISPLAYFORM0 weighted by w k and where \\u03c0 corresponds to \\u03c0 lo when the state s t and goal g are fixed. While the first term of Equation 16 is straightforward to compute, the log probabilities log \\u03c1(s t+k ), log P \\u03c0 (s t+k |s t ) are in general unknown. To approach this issue, we take advantage of the representation learning objective for f, \\u03d5. When f, \\u03d5 are optimized as dictated by Equation 8, we have DISPLAYFORM1 We may therefore approximate the low-level reward as DISPLAYFORM2 As in Section 5.1, we use the sampled actions a t:t+c\\u22121 to represent \\u03c0 as input to \\u03d5. We approximate the third term of Equation 18 analogously to Equation 15. Note that this is a slight difference from standard low-level rewards, which use only the first term of Equation 18 and are unweighted. Representation learning for RL has a rich and diverse existing literature, often interpreted as an abstraction of the original MDP. Previous works have interpreted the hierarchy introduced in hierarchical RL as an MDP abstraction of state, action, and temporal spaces BID25 BID8 BID26 BID2 . In goal-conditioned hierarchical designs, although the representation is learned on states, it is in fact a form of action abstraction (since goals g are high-level actions). While previous successful applications of goal-conditioned hierarchical designs have either learned representations naively end-to-end (Vezhnevets et al., 2017), or not learned them at all BID14 BID17 , we take a principled approach to representation learning in hierarchical RL, translating a bound on sub-optimality to a practical learning objective. Bounding sub-optimality in abstracted MDPs has a long history, from early work in theoretical analysis on approximations to dynamic programming models (Whitt, 1978; BID4 . Extensive theoretical work on state abstraction, also known as state aggregation or model minimization, has been done in both operational research BID20 Van Roy, 2006) and RL BID7 BID19 BID0 . Notably, BID15 introduce a formalism for categorizing classic work on state abstractions such as bisimulation BID7 and homomorphism BID19 based on what information is preserved, which is similar in spirit to our approach. Exact state abstractions BID15 incur no performance loss BID7 BID19 , while their approximate variants generally have bounded sub-optimality BID4 BID7 BID23 BID0 . While some of the prior work also focuses on learning state abstractions BID15 BID23 BID0 , they often exclusively apply to simple MDP domains as they rely on techniques such as state partitioning or Q-value based aggregation, which are difficult to scale to our experimented domains. Thus, the key differentiation of our work from these prior works is that we derive bounds which may be translated to practical representation learning objectives. Our impressive results on difficult continuous-control, high-dimensional domains is a testament to the potential impact of our theoretical findings. Lastly, we note the similarity of our representation learning algorithm to recently introduced scalable mutual information maximization objectives such as CPC (van den Oord et al., 2018) and MINE BID12 . This is not a surprise, since maximizing mutual information relates closely with maximum likelihood learning of energy-based models, and our bounds effectively correspond to bounds based on model-based predictive errors, a basic family of bounds in representation learning in MDPs BID23 BID5 BID0 . Although similar information theoretic measures have been used previously for exploration in RL BID24 , to our knowledge, no prior work has connected these mutual information estimators to representation learning in hierarchical RL, and ours is the first to formulate theoretical guarantees on sub-optimality of the resulting representations in such a framework. We evaluate our proposed representation learning objective compared to a number of baselines:\\u2022 XY: The oracle baseline which uses the x, y position of the agent as the representation.\\u2022 VAE: A variational autoencoder (Kingma & Welling, 2013) on raw observations.\\u2022 E2C: Embed to control (Watter et al., 2015) . A method which uses variational objectives to train a representation of states and actions which have locally linear dynamics.\\u2022 E2E: End-to-end learning of the representation. The representation is fed as input to the higher-level policy and learned using gradients from the RL objective.\\u2022 Whole obs: The raw observation is used as the representation. No representation learning. This is distinct from BID17 , in which a subset of the observation space was pre-determined for use as the goal space. Figure 2: Learned representations (2D embeddings) of our method and a number of variants on a MuJoCo Ant Maze environment, with color gradient based on episode time-step (black for beginning of episode, yellow for end). The ant travels from beginning to end of a \\u2283-shaped corridor along an x, y trajectory shown under XY. Without any supervision, our method is able to deduce this nearideal representation, even when the raw observation is given as a top-down image. Other approaches are unable to properly recover a good representation. Figure 3: Results of our method and a number of variants on a suite of tasks in 10M steps of training, plotted according to median over 10 trials with 30 th and 70 th percentiles. We find that outside of simple point environments, our method is the only one which can approach the performance of oracle x, y representations. These results show that our method can be successful, even when the representation is learned online concurrently while learning a hierarchical policy. We evaluate on the following continuous-control MuJoCo (Todorov et al., 2012) tasks (see Appendix C for details):\\u2022 Ant (or Point) Maze: An ant (or point mass) must navigate a \\u2283-shaped corridor.\\u2022 Ant Push: An ant must push a large block to the side to reach a point behind it.\\u2022 Ant Fall: An ant must push a large block into a chasm so that it may walk over it to the other side without falling.\\u2022 Ant Block: An ant must push a small block to various locations in a square room.\\u2022 Ant Block Maze: An ant must push a small block through a \\u2283-shaped corridor. In these tasks, the raw observation is the agent's x, y coordinates and orientation as well as local coordinates and orientations of its limbs. In the Ant Block and Ant Block Maze environments we also include the x, y coordinates and orientation of the block. We also experiment with more difficult raw representations by replacing the x, y coordinates of the agent with a low-resolution 5 \\u00d7 5 \\u00d7 3 top-down image of the agent and its surroundings. These experiments are labeled 'Images'.For the baseline representation learning methods which are agnostic to the RL training (VAE and E2C), we provide comparative qualitative results in Figure 2 . These representations are the result Ant and block Ant pushing small block through corridor Representations Figure 4 : We investigate importance of various observation coordinates in learned representations on a difficult block-moving task. In this task, a simulated robotic ant must move a small red block from beginning to end of a \\u2283-shaped corridor. Observations include both ant and block x, y coordinates. We show the trajectory of the learned representations on the right (cyan). At four time steps, we also plot the resulting representations after perturbing the observation's ant coordinates (green) or the observation's block coordinates (magenta). The learned representations put a greater emphasis (i.e., higher sensitivity) on the block coordinates, which makes sense for this task as the external reward is primarily determined by the position of the block.of taking a trained policy, fixing it, and using its sampled experience to learn 2D representations of the raw observations. We find that our method can successfully deduce the underlying near-optimal x, y representation, even when the raw observation is given as an image. We provide quantitative results in Figure 3 . In these experiments, the representation is learned concurrently while learning a full hierarchical policy (according to the procedure in BID17 ). Therefore, this setting is especially difficult since the representation learning must learn good representations even when the behavior policy is very far from optimal. Accordingly, we find that most baseline methods completely fail to make any progress. Only our proposed method is able to approach the performance of the XY oracle. For the 'Block' environments, we were curious what our representation learning objective would learn, since the x, y coordinate of the agent is not the only near-optimal representation. For example, another suitable representation is the x, y coordinates of the small block. To investigate this, we plotted ( Figure 4 ) the trajectory of the learned representations of a successful policy (cyan), along with the representations of the same observations with agent x, y perturbed (green) or with block x, y perturbed (magenta). We find that the learned representations greatly emphasize the block x, y coordinates over the agent x, y coordinates, although in the beginning of the episode, there is a healthy mix of the two. We have presented a principled approach to representation learning in hierarchical RL. Our approach is motivated by the desire to achieve maximum possible return, hence our notion of sub-optimality is in terms of optimal state values. Although this notion of sub-optimality is intractable to optimize directly, we are able to derive a mathematical relationship between it and a specific form of representation learning. Our resulting representation learning objective is practical and achieves impressive results on a suite of high-dimensional, continuous-control tasks. We thank Bo Dai, Luke Metz, and others on the Google Brain team for insightful comments and discussions. A PROOF OF THEOREM 3 (GENERALIZATION OF THEOREM 1)Consider the sub-optimality with respect to a specific state s 0 , V DISPLAYFORM0 Recall that \\u03c0 * is the hierarchical result of a policy \\u03c0 * * hi : S \\u2192 \\u2206(\\u03a0), and note that \\u03c0 * * hi may be assumed to be deterministic due to the Markovian nature of M. We may use the mapping \\u03d5 to transform \\u03c0 * * hi to a high-level policy \\u03c0 hi on G and using the mapping \\u03a8: DISPLAYFORM1 Let \\u03c0 hier be the corresponding hierarchical policy. We will bound the quantity V DISPLAYFORM2 hier (s 0 ). We follow logic similar to BID1 and begin by bounding the total variation divergence between the \\u03b3-discounted state visitation frequencies of the two policies. Denote the k-step state transition distributions using either \\u03c0 * or \\u03c0 hier as, DISPLAYFORM3 DISPLAYFORM4 for k \\u2208 [1, c]. Considering P * , P hier as linear operators, we may express the state visitation frequencies d * , d hier of \\u03c0 * , \\u03c0 hier , respectively, as DISPLAYFORM5 DISPLAYFORM6 where \\u00b5 is a Dirac \\u03b4 distribution centered at s 0 and DISPLAYFORM7 DISPLAYFORM8 We will use d c * , dc hier to denote the every-c-steps \\u03b3-discounted state frequencies of \\u03c0 * , \\u03c0 hier ; i.e., DISPLAYFORM9 DISPLAYFORM10 By the triangle inequality, we have the following bound on the total variation divergence |d hier \\u2212d * |: DISPLAYFORM11 We begin by attacking the first term of Equation 28. We note that DISPLAYFORM12 Thus the first term of Equation 28 is bounded by DISPLAYFORM13 \\u22121 as a geometric series and employing the triangle inequality, we have DISPLAYFORM14 , and we thus bound the whole quantity (30) by DISPLAYFORM15 We now move to attack the second term of Equation 28. We may express this term as DISPLAYFORM16 Furthermore, by the triangle inequality we have DISPLAYFORM17 Therefore, recalling w k = 1 for k < c and w k = (1 \\u2212 \\u03b3) \\u22121 for k = c, we may bound the total variation of the state visitation frequencies as DISPLAYFORM18 By condition 7 of Theorem 3 we have, DISPLAYFORM19 We now move to considering the difference in values. We have DISPLAYFORM20 DISPLAYFORM21 Therefore, we have DISPLAYFORM22 as desired. Consider a specific s t , \\u03c0. Let K(s |s t , \\u03c0) \\u221d \\u03c1(s ) exp(\\u2212D(f (s ), \\u03d5(s t , \\u03c0))). Note that the definition of \\u03a8(s t , \\u03d5(s t , \\u03c0)) may be expressed in terms of a KL: DISPLAYFORM0 Therefore we have, DISPLAYFORM1 By condition 8 we have, DISPLAYFORM2 Jensen's inequality on the sqrt function then implies DISPLAYFORM3 Pinsker's inequality now yields, DISPLAYFORM4 Similarly Jensen's and Pinsker's inequality on the LHS of Equation 43 yields DISPLAYFORM5 The triangle inequality and Equations 46 and 47 then give us, DISPLAYFORM6 as desired. C EXPERIMENTAL DETAILS The environments for Ant Maze, Ant Push, and Ant Fall are as described in BID17 . During training, target (x, y) locations are selected randomly from all possible points in the environment (in Ant Fall, the target includes a z coordinate as well). Final results are evaluated on a single difficult target point, equal to that used in BID17 .The Point Maze is equivalent to the Ant Maze, with size scaled down by a factor of 2 and the agent replaced with a point mass, which is controlled by actions of dimension two -one action determines a rotation on the pivot of the point mass and the other action determines a push or pull on the point mass in the direction of the pivot. For the 'Images' versions of these environments, we zero-out the x, y coordinates in the observation and append a low-resolution 5 \\u00d7 5 \\u00d7 3 top-down view of the environment. The view is centered on the agent and each pixel covers the size of a large block (size equal to width of the corridor in Ant Maze). The 3 channels correspond to (1) immovable blocks (walls, gray in the videos), (2) movable blocks (shown in red in videos), and (3) chasms where the agent may fall. Figure 5 : The tasks we consider in this paper. Each task is a form of navigation. The agent must navigate itself (or a small red block in 'Block' tasks) to the target location (green arrow). We also show an example top-down view image (from an episode on the Ant Maze task). The image is centered on the agent and shows walls and blocks (at times split across multiple pixels).The Ant Block environment puts the ant in a 16 \\u00d7 16 square room next to a 0.8 \\u00d7 0.8 \\u00d7 0.4 small movable block. The agent is rewarded based on negative L2 distance of the block to a desired target location. During training, these target locations are sampled randomly from all possible locations. Evaluation is on a target location diagonally opposite the ant. The Ant Block Maze environment consists of the same ant and small movable block in a \\u2283-shaped corridor. During training, these target locations are sampled randomly from all possible locations. Evaluation is on a target location at the end of the corridor. We follow the basic training details used in BID17 . Some differences are listed below:\\u2022 We input the whole observation to the lower-level policy BID17 zero-out the x, y coordinates for the lower-level policy).\\u2022 We use a Huber function for D, the distance function used to compute the low-level reward.\\u2022 We use a goal dimension of size 2. We train the higher-level policy to output actions in [\\u221210, 10] 2 . These actions correspond to desired deltas in state representation.\\u2022 We use a Gaussian with standard deviation 5 for high-level exploration.\\u2022 Additional differences in low-level training (e.g. reward weights and discounting) are implemented according to Section 5.We parameterize f \\u03b8 with a feed-forward neural network with two hidden layers of dimension 100 using relu activations. The network structure for \\u03d5 \\u03b8 is identical, except using hidden layer dimensions 400 and 300. We also parameterize \\u03d5(s, \\u03c0) := f \\u03b8 (s) + \\u03d5 \\u03b8 (s, \\u03c0). These networks are trained with the Adam optimizer using learning rate 0.0001. FORMULA0 ). We compare to variants of our method that are implemented more in the style of CPC. Although we find that using a dot product rather than distance function D is detrimental, a number of distance-based variants of our approach may perform similarly. Figure 7: We provide additional results comparing to variants of \\u03b2-VAE BID10 . We find that even with this additional hyperparameter, the VAE approach to representation learning does not perform well outside of the simple point mass environment. The drawback of the VAE is that it is encouraged to reconstruct the entire observation, despite the fact that much of it is unimportant and possibly exhibits high variance (e.g. ant joint velocities). This means that outside of environments with high-information state observation features, a VAE approach to representation learning will suffer. Figure 8: We evaluate the ability of our learned representations to transfer from one task to another. For these experiments, we took a representation function f learned on Ant Maze, fixed it, and then used it to learn a hierarchical policy on a completely different task. We evaluated the ability of the representation to transfer to \\\"Reflected Ant Maze\\\" (same as Ant Maze but the maze shape is changed from '\\u2283' to '\\u2229') and \\\"Ant Push\\\". We find that the representations are robust these changes to the environment and can generalize successfully. We are able to learn well-performing policies in these distinct environments even though the representation used was learned with respect to a different task. Ant Maze Env XY Ours Ours (Images) Figure 9 : We replicate the results of Figure 2 but with representations learned according to data collected by a random higher-level policy. In this setting, when there is even less of a connection between the representation learning objective and the task objective, our method is able to recover near-ideal representations. BID17 . The representation used in the original formulation of HIRO is a type of oracle -sub-goals are defined as only the position-based (i.e., not velocity-based) components of the agent observation. In our own experiments, we found this method to perform similarly to the XY oracle in non-image tasks. However, when the state observation is more complex (images) performance is much worse.\",\n          \"Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation. Despite decades of research, the design of neural networks is still an empirical process. Practitioners make design choices, such as the number of layers, type of layers, number of filters per layer, etc., based on intuition or brute-force search. Nevertheless, the performance of these algorithms, together with the advances of GPU devices, have led to a growing popularity of these techniques in both academia and industry. Recent advances are unveiling some properties of neural networks. For example, there is a consensus that depth can accelerate learning, and that wider layers help optimization BID2 BID29 . However, in practical applications, the size of these networks is often a limiting factor when deploying on devices with constrained storage, memory, and computation resources. Another known neural network property is that the responses of a layer exhibit considerable correlation BID12 , inspiring the idea of learning decorrelated filters BID8 BID38 . These algorithms propose a modified loss function to encourage decorrelation during training and show that accuracy improves with decorrelated filters. However, such algorithms focus on training and do not address network compression. Our hypothesis is that layers that exhibit high correlation in filter responses could learn equally well using a smaller number of filters. Principal Filter Analysis (PFA) draws from the recent findings that it is easier to start with an overparametrized network and it then exploits intra-layer correlation for guiding network compression. PFA analyzes a trained network and is agnostic to the training methodology and the loss function. Inference is performed on a dataset, and the correlation within the responses of each layer is used to provide a compression recipe. A new smaller architecture based on this recipe can then be retrained. We propose two closed-form algorithms based on spectral energy analysis for suggesting the number of filters to remove in a layer:PFA-En uses Principal Component Analysis (PCA) BID21 to allow a user to specify the proportion of the energy in the original response that should be preserved in each layer; PFA-KL is a heuristic that leads to a parameter-free approach that uses Kullback-Leibler (KL) divergence BID24 to identify the number of redundant filters. Based on the new suggested number of filters per layer identified by PFA, we remove those that are maximally correlated with other filters and fine-tune the network by retraining. Both PFA algorithms are straightforward to implement and, as shown in Sec. 4, they achieve better compression and, in most cases, better accuracy than the state of the art on several datasets and architectures. In Sec. 4 we also show how PFA can be used to perform simulations compression and domain adaptation. The field of network compression encompasses a wide range of techniques that can be grouped into the following families: quantization, knowledge distillation, tensor factorization and network pruning. Quantization algorithms compress networks by reducing the number of bits used to represent each weight BID14 BID37 BID18 BID47 .Knowledge distillation BID20 and model compression BID4 aim to create a simpler model that mimics the output of a more complex model. Variations on this concept include Ba & Caurana (2014); BID39 ; .Tensor factorization algorithms exploit the redundancy present in convolution layers by replacing the original tensors with a sequence of smaller or sparser counterparts that produce similar responses BID13 BID25 BID22 BID32 BID45 BID50 BID1 BID35 .Network pruning is a family of techniques that compress networks by iteratively removing connections based on the salience of their weights. Early work, like Optimal Brain Damage BID26 and Optimal Brain Surgeon BID16 , targeted fully connected networks. Recent work can be divided into two sub-families: sparse pruning BID15 BID42 BID44 BID43 BID0 BID5 BID10 , where individual neurons are removed, and structured pruning BID19 BID31 BID33 BID49 , where entire filters are removed. PFA falls within the family of structured network pruning. Some of these techniques, e.g., , require user defined parameters that are hard to choose and whose effect on the footprint is difficult to predict (see Sec. 4.6 for a more detailed discussion). Others also require modification of the loss function, e.g., . In contrast, PFA-En has only one intuitive parameter, which is the proportion of the response energy to be preserved at each layer, and PFA-KL is parameter-free. Furthermore, instead of learning the saliency of the filters during training by modifying the loss function, PFA estimates it after training without requiring knowledge of the training details. This makes PFA applicable to any trained network, without the need to know its loss function. Within the structured pruning family, there are approaches based on the singular value decomposition (SVD) BID48 BID34 BID36 , where a new set of filters are obtained by projecting the original weights onto a lower dimensional space. PFA differs from these methods by the fact that SVD is performed on the responses of the layers rather than on the filter weights, and no projection is done. This is particularly relevant for domain adaption applications, where a trained network is specialized for a different task. Techniques that make compression decisions based on weights, rather than the responses, cannot take into account the specificity of the task. As shown in Sec. 4.7, PFA derives different architectures from the same initial model when analyzed for different tasks. Some methods, e.g., ; BID32 BID35 , also reason on the layer responses. These techniques aim to find a smaller set of filters that, in different ways, minimize the reconstruction error of the feature maps or the response output. Note that PFA uses the spectral energy of the filters' responses only to decide how many filters should be preserved. Similarly, the subsequent filter selection process does not take the reconstruction error into account. PFA is also philosophically and practically different: PFA uses the concept of correlation within the responses to identify redundancy within a layer. In practice this means that PFA can compress all layers in one run, while the majority of the techniques that use responses need to operate on one layer at the time. Finally, PFA is orthogonal to the quantization, tensor factorization and distillation methods, and could be used as a complementary strategy to further compress neural networks. In this section, PFA-En and PFA-KL algorithms are described in detail. Both algorithms share the idea of exploiting correlations between responses in convolutional layers and neurons in fully connected layers to obtain a principled recommendation for network compression. PFA is inherently data driven and thus takes advantage of a dataset {X i } \\u2208 R M \\u00d7I where X i is the i th input data sample, M is the number of samples in the dataset, and I is the input dimensionality. Typically, this dataset is the data used to train the network, but it can also be a representative set that covers the distribution of inputs likely to be encountered. Without loss of generality, we assume that the input data are images:{X i } \\u2208 R M \\u00d7H\\u00d7W \\u00d7C , where H is the image height, W is the image width and C is the number channels. ] be the output tensor produced by a given layer of a network on the i th input sample. Any operation in the network is considered a layer (e.g., batch normalization, ReLU, etc.) . In this work we analyze the output of convolutional and fully connected layers. However, PFA can be used to analyze the output of any layer in the network. Recurrent neural networks could also be analyzed, but are out of the scope for this paper. DISPLAYFORM0 For a convolutional layer , let DISPLAYFORM1 , where * denotes the convolution operator. We omit the bias term to improve readability. We define the response vector a DISPLAYFORM2 of a given layer with respect to an input X i to be the spatially max-pooled and flattened tensor T DISPLAYFORM3 , and since no pooling is required, the response vector is a DISPLAYFORM4 be the matrix of responses of layer given a dataset with M samples. DISPLAYFORM5 be the distribution of eigenvalues of the covariance matrix of A [ ] sorted in descending order and normalized to sum to 1. In the following sections we present two algorithms that exploit the distribution \\u03bb [ ] to guide network compression. The distribution \\u03bb [ ] provides insight into the correlation within layer . The closer \\u03bb [ ] is to a uniform distribution, the more decorrelated the response of the filters and the more uniform their contribution to the overall response energy. Conversely, the closer \\u03bb [ ] is to a Dirac \\u03b4-distribution, the more correlated the filters. Our hypothesis is that layers that exhibit high correlation in filter responses could learn equally well using a smaller number of filters. Now that we have defined the key ingredient (\\u03bb [ ] ) for PFA, we present two strategies that produce a compression recipe with the goal of maximizing compression while reducing correlation. Let a compression recipe DISPLAYFORM0 , be the set of compression factors applied to each of the L layers included in the analysis. For example, \\u03b3[3] = 0.6 means that we keep 60% of the filters in layer 3.Once the correct number of filters have been determined by the recipe, one could further proceed to choose which filters should be kept. We call this filter selection and we outline it in Sec. 3.2.3. PCA can be used for dimensionality reduction by performing a linear mapping to a lower dimensional space that maximizes the variance of the data in this space. This can be accomplished by extracting the eigenvectors and eigenvalues of the covariance matrix. The original data are then reconstructed using the minimum number of eigenvectors that correspond to the eigenvalues that sum up to the desired energy factor \\u03c4 . Inspired by this strategy, we propose to keep the minimum set of filters such that a fraction of response energy greater or equal to a user defined \\u03c4 is preserved. We define the energy at a given compression ratio for a layer as DISPLAYFORM0 and we propose to re-architect the network according to the following recipe: DISPLAYFORM1 The parameter \\u03c4 provides the user with the ability to guide the compression ratio. PFA-En has the advantage of being tightly connected to well-established dimensionality reduction techniques based on PCA, it is simple to implement and uses a single, highly intuitive parameter. Furthermore, since evaluating the size of a model (or its FLOPs) obtained at different energy thresholds is easy and fast, it is straightforward to replace the parameter \\u03c4 with the desired footprint (or FLOPs) after compression. Being able to specify a target footprint instead of an energy threshold gives PFA-En even more appeal for practical use cases. We propose an alternative formulation to obtain an optimal recipe \\u0393 KL , based on KL divergence. This formulation is a heuristic that frees PFA from the use of any parameter. As previously mentioned, a distribution \\u03bb [ ] similar to a flat distribution implies an uncorrelated response of the filters in layer . Therefore, the farther the distribution \\u03bb [ ] is from a flat distribution the more layer can be compressed. Let us define u as the desired uniform distribution (no correlation between filters), and d = Dirac(k) as the worst case distribution (all filters are perfectly correlated). We can measure the dissimilarity of the actual distribution, \\u03bb [ ] , from the desired distribution, u, as the KL divergence KL(\\u03bb [ ] , u). The upper bound of which is given by u KL = KL(d, u), while the lower bound is 0. Notice that one could replace the KL divergence with any dissimalarity measure between distributions, such as \\u03c7 2 or the earth mover's distance BID40 .Intuitively, when the actual distribution is identical to the ideal distribution (i.e., no correlation found) then we would like to preserve all filters. On the other hand, when the actual distribution is identical to the worst case distribution (i.e., all filters are maximally correlated) then one single filter would be sufficient. The proposed KL divergence-based recipe is a mapping \\u03c8 : [0, u KL ] \\u2192 (0, 1]; a divergence close to the upper bound results in a strong compression and a divergence close to the lower bound results in a milder compression: DISPLAYFORM0 In this work, we use a simple linear mapping \\u03c8(x, u KL ) = 1\\u2212 x /u KL . Other mappings were explored, leading to different degrees of compression; however, we have empirically observed that a linear mapping produces good results that generalize well across networks. The recipes produced by PFA-En and PFA-KL provide the number of filters, DISPLAYFORM0 , that should be kept in each layer, but do not indicate which filters should be kept. One option is to retrain the compressed network from a random initialization (from scratch). In this case it does not matter which filters are chosen. An alternative is to select which filters to keep and use their values for initialization. We do this by removing those filters in each layer that are maximally correlated. We compute the 1 -norm of each row of the correlation matrix from the matrix A [ ] and remove the filter with the largest norm. If more filters need to be chosen then we update the correlation matrix by removing the previously selected filter, and iterate until the desired number of filters has been removed. In the rare, but theoretically possible, case in which two filters have the same 1 -norm we choose the one with the highest individual correlation coefficient. To evaluate PFA, we analyze several network architectures and datasets, and our results are compared to the state of the art. Specifically we will contrast PFA against the filter pruning approach (FP) in , the network slimming approach (NS) in , the variational information bottleneck approach (VIB) in BID10 and the filter group approximation approach (FGA) in BID35 . For comparison, we focus on the compression ratio and the accuracy change, measured in percentage points (pp), obtained by the compressed architectures. This enables plotting various techniques in the same plot, even if the accuracy of each original architecture is slightly different because of different training strategies used. After the Sate of the Art quantitative comparison, PFA is tested on the task of simultaneous compression and domain adaptation. Three networks are evaluated in this paper: VGG-16 BID41 (we use the version proposed by BID51 for CIFAR), ResNet-56 BID17 , and SimpleCNN, a small CNN we use to demonstrate the capabilities of PFA. Refer to Appendix A for the architecture details. We train and test all the networks on CIFAR-10 and CIFAR-100 BID23 ). In addition we test VGG-16 BID30 on ImageNet BID11 ). A baseline for each architecture is obtained by training using 10 random initializations -we choose the initialization that leads to the highest test accuracy and perform inference on the training set to obtain the responses at each layer (A [ ] ) needed for the PFA analysis. PFA analyzes all layers in parallel to obtain its recipes. Compressed architectures are created by following the recipes from each PFA strategy. For each new compressed architecture, we perform two types of training. First, we retrain with 10 different random initializations and this set of results are referred to as Scratch. Second, we retrain 10 times using the filter selection method described in Sec. 3.2.3, and fine-tune the compressed network starting from the weights of the selected filters. The accuracy reported is the mean of these 10 retrainings. Note that the retraining is done without hyper-parameter tuning. While this is a sub-optimal strategy, it removes any ambiguity on how well the parameters were tuned for the baseline compared to the compressed networks. In practice, one would expect to attain even better results if parameter sweeping was done on the compressed networks. In all experiments we use the SGD optimizer with no decay, learning rate 0.1, Nesterov momentum 0.9, 50 epochs for SimpleCNN and 160 for VGG-16 and ResNet-56.PFA-En is computed for every energy value: \\u03c4 \\u2208 {0.8, 0.85, 0.9, 0.925, 0.95, 0.96, 0.97, 0.98, 0.99}, whereas PFA-KL is parameter-free and is computed once per baseline network. An empirical upper bound on the accuracy at different compression ratios is obtained by randomly choosing how many filters to remove at each layer. By repeating this a sufficient number of times, the best result at each compression ratio can be considered an empirical upper bound for that architecture and footprint, labeled Upper bound. On the other hand, the result averaged across all trials is representative of how easy (or difficult) it is to randomly compress a network without hurting its accuracy, labeled Avg. random. Results tagged with the word Scratch were trained from a random initialization while those without the tag were obtained using filter selection. In these experiments we generated and trained 300 randomly pruned architectures for each compression ratio. In FIG0 the On CIFAR-10, both PFA strategies trained from scratch produce results close to Upper bound Scratch and significantly better than the Avg. random Scratch results. This indicates that choosing the correct number of filters for each layer is crucial even for footprints that are close to the original model, and shows that PFA is able to perform this choice close to optimally. PFA-En Scratchand PFA-KL Scratch both achieve a footprint close to 60% with an accuracy drop of only 0.5 pp. When performing filter selection, rather than retraining from scratch, both PFA-En and PFA-KL improve their accuracy by about 1 pp above the Scratch version for footprints above 40%. For footprints above 42%, the filter selection strategy performs even better than Upper bound Scratch, whereas for footprints less than 40%, the accuracy starts to degrade. On CIFAR-100, filter selection increases its gain up to 2 pp compared to the Scratch version, despite the more challenging nature of the dataset as indicated by the faster drop in accuracy past the 40% footprint mark. We have found that the filter selection strategy converges faster during training and performs consistently better across different architectures and datasets, hence from now on we will only report results using PFA with filter selection. Interestingly, at the 10% footprint mark a random initialization appears to be better than the use of filter selection. It is possible that when keeping an extremely small number of filters, the starting point provided by the filter selection becomes a local minimum that is difficult to escape. For thin layers in relatively small architectures (like SimpleCNN), a random initialization may give more room for exploration during the learning phase. For VGG-16 we compare the results of PFA with those reported by FP, NS, VIB 1 and FGA 2 , after a single iteration of NS for a direct comparison with the other methods (see FIG1 and Tab. 1). Note that VIB performs sparse pruning (single neurons) rather than structured pruning. The main results are summarized in Tab. 1.PFA-En and PFA-KL outperform all the other methods both on CIFAR-10 and CIFAR-100 in terms of compression ratio. On CIFAR-10, PFA-En (\\u03c4 = 0.98) obtains a model 2.5x, 2.8x and 4.5x smaller than NS, FP and VIB respectively. On CIFAR-100, PFA-En also obtains models 1.2x, 2x smaller than NS and VIB. Despite achieving smaller models the accuracy change is comparable to that of the other techniques: all techniques achieve an accuracy change between -1pp and +2pp from the baseline. Similarly, the FLOPs reduction is inline with that achieved by other techniques. Note that DISPLAYFORM0 PFA-En obtains similar model sizes as FGA on CIFAR-100, but with better accuracy for compression ratios over 13%. At a comparable compression ratio of 40%, PFA-En achieves +1pp better accuracy than FGA.For ResNet-56 on CIFAR-10 the same conclusions hold true. PFA-En (\\u03c4 = 0.98) achieves an accuracy similar to FP but with a smaller footprint: 61.5% compared to 86.3%. PFA-KL achieves an even smaller footprint: 59.6%, with an accuracy drop of only 0.6 pp. Similar results are observed on CIFAR-100, as shown in Fig. B .1b in Appendix B.For VGG-16 on ImageNet we compare PFA-KL with FGA (note that NS also reports results on VGG-16 but with fully connected layers). While FGA reports footprints between 74.4% and 23.18% we report in Tab. 1 the footprint most closely comparable with PFA-KL: at a slightly smaller footprint PFA-KL achieves a better accuracy improvements than FGA, increasing the accuracy over the baseline by 2.4% in the top-1 accuracy. In summary, independently of the architecture or dataset, PFA consistently provides better compression or accuracy than the state of the art. The complexity of PFA (excluding the inference step), with respect to number of filters and dataset size, is dominated by the the PCA analysis which, for a given layer, is O(mn 2 + n 3 ), n being the number of filters, and m the number of samples. For example, for ImageNet BID11 , m=1.2M, and assuming a VGG-16 architecture with layers of size n equal to 64, 128, 256, 512, and 4096 , the time to compute PFA per layer is roughly 1.24s, 2.8s, 4.6s, 9.3s, and 127.5s respectively (single CPU @ 2.30GHz). The complexity of the filter selection only depends on the layer size. In the worst case is O(\\u00f1n 2 ), where\\u00f1 is the number of filters to remove. Considering that PFA has to run once at the end of the training step, the time consumed by PFA is negligible compared to the whole training time. In exchange for this negligible extra-time, PFA provides the long-term benefit of a smaller footprint and faster inference, which, in the lifetime of a deployed network, including re-training when new data becomes available, will quickly surpass the time initially required by PFA. All the techniques we used for comparison achieve good results in term of maintaining accuracy and reducing FLOPs. In our view their main limitation, and difference with respect to PFA, is in the user defined parameters that they rely upon. NS has two crucial parameters: the weight of the sparsity regularizer, and the percentage of filters to be pruned. The weight has a direct impact on the induced sparsity, however, there is no intuitive way to set this parameter and it needs to be tuned for each architecture and dataset. In addition, setting the same percentage of filters to be pruned at each layer for the whole network ignores the relative effect of those filters on the accuracy and the footprint of the network. Setting the thresholds in FP is non-trivial and it requires the user to choose the compression thresholds based on a pre-analysis that provides insight on the sensitivity of each layer to pruning. Both VIB and FGA require a user tuned parameter that controls how much each layer is compressed. The tuning is different depending on the network-dataset combination and depending on the layer. From the results there seem to be no intuitive way to set this parameter other than trial and error. In contrast, PFA achieves comparable accuracy and FLOPs with a much higher compression ratio. In addition, one of the advantages of PFA is that it requires a single intuitive parameter (for example the desired footprint in PFA-En), or it is parameter-free (PFA-KL). On the other hand, all the state of the art techniques we used for comparison require a tuning step. The typical PFA use case is to compress a network for the same initial domain D A used for training. However, PFA can be computed using data from a different domain, D Z . The result is a compressed architecture specialized for the target domain D Z that takes advantage of the training initially performed on D A . In this section we show that PFA derives different architectures from the same initial model when analyzed for different tasks, hence, in addition to compression PFA can also be used for domain adaptation. In order to test PFA on domain adaptation we randomly sample classes out of the original 100 contained in CIFAR-100. We generate two sub-sets (D \\u2022 PFA fine: Train from scratch on domain D A , run PFA-KL on domain D Z and train compressed architecture using filter selection on D Z .The results in Fig. 3 show how the PFA fine strategy performs very similarly to the full fined tuned model (Full fine), while obtaining models more than 4 times smaller. Moreover, The PFA fine strategy largely outperforms the full model trained from scratch on the target domain (Full scratch).The PFA-KL recipes when adapting from D A are very similar to each other, independently of the target doamin (Fig. 3d) , while those obtained adapting from D C100 A vary (Fig. 3b) . We believe that the fact that the target domain is a subset of D C100 A and not of D A is the explanation to this effect. In the D C100 A case responses are stronger and selective, since the images used for PFA were also used for training, leading to stronger correlations. PFA is able to exploit such effect, obtaining recipes adapted to the complexity of the task. Note how PFA obtains recipes with more filters for the 10 class subsets (Fig. 3b) . On the other hand, the model trained on D A has never seen the target domain, therefore this generates less correlation. As a result PFA generates more uniform recipes, as shown in Fig. 3d .These results show how PFA is able to transfer knowledge to different domains. When the original network had been exposed to the target domain (Figs. 3a, 3b) , PFA is able to exploit such knowledge and create recipes adapted to the task. On the other hand, PFA obtains more uniform (and still accurate, Figs. 3c, 3d ) recipes when the original network had not been exposed to the target domain. Two effective, and yet easy to implement techniques for the compression of neural networks using Principal Filter Analysis were presented: PFA-En and PFA-KL. These techniques exploit the inherent correlation of filter responses within layers to compress networks without compromising accuracy. These techniques can be applied to the output response of any layer with no knowledge of the training procedure or the loss function. While easy to implement, both algorithms surpass state of the art results in terms or compression ratio with the advantage that PFA-KL is parameter free and PFA-En has only a single intuitive parameter: the energy to be preserved in each layer or a desired network characteristic (such as a target footprint or FLOPs).By performing spectral analysis on the responses rather than the weights, PFA allows users to take advantage of transfer learning in order to perform domain adaptation and derive smaller specialized networks that are optimally designed for a specific task, while starting from the same base model. It is interesting to observe that all compression algorithms in the state of art, including PFA-KL, do not converge if applied repeatedly. This is due to the retraining step which alters the weights in order to optimize an objective function that does not take into account compression criteria. In practical applications this is not a limitation since most of the models tend to be over specified and users can apply compression algorithms until the accuracy falls below an acceptable level. However, it is an interesting theoretical limitation that could inspire future work. Specifically for PFA, this could be a strategy that can also suggest expanding layers so that an optimal size could be reached at convergence.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"rkgd0iA9FQ\",\n          \"H1emus0qF7\",\n          \"rkl42iA5t7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"In this paper we prove convergence to criticality of (stochastic and deterministic) RMSProp and deterministic ADAM for smooth non-convex objectives and we demonstrate an interesting beta_1 sensitivity for ADAM on autoencoders.  This paper presents a convergence analysis of RMSProp and ADAM in the case of smooth non-convex functions\",\n          \"We translate a bound on sub-optimality of representations to a practical training objective in the context of hierarchical reinforcement learning. The authors proposes a novel approach in learning a representation for HRL and state an intriguing connection between representation learning and bounding the sub-optimality which results in a gradient based algorithm This paper proposes a way to handle sub-optimality in the context of learning representations which refer to the sub-optimality of hierarchical polity with respect to the task reward.\",\n          \"We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints. Proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. This paper introduces an approach to compressing neural networks by looking at the correlation of filter responses in each layer via two strategies. This paper proposes a compression method based on spectral analysis\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Convergence Guarantees for RMSProp and ADAM in Non-Convex Optimization and an Empirical Comparison to Nesterov Acceleration\",\n          \"Near-Optimal Representation Learning for Hierarchical Reinforcement Learning\",\n          \"NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26,\n        \"min\": 30,\n        \"max\": 90,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          48,\n          79,\n          90\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"We show that at very high values of the momentum parameter (\\\\beta_1 = 0.99) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses. While (to the best of our knowledge) there is no clear theoretical justification in the stochastic case of the benefits of NAG and HB over regular SGD in general (Yuan et al., 2016; Kidambi et al., 2018; Wiegerinck et al., 1994; Orr & Leen, 1994; Yang et al., 2016; Gadat et al., 2018) , unless considering specialized function classes (Loizou & Richt\\u00e1rik, 2017) ; in practice, these momentum methods, and in particular NAG, have been repeatedly shown to have good convergence and generalization on a range of neural net problems (Sutskever et al., 2013; Lucas et al., 2018; Kidambi et al., 2018) .The performance of NAG (as well as HB and SGD), however, are typically quite sensitive to the selection of its hyper-parameters: step size, momentum and batch size (Sutskever et al., 2013) . Adaptive gradient methods use as their update direction a vector which is the image under a linear transformation (often called the \\\"diagonal pre-conditioner\\\") constructed out of the history of the gradients, of a linear combination of all the gradients seen till now. Further, there are also important motivations to study the behavior of these algorithms in the deterministic setting because of usecases where the amount of noise is controlled during optimization, either by using larger batches (Martens & Grosse, 2015; De et al., 2017; Babanezhad et al., 2015) or by employing variance-reducing techniques (Johnson & Zhang, 2013; Defazio et al., 2014) .Further, works like Wilson et al. (2017) and Keskar & Socher (2017) have shown cases where SGD (no momentum) and HB (classical momentum) generalize much better than RMSProp and ADAM with stochastic gradients. In the full-batch setting, we demonstrate that ADAM with very high values of the momentum parameter (\\u03b2 1 = 0.99) matches or outperforms carefully tuned NAG and RMSProp, in terms of getting lower training and test losses. We further validate this behavior on an image classification task on CIFAR-10 using a VGG-9 convolutional neural network, the results to which we present in the Appendix E. We note that recently it has been shown by Lucas et al. (2018) , that there are problems where NAG generalizes better than ADAM even after tuning \\u03b2 1 . Much after this work was completed we came to know of a related paper (Li & Orabona, 2018 ) which analyzes convergence rates of a modification of AdaGrad (not RMSPRop or ADAM).After the initial version of our work was made public, a few other analysis of adaptive gradient methods have also appeared like Chen et al. (2018) , Zhou et al. (2018) and Zaheer et al. (2018) . Square root of the Penrose inverse DISPLAYFORM1 , where {e i } {i=1,...,d} is the standard basis of R d Now we list out the pseudocodes used for NAG, RMSProp and ADAM in theory and experiments, Nesterov's Accelerated Gradient (NAG) Algorithm Algorithm 1 NAG 1: Convergence of deterministic RMSProp -the version with standard speeds (Proof in subsection A.2) Let f : R d \\u2192 R be L\\u2212smooth and let \\u03c3 < \\u221e be an upperbound on the norm of the gradient of f . It is sometimes believed that ADAM gains over RMSProp because of its \\\"bias correction term\\\" which refers to the step length of ADAM having an iteration dependence of the following form, 1 \\u2212 \\u03b2 t 2 /(1 \\u2212 \\u03b2 t 1 ). Then the output\\u1e91 \\u2208 R d of the autoencoder is defined as\\u1e91 DISPLAYFORM10 This defines an autoencoder with 2 \\u2212 1 hidden layers using weight matrices and 2 bias vectors. However, for larger nets, the full-batch behavior continues, i.e., when ADAM's momentum parameter \\u03b2 1 is pushed closer to 1, it gets better generalization (significantly lower test losses) than NAG at any momentum tested.\\u2022 In general, for all metrics (test loss, training loss and gradient norm reduction) both ADAM as well as NAG seem to improve in performance when their momentum parameter (\\u00b5 for NAG and \\u03b2 1 for ADAM) is pushed closer to 1. This lets us write the following bounds, DISPLAYFORM1 i and this lets us get the following bounds, DISPLAYFORM2 Now we invoke the bounded gradient assumption about the f i functions and replace in the above equation the eigenvalue bounds of the pre-conditioner by worst-case estimates \\u00b5 max and \\u00b5 min defined as, DISPLAYFORM3 Using the L-smoothness of f between consecutive iterates x t and x t+1 we have, DISPLAYFORM4 We note that the update step of stochastic RMSProp is x t+1 = x t \\u2212 \\u03b1(V t ) \\u2212 1 2 g t where g t is the stochastic gradient at iterate x t . Now we can invoke these stochastic oracle's properties and take a conditional (on H t ) expectation over g t of the L\\u2212smoothness in equation to get, DISPLAYFORM6 We now separately analyze the middle term in the RHS above in Lemma A.1 below and we get, DISPLAYFORM7 We substitute the above into equation 1 and take expectations over H t to get, DISPLAYFORM8 Doing the above replacements to upperbound the RHS of equation 2 and summing the inequation over t = 1 to t = T and taking the average and replacing the LHS by a lowerbound of it, we get, DISPLAYFORM9 Replacing into the RHS above the optimal choice of, DISPLAYFORM10 Thus stochastic RMSProp with the above step-length is guaranteed is reach \\u2212criticality in number of iterations given by, T \\u2264 DISPLAYFORM11 Lemma A.1. Further our instantiation of the oracle is equivalent to doing the uniformly at random sampling, (g t ) i \\u223c {a pi } p=1,...,k .Given that we have, DISPLAYFORM15 i +di where we have defined DISPLAYFORM16 This leads to an explicit form of the needed expectation over the t th \\u2212oracle call as, DISPLAYFORM17 Substituting the above (and the definition of the constants a pi ) back into equation 3 we have, DISPLAYFORM18 Substituting this, the above expression can be written as, DISPLAYFORM19 Note that with this substitution, the RHS of the claimed lemma becomes, DISPLAYFORM20 Therefore our claim is proved if we show that for all i, DISPLAYFORM21 To further simplify, we define DISPLAYFORM22 \\u2212\\u00b5 min . In Figure 5 , we show the same effect of changing \\u03be as in Section 5.1 on a 1 hidden layer network of 1000 nodes, while keeping all other hyper-parameters fixed (such as learning rate, \\u03b2 1 , \\u03b2 2 ).\",\n          \"To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. In the following section, we derive a series of bounds which establish a more direct relationship between SubOpt(\\u03a8) and f . Our main result will show that if one defines \\u03a8 as a slight modification of the traditional objective given in Equation 1, then one may translate sub-optimality of \\u03a8 to a practical representation learning objective for f . This is essentially a representation learning objective for choosing f , and in Section 5 we describe how to optimize it in practice. To approach this issue, we take advantage of the representation learning objective for f, \\u03d5. When f, \\u03d5 are optimized as dictated by Equation 8, we have DISPLAYFORM1 We may therefore approximate the low-level reward as DISPLAYFORM2 As in Section 5.1, we use the sampled actions a t:t+c\\u22121 to represent \\u03c0 as input to \\u03d5. We approximate the third term of Equation 18 analogously to Equation 15. While some of the prior work also focuses on learning state abstractions BID15 BID23 BID0 , they often exclusively apply to simple MDP domains as they rely on techniques such as state partitioning or Q-value based aggregation, which are difficult to scale to our experimented domains. We evaluate our proposed representation learning objective compared to a number of baselines:\\u2022 XY: The oracle baseline which uses the x, y position of the agent as the representation.\\u2022 VAE: A variational autoencoder (Kingma & Welling, 2013) on raw observations.\\u2022 E2C: Embed to control (Watter et al., 2015) . We also experiment with more difficult raw representations by replacing the x, y coordinates of the agent with a low-resolution 5 \\u00d7 5 \\u00d7 3 top-down image of the agent and its surroundings. The learned representations put a greater emphasis (i.e., higher sensitivity) on the block coordinates, which makes sense for this task as the external reward is primarily determined by the position of the block.of taking a trained policy, fixing it, and using its sampled experience to learn 2D representations of the raw observations. To investigate this, we plotted ( Figure 4 ) the trajectory of the learned representations of a successful policy (cyan), along with the representations of the same observations with agent x, y perturbed (green) or with block x, y perturbed (magenta). Although this notion of sub-optimality is intractable to optimize directly, we are able to derive a mathematical relationship between it and a specific form of representation learning. Considering P * , P hier as linear operators, we may express the state visitation frequencies d * , d hier of \\u03c0 * , \\u03c0 hier , respectively, as DISPLAYFORM5 DISPLAYFORM6 where \\u00b5 is a Dirac \\u03b4 distribution centered at s 0 and DISPLAYFORM7 DISPLAYFORM8 We will use d c * , dc hier to denote the every-c-steps \\u03b3-discounted state frequencies of \\u03c0 * , \\u03c0 hier ; i.e., DISPLAYFORM9 DISPLAYFORM10 By the triangle inequality, we have the following bound on the total variation divergence |d hier \\u2212d * |: DISPLAYFORM11 We begin by attacking the first term of Equation 28. Final results are evaluated on a single difficult target point, equal to that used in BID17 .The Point Maze is equivalent to the Ant Maze, with size scaled down by a factor of 2 and the agent replaced with a point mass, which is controlled by actions of dimension two -one action determines a rotation on the pivot of the point mass and the other action determines a push or pull on the point mass in the direction of the pivot. Ant Maze Env XY Ours Ours (Images) Figure 9 : We replicate the results of Figure 2 but with representations learned according to data collected by a random higher-level policy. The representation used in the original formulation of HIRO is a type of oracle -sub-goals are defined as only the position-based (i.e., not velocity-based) components of the agent observation. In our own experiments, we found this method to perform similarly to the XY oracle in non-image tasks.\",\n          \"We propose two closed-form algorithms based on spectral energy analysis for suggesting the number of filters to remove in a layer:PFA-En uses Principal Component Analysis (PCA) BID21 to allow a user to specify the proportion of the energy in the original response that should be preserved in each layer; PFA-KL is a heuristic that leads to a parameter-free approach that uses Kullback-Leibler (KL) divergence BID24 to identify the number of redundant filters. Both PFA algorithms are straightforward to implement and, as shown in Sec. 4, they achieve better compression and, in most cases, better accuracy than the state of the art on several datasets and architectures. Within the structured pruning family, there are approaches based on the singular value decomposition (SVD) BID48 BID34 BID36 , where a new set of filters are obtained by projecting the original weights onto a lower dimensional space. In practice this means that PFA can compress all layers in one run, while the majority of the techniques that use responses need to operate on one layer at the time. PCA can be used for dimensionality reduction by performing a linear mapping to a lower dimensional space that maximizes the variance of the data in this space. A baseline for each architecture is obtained by training using 10 random initializations -we choose the initialization that leads to the highest test accuracy and perform inference on the training set to obtain the responses at each layer (A [ ] ) needed for the PFA analysis. An empirical upper bound on the accuracy at different compression ratios is obtained by randomly choosing how many filters to remove at each layer. On CIFAR-100, filter selection increases its gain up to 2 pp compared to the Scratch version, despite the more challenging nature of the dataset as indicated by the faster drop in accuracy past the 40% footprint mark. Interestingly, at the 10% footprint mark a random initialization appears to be better than the use of filter selection. In our view their main limitation, and difference with respect to PFA, is in the user defined parameters that they rely upon. In addition, setting the same percentage of filters to be pruned at each layer for the whole network ignores the relative effect of those filters on the accuracy and the footprint of the network. The result is a compressed architecture specialized for the target domain D Z that takes advantage of the training initially performed on D A . We generate two sub-sets (D \\u2022 PFA fine: Train from scratch on domain D A , run PFA-KL on domain D Z and train compressed architecture using filter selection on D Z .The results in Fig. 3 show how the PFA fine strategy performs very similarly to the full fined tuned model (Full fine), while obtaining models more than 4 times smaller. On the other hand, PFA obtains more uniform (and still accurate, Figs. 3c, 3d ) recipes when the original network had not been exposed to the target domain. These techniques can be applied to the output response of any layer with no knowledge of the training procedure or the loss function. While easy to implement, both algorithms surpass state of the art results in terms or compression ratio with the advantage that PFA-KL is parameter free and PFA-En has only a single intuitive parameter: the energy to be preserved in each layer or a desired network characteristic (such as a target footprint or FLOPs).By performing spectral analysis on the responses rather than the weights, PFA allows users to take advantage of transfer learning in order to perform domain adaptation and derive smaller specialized networks that are optimally designed for a specific task, while starting from the same base model.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random.seed(42)\n",
        "\n",
        "# def shuffle_list(text):\n",
        "#     lst = text.split('. ')\n",
        "#     lst[:-1] = [sentence + '.' for sentence in lst[:-1]]\n",
        "#     random.shuffle(lst)\n",
        "#     return ' '.join(lst)\n",
        "\n",
        "# data['target'] = data['target'].apply(shuffle_list)"
      ],
      "metadata": {
        "id": "03zjLD42tXLy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "66d5f1d3-3d01-40a5-b851-1eda1adab99b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "9a808703-fdcf-4657-e1a5-c1ea632d3e08"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABat0lEQVR4nO3deVgUV/o24KfZGhCalh0UEFcU16AioxgVBJG4Jy5xIhrUxEgySmKMTlRwTHAbNTpuk0VNJibRZKITd1wxikRRNC4hYjBGBVQIIKDQ0uf7w4/62Ta7QFfrc19XX1qnTle9dag+/XZVnSqFEEKAiIiISEZMDB0AERER0eOYoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkhwkKERERyQ4TFCIiIpIdJihEREQkO0xQiIiISHaYoMjQ4cOHoVAo8O233xo6FNlq1qwZxo8fb+gwiIwG+5WqsV+RFyYoRP/fzZs3ERMTg5SUFEOHUi3GFi8RUU0wQSH6/27evInY2Fij+cI3tniJiGqCCcozrLCw0NAhVEjOsdXU07QtRFWR8/4u59jqmxAC9+7dM3QYNcIEBUBMTAwUCgXS0tIwfvx4qNVq2NnZYcKECSgqKgIAXL16FQqFAhs3btR7v0KhQExMjN7yfv31V/z1r3+FnZ0dnJycMGfOHAgh8Mcff2DIkCFQqVRwdXXFP//5z3LjKi0txezZs+Hq6opGjRph8ODB+OOPP/TqJSUlYcCAAbCzs4O1tTWef/55HDt2rNxtvHjxIl5++WU0btwYvXr1qrJthBBwdHREdHS0VKbVaqFWq2Fqaorc3FypfNGiRTAzM0NBQYFUdvDgQQQGBqJRo0ZQq9UYMmQILl26VO3YhBBYsGABmjZtCmtra/Tt2xcXLlzQi1Oj0SA2NhatWrWCpaUlHBwc0KtXL8THx1e5jcDD8/PdunUDAEyYMAEKhULn73306FG89NJL8PT0hFKphIeHB6ZPn673gR8/fjxsbGxw5coVDBw4ELa2thg7diwA4N69e3jrrbfg6OgIW1tbDB48GDdu3NDbfwDgxo0bePXVV+Hi4gKlUglfX1989tln1Y6XDI/9SsWelX6lzKpVq+Dr6wtra2s0btwYXbt2xebNm3XqnDlzBmFhYVCpVLCxsUFQUBBOnDhR7jY9buPGjVAoFLh69apU1qxZM7zwwgvYu3cvunbtCisrK6xfvx4AkJubi+nTp6NZs2ZQKpVo2rQpxo0bhzt37kjvLy4uxrx589CyZUupz3v33XdRXFxco21/EmYNtiYjMHLkSHh7eyMuLg6nT5/GJ598AmdnZyxatKhWyxs1ahTatm2LhQsXYufOnViwYAHs7e2xfv169OvXD4sWLcKXX36Jd955B926dUPv3r113v/BBx9AoVBg5syZuHXrFlasWIHg4GCkpKTAysoKwMMPalhYGPz8/DBv3jyYmJhgw4YN6NevH44ePYru3bvrLPOll15Cq1at8OGHH0IIUeU2KBQK9OzZEwkJCVLZuXPnkJeXBxMTExw7dgzh4eEAHn6Jd+nSBTY2NgCA/fv3IywsDM2bN0dMTAzu3buHVatWoWfPnjh9+jSaNWtWZWxz587FggULMHDgQAwcOBCnT59GSEgISkpKdN4bExODuLg4TJw4Ed27d0d+fj5OnTqF06dPo3///lVuZ9u2bTF//nzMnTsXkydPRmBgIADgL3/5CwBg69atKCoqwpQpU+Dg4ICffvoJq1atwvXr17F161adZT148AChoaHo1asXli5dCmtrawAPk5ctW7bglVdeQY8ePXDkyBGp7R6VlZWFHj16QKFQICoqCk5OTti9ezciIyORn5+PadOmVRkvyQf7FX3PSr8CAB9//DHeeustvPjii/jb3/6G+/fv49y5c0hKSsLLL78MALhw4QICAwOhUqnw7rvvwtzcHOvXr0efPn1w5MgR+Pv7V2tdj0tNTcWYMWPw2muvYdKkSWjTpg0KCgoQGBiIS5cu4dVXX8Vzzz2HO3fu4H//+x+uX78OR0dHaLVaDB48GD/++CMmT56Mtm3b4ueff8by5cvx66+/Ytu2bbWKp8YEiXnz5gkA4tVXX9UpHzZsmHBwcBBCCJGeni4AiA0bNui9H4CYN2+e3vImT54slT148EA0bdpUKBQKsXDhQqn8zz//FFZWViIiIkIqO3TokAAgmjRpIvLz86XyLVu2CADio48+EkIIodVqRatWrURoaKjQarVSvaKiIuHt7S369++vF9OYMWNq1jhCiCVLlghTU1MplpUrVwovLy/RvXt3MXPmTCGEEKWlpUKtVovp06dL7+vcubNwdnYW2dnZUtnZs2eFiYmJGDduXJWx3bp1S1hYWIjw8HCd7Zs9e7YAoNNmnTp1EuHh4TXetkedPHmywr9xUVGRXllcXJxQKBTi999/l8oiIiIEAPHee+/p1E1OThYAxLRp03TKx48fr7f/REZGCjc3N3Hnzh2duqNHjxZ2dnZSLJXFS4bHfqVyz0q/MmTIEOHr61tpnaFDhwoLCwtx5coVqezmzZvC1tZW9O7dW2+bHrdhwwYBQKSnp0tlXl5eAoDYs2ePTt25c+cKAOK///2v3nLK2uOLL74QJiYm4ujRozrz161bJwCIY8eOVbo9dYWneB7x+uuv60wHBgYiOzsb+fn5tVrexIkTpf+bmpqia9euEEIgMjJSKler1WjTpg1+++03vfePGzcOtra20vSLL74INzc37Nq1CwCQkpKCy5cv4+WXX0Z2djbu3LmDO3fuoLCwEEFBQUhISIBWq610G6sjMDAQpaWlOH78OICHv2gCAwMRGBiIo0ePAgDOnz+P3Nxc6Zd8RkYGUlJSMH78eNjb20vL6tixI/r37y9tQ2Wx7d+/HyUlJXjzzTd1DmtOmzZN771qtRoXLlzA5cuXa7x91VH2yxJ4eB77zp07+Mtf/gIhBM6cOaNXf8qUKTrTe/bsAQC88cYbOuVvvvmmzrQQAt999x0GDRoEIYT0N71z5w5CQ0ORl5eH06dP19VmUQNgv1K+Z6VfUavVuH79Ok6ePFnu/NLSUuzbtw9Dhw5F8+bNpXI3Nze8/PLL+PHHH2u9r3h7eyM0NFSn7LvvvkOnTp0wbNgwvfpl7bF161a0bdsWPj4+On1Qv379AACHDh2qVTw1xQTlEZ6enjrTjRs3BgD8+eefdbI8Ozs7WFpawtHRUa+8vHW0atVKZ1qhUKBly5bSecayD01ERAScnJx0Xp988gmKi4uRl5enswxvb+8ab8dzzz0Ha2trqdMo60h69+6NU6dO4f79+9K8snO8v//+OwCgTZs2estr27at1OFVFlvZMh5vBycnJ+lvU2b+/PnIzc1F69at0aFDB8yYMQPnzp2r8bZW5Nq1a1KnaGNjAycnJzz//PMAoNfGZmZmaNq0qd62mJiY6G1jy5YtdaZv376N3Nxc/Pvf/9b7m06YMAEAcOvWrTrbLqp/7FfK96z0KzNnzoSNjQ26d++OVq1aYerUqTrX8ty+fRtFRUUVbpNWqy33GqHqKO/vcuXKFbRv377S912+fBkXLlzQ+/u3bt0aQMP1QbwG5RGmpqbllgshyr0wCXiY/dZkeZWto6bKfsUsWbIEnTt3LrdO2XnbMo8eCaguc3Nz+Pv7IyEhAWlpacjMzERgYCBcXFyg0WiQlJSEo0ePwsfHB05OTjVe/pPEVqZ37964cuUKtm/fjn379uGTTz7B8uXLsW7dOp1fnLVRWlqK/v37IycnBzNnzoSPjw8aNWqEGzduYPz48Xq/JpVKJUxMapf7ly3rr3/9KyIiIsqt07Fjx1otmwyD/Ur5npV+pW3btkhNTcWOHTuwZ88efPfdd1izZg3mzp2L2NjYGsVT0/2lttuu1WrRoUMHLFu2rNz5Hh4etVpuTTFBqaayzPrRq8uB/8vG68PjhxWFEEhLS5O+oFq0aAEAUKlUCA4Orrc4gIeHYxctWoT9+/fD0dERPj4+UCgU8PX1xdGjR3H06FG88MILUn0vLy8ADy/Setwvv/wCR0dHNGrUqNJ1li3j8uXLOoc+b9++Xe4vQ3t7e0yYMAETJkxAQUEBevfujZiYmGp3JBV9+H/++Wf8+uuv2LRpE8aNGyeV1+RKfi8vL2i1WqSnp+v8cktLS9Op5+TkBFtbW5SWllb5N60oXjIe7Fee/n4FABo1aoRRo0Zh1KhRKCkpwfDhw/HBBx9g1qxZcHJygrW1dYXbZGJiIiUEj+4varVaqleT/aVFixY4f/58lXXOnj2LoKAgg/YzPMVTTSqVCo6OjjpXnQPAmjVr6m2dn3/+Oe7evStNf/vtt8jIyEBYWBgAwM/PDy1atMDSpUt1huCVuX37dp3FEhgYiOLiYqxYsQK9evWSdtrAwEB88cUXuHnzpnSeGHh4/rRz587YtGmTTud7/vx57Nu3DwMHDqxyncHBwTA3N8eqVat0fgmuWLFCr252drbOtI2NDVq2bFmjIXFlHdvjXxZlv04fjUEIgY8++qjayy47D/z4/rJq1Sq9dY0YMQLfffdduZ3Io3/TiuIl48F+5envVx5fhoWFBdq1awchBDQaDUxNTRESEoLt27frDBPOysrC5s2b0atXL6hUKgD/lzw+ur8UFhZi06ZN1Y5nxIgROHv2LL7//nu9eWXtMXLkSNy4cQMff/yxXp179+412P1keASlBiZOnIiFCxdi4sSJ6Nq1KxISEvDrr7/W2/rs7e3Rq1cvTJgwAVlZWVixYgVatmyJSZMmAQBMTEzwySefICwsDL6+vpgwYQKaNGmCGzdu4NChQ1CpVPjhhx/qJJaAgACYmZkhNTUVkydPlsp79+6NtWvXAoBORwI8PEQcFhaGgIAAREZGSsMB7ezs9O77UR4nJye88847iIuLwwsvvICBAwfizJkz2L17t9759nbt2qFPnz7w8/ODvb09Tp06hW+//RZRUVHV3sYWLVpArVZj3bp1sLW1RaNGjeDv7w8fHx+0aNEC77zzDm7cuAGVSoXvvvuuRtcQ+Pn5YcSIEVixYgWys7OlYcZl+8+jv1IWLlyIQ4cOwd/fH5MmTUK7du2Qk5OD06dPY//+/cjJyak03tpcD0CGw37l6e5XQkJC4Orqip49e8LFxQWXLl3Cv/71L4SHh0sXKy9YsADx8fHo1asX3njjDZiZmWH9+vUoLi7G4sWLdZbl6emJyMhIzJgxA6ampvjss8/g5OSEa9euVSueGTNm4Ntvv8VLL72EV199FX5+fsjJycH//vc/rFu3Dp06dcIrr7yCLVu24PXXX8ehQ4fQs2dPlJaW4pdffsGWLVuke6vUuwYZKyRzZUO3bt++rVP++NCtoqIiERkZKezs7IStra0YOXKkuHXrVoXDAR9fXkREhGjUqJHe+p9//nmdYWhlwwG/+uorMWvWLOHs7CysrKxEeHi4zpDWMmfOnBHDhw8XDg4OQqlUCi8vLzFy5Ehx4MCBKmOqiW7dugkAIikpSSq7fv26ACA8PDzKfc/+/ftFz549hZWVlVCpVGLQoEHi4sWLOnUqi620tFTExsYKNzc3YWVlJfr06SPOnz8vvLy8dIYDLliwQHTv3l2o1WphZWUlfHx8xAcffCBKSkpqtI3bt28X7dq1E2ZmZjrDPy9evCiCg4OFjY2NcHR0FJMmTRJnz57VGyJa0d9YCCEKCwvF1KlThb29vbCxsRFDhw4VqampAoDOEFEhhMjKyhJTp04VHh4ewtzcXLi6uoqgoCDx73//u1rxkuGxX6mep71fWb9+vejdu7fUji1atBAzZswQeXl5OvVOnz4tQkNDhY2NjbC2thZ9+/YVx48f11tecnKy8Pf3FxYWFsLT01MsW7aswmHGFQ2Rzs7OFlFRUaJJkybCwsJCNG3aVEREROjc2qCkpEQsWrRI+Pr6CqVSKRo3biz8/PxEbGysXuz1RSFELa6iIqI6kZKSgi5duuA///mPdMdZIiLiNShEDaa852CsWLECJiYmenf7JCJ61vEalGdYSUmJdD1DRezs7J5omJ4cyGU7Fy9ejOTkZPTt2xdmZmbYvXs3du/ejcmTJzfYsD2i+iaXz1t9e1a206Aa5EQSyVLZOenKXk/DNQ1y2c59+/aJnj17isaNGwtzc3PRokULERMTIzQaTb2vm6ihyOXzVt+ele00pBpfg5KQkIAlS5YgOTkZGRkZ+P777zF06FBp/vjx4/WGPIWGhkq3+gaAnJwcvPnmm/jhhx9gYmKCESNG4KOPPtK7+Q/Vrz///BPJycmV1vH19YWbm1sDRVQ/npXtJJKDZ+Xz9qxspyHVOEHZvXs3jh07Bj8/PwwfPrzcBCUrKwsbNmyQypRKpc4thMPCwpCRkYH169dDo9FgwoQJ6Natm97jp4no2bJ27VqsXbtWuh+Er68v5s6dK92j4/79+3j77bfx9ddfo7i4GKGhoVizZg1cXFykZVy7dg1TpkzBoUOHYGNjg4iICMTFxcHMjGe0iYxJjT+xYWFhUmdREaVSCVdX13LnXbp0CXv27MHJkyelcdSrVq3CwIEDsXTpUri7u9c0JCJ6SjRt2hQLFy5Eq1atIITApk2bMGTIEJw5cwa+vr6YPn06du7cia1bt8LOzg5RUVEYPny49GyT0tJShIeHw9XVFcePH0dGRgbGjRsHc3NzfPjhhwbeOiKqiScaZqxQKMo9grJt2zZYWFigcePG6NevHxYsWAAHBwcAwGeffYa3335b5yZXDx48gKWlJbZu3VruExaLi4t17tyn1WqRk5MDBwcH3u6bqI4JIXD37l24u7vX+plCdcne3h5LlizBiy++CCcnJ2zevBkvvvgigIe3Am/bti0SExPRo0cP7N69Gy+88AJu3rwpHVVZt24dZs6cidu3b8PCwqLK9Wm1Wty8eRO2trbsX4jqWE36lzo/5jlgwAAMHz4c3t7euHLlCmbPno2wsDAkJibC1NQUmZmZcHZ21g3CzAz29vbIzMwsd5lxcXE1fqgSET2ZP/74Q++pzA2ptLQUW7duRWFhIQICApCcnAyNRqPzfBgfHx94enpKCUpiYiI6dOigc8onNDQUU6ZMwYULF9ClSxe99Tz+A+jGjRto165d/W4c0TOuOv1LnScoo0ePlv7foUMHdOzYES1atMDhw4cRFBRUq2XOmjUL0dHR0nReXh48PT2Rnp4u3SpYrjQaDQ4dOoS+ffvC3Nzc0OEYLbZj3amqLe/evQtvb2+DfbZ+/vlnBAQE4P79+7CxscH333+Pdu3aISUlBRYWFjoPSQMAFxcX6cdNZmamTnJSNr9sXnkq+gH0ySefwNraug62iIjKFBUVYeLEidXqX+r9qrHmzZvD0dERaWlpCAoKgqurK27duqVT58GDB8jJyanwuhWlUgmlUqlXbm9vLz1ESa40Gg2sra3h4ODAL9YnwHasO1W1ZVmZoU5vtGnTBikpKcjLy8O3336LiIgIHDlypN7W9/gPoPz8fHh4eGDo0KEV9i8ajQbx8fHo378/98daYPs9OWNtw/z8fEycOLFa/Uu9JyjXr19Hdna2NNQqICAAubm5SE5Ohp+fHwDg4MGD0Gq18Pf3r+9wiEjmLCws0LJlSwAPH7J48uRJfPTRR9Kj6h9/1HxWVpb048bV1RU//fSTzvKysrKkeeWp6AeQubl5lR1/depQxdh+T87Y2rAmsdb4CriCggKkpKQgJSUFAJCeno6UlBRcu3YNBQUFmDFjBk6cOIGrV6/iwIEDGDJkCFq2bCk9br5t27YYMGAAJk2ahJ9++gnHjh1DVFQURo8ezRE8RKRHq9WiuLgYfn5+MDc3x4EDB6R5qampuHbtGgICAgA8/AH0888/6xyljY+Ph0ql4nUlREamxkdQTp06hb59+0rTZYdGIyIisHbtWpw7dw6bNm1Cbm4u3N3dERISgn/84x86v1C+/PJLREVFISgoSLpR28qVK+tgc4jImM2aNQthYWHw9PTE3bt3sXnzZhw+fBh79+6FnZ0dIiMjER0dLZ3effPNNxEQEIAePXoAePg4+nbt2uGVV17B4sWLkZmZiffffx9Tp04t9ygJEclXjROUPn36oLKRyXv37q1yGfb29rwpGxHpuXXrFsaNG4eMjAzY2dmhY8eO2Lt3L/r37w8AWL58ufSj5tEbtZUxNTXFjh07MGXKFAQEBKBRo0aIiIjA/PnzDbVJRFRLT/2tFZu9t7Pc8qsLwxs4EiKqyqefflrpfEtLS6xevRqrV6+usI6Xlxd27dpV16HJFvs4eloZ/i5MRERERI9hgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHTNDB0BERHWv2Xs7K5x3dWF4A0ZCVDs8gkJERESywwSFiIiIZIcJChEREckOExQiIiKSnWf2IlleQEZERCRfPIJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZeWZv1CZ3Fd1IjjeRIyKiZwGPoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkhwkKERERyQ4TFCIiIpKdGicoCQkJGDRoENzd3aFQKLBt2zad+UIIzJ07F25ubrCyskJwcDAuX76sUycnJwdjx46FSqWCWq1GZGQkCgoKnmhDiMi4xcXFoVu3brC1tYWzszOGDh2K1NRUnTr379/H1KlT4eDgABsbG4wYMQJZWVk6da5du4bw8HBYW1vD2dkZM2bMwIMHDxpyU+pcs/d2VvgielrVOEEpLCxEp06dsHr16nLnL168GCtXrsS6deuQlJSERo0aITQ0FPfv35fqjB07FhcuXEB8fDx27NiBhIQETJ48ufZbQURG78iRI5g6dSpOnDiB+Ph4aDQahISEoLCwUKozffp0/PDDD9i6dSuOHDmCmzdvYvjw4dL80tJShIeHo6SkBMePH8emTZuwceNGzJ071xCbRERPoMY3agsLC0NYWFi584QQWLFiBd5//30MGTIEAPD555/DxcUF27Ztw+jRo3Hp0iXs2bMHJ0+eRNeuXQEAq1atwsCBA7F06VK4u7vrLbe4uBjFxcXSdH5+PgBAo9FAo9FUGq/SVNR0E6tcZm2WVdNlVhR3XcZmTGrbjqSvqrY0VBvv2bNHZ3rjxo1wdnZGcnIyevfujby8PHz66afYvHkz+vXrBwDYsGED2rZtixMnTqBHjx7Yt28fLl68iP3798PFxQWdO3fGP/7xD8ycORMxMTGwsLAwxKYRUS3U6Z1k09PTkZmZieDgYKnMzs4O/v7+SExMxOjRo5GYmAi1Wi0lJwAQHBwMExMTJCUlYdiwYXrLjYuLQ2xsrF75vn37YG1tXWlMi7vXfDt27dpV8zdVIT4+vkb1K4q7PmIzJjVtR6pYRW1ZVFTUwJGULy8vDwBgb28PAEhOToZGo9HpX3x8fODp6YnExET06NEDiYmJ6NChA1xcXKQ6oaGhmDJlCi5cuIAuXbrorac2P4AaOmGuzQ+tyhg60ecPjidnrG1Yk3jrNEHJzMwEAJ3OoWy6bF5mZiacnZ11gzAzg729vVTncbNmzUJ0dLQ0nZ+fDw8PD4SEhEClUlUaU/uYvTXejvMxoTV+T0U0Gg3i4+PRv39/mJubV/t9FcVdl7EZk9q2I+mrqi3LvqANSavVYtq0aejZsyfat28P4GHfYWFhAbVarVP38f6lvP6nbF55nuQHUEMlzLX5oVUZufzQ4Q+OJ2dsbViTH0BG8SwepVIJpVKpV25ubl7ll1VxqaLG66uPL8DqxPqoiuJ+1r+ca9qOVLGK2lIO7Tt16lScP38eP/74Y72vqzY/gBo6Ya7ND63KGPqHDn9wPDljbcOa/ACq0wTF1dUVAJCVlQU3NzepPCsrC507d5bq3Lp1S+d9Dx48QE5OjvR+Inp2RUVFSRfPN23aVCp3dXVFSUkJcnNzdY6iZGVlSX2Hq6srfvrpJ53llY3yqah/eZIfQA2VMNfmh1Zl5PKFxh8cT87Y2rAmsdbpfVC8vb3h6uqKAwcOSGX5+flISkpCQEAAACAgIAC5ublITk6W6hw8eBBarRb+/v51GQ4RGREhBKKiovD999/j4MGD8Pb21pnv5+cHc3Nznf4lNTUV165d0+lffv75Z50fQfHx8VCpVGjXrl3DbAgR1YkaH0EpKChAWlqaNJ2eno6UlBTY29vD09MT06ZNw4IFC9CqVSt4e3tjzpw5cHd3x9ChQwEAbdu2xYABAzBp0iSsW7cOGo0GUVFRGD16dLkjeIjo2TB16lRs3rwZ27dvh62trXTNiJ2dHaysrGBnZ4fIyEhER0fD3t4eKpUKb775JgICAtCjRw8AQEhICNq1a4dXXnkFixcvRmZmJt5//31MnTq13KMkRCRfNU5QTp06hb59+0rTZeduIyIisHHjRrz77rsoLCzE5MmTkZubi169emHPnj2wtLSU3vPll18iKioKQUFBMDExwYgRI7By5co62BwiMlZr164FAPTp00enfMOGDRg/fjwAYPny5VKfUVxcjNDQUKxZs0aqa2pqih07dmDKlCkICAhAo0aNEBERgfnz5zfUZhBRHalxgtKnTx8IUfGQN4VCgfnz51faIdjb22Pz5s01XTURPcUq61fKWFpaYvXq1RXeKBIAvLy8ZDNKhYhqj8/iISIiItlhgkJERESywwSFiIiIZMcobtT2tOKTSImIiMrHIyhEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2eFFskamsgtrry4Mr7P3ENHTi30CGQMeQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkezwRm3l4E2MiIiIDIsJSh2pKKlRmgos7t7AwRARERk5nuIhIiIi2WGCQkRERLLDUzxPkcqunSEiIjImTFAaSPuYvSguVRg6DD0VJTW8GJiIiAyJp3iIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDocZ1xDvNUJERFT/eASFiIiIZIcJChEREckOT/FQuSo7lcW7zBIRUX3jERQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSnTpPUGJiYqBQKHRePj4+0vz79+9j6tSpcHBwgI2NDUaMGIGsrKy6DoOIjFBCQgIGDRoEd3d3KBQKbNu2TWe+EAJz586Fm5sbrKysEBwcjMuXL+vUycnJwdixY6FSqaBWqxEZGYmCgoIG3Aoiqgv1cgTF19cXGRkZ0uvHH3+U5k2fPh0//PADtm7diiNHjuDmzZsYPnx4fYRBREamsLAQnTp1wurVq8udv3jxYqxcuRLr1q1DUlISGjVqhNDQUNy/f1+qM3bsWFy4cAHx8fHYsWMHEhISMHny5IbaBCKqI/XyLB4zMzO4urrqlefl5eHTTz/F5s2b0a9fPwDAhg0b0LZtW5w4cQI9evSoj3CIyEiEhYUhLCys3HlCCKxYsQLvv/8+hgwZAgD4/PPP4eLigm3btmH06NG4dOkS9uzZg5MnT6Jr164AgFWrVmHgwIFYunQp3N3dG2xbiOjJ1EuCcvnyZbi7u8PS0hIBAQGIi4uDp6cnkpOTodFoEBwcLNX18fGBp6cnEhMTK0xQiouLUVxcLE3n5+cDADQaDTQaTaWxKE1FHWxR7SlNhM6/T4Oq2rw+12mIdT9tqmpLubZxeno6MjMzdfoPOzs7+Pv7IzExEaNHj0ZiYiLUarWUnABAcHAwTExMkJSUhGHDhukttzb9S0Pvjw3ZjzXENvHz/OSMtQ1rEm+dJyj+/v7YuHEj2rRpg4yMDMTGxiIwMBDnz59HZmYmLCwsoFardd7j4uKCzMzMCpcZFxeH2NhYvfJ9+/bB2tq60ngWd6/VZtS5f3TVGjqEOrNr1y6DrTs+Pt5g637aVNSWRUVFDRxJ9ZT1ES4uLjrlj/YfmZmZcHZ21plvZmYGe3v7CvuYJ+lfGmp/bMh+rCE/3/w8Pzlja8Oa9C91nqA8eni2Y8eO8Pf3h5eXF7Zs2QIrK6taLXPWrFmIjo6WpvPz8+Hh4YGQkBCoVKpK39s+Zm+t1llXlCYC/+iqxZxTJijWKgwaS105HxPa4OvUaDSIj49H//79YW5u3uDrf5pU1ZZlRxCeFbXpXxp6fzR0PwbU7eeen+cnZ6xtWJP+pV5O8TxKrVajdevWSEtLQ//+/VFSUoLc3FydoyhZWVnlXrNSRqlUQqlU6pWbm5tX+YcpLpVHUlCsVcgmlidlyA9Ddf7mVD0VtaVc27esj8jKyoKbm5tUnpWVhc6dO0t1bt26pfO+Bw8eICcnp8I+5kn6l4baH+XQd9THdvLz/OSMrQ1rEmu93weloKAAV65cgZubG/z8/GBubo4DBw5I81NTU3Ht2jUEBATUdyhEZMS8vb3h6uqq03/k5+cjKSlJ6j8CAgKQm5uL5ORkqc7Bgweh1Wrh7+/f4DETUe3V+RGUd955B4MGDYKXlxdu3ryJefPmwdTUFGPGjIGdnR0iIyMRHR0Ne3t7qFQqvPnmmwgICOAIHiJCQUEB0tLSpOn09HSkpKTA3t4enp6emDZtGhYsWIBWrVrB29sbc+bMgbu7O4YOHQoAaNu2LQYMGIBJkyZh3bp10Gg0iIqKwujRozmCh8jI1HmCcv36dYwZMwbZ2dlwcnJCr169cOLECTg5OQEAli9fDhMTE4wYMQLFxcUIDQ3FmjVr6joMIjJCp06dQt++faXpsmtDIiIisHHjRrz77rsoLCzE5MmTkZubi169emHPnj2wtLSU3vPll18iKioKQUFBUl+zcuXKBt8WInoydZ6gfP3115XOt7S0xOrVqyu8ERMRPbv69OkDISoeUqtQKDB//nzMnz+/wjr29vbYvHlzfYRHRA2Iz+IhIiIi2an3UTz07Gj23s4K511dGN6AkRARkbHjERQiIiKSHSYoREREJDs8xUM1VtmpHCIiorrAIyhEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2eFFskREMsEL0In+D4+gEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHF8mSQfEBg0REVB4eQSEiIiLZYYJCREREssNTPNQgeH8HIiKqCR5BISIiItlhgkJERESyw1M8RERUJY64o4bGIyhEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2eFFskRE9EQquoCWF8/Sk2CCQkRUS+1j9qK4VFHuPH45Ez0ZnuIhIiIi2eERFCKiBsTHPhBVD4+gEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0OMybZenQ4ptJUYHH3/7sxFm+CRXLH4cRET4YJCj1VKvtSYFJDRGQ8eIqHiIiIZIdHUMgo8fA5kfEr71lGPNJJZZigEBFRvajoh0TZNWVEleEpHiIiIpIdHkGhZx4vrCUikh8eQSEiIiLZMegRlNWrV2PJkiXIzMxEp06dsGrVKnTvzhOTJH886iJ/7F+MEz9bVMZgCco333yD6OhorFu3Dv7+/lixYgVCQ0ORmpoKZ2dnQ4VFpKMuRwux42047F+IjJ/BEpRly5Zh0qRJmDBhAgBg3bp12LlzJz777DO89957OnWLi4tRXFwsTefl5QEAcnJyoNFoKl2P2YPCOo68Zsy0AkVFWphpTFCqVVT9BipXXbRjy3e2lL/sJwmsHNnZ2eWvp5J9saL31IZ/3IEK5yXNCoJGo0FRURGys7Nhbm6uV+fu3bsAACFEncXU0Oq7fylrQ36ua6e2n+eKPieV7fMVSZoVVOP3yElVn2O5qlH/IgyguLhYmJqaiu+//16nfNy4cWLw4MF69efNmycA8MUXXw34+uOPPxqoR6hb7F/44kv+r+r0LwY5gnLnzh2UlpbCxcVFp9zFxQW//PKLXv1Zs2YhOjpamtZqtcjJyYGDgwMUCnn/esnPz4eHhwf++OMPqFQqQ4djtNiOdaeqthRC4O7du3B3dzdAdE+uIfoX7o9Phu335Iy1DWvSvxjFMGOlUgmlUqlTplarDRNMLalUKqPaieSK7Vh3KmtLOzu7Bo7GcJ6kf+H++GTYfk/OGNuwuv2LQYYZOzo6wtTUFFlZWTrlWVlZcHV1NURIRPSUYP9C9HQwSIJiYWEBPz8/HDjwfxc2abVaHDhwAAEBAYYIiYieEuxfiJ4OBjvFEx0djYiICHTt2hXdu3fHihUrUFhYKF11/7RQKpWYN2+e3iFkqhm2Y915FtqyvvuXZ6EN6xPb78k9C22oEMJwYwn/9a9/STdS6ty5M1auXAl/f39DhUNETxH2L0TGzaAJChEREVF5+CweIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQakDMTExUCgUOi8fHx9p/v379zF16lQ4ODjAxsYGI0aM0LuJ1LMqISEBgwYNgru7OxQKBbZt26YzXwiBuXPnws3NDVZWVggODsbly5d16uTk5GDs2LFQqVRQq9WIjIxEQUFBA26F4VXVjuPHj9fbRwcMGKBTh+1YfatXr0azZs1gaWkJf39//PTTT4YOqd411Gf13LlzCAwMhKWlJTw8PLB48WK9WLZu3QofHx9YWlqiQ4cO2LVrV51vb12Li4tDt27dYGtrC2dnZwwdOhSpqak6darzXXHt2jWEh4fD2toazs7OmDFjBh48eKBT5/Dhw3juueegVCrRsmVLbNy4US8eo9iHn/TBXPTwYWO+vr4iIyNDet2+fVua//rrrwsPDw9x4MABcerUKdGjRw/xl7/8xYARy8euXbvE3//+d/Hf//5XANB7wNvChQuFnZ2d2LZtmzh79qwYPHiw8Pb2Fvfu3ZPqDBgwQHTq1EmcOHFCHD16VLRs2VKMGTOmgbfEsKpqx4iICDFgwACdfTQnJ0enDtuxer7++mthYWEhPvvsM3HhwgUxadIkoVarRVZWlqFDq1cN8VnNy8sTLi4uYuzYseL8+fPiq6++ElZWVmL9+vVSnWPHjglTU1OxePFicfHiRfH+++8Lc3Nz8fPPP9d7GzyJ0NBQsWHDBnH+/HmRkpIiBg4cKDw9PUVBQYFUp6rvigcPHoj27duL4OBgcebMGbFr1y7h6OgoZs2aJdX57bffhLW1tYiOjhYXL14Uq1atEqampmLPnj1SHWPZh5mg1IF58+aJTp06lTsvNzdXmJubi61bt0plly5dEgBEYmJiA0VoHB7v9LRarXB1dRVLliyRynJzc4VSqRRfffWVEEKIixcvCgDi5MmTUp3du3cLhUIhbty40WCxy0lFCcqQIUMqfA/bsfq6d+8upk6dKk2XlpYKd3d3ERcXZ8CoGlZ9fVbXrFkjGjduLIqLi6U6M2fOFG3atJGmR44cKcLDw3Xi8ff3F6+99lqdbmN9u3XrlgAgjhw5IoSo3nfFrl27hImJicjMzJTqrF27VqhUKqnN3n33XeHr66uzrlGjRonQ0FBp2lj2YZ7iqSOXL1+Gu7s7mjdvjrFjx+LatWsAgOTkZGg0GgQHB0t1fXx84OnpicTEREOFaxTS09ORmZmp03Z2dnbw9/eX2i4xMRFqtRpdu3aV6gQHB8PExARJSUkNHrOcHT58GM7OzmjTpg2mTJmC7OxsaR7bsXpKSkqQnJyss0+amJggODj4mf4819VnNTExEb1794aFhYVUJzQ0FKmpqfjzzz+lOo+up6yOsbV/Xl4eAMDe3h5A9b4rEhMT0aFDB50ndYeGhiI/Px8XLlyQ6lTWPsa0DzNBqQP+/v7YuHEj9uzZg7Vr1yI9PR2BgYG4e/cuMjMzYWFhofd0VBcXF2RmZhomYCNR1j6PfhjLpsvmZWZmwtnZWWe+mZkZ7O3t2b6PGDBgAD7//HMcOHAAixYtwpEjRxAWFobS0lIAbMfqunPnDkpLSyvdJ59FdfVZzczMLHcZj66jojrG1P5arRbTpk1Dz5490b59ewCo1nfFk7RPfn4+7t27Z1T7sMGexfM0CQsLk/7fsWNH+Pv7w8vLC1u2bIGVlZUBIyN6aPTo0dL/O3TogI4dO6JFixY4fPgwgoKCDBgZ0bNn6tSpOH/+PH788UdDhyJrPIJSD9RqNVq3bo20tDS4urqipKQEubm5OnX46PeqlbXP41exP9p2rq6uuHXrls78Bw8eICcnh+1biebNm8PR0RFpaWkA2I7V5ejoCFNT00r3yWdRXX1WXV1dy13Go+uoqI6xtH9UVBR27NiBQ4cOoWnTplJ5db4rnqR9VCoVrKysjGofZoJSDwoKCnDlyhW4ubnBz88P5ubmOo9+T01NxbVr1/jo9yp4e3vD1dVVp+3y8/ORlJQktV1AQAByc3ORnJws1Tl48CC0Wi0fDFeJ69evIzs7G25ubgDYjtVlYWEBPz8/nX1Sq9XiwIEDz/Tnua4+qwEBAUhISIBGo5HqxMfHo02bNmjcuLFU59H1lNWRe/sLIRAVFYXvv/8eBw8ehLe3t8786nxXBAQE4Oeff9ZJ9OLj46FSqdCuXTupTmXtY1T7sKGv0n0avP322+Lw4cMiPT1dHDt2TAQHBwtHR0dx69YtIcTDoWOenp7i4MGD4tSpUyIgIEAEBAQYOGp5uHv3rjhz5ow4c+aMACCWLVsmzpw5I37//XchxMOhi2q1Wmzfvl2cO3dODBkypNyhi126dBFJSUnixx9/FK1atXrmhsdW1o53794V77zzjkhMTBTp6eli//794rnnnhOtWrUS9+/fl5bBdqyer7/+WiiVSrFx40Zx8eJFMXnyZKFWq3VGVjyNGuKzmpubK1xcXMQrr7wizp8/L77++mthbW2tN8zYzMxMLF26VFy6dEnMmzfPKIYZT5kyRdjZ2YnDhw/rDPcvKiqS6lT1XVE2zDgkJESkpKSIPXv2CCcnp3KHGc+YMUNcunRJrF69utxhxsawDzNBqQOjRo0Sbm5uwsLCQjRp0kSMGjVKpKWlSfPv3bsn3njjDdG4cWNhbW0thg0bJjIyMgwYsXwcOnRIANB7RURECCEeDl+cM2eOcHFxEUqlUgQFBYnU1FSdZWRnZ4sxY8YIGxsboVKpxIQJE8Tdu3cNsDWGU1k7FhUViZCQEOHk5CTMzc2Fl5eXmDRpkl5nxHasvlWrVglPT09hYWEhunfvLk6cOGHokOpdQ31Wz549K3r16iWUSqVo0qSJWLhwoV4sW7ZsEa1btxYWFhbC19dX7Ny5s962u66U13YAxIYNG6Q61fmuuHr1qggLCxNWVlbC0dFRvP3220Kj0ejUOXTokOjcubOwsLAQzZs311lHGWPYhxVCCNFwx2uIiIiIqsZrUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSljh0+fBgKhQLffvutoUORrWbNmmH8+PGGDsNoXb16FQqFotxHqBMRPS2YoBCV4+LFi4iJicHVq1cNFsPmzZuxYsUKg62fiMiQmKAQlePixYuIjY2VZYLi5eWFe/fu4ZVXXmn4oIiIGggTFCNVWFho6BAqJOfY6oMQAvfu3Wuw9SkUClhaWsLU1LTB1klE1NCMPkGJiYmBQqFAWloaxo8fD7VaDTs7O0yYMAFFRUUAKj9nr1AoEBMTo7e8X3/9FX/9619hZ2cHJycnzJkzB0II/PHHHxgyZAhUKhVcXV3xz3/+s9y4SktLMXv2bLi6uqJRo0YYPHgw/vjjD716SUlJGDBgAOzs7GBtbY3nn38ex44dK3cbL168iJdffhmNGzdGr169qmwbIQQcHR0RHR0tlWm1WqjVapiamuo8lGrRokUwMzNDQUGBVHbw4EEEBgaiUaNGUKvVGDJkCC5dulTt2IQQWLBgAZo2bQpra2v07dsXFy5c0ItTo9EgNjYWrVq1gqWlJRwcHNCrVy/Ex8dXuY2PKi4uxrx589CyZUsolUp4eHjg3XffRXFxsVQnIiIClpaWetsRGhqKxo0b4+bNm9i4cSNeeuklAEDfvn2hUCigUChw+PBhAA+voXnhhRewd+9edO3aFVZWVli/fj0AYMOGDejXrx+cnZ2hVCrRrl07rF27ttx4d+/ejeeffx62trZQqVTo1q0bNm/eDADo06cPdu7cid9//11af7NmzQDo789Lly6FQqHA77//rreOWbNmwcLCAn/++adUVp19jojI0Iw+QSkzcuRI3L17F3FxcRg5ciQ2btyI2NjYWi9v1KhR0Gq1WLhwIfz9/bFgwQKsWLEC/fv3R5MmTbBo0SK0bNkS77zzDhISEvTe/8EHH2Dnzp2YOXMm3nrrLcTHxyM4OFjnl/bBgwfRu3dv5OfnY968efjwww+Rm5uLfv364aefftJb5ksvvYSioiJ8+OGHmDRpUpXboFAo0LNnT534zp07h7y8PADQ+VI6evQounTpAhsbGwDA/v37ERoailu3biEmJgbR0dE4fvw4evbsWe5pj/Jimzt3LubMmYNOnTphyZIlaN68OUJCQvSOsMTExCA2NhZ9+/bFv/71L/z973+Hp6cnTp8+XeU2ltFqtRg8eDCWLl2KQYMGYdWqVRg6dCiWL1+OUaNGSfU++ugjODk5ISIiAqWlpQCA9evXY9++fVi1ahXc3d3Ru3dvvPXWWwCA2bNn44svvsAXX3yBtm3bSstJTU3FmDFj0L9/f3z00Ufo3LkzAGDt2rXw8vLC7Nmz8c9//hMeHh544403sHr1ap14N27ciPDwcOTk5GDWrFlYuHAhOnfujD179gAA/v73v6Nz585wdHSU1l/R9SgjR46EQqHAli1b9OZt2bIFISEh0oPWarrPEREZjCHvs18X5s2bJwCIV199Vad82LBhwsHBQQghRHp6ut4zD8oAEPPmzdNb3uTJk6WyBw8eiKZNmwqFQqHzXIg///xTWFlZSc+iEOL/nlfRpEkTkZ+fL5Vv2bJFABAfffSREOLhcytatWolQkNDhVarleoVFRUJb29v0b9/f72YavPgtiVLlghTU1MplpUrVwovLy/RvXt3MXPmTCGEEKWlpUKtVovp06dL7+vcubNwdnYW2dnZUtnZs2eFiYmJGDduXJWx3bp1S1hYWIjw8HCd7Zs9e7bO8zuEEKJTp04iPDy8xtv2qC+++EKYmJiIo0eP6pSvW7dOABDHjh2Tyvbu3SsAiAULFojffvtN2NjYiKFDh+q8b+vWrQKAOHTokN66vLy8BACdh2+VefTBX2VCQ0NF8+bNpenc3Fxha2sr/P39dR6kJoTQaavw8HDh5eWlt7zy9ueAgADh5+enU++nn34SAMTnn38uLbu6+xwRkaE9NUdQXn/9dZ3pwMBAZGdnIz8/v1bLmzhxovR/U1NTdO3aFUIIREZGSuVqtRpt2rTBb7/9pvf+cePGwdbWVpp+8cUX4ebmhl27dgEAUlJScPnyZbz88svIzs7GnTt3cOfOHRQWFiIoKAgJCQnQarWVbmN1BAYGorS0FMePHwfw8EhJYGAgAgMDcfToUQDA+fPnkZubi8DAQABARkYGUlJSMH78eNjb20vL6tixI/r37y9tQ2Wx7d+/HyUlJXjzzTehUCik8mnTpum9V61W48KFC7h8+XKNt6/M1q1b0bZtW/j4+EhteefOHfTr1w8AcOjQIaluSEgIXnvtNcyfPx/Dhw+HpaWldIqmury9vREaGqpXbmVlJf0/Ly8Pd+7cwfPPP4/ffvtNOnIVHx+Pu3fv4r333oOlpaXO+x9tq5oYNWoUkpOTceXKFansm2++gVKpxJAhQwDUbp8jIjKUpyZB8fT01JkuO6T96Ln3J1menZ0dLC0t4ejoqFde3jpatWqlM61QKNCyZUvp9EjZl3FERAScnJx0Xp988gmKi4ulL7Qy3t7eNd6O5557DtbW1lIyUpag9O7dG6dOncL9+/eleWXXjpRdy9CmTRu95bVt21b6UqsstrJlPN4OTk5O0t+mzPz585Gbm4vWrVujQ4cOmDFjBs6dO1ej7bx8+TIuXLig15atW7cGANy6dUun/tKlS2Fvb4+UlBSsXLkSzs7ONVpfRX+LY8eOITg4WLpux8nJCbNnzwYA6e9ZlkS0b9++RuuszEsvvQQTExN88803AB5e/7N161aEhYVBpVIBqN0+R0RkKGaGDqCuVDSiQQhR4a/SsmsQqru8ytZRU2W/VJcsWSJdv/C4sutByjz667y6zM3N4e/vj4SEBKSlpSEzMxOBgYFwcXGBRqNBUlISjh49Ch8fHzg5OdV4+U8SW5nevXvjypUr2L59O/bt24dPPvkEy5cvx7p163SOZFVGq9WiQ4cOWLZsWbnzPTw8dKbPnDkjJS0///wzxowZU6OYy9veK1euICgoCD4+Pli2bBk8PDxgYWGBXbt2Yfny5fV6dMLd3R2BgYHYsmULZs+ejRMnTuDatWtYtGiRVKc2+xwRkaE8NQlKZcp+sT86agVAuaMe6srjpyuEEEhLS0PHjh0BAC1atAAAqFQqBAcH11scwMPTPIsWLcL+/fvh6OgIHx8fKBQK+Pr64ujRozh69CheeOEFqb6XlxeAhxeCPu6XX36Bo6MjGjVqVOk6y5Zx+fJlNG/eXCq/fft2uUec7O3tMWHCBEyYMAEFBQXo3bs3YmJiqp2gtGjRAmfPnkVQUFCVp0kKCwsxYcIEtGvXDn/5y1+wePFiDBs2DN26dZPq1OZUyw8//IDi4mL873//0zkC9+jppbJYgYen1lq2bFnh8moaw6hRo/DGG28gNTUV33zzDaytrTFo0CC99TbEPkdE9KSemlM8lVGpVHB0dNQbbbNmzZp6W+fnn3+Ou3fvStPffvstMjIyEBYWBgDw8/NDixYtsHTpUp2hvWVu375dZ7EEBgaiuLgYK1asQK9evaQvvsDAQHzxxRe4efOmdP0JALi5uaFz587YtGmTTlJ3/vx57Nu3DwMHDqxyncHBwTA3N8eqVat0jjCVNxIlOztbZ9rGxgYtW7bUGR5clZEjR+LGjRv4+OOP9ebdu3dP55TUzJkzce3aNWzatAnLli1Ds2bNEBERobO+sgTs8aS2MmVH2B7d3ry8PGzYsEGnXkhICGxtbREXF4f79+/rzHv0vY0aNarRKZcRI0bA1NQUX331FbZu3YoXXnhBJ5FsyH2OiOhJPRNHUICHF70uXLgQEydORNeuXZGQkIBff/213tZnb2+PXr16YcKECcjKysKKFSvQsmVLaQiuiYkJPvnkE4SFhcHX1xcTJkxAkyZNcOPGDRw6dAgqlQo//PBDncQSEBAAMzMzpKamYvLkyVJ57969pXt0PJqgAA9PA4SFhSEgIACRkZG4d+8eVq1aBTs7O537xlTEyckJ77zzDuLi4vDCCy9g4MCBOHPmDHbv3q13HU+7du3Qp08f+Pn5wd7eHqdOncK3336LqKioam/jK6+8gi1btuD111/HoUOH0LNnT5SWluKXX37Bli1bpHuWHDx4EGvWrMG8efPw3HPPAXh475I+ffpgzpw5WLx4MQCgc+fOMDU1xaJFi5CXlwelUind36QiISEhsLCwwKBBg/Daa6+hoKAAH3/8MZydnZGRkSHVU6lUWL58OSZOnIhu3bpJ9485e/YsioqKsGnTJgAPE4pvvvkG0dHR6NatG2xsbHSOiDzO2dkZffv2xbJly3D37l2d4dVAw+5zRERPzHADiOpG2TDX27dv65Rv2LBBABDp6elCiIdDKSMjI4WdnZ2wtbUVI0eOFLdu3apwmPHjy4uIiBCNGjXSW//zzz8vfH19pemyYcZfffWVmDVrlnB2dhZWVlYiPDxc/P7773rvP3PmjBg+fLhwcHAQSqVSeHl5iZEjR4oDBw5UGVNNdOvWTQAQSUlJUtn169cFAOHh4VHue/bv3y969uwprKyshEqlEoMGDRIXL17UqVNZbKWlpSI2Nla4ubkJKysr0adPH3H+/Hnh5eWlM8x4wYIFonv37kKtVgsrKyvh4+MjPvjgA1FSUlKjbSwpKRGLFi0Svr6+QqlUisaNGws/Pz8RGxsr8vLyRH5+vvDy8hLPPfec0Gg0Ou+dPn26MDExEYmJiVLZxx9/LJo3by5MTU11hhx7eXlVOCz6f//7n+jYsaOwtLQUzZo1E4sWLRKfffaZzr74aN2//OUvUvt2795dfPXVV9L8goIC8fLLLwu1Wi0ASEOOKxs2//HHHwsAwtbWVm8Ic5nq7HNERIamEKIWV3gSERER1aNn4hoUIiIiMi7PzDUoT5uSkhLk5ORUWsfOzu6Jhv/KwbOynUREpIsJipE6fvw4+vbtW2mdDRs2YPz48Q0TUD15VraTiIh08RoUI/Xnn38iOTm50jq+vr5wc3NroIjqx7OynUREpIsJChEREckOL5IlIiIi2THKa1C0Wi1u3rwJW1vbWj/9lYjKJ4TA3bt34e7uDhMT/oYhIsMwygTl5s2beg9/I6K69ccff6Bp06aGDoOInlFGmaDY2toCeNiBlj1KvqFpNBrs27cPISEhMDc3N0gMNWWMMQOMuyFpNBps27YNEydOlD5nRESGYJQJStlpHZVKZdAExdraGiqVyqi+fIwtZoBxN6SymIHaPdGZiKiu8AQzERERyQ4TFCIiIpIdJihEREQkO0xQiIiISHaM8iJZMqxm7+0st/zqwvAGjoSIiJ5WPIJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkezUOEFJSEjAoEGD4O7uDoVCgW3btunMHz9+PBQKhc5rwIABOnVycnIwduxYqFQqqNVqREZGoqCg4Ik2hIiIiJ4eNU5QCgsL0alTJ6xevbrCOgMGDEBGRob0+uqrr3Tmjx07FhcuXEB8fDx27NiBhIQETJ48uebRExER0VPJrKZvCAsLQ1hYWKV1lEolXF1dy5136dIl7NmzBydPnkTXrl0BAKtWrcLAgQOxdOlSuLu71zQkIiIiesrUOEGpjsOHD8PZ2RmNGzdGv379sGDBAjg4OAAAEhMToVarpeQEAIKDg2FiYoKkpCQMGzZMb3nFxcUoLi6WpvPz8wEAGo0GGo2mPjahSmXrNdT6a6OuYlaaikqXX9eMsa0B44zbmGIloqdbnScoAwYMwPDhw+Ht7Y0rV65g9uzZCAsLQ2JiIkxNTZGZmQlnZ2fdIMzMYG9vj8zMzHKXGRcXh9jYWL3yffv2wdrauq43oUbi4+MNuv7aeNKYF3cvv3zXrl1PtNyqGGNbA8YbNxGRIdV5gjJ69Gjp/x06dEDHjh3RokULHD58GEFBQbVa5qxZsxAdHS1N5+fnw8PDAyEhIVCpVE8cc21oNBrEx8ejf//+MDc3N0gMNVVezO1j9tbZ8s/HhNbZsh5ljG0NGGfcGo0G27dvN3QYRET1c4rnUc2bN4ejoyPS0tIQFBQEV1dX3Lp1S6fOgwcPkJOTU+F1K0qlEkqlUq/c3Nzc4B2/HGKoqUdjLi5V1Oly65MxtjVgvHETERlSvd8H5fr168jOzoabmxsAICAgALm5uUhOTpbqHDx4EFqtFv7+/vUdDhERERmBGh9BKSgoQFpamjSdnp6OlJQU2Nvbw97eHrGxsRgxYgRcXV1x5coVvPvuu2jZsiVCQx8e/m/bti0GDBiASZMmYd26ddBoNIiKisLo0aM5goeIiIgA1OIIyqlTp9ClSxd06dIFABAdHY0uXbpg7ty5MDU1xblz5zB48GC0bt0akZGR8PPzw9GjR3VO0Xz55Zfw8fFBUFAQBg4ciF69euHf//533W0VERERGbUaH0Hp06cPhCh/mCkA7N1b9UWX9vb22Lx5c01XTURERM8IPouHiIiIZIcJChEREclOvQ8zpmdHs/d2Vjjv6sLwBoyEiIiMHY+gEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7ZoYOgJ4Nzd7bWW751YXhDRwJEREZAx5BISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOHxb4DCh7UJ/SVGBxd6B9zF4UlyoMHBUREVHFeASFiIiIZKfGCUpCQgIGDRoEd3d3KBQKbNu2TWe+EAJz586Fm5sbrKysEBwcjMuXL+vUycnJwdixY6FSqaBWqxEZGYmCgoIn2hAiIiJ6etQ4QSksLESnTp2wevXqcucvXrwYK1euxLp165CUlIRGjRohNDQU9+/fl+qMHTsWFy5cQHx8PHbs2IGEhARMnjy59ltBRERET5UaX4MSFhaGsLCwcucJIbBixQq8//77GDJkCADg888/h4uLC7Zt24bRo0fj0qVL2LNnD06ePImuXbsCAFatWoWBAwdi6dKlcHd3f4LNISIioqdBnV4km56ejszMTAQHB0tldnZ28Pf3R2JiIkaPHo3ExESo1WopOQGA4OBgmJiYICkpCcOGDdNbbnFxMYqLi6Xp/Px8AIBGo4FGo6nLTai2svUaav01oTQVD/810f1XDqrTfsbU1o8yxriNKVYierrVaYKSmZkJAHBxcdEpd3FxkeZlZmbC2dlZNwgzM9jb20t1HhcXF4fY2Fi98n379sHa2rouQq+1+Ph4g66/OhZ3153+R1etYQIpx65du6pd1xjaujzGGjcRkSEZxTDjWbNmITo6WprOz8+Hh4cHQkJCoFKpDBKTRqNBfHw8+vfvD3Nzc4PEUF3tY/YCeHjk5B9dtZhzygTFWnkMMz4fE1plHWNq60cZY9wajQbbt283dBhERHWboLi6ugIAsrKy4ObmJpVnZWWhc+fOUp1bt27pvO/BgwfIycmR3v84pVIJpVKpV25ubm7wjl8OMVTl8XueFGsVsrkPSk3azhjaujzGGjcRkSHV6X1QvL294erqigMHDkhl+fn5SEpKQkBAAAAgICAAubm5SE5OluocPHgQWq0W/v7+dRkOERERGakaH0EpKChAWlqaNJ2eno6UlBTY29vD09MT06ZNw4IFC9CqVSt4e3tjzpw5cHd3x9ChQwEAbdu2xYABAzBp0iSsW7cOGo0GUVFRGD16NEfwEBEREYBaJCinTp1C3759pemya0MiIiKwceNGvPvuuygsLMTkyZORm5uLXr16Yc+ePbC0tJTe8+WXXyIqKgpBQUEwMTHBiBEjsHLlyjrYHCIiInoa1DhB6dOnD4SoeJiqQqHA/PnzMX/+/Arr2NvbY/PmzTVdNRERET0j+CweIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7Nb7VPVFdavbezgrnXV0Y3oCREBGRnPAIChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHd7q/ilS2W3jiYiIjAmPoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkhwkKERERyQ4TFCIiIpIdJihEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkp84TlJiYGCgUCp2Xj4+PNP/+/fuYOnUqHBwcYGNjgxEjRiArK6uuwyAiIiIjVi9HUHx9fZGRkSG9fvzxR2ne9OnT8cMPP2Dr1q04cuQIbt68ieHDh9dHGERERGSkzOploWZmcHV11SvPy8vDp59+is2bN6Nfv34AgA0bNqBt27Y4ceIEevToUR/hkJFq9t5OAIDSVGBxd6B9zF4UlyoAAFcXhhsyNCIiqmf1kqBcvnwZ7u7usLS0REBAAOLi4uDp6Ynk5GRoNBoEBwdLdX18fODp6YnExMQKE5Ti4mIUFxdL0/n5+QAAjUYDjUZTH5tQpbL1Gmr95VGaisrnmwidf41FeXHLqd0rIsd9pCrGFCsRPd0UQog6/bbavXs3CgoK0KZNG2RkZCA2NhY3btzA+fPn8cMPP2DChAk6yQYAdO/eHX379sWiRYvKXWZMTAxiY2P1yjdv3gxra+u6DJ/omVdUVISXX34ZeXl5UKlUhg6HiJ5RdX4EJSwsTPp/x44d4e/vDy8vL2zZsgVWVla1WuasWbMQHR0tTefn58PDwwMhISEG60A1Gg3i4+PRv39/mJubGySGx7WP2VvpfKWJwD+6ajHnlAmKtYoGiurJ1STu8zGhDRRV1eS4j1RFo9Fg+/bthg6DiKh+TvE8Sq1Wo3Xr1khLS0P//v1RUlKC3NxcqNVqqU5WVla516yUUSqVUCqVeuXm5uYG7/jlEEOZsuszqqynVVS7rpxUJ265/C0eJad9hIjIWNT7fVAKCgpw5coVuLm5wc/PD+bm5jhw4IA0PzU1FdeuXUNAQEB9h0JERERGos6PoLzzzjsYNGgQvLy8cPPmTcybNw+mpqYYM2YM7OzsEBkZiejoaNjb20OlUuHNN99EQEAAR/AQERGRpM4TlOvXr2PMmDHIzs6Gk5MTevXqhRMnTsDJyQkAsHz5cpiYmGDEiBEoLi5GaGgo1qxZU9dhEBERkRGr8wTl66+/rnS+paUlVq9ejdWrV9f1qomIiOgpwWfxEBERkewwQSEiIiLZYYJCREREslPv90GhulX2fBoiIqKnGY+gEBERkezwCAo9VSo7wsQnIBMRGQ8eQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdngfFHrm8d4pRETywyMoREREJDtMUIiIiEh2eIqHnhl80CIRkfHgERQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7PAiWaJKVHRhLe+PQkRUv5igyBRHnBAR0bOMp3iIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJzlM/zLg297GobIgv739BAPcRIqL69tQnKEQNrSx5UZoKLO4OtI/Zi+JSBRMXIqIa4CkeIiIikp1n9ggK79RKREQkXwZNUFavXo0lS5YgMzMTnTp1wqpVq9C9e3dDhtSgmCQRERGVz2AJyjfffIPo6GisW7cO/v7+WLFiBUJDQ5GamgpnZ2dDhUVUb3hhLRFR9RksQVm2bBkmTZqECRMmAADWrVuHnTt34rPPPsN7771nqLCqxAsgqT7U5mhaRfscEyEiehoYJEEpKSlBcnIyZs2aJZWZmJggODgYiYmJevWLi4tRXFwsTefl5QEAcnJyoNFoKl2X2YPCOor6seVqBYqKtDDTmKBUq0B2dnaFdf3jDpS/jHqJrGKPx2wsGHf5KtrnKtvnK9tPAUCj0aCoqAgAIISofXBERE/IIAnKnTt3UFpaChcXF51yFxcX/PLLL3r14+LiEBsbq1fu7e1dbzFWx8uP/N/xnwYLo0ZerrqKLDFufbXZ52rynrt378LOzq7mKyEiqgNGMYpn1qxZiI6Olqa1Wi1ycnLg4OAAhcIwv6jz8/Ph4eGBP/74AyqVyiAx1JQxxgww7oZUFvPFixfh7u5u6HCI6BlmkATF0dERpqamyMrK0inPysqCq6urXn2lUgmlUqlTplar6zPEalOpVEbz5VPGGGMGGHdDatKkCUxMeJskIjIcg/RAFhYW8PPzw4ED/3dthlarxYEDBxAQEGCIkIiIiEhGDHaKJzo6GhEREejatSu6d++OFStWoLCwUBrVQ0RERM8ugyUoo0aNwu3btzF37lxkZmaic+fO2LNnj96Fs3KlVCoxb948vVNPcmaMMQOMuyEZY8xE9HRSCI4lJCIiIpnhVXBEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2WGCQkRERLLDBOX/i4uLQ7du3WBrawtnZ2cMHToUqampOnX69OkDhUKh83r99dd16ly7dg3h4eGwtraGs7MzZsyYgQcPHtRb3DExMXox+fj4SPPv37+PqVOnwsHBATY2NhgxYoTeHXwbOmYAaNasmV7cCoUCU6dOBSCftk5ISMCgQYPg7u4OhUKBbdu26cwXQmDu3Llwc3ODlZUVgoODcfnyZZ06OTk5GDt2LFQqFdRqNSIjI1FQUKBT59y5cwgMDISlpSU8PDywePHieolZo9Fg5syZ6NChAxo1agR3d3eMGzcON2/e1FlGeX+fhQsX1lvMRESPY4Ly/x05cgRTp07FiRMnEB8fD41Gg5CQEBQW6j4ZdtKkScjIyJBej3bKpaWlCA8PR0lJCY4fP45NmzZh48aNmDt3br3G7uvrqxPTjz/+KM2bPn06fvjhB2zduhVHjhzBzZs3MXz4cIPHfPLkSZ2Y4+PjAQAvvfSSVEcObV1YWIhOnTph9erV5c5fvHgxVq5ciXXr1iEpKQmNGjVCaGgo7t+/L9UZO3YsLly4gPj4eOzYsQMJCQmYPHmyND8/Px8hISHw8vJCcnIylixZgpiYGPz73/+u85iLiopw+vRpzJkzB6dPn8Z///tfpKamYvDgwXp158+fr9P+b775Zr3FTESkR1C5bt26JQCII0eOSGXPP/+8+Nvf/lbhe3bt2iVMTExEZmamVLZ27VqhUqlEcXFxvcQ5b9480alTp3Ln5ebmCnNzc7F161ap7NKlSwKASExMNFjM5fnb3/4mWrRoIbRarRBCnm0NQHz//ffStFarFa6urmLJkiVSWW5urlAqleKrr74SQghx8eJFAUCcPHlSqrN7926hUCjEjRs3hBBCrFmzRjRu3Fgn7pkzZ4o2bdrUeczl+emnnwQA8fvvv0tlXl5eYvny5RW+pz5jJiISQggeQalAXl4eAMDe3l6n/Msvv4SjoyPat2+PWbNmoaioSJqXmJiIDh066NwNNzQ0FPn5+bhw4UK9xXr58mW4u7ujefPmGDt2LK5duwYASE5OhkajQXBwsFTXx8cHnp6eSExMNGjMjyopKcF//vMfvPrqqzpPp5ZjWz8qPT0dmZmZOu1rZ2cHf39/nfZVq9Xo2rWrVCc4OBgmJiZISkqS6vTu3RsWFhY625Kamoo///yz3rcjLy8PCoVC7wGcCxcuhIODA7p06YIlS5bonD4zdMxE9PQz2K3u5Uyr1WLatGno2bMn2rdvL5W//PLL8PLygru7O86dO4eZM2ciNTUV//3vfwEAmZmZerfqL5vOzMysl1j9/f2xceNGtGnTBhkZGYiNjUVgYCDOnz+PzMxMWFhY6H3xuLi4SPEYIubHbdu2Dbm5uRg/frxUJse2flzZesqL49H2dXZ21plvZmYGe3t7nTre3t56yyib17hx43qJH3h4jdLMmTMxZswYnScuv/XWW3juuedgb2+P48ePY9asWcjIyMCyZcsMHjMRPRuYoJRj6tSpOH/+vM61HAB0rhvo0KED3NzcEBQUhCtXrqBFixYNHSYAICwsTPp/x44d4e/vDy8vL2zZsgVWVlYGiammPv30U4SFhcHd3V0qk2NbP200Gg1GjhwJIQTWrl2rMy86Olr6f8eOHWFhYYHXXnsNcXFxfE4PETUInuJ5TFRUFHbs2IFDhw6hadOmldb19/cHAKSlpQEAXF1d9UbIlE27urrWQ7T61Go1WrdujbS0NLi6uqKkpAS5ubl6MZXFY+iYf//9d+zfvx8TJ06stJ4c27psPeXF8Wj73rp1S2f+gwcPkJOTY9C/QVly8vvvvyM+Pl7n6El5/P398eDBA1y9etVgMRPRs4UJyv8nhEBUVBS+//57HDx4UO/wdXlSUlIAAG5ubgCAgIAA/PzzzzpfSGWdf7t27eol7scVFBTgypUrcHNzg5+fH8zNzXHgwAFpfmpqKq5du4aAgABZxLxhwwY4OzsjPDy80npybGtvb2+4urrqtG9+fj6SkpJ02jc3NxfJyclSnYMHD0Kr1UpJV0BAABISEqDRaHS2pU2bNvVyqqQsObl8+TL2798PBweHKt+TkpICExMT6XRVQ8dMRM8gQ1+lKxdTpkwRdnZ24vDhwyIjI0N6FRUVCSGESEtLE/PnzxenTp0S6enpYvv27aJ58+aid+/e0jIePHgg2rdvL0JCQkRKSorYs2ePcHJyErNmzaq3uN9++21x+PBhkZ6eLo4dOyaCg4OFo6OjuHXrlhBCiNdff114enqKgwcPilOnTomAgAAREBBg0JjLlJaWCk9PTzFz5kydcjm19d27d8WZM2fEmTNnBACxbNkycebMGWnEy8KFC4VarRbbt28X586dE0OGDBHe3t7i3r170jIGDBggunTpIpKSksSPP/4oWrVqJcaMGSPNz83NFS4uLuKVV14R58+fF19//bWwtrYW69evr/OYS0pKxODBg0XTpk1FSkqKzr5eNiLn+PHjYvny5SIlJUVcuXJF/Oc//xFOTk5i3Lhx9RYzEdHjmKD8fwDKfW3YsEEIIcS1a9dE7969hb29vVAqlaJly5ZixowZIi8vT2c5V69eFWFhYcLKyko4OjqKt99+W2g0mnqLe9SoUcLNzU1YWFiIJk2aiFGjRom0tDRp/r1798Qbb7whGjduLKytrcWwYcNERkaGQWMus3fvXgFApKam6pTLqa0PHTpU7n4REREhhHg41HjOnDnCxcVFKJVKERQUpLc92dnZYsyYMcLGxkaoVCoxYcIEcffuXZ06Z8+eFb169RJKpVI0adJELFy4sF5iTk9Pr3BfP3TokBBCiOTkZOHv7y/s7OyEpaWlaNu2rfjwww/F/fv36y1mIqLHKYQQosEO1xARERFVA69BISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZOf/Ado/z71f+SF/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "bb2a6be8-6dae-471d-f965-8734e77b6b07"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      shuffle=True,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        shuffle=True,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data_train['number_words_source'], bins=25, edgecolor='black', alpha=0.5, label='Training')\n",
        "plt.hist(data_val['number_words_source'], bins=25, edgecolor='black', alpha=0.5, label='Validation')\n",
        "plt.hist(data_test['number_words_source'], bins=25, edgecolor='black', alpha=0.5, label='Test')\n",
        "\n",
        "\n",
        "plt.xlabel('N¬∫ of words in document')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "FzpOP09JQyTU",
        "outputId": "9f169d8d-9976-4c84-9c99-e89ee46e8399"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7de257334820>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data_train['number_words_target'], bins=25, edgecolor='black', alpha=0.5, label='Training')\n",
        "plt.hist(data_val['number_words_target'], bins=25, edgecolor='black', alpha=0.5, label='Validation')\n",
        "plt.hist(data_test['number_words_target'], bins=25, edgecolor='black', alpha=0.5, label='Test')\n",
        "\n",
        "\n",
        "plt.xlabel('N¬∫ of words in target')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "OC8JMu-aUOHV",
        "outputId": "18a154fe-7664-4fc2-d026-87d10d6e2c3c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7de256ae4cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,3, figsize=(16, 8),  sharey=True)\n",
        "\n",
        "clean_data[['number_words_source', 'number_words_target']].hist(ax=ax,\n",
        "              bins=25,\n",
        "              edgecolor='black',\n",
        "              grid=False)\n",
        "\n",
        "ax[0].set_xlabel('N¬∫ of words in document')\n",
        "ax[1].set_xlabel('N¬∫ of words in target')"
      ],
      "metadata": {
        "id": "8VA7P8ggQdrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "o46lFUVw5taI",
        "outputId": "e7008bbc-beb9-4433-a84c-bef5553ece0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                source    paper_id  \\\n",
              "631  With the recently rapid development in deep le...  ByxmXnA9FQ   \n",
              "634  Neural machine translation (NMT) models learn ...  H1z-PsR5KX   \n",
              "963  Recent results from linear algebra stating tha...  SkeUG30cFQ   \n",
              "625  Analogical reasoning has been a principal focu...  SylLYsCcFm   \n",
              "365  Recent advances in computing technology and se...   ByJbJwxCW   \n",
              "\n",
              "                                                target  \\\n",
              "631  A new framework based variational inference fo...   \n",
              "634  Unsupervised methods for finding, analyzing, a...   \n",
              "963  We provide a theoretical study of the properti...   \n",
              "625  The most robust capacity for analogical reason...   \n",
              "365  We propose a deep Multi Instance Learning fram...   \n",
              "\n",
              "                                                 title  number_words_target  \\\n",
              "631  A Variational Dirichlet Framework for Out-of-D...                   78   \n",
              "634  Identifying and Controlling Important Neurons ...                   54   \n",
              "963  The Expressive Power of Deep Neural Networks w...                   54   \n",
              "625  Learning to Make Analogies by Contrasting Abst...                   67   \n",
              "365  Relational Multi-Instance Learning for Concept...                   97   \n",
              "\n",
              "                                    extractive_summary  number_words_source  \\\n",
              "631  Therefore, it is very essential to design a ro...                 3289   \n",
              "634  First, it targets the whole vector representat...                 4933   \n",
              "963  Recent results from linear algebra stating tha...                 4143   \n",
              "625  It is natural to consider, however, whether th...                 6473   \n",
              "365  Most of the medical time series lack annotatio...                 4819   \n",
              "\n",
              "     number_words_extractive  \n",
              "631                      695  \n",
              "634                      456  \n",
              "963                      594  \n",
              "625                      673  \n",
              "365                      513  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93fa64ac-3750-4aff-9d06-36ef63158ba9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "      <th>number_words_source</th>\n",
              "      <th>number_words_extractive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>With the recently rapid development in deep le...</td>\n",
              "      <td>ByxmXnA9FQ</td>\n",
              "      <td>A new framework based variational inference fo...</td>\n",
              "      <td>A Variational Dirichlet Framework for Out-of-D...</td>\n",
              "      <td>78</td>\n",
              "      <td>Therefore, it is very essential to design a ro...</td>\n",
              "      <td>3289</td>\n",
              "      <td>695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>Neural machine translation (NMT) models learn ...</td>\n",
              "      <td>H1z-PsR5KX</td>\n",
              "      <td>Unsupervised methods for finding, analyzing, a...</td>\n",
              "      <td>Identifying and Controlling Important Neurons ...</td>\n",
              "      <td>54</td>\n",
              "      <td>First, it targets the whole vector representat...</td>\n",
              "      <td>4933</td>\n",
              "      <td>456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>Recent results from linear algebra stating tha...</td>\n",
              "      <td>SkeUG30cFQ</td>\n",
              "      <td>We provide a theoretical study of the properti...</td>\n",
              "      <td>The Expressive Power of Deep Neural Networks w...</td>\n",
              "      <td>54</td>\n",
              "      <td>Recent results from linear algebra stating tha...</td>\n",
              "      <td>4143</td>\n",
              "      <td>594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>Analogical reasoning has been a principal focu...</td>\n",
              "      <td>SylLYsCcFm</td>\n",
              "      <td>The most robust capacity for analogical reason...</td>\n",
              "      <td>Learning to Make Analogies by Contrasting Abst...</td>\n",
              "      <td>67</td>\n",
              "      <td>It is natural to consider, however, whether th...</td>\n",
              "      <td>6473</td>\n",
              "      <td>673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>Recent advances in computing technology and se...</td>\n",
              "      <td>ByJbJwxCW</td>\n",
              "      <td>We propose a deep Multi Instance Learning fram...</td>\n",
              "      <td>Relational Multi-Instance Learning for Concept...</td>\n",
              "      <td>97</td>\n",
              "      <td>Most of the medical time series lack annotatio...</td>\n",
              "      <td>4819</td>\n",
              "      <td>513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93fa64ac-3750-4aff-9d06-36ef63158ba9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-93fa64ac-3750-4aff-9d06-36ef63158ba9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-93fa64ac-3750-4aff-9d06-36ef63158ba9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dab96332-f1fe-4190-b84a-0c00056ee215\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dab96332-f1fe-4190-b84a-0c00056ee215')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dab96332-f1fe-4190-b84a-0c00056ee215 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_test",
              "summary": "{\n  \"name\": \"data_test\",\n  \"rows\": 203,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"Quantum computers promise significant advantages over classical computers for a number of different applications. We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer. We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification. We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously. We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network. Finding a suitable set of weights for a neural network has become one of the most studied problems of modern machine learning. It has presented a significant challenge to computer scientists for whom few successful alternatives to back-propagation are available. It can be difficult to explore very large search spaces efficiently and, worse, optimization may converge to a local minima far from global optimum BID2 . Understanding the cost function landscape is also hard, and choosing hyper-parameters and designing neural networks remains mostly a manual process. As Moore's law approaches its end, two new computing paradigms have been explored, neuromorphic and quantum computers. Quantum computing is based on quantum bits (or qbits) obeying the laws of quantum physics as opposed to the classical bits of today that are based on classical physics. Note that in physics the term classical is used to mean non-quantum and we use this terminology throughout. Quantum machine learning aims to find an advantage in applying quantum computing to machine learning. Current research into quantum machine learning falls into one of two catgeories. Some quantum algorithms promise a revolution in machine learning in theory, but contain many gaps in their implementation in practice. In contrast, others are more realistic in their method, but struggle to justify a place amongst the well-established methods of machine learning. In this paper, it is shown that a quantum computer can output a quantum state that represents the entire cost landscape for a given neural network. The method is shown to be versatile and even able to represent a meta-cost landscape of all possible hyperparameters and parameters. Applying it to the connectivities and weights of a binary neural network and simulating the quantum algorithm on a classical computer, we further show that this landscape state can be used for training and metatraining the binary neural network for a small toy problem using quantum amplitude amplification, a standard quantum algorithm. Binary Neural Networks (BNNs) are neural networks with weights and activations restricted to taking only binary values, usually \\u00b11. The greatest advantage of BNNs is in their deployment as using binary provides great advantages in compression and inference time, as well as computational efficiency through the use of bitwise operations. On the other hand they are relatively tricky to train as the sign function has a derivative of zero nearly everywhere, the search space is discrete, and alternative training methods take significantly longer than non-binarized neural networks. Nonetheless, BNNs have achieved state-of-the-art performance on smaller datasets such as MNIST and CIFAR10 BID4 but initially suffered when applied to larger datasets such as ImageNet. A popular approach to solving this issue has been to relax the binarisation constraints. This has been achieved by using multiple binary activations BID13 or by introducing scale factors BID16 , both of which result in improvements in accuracy. On the other hand, it has been argued that a better training strategy for BNNs is sufficient to achieve high accuracy on large datasets without compromising on the pure binary nature BID22 . After investigating the accuracy failures of the previous methods, a number of improvements to the BNN training process have been suggested such as changing the activation function, lowering the learning rate and using a different regularization term. These changes helped achieve both high accuracy and high compression rates on ImageNet. Again, this solution is not entirely ideal, as training BNNs is already relatively slow, and a lower learning rate exacerbates this issue. Between the efficient deployment, discrete search space, slow training and relatively small problem size (near-term quantum computers favor problems that require fewer bits), training a binary neural network represents an ideal test case for a quantum computer. Finally, BNNs have been suggested as a candidate for efficient hybrid architectures through transfer learning. The idea is that a BNN pretrained on ImageNet may be used as a feature extractor for other datasets by retraining a final non-binarised layer. In this way, a hybrid hardware-software architecture can implement the binary part using efficient hardware and the non-binary final layer in software BID12 . Quantum computers use quantum bits, manipulated with quantum gates in quantum circuits according to quantum algorithms. The advantage of quantum computers over classical computers is that certain quantum algorithms show significantly improved computational complexity compared to the best known classical algorithms. Such improved scaling, combined with the exponentially growing computational power of qubits suggests that (large, error-free) quantum computers would be able to easily handle and process very large amounts of data. Most relevant to this paper is the quantum search algorithm known as Grover's algorithm BID8 , itself a specific case of another algorithm known as quantum amplitude amplification BID0 . These algorithms can search for an element of an unstructured dataset of size N in O( \\u221a N ) operations, over the classical O(N ). It is important to keep in mind that these are compared to the best-known classical algorithms, and not that they are better than all possible classical algorithms. A recent paper BID21 has challenged the presumed superiority of a quantum recommendation algorithm with a new classical algorithm inspired by the quantum method that shows similar scaling. In our case, the optimality of Grover's algorithm has been proven BID24 and so the assumption of its inherent advantage is robust. Some quantum algorithms are able to efficiently perform k-means clustering BID14 and solve linear systems of equations BID9 , among other such achievements (see BID3 for a review). All of these algorithms require the classical data to be encoded into an accessible quantum form of RAM known as a qRAM. Although there is some work on how this might be done BID7 it is not known to even be possible to construct a qRAM in an efficient manner for a completely general dataset. To many, this is a significant drawback that cannot be ignored, and places a heavy burden on the feasibility of these methods. An alternative approach has been to mimic the progress of classical machine learning by using methods classically known to work. Many have taken to using classical computers to train parametrized quantum circuits to perform classification BID19 or to learn generative models BID6 . Some, but not all, of these circuits mimic neural networks in that they are layered and try to utilize non-linearities . The biggest issue with this approach is the lack of an efficient algorithm for training quantum circuits and so current methods are akin to black box optimization. The motivation is that the output of quantum circuits are known to be impossible to efficiently simulate with classical computers and could therefore provide superior performance on that basis. A slightly different approach to training a perceptron using quantum amplitude amplification has been explored before and its complexity studied compared to classical methods BID10 . Previous work has demonstrated and experimentally implemented the use of quantum hardware to perform binary classification, BID15 ) but this is not the same as the method proposed in this paper, as this work is based on a different, more general gate-based form of quantum computation as opposed to the quantum annealing devices of the former. Quantum computing follows the structure of classical computing very closely. Quantum bits, or qubits, are the fundamental unit of quantum information. Their values are manipulated by applying quantum (logic) gates to them in the form of quantum circuits. Qubits are challenging to manufacture in practice due to the noise-sensitive nature of quantum properties. The biggest such device in existence today contains just 72 highly imperfect qubits, but it is worth noting that progress has advanced at a particularly rapid pace over the past few years and a number are available for public access on the cloud. In addition, simulating the behaviour of qubits using classical computers is difficult, requiring exponentially increasing resources as the number of qubits increases -with an upper limit of 50 (perfect) qubits often cited for the most powerful supercomputers. Therefore, quantum algorithms are almost always defined in terms of their circuit implementation, as opposed to the higher level abstraction of classical algorithms. Qubits are the unit of quantum information and are fundamentally different to classical bits. Whilst classical bits are completely described as being in one of two states, either 0 or 1, the state of a qubit cannot be fully described by just a single number. It can be in the 0 state, the 1 state or a quantum superposition of both. Mathematically the state of a qubit is a two dimensional vector with complex elements and a unit norm. We can write a general form for this vector as DISPLAYFORM0 Here \\u03b1 and \\u03b2 are the probability amplitudes of the zero state |0 and the one state |1 respectively. Qubits cannot be simply read out as classical bits are, but are instead measured. Measurement is a unique feature of quantum mechanics. If the qubit given above is measured, it will be found in the zero state with probability |\\u03b1| 2 , outputting a value of 0, and the one state with probability |\\u03b2| 2 outputting a value of 1. Therefore measurement of a qubit state always produces a binary outcome, no matter the actual state itself. Measurement is fundamentally indeterministic, probabilistic and irreversible. Upon measurement, the original state is lost along with the values of \\u03b1 and \\u03b2 as the qubit collapses to the state |0 or |1 corresponding to the measurement outcome. As a result, the values \\u03b1 and \\u03b2 cannot be obtained without repeated measurements of many identical copies of the state. Here \\u03c6 is a phase that does not affect measurement outcome, but can be manipulated with quantum gates and play a role in quantum algorithms. Part of the power of quantum computing is the ability to harness superposition to parallelize certain computations and processes. An important feature of qubits is the way in which they are combined. N qubits are collectively described by a complex vector of unit norm in a similar way as the above, but the length of this vector is given by 2 N . It is this exponential scaling that makes even modest numbers of qubits unfeasible to simulate on a classical computer. In both classical and quantum computing, gates manipulate the states of bits and qubits. As complex vectors, qubit states are transformed into one another by applying complex matrices called operators or simply, quantum gates. This transformation follows the rules of linear algebra and a state |\\u03c8 is transformed into a different state |\\u03c6 by a gate U according to the matrix transformation |\\u03c6 = U |\\u03c8 . In order to maintain the stringent requirement of a unit norm, these matrices are restricted to being unitary. A unitary matrix is defined as any square matrix who's inverse is its complex conjugate transpose. Unitarity implies that every quantum gate is reversible, in a manner similar to reversible computing. This fundamental difference in the kinds of operations that can be performed on qubits compared to classical bits is part of the power of quantum computing, but can make analogies to classical computing difficult. Many quantum operations have no classical analogue and conversely, certain simple classical operations (e.g copying the state of a general qubit) are impossible in quantum computing. Just as in classical computing, small sets of quantum gates are universal in that they can be combined to generate any other. It transpires that a small set of quantum gates are sufficient to our work and we choose to list them here, both in terms of their actions and their matrix forms. The X (NOT) gate flips the state of a qubit from |1 to |0 and vice versa. For qubits in superposition, it swaps the amplitudes of the |1 and |0 states. Its matrix form is DISPLAYFORM0 The Z gate has no classical analogue and takes the matrix form DISPLAYFORM1 It transforms an arbitrary state \\u03b1 |0 + \\u03b2 |1 into the state \\u03b1 |0 \\u2212 \\u03b2 |1 . The probability amplitude of the |1 component has changed sign, but the probabilities associated with measurement outcome, as squares of the probability amplitudes, remain unchanged. Note that this still represents a completely different state. The Hadamard (H) gate also has no classical analogue. It is used to transform qubits from their initial state |0 into the state DISPLAYFORM2 |1 -an equal quantum superposition of 0 and 1. As a matrix it is DISPLAYFORM3 The controlled-not (CNOT) gate can be thought of as a generalisation of the classical XOR gate. It performs a NOT gate on a target qubit if a control qubit is in the state |1 . We write this as DISPLAYFORM4 Note that controlled gates can be extended both to arbitrary gates (e.g. CZ) and to arbitrary numbers of control qubits (e.g. CCCNOT). The main advantage of qubits over classical bits is their ability to be placed and processed in quantum superpositions of states. The key to our method is to use superposition to parallelize the processing of weights in a way not possible classically. Our scheme proceeds as follows: Step 1: The weights are represented in some way by the quantum state of a set of qubits. Setting those qubits into a state that represents an equal superposition of every possible set of weights allows them to define the domain. Step 2: We then build a quantum circuit analogue of the neural network U QN N that takes in a given set of weights (encoded within qubits as above) and an empty register, and outputs onto the register the corresponding accuracy according to the chosen neural network i.e U QN N (w, 0) = (w, acc w ).Step 3: Since U QN N is a quantum circuit, inputting weights in superposition form allows them to be processed in parallel. Thus by using the domain-defining qubits as the weights input to U QN N the output will be a superposition correlating all possible weights to their corresponding accuracies. This is what we refer to as the landscape state. We can write this as DISPLAYFORM0 where W is the set of all possible weights, W is its size and O w the accuracy of the neural network given the set of weights w. This is a single quantum state representing the entire landscape of the neural network by correlating every possible set of weights with its resultant accuracy. In the language of quantum physics the weights and the accuracies are entangled. This method can be adapted in many ways. For example, if just a single weight is set it to superposition and the rest kept to a given value, then the output is the cost landscape of just that one weight conditional on the value of the others. We are not limited to only setting weights in superposition. We note that a meta-neural network with the presence/absence of the connections within the neural network themselves represented by binary parameters can also be created. These meta-parameters can also be encoded in qubits, formed into a quantum circuit and set to superposition. If we set both the weights and the connection meta-parameters to superposition then the output state of the quantum circuit contains an entire meta-cost landscape of every possible weight with every possible connectivity of a neural network simultaneously correlated with the respective accuracy. We demonstrate our method by generating the landscape state for a small binary neural network on simple toy problems and use it to train the network. The advantage of binary neural networks is that each weight can be naturally represented by just one qubit and so are therefore a suitable demonstration given the fundamentally small number of qubits that can be simulated on a nonquantum device. We construct two toy problems, both of which are a binary classification on three binary features x i \\u2208 {\\u22121, 1} of eight data points corresponding to every 2 3 arrangement of those features. In problem 1, the label is given by the function DISPLAYFORM0 and for problem 2 the label is given by DISPLAYFORM1 In both cases we define the sign function as: DISPLAYFORM2 We choose to implement the BNN given in figure 1 meaning that we are aiming to find eight binary weights. To construct a quantum circuit equivalent to the BNN, henceforth known as the Quantum Binary Neural Network (QBNN), every operation in the implementation of a BNN must be mapped to a quantum equivalent. Below we detail each of these and their quantum implementation. Representing numerical values with qubits is already well established in the literature BID20 . Other parts of our construction are, however, incompatible with non-binary input and so we restrict ourselves to the simple case of a binary data input. In this case, the qubit states |1 and |0 represent the values +1, \\u22121 respectively. In a quantum circuit, all qubits begin in the |0 state and need only an application of a single NOT gate to be set to |1 where appropriate. Given two qubits representing binary values \\u00b11 as described above, we can multiply them using an anti-CNOT gate. An anti-CNOT gate applies a NOT gate to a target qubit if the control qubit is in the state |0 instead of |1 . Its truth table is identical to an XNOR gate and outputs |1 if both input values are equal, and |0 otherwise. This truth table matches the truth table of multiplying two binary values and thus performs the same function. It can be constructed using two NOT gates and a CNOT gate. Qubits that encode weights must always be used as control qubits to preserve the values they encode. Since the sign function is highly non-linear, it poses the greatest challenge to translate to the linear algebra-based language of quantum mechanics. Generally, the problem can be overcome by the addition of extra helper or 'ancilla' qubits. If we restrict the problem to the special case of binary arguments only, the sign function 1 is reduced to finding whether there exist N/2 qubits out of N in state |1 . This can be achieved by constructing a quantum analogue of a classical majority function by replacing AND gates with CCNOT gates and constructing OR gates out of CNOT and NOT gates. The number of gates needed scales as the binomial coefficient N choose N/2. As an example, figure 2 shows a three input neuron and its quantum circuit implementation. Note that this is just a single neuron, and not our entire network. In practice, it works in the same manner as a classical neural network. The activations of each neuron in one layer are then weighted by their own weight qubits and used as input to the next layer and so on. This whole circuit is what we refer to as the QBNN. For each data point on the training set we must compare the prediction to the label in order to find the accuracy. We initialise a register of qubits to store the predictions. The reversibility of quantum circuits allows us to apply the QBNN for a given data point, store its output value onto its corresponding qubit on the register, perform the same QBNN in reverse order -its inverse -to refresh the other qubits, and continue for the next data point in the training set. This resetting is a common, necessary workaround for small quantum computers and is easily avoided by parallelization given more qubits. For a training set of size N , we obtain a register of N qubits containing the predictions of the QBNN for each of them. Since both the labels and the outputs are binary, we can represent the accuracy of each of these predictions by performing a NOT gate on all the qubits corresponding to a data point with a label of 0. Each qubit in this register will then be in the state |1 if it corresponds to a correctly classified data point and |0 if it does not. By applying the QBNN over the entire training set with the weights initialized in superposition, our circuit output is the cost landscape state. Training the BNN can be seen as a search for a single state within the cost function landscape, for which we use a quantum algorithm known as quantum amplitude amplification. It is not the first time that quantum amplitude amplification has been suggested as a means to train quantum neural networks BID17 ), but they did not construct the actual details of an implementation such as the method of generating a nonlinearity. Quantum amplitude amplification is a technique to amplify the probability amplitudes that correspond to desired state(s) within the superposition and therefore increase the probability of measuring one of these. It works by splitting the space of all states into a 'good' and a 'bad' subspace and rotating their relative probabilities when measured. In this case the 'good' subspace is defined as that which has all the qubits in the prediction register in the state |1 implying that all data points have been correctly classified. It is known that quantum amplitude amplification requires just O(1/ \\u221a a) to search for an entry with an occurrence probability of a BID0 . Quantum amplitude amplification works by first constructing the amplifying operator, Q. DISPLAYFORM0 The composite operation, Q, is interpreted as a sequence of operations applied from right to left as read in the equation above. U QBN N is our entire QBNN circuit (for all data points), and U \\u22121 QBN N is its (matrix) inverse. Since quantum gates are reversible, and every gate we have used is self-inverse, we obtain this by applying all of the gates of U QBN N in reverse order. The operations S 0 and S \\u03c7 reverse the sign of the probability amplitudes of the initial state and the target state(s) respectively. In this case, our target states correspond to those with an accuracy of 100% and S \\u03c7 is a controlled-Z gate performed on each of the target qubits. Similarly, the initial state of any quantum computer is defined as having all the qubits in the state |0 , and thus we can implement S 0 by first applying a NOT gate to each qubit and then applying the same controlled-Z gates as for S \\u03c7 . FIG2 is a pictorial representation of how quantum amplitude amplification changes the probability distribution of the measured weights. If we write the initial probability of obtaining the correct weights by random as p and the number of successive applications of operator Q to be k, it can be shown that the probability of obtaining the optimal weights when measuring the circuit after k amplifications is sin 2 (2k + 1)\\u03b8where p and \\u03b8 obey the relation p = sin 2 \\u03b8 BID0 . The probability of success is therefore highly periodic in k. The problem of training the BNN essentially reduces to a probabilistic search on this one hyper-parameter and its regular periodic landscape. The location of the first maximum, i.e of k * , is inversely proportional to \\u03b8 and hence to the probability of obtaining the weights by random. In other words, a harder problem with more weights to search requires a greater number of quantum amplifications to find. In practical terms the landscape state is a set of 8 weight qubits and 8 prediction qubits. After the search, at the end of the entire process, all the qubits are measured. If the prediction qubits are all in the state |1 the training was a success and the appropriate weights can be simply read off their corresponding qubits. We constructed and simulated the QBNN and quantum amplitude amplification circuits on the projectQ framework BID18 . The use of an actual quantum computer was not possible as the number of gates used during the computation (called circuit depth) exceeds the maximum possible circuit depth for the current generation of imperfect noisy qubits. Furthermore, we use more qubits than are available on current publicly accessible quantum hardware. For each of the two problems defined, we plotted the probability of obtaining an optimal set of weights against the number of iterations of the quantum amplitude amplification and obtained results, shown in FIG3 , that match well with the expected periodic behavior described in equation 3. This confirms that a quantum search of the landscape state can indeed be used to train a BNN in exactly the manner as predicted theoretically. We emphasize here that every reference to finding optimal weights means that the BNN has been trained to an accuracy of 100% on the training data. In order to demonstrate the performance of this method in actual training, we follow the simple algorithm described in BID0 for probing this landscape. This simple algorithm begins with n = 0 and chooses a random integer k of quantum amplifications between 0 and n. n increases by 1 until the training succeeds. In our experiment, we perform 100 runs of this algorithm and present in figure 5 a cumulative plot of the proportion of these runs that were successful against the number of iterations this algorithm required. We find that training succeeds with a probability over 90% after just 5 steps for the first problem and 6 steps for the second. In order to compare this to a classical search, we search the entire space of 2 8 = 256 possible sets of weights and find that there are eight and four correct sets of weights (giving 100% accuracy) for the first and second problem respectively. Statistically, if these weights were to be searched through the analogous classical brute data is the cumulative probability of success over 100 runs of the algorithm. Classical results are analytically derived from the known probability of obtaining a solution by random search. The superior scaling of the quantum algorithm becomes more prominent for harder problems.force search, one would find that it requires 28 and 57 steps respectively to succeed with a confidence over 90%. This matches our expectation of a quadratic speedup of the quantum search over the classical. We then construct a more complex QBNN which can incorporate meta-training by introducing a set of binary indicators that correspond to the presence or absence of a set of connections within the BNN and encode these within qubits in the exact same way as was done with the weights. With the weights and connection parameters both set to superpositions, the output of this circuit is the meta-cost landscape, where weights, connections and accuracy are all entangled with one another. As before quantum amplitude amplification is used to search for the state with all points correctly classified. Again this has been suggested before, but we present a full circuit implementation of this idea (da BID5 . In practice, due to qubit number constraints, we choose to only learn the structure of the first layer of the BNN. The second layer remains fixed. Due to the increased size of the circuit, and the significant increase in computational cost, we did not perform a complete classical search of the space as before but it is clear to see that the space of parameters we are searching has increased and therefore the number of amplifications required has similarly increased. Between 16 and 20 amplifications were found to be sufficient to produce results with a reasonable probability. FIG5 (a) shows the meta-BNN that was used, and (b) and (c) show two solutions to problems 1 and 2 respectively learnt by our meta-QBNN. It is particularly interesting to note that the learned structures of the two BNN solutions seem to match well with their problem definitions (equation 1 and equation 2). Note that due to our circuit construction a neuron that receives no input will always output \\u22121. We show that quantum superposition can be used to represent many parameters of a neural network at once and efficiently encode entire loss landscapes in a quantum state using just a single run of a quantum circuit. We demonstrate this explicitly for both parameters and hyper-parameters of a BNN, and show that further processing of this state can lead to quantum advantage in training and metatraining. As a training method it possesses significant advantages as it is landscape-independent, has a quadratic speedup over a classical search of the same kind, and would be able to solve statistically neutral problems such as parity problems BID23 . It is not, however, without shortcomings. One potential criticism is the issue of over-fitting. Since our problem is so small, we chose to define a target state as one where the accuracy is 100% on the training set but this is rarely desirable in real machine learning. One solution may be to simply run the quantum algorithm and, upon finding a particular set of weights that represents an overfit, run the algorithm again but with a deselection of that particular set of weights. This can be done by simply changing the sign of the probability amplitude corresponding to that state during each iteration of the quantum amplitude amplification. A similar issue is that regular machine learning typically uses batch learning, whilst our method incorporates the entire dataset at once. This too can be fixed by altering our method to use a different batch of the data for each quantum amplitude amplification iteration. This works since no matter what batch we use, a good set of weights should still be amplified by the circuit. In fact, such an implementation is advantageous since it would allow us to use less qubits which in practical terms are limited in number in the near term. A significant limitation in our method is the requirement that the input is binary, and the poor scaling of the activation function. Both of these problems arise completely from our implementation of the sign function, which could either be improved or replaced entirely with a different binary activation function that could be implemented more efficiently on a quantum computer and would be compatible with non-binary input. There has been progress on creating effective non-linear activation functions by so-called repeat-until-success circuits BID1 ). An alternative approach would be to use floating point representations as in classical computing and the quantum equivalent of full-adders, but this would require an overhead in the number of qubits that would take us beyond the limit of classical simulation. Finally, we note that this method scales poorly compared to backpropagation and that the advantage only appears in like for like comparisons of unstructured classical/quantum searches. The cost function landscape is not unstructured and algorithms such as backpropagation take advantage of this. We conjecture that a quantum search method that applies quantum advantage to structured searches, if it exists, can be applied to the cost landscape in place of quantum amplitude amplification. Finding ways to harness quantum computers to aid classical machine learning methods in a meaningful way remains an open problem and we present the loss landscape state as a plausible candidate towards this goal. Whilst we used the example of quantum training, the most fruitful approach in the short term is to ask whether some property of the state can be used to glean useful information for classical machine learning methods. This might take the form of understanding the roughness of the landscape, identifying certain features, or even choosing an appropriate learning rate. Further work in investigating the relationship between the landscape as a quantum state and its features from a machine learning perspective would be a step forward in this direction.\",\n          \"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a more straightforward learning signal. Humans effortlessly combine the two, and engage in chit-chat for example with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning via self-play against an imitation-learned chit-chat model with two new approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-k utterances. We show that both models outperform a strong inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals. In the literature on artificial dialogue agents, a distinction is often made between \\\"goal-oriented\\\" dialogue, where an agent is tasked with filling slots or otherwise obtaining or disseminating specified information from the user to help complete a task, and \\\"chit-chat\\\", where an agent should imitate human small talk. Modeling goal-oriented dialogue can have advantages over chit-chat imitation as it gives clearer metrics of success and perhaps more meaningful learning signals; but goal-oriented dialogue data is often more specialized, covering only a narrow slice of natural language. Current goal-oriented datasets study setting like booking restaurants or airline tickets, or obtaining weather information, as standalone tasks (Raux et al., 2005; Henderson et al., 2014; Bordes et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) . Chit-chat agents, by contrast, might focus on coarse statistical regularities of dialogue data without accurately modeling the underlying \\\"meaning\\\"; but the data often covers a much wider space of natural language. For example, Twitter or Reddit chitchat tasks (Li et al., 2016a; Yang et al., 2018; Mazar\\u00e9 et al., 2018 ) cover a huge spectrum of language and diverse topics. Chit-chat and goal-oriented dialogue are not mutually exclusive: when humans engage in chit-chat, their aim is to exchange information, or to elicit specific responses from their partners. Modeling such goals, however, is made difficult by the fact that it requires large amounts of world knowledge, and that goals in real life are implicit. In this work, we study goal-oriented dialogue agents in the setting of a multi-player text-based fantasy environment (Urbanek et al., 2019) . The environment is built on top of a game engine that grounds actions and reference objects, and thus codifies a body of world-knowledge. Although the interactions between objects and characters are simulated, the choice and types of interactions, the text used to describe them, and the dialogues between characters, are \\\"natural\\\" and wide-ranging, having been collected from human crowdworkers. We define the general task of, given a particular character in a particular scenario (location, set of objects and other characters to interact with) to conduct open-ended dialogue such that a given action is executed in the future by their dialogue partner. The given action could be an emote action (smile, laugh, ponder, . . . ), or a game action (wear chain mail, drink mead, put glass on table, . . . ). The richness of the environment means that there are a huge set of possible tasks and scenarios in which to achieve a wide range of actions. Thus, this task is ideally suited for bridging the divide between goal-oriented and chit-chat dialogue, combining clearer metrics and learning signals on the one hand, with the richness and complexity of situated but open-domain natural language on the other. Figure 1 : Example episode from the LIGHT dataset, consisting of an environment (location setting, characters with given personas, objects), utterances and game actions. There are 10,777 such humanhuman gameplay episodes, and a rich world of 663 locations, 1755 characters and 3462 objects. We train models to achieve these tasks using reinforcement learning (RL) and a type of self-play between two agents. The first agent, which we call the environment agent, is trained with imitation learning on human-human interactions (game actions, utterances and emotes) and subsequently kept fixed. The second agent, the RL agent, is trained to conduct dialogue given the goal, and the two agents interact within a given environment until the goal is either reached or a given number of turns has expired. At that point, rewards are given, and the RL agent is updated. We compare agents that have been trained to imitate human actions given a goal (an \\\"inverse model\\\") to two different RL approaches: optimizing actions with latent discrete variables (topics), or via rewarding actions sampled from the model (via the top-K outputs). We show that both types of RL agent are able to learn effectively, outperforming the inverse model approach or a vanilla chit-chat imitation baseline, and can converse naturally with their dialogue partner to achieve goals. We work in the LIGHT game environment (Urbanek et al., 2019) , which is a multi-user text-based game, involving many characters playing the game at once. Characters (either played by humans or run by models) can speak to to each other via free text, send emote actions like applaud, nod or pout (22 emote types in total), and take actions to move to different locations and interact with objects (e.g. get cutlery, put cutlery in drawer, etc.), see Appendix A for a full list of game actions. LIGHT at its core has a game engine which can formally be defined as a graph, where each location, object and character is a node, and they are connected by labeled edges representing relationships, for example contained-in, path-to or has-property. Actions in the game result in changes in state of the graph. To a player (agent) a local view of the graph can be seen and this is expressed in text, as are the game actions and changes of state. This text then naturally interleaves with the dialogue utterances of the speakers as well to form a input context sequence from which a character can base their subsequent actions. See Fig. 1 for an example. To make the world and its textual descriptions, LIGHT consists of a large set of human-written game locations, characters, and objects, all based within a fantasy medieval setting. Their names, descriptions and properties were crowd-sourced, yielding a total of 663 locations, 1755 characters, and 3462 objects. They range from beaches with crabs and seaweed to crypts with archaeologists and coffins, yielding an extremely rich environment for agents to learn within. An additional set of crowdworkers were then asked to play the role of characters (randomly picked from the set of 1755) within the created world as rendered by the game engine. This involved them making utterances, game actions and emotes, while interacting with each other (in pairs). The resulting gameplay data consists of 10,777 episodes with an average of 18.3 actions each (game actions, emotes and utterances) of rich human play. These are split into train (8538), validation (500) and test (1739) portions, the latter being split into new episodes in existing settings (test seen, 1000) and completely new settings (test unseen, 739) . This gameplay data can be used for training models using imitation learning, as well as for obtaining \\\"common sense\\\" knowledge about how the world works, i.e., what kinds of things certain characters say; what actions they use with certain objects; what they say and how they act in certain environments or while interacting with certain other characters. The whole environment is thus intended as a proxy for learning about the world within a rich simulation, while avoiding the complexities and bandwidth of rendering (3D) computer graphics. While players were not given specific goals, but instead asked to play the role convincingly of the character given, during play some of them effectively defined their own goals during the interactions, see Fig. 1 . The tasks we consider in this work involve interaction between two agents in a given LIGHT scenario. One of the agents, which we will call M env , together with the game engine, effectively functions as an environment for the other agent, which we will call M RL . Because we will formulate our tasks as a reinforcement learning problem, we will also refer to M env as the \\\"environment agent\\\" and M RL as the \\\"RL agent\\\". We assume that the environment agent is fixed; in this work it will be a model trained via behavioral cloning from human-human interaction data. The RL agent must conduct open-ended dialogue such that a given goal action is executed in the future by the environment agent. Our task is formally defined as follows. The two agents M env and M RL are given their views of the scenario (D env and D RL respectively). These consist of the setting name, scenario description, character names, and their own persona, all described as a sequence of text (see Fig 2) . Note that each agent can only access their own persona but not the persona of the partner with whom they are conversing, but they do know the name of their partner. Denote by t the time-step of the environment, U RL t and U env t the utterances of the agents M RL and M env respectively, and denote by A env t the environment actions by M env . Hence the interaction sequence looks like Note that there is an inversion from the usual reinforcement literature language, as the \\\"actions\\\" of the RL agent are its utterances U RL t ; the actions A env t of the environment agent should be considered as internal mechanics of the environment. The agent M RL is additionally given a goal g to achieve, which consists of an action which must be executed by the other agent. That is, the objective of M RL is for M env to take the action g. An episode ends when A env t == g or when n becomes larger than a set number of turns. The RL agent only speaks, but does not perform game or emote actions. This was chosen for simplicity, but also to guarantee that the RL agent cannot help force the goal to be reached by performing actions itself -it has to pick the appropriate utterances U RL such that M env eventually takes the action g. Goals We experiment separately with two different types of goals: game actions and emote actions. We use the same train, valid, test (seen and unseen) split of the original human-human LIGHT episodes, assign roles M RL and M env randomly, and randomly pick an action by M env that occurs in the episode as the goal. We can then present the corresponding setting to our agents in order to form a new interaction, but within the same scenario and with a goal that was naturally desirable and achievable within that setting. Observations The state observation O t = (D RL , S t\\u22121 , g) at time t given to an RL model consists of the RL agent's setting description (D RL ), the utterance and action history up to that time step (S t\\u22121 ), and the agent's goal (g). Our RL agent models consume O t as a flattened sequence of tokens, and return a dialogue utterance U RL t . Each structured component is represented in the flattened sequenced separated by a special token denoting the types, e.g. names, settings, etc., see Fig. 2 . Note that because the entire history and goal is given to the RL agent, the environment is Markovian. Reward We have a terminal reward of +1 only if the goal g is achieved and 0 otherwise, i.e, it is +1 if the environment agent takes the goal action g. The episode ends after n steps. In our experiments we consider n = 1 and n = 3. In this section we describe the models for M env and M RL . In this work these are retrieval models, using the LIGHT dialogue corpus as candidates. We leave generative models to future work. Base Agent Architecture For all our models we adopt the same base architecture, which is a 12-layer bidirectional transformer (Vaswani et al., 2017) pre-trained on a large dialogue corpus (Reddit, 174M examples), and then fine-tuned on our task 1 . To score retrieval candidates, we use a biencoder as in Urbanek et al., 2019) . That is, two transformers are used, one to encode the context, and another to encoder a candidate dialogue, and a dot product between the first output vector of each scores the match. To produce a dialogue utterance one then takes the utterance with the largest output from the training set candidates (111k in this case). For emotes and actions, the same procedure is used, but with those candidate sets instead. For actions, the candidates are the set of admissible actions at that game state, which are provided by the game engine, for example get apple is only available in the candidate set if it is a valid action (an apple is present in the room). For emotes, all 22 candidates are always available. To train the model, a cross entropy loss is used. Similar to Mazar\\u00e9 et al. (2018) , during training we consider the other elements of the batch as negatives. Environment agent The environment agent is the base agent described above, and stays fixed during episodes where the RL agent is trained. This helps guarantee that our RL models stick to using the semantics of natural language (English) rather than so-called language drift, of learning a new emergent language with the same tokens (Lee et al., 2019) . RL agents We design two RL approaches for our tasks -learn to pick the right latent discrete variables (topics) that lead to the correct U RL i ; and learn to pick the correct U RL i from the top K candidates. These are described in more detail in Sections 4.2 and 4.3. We also discuss a baseline \\\"inverse\\\" model trained via behavioral cloning on the human-human data. We consider an inverse model, trained to imitate human actions given a goal, as both a baseline for comparing to RL models, and for producing weights form which we can fine-tune. The inverse model consists of a Bi-encoder, as described above, which takes as input an observation O t similar to our RL models, and outputs an utterance. We train it by extracting from the human-human game logs training set (which does not have goals) every instance where a game action occurs at time t in S t , that is where for 0 < i < t might be null). We then construct a training example for the inverse model with observation (D RL , g = A env t , S t\\u22121 ). i.e. setting the goal g to be A env t , and with the desired action to be taken by the agent as U RL t . Here we use the subscripts \\\"RL\\\" and \\\"env\\\" just to mark the relative positions in the sequence, as all actions and utterances come from the human logs. Note also that unlike the RL agents we train, the human in the RL agent \\\"position\\\" can take game actions. We can thus train this model in a supervised manner using a cross entropy loss as described before. This model does not learn a policy interactively, and hence might not learn to plan or strategize optimally for goal completion. The data distribution it is trained on is different than the data distribution seen by the RL agents. Nevertheless, it can serve as a strong baseline. Further, when training our RL agents, we initialize their weights to the weights of this model, and then fine-tune from that point. Optimizing all the parameters of a large transformer architecture by RL is both incredibly costly in data efficiency and computing time, and is also known to have the problem of language drift (Lee et al., 2019) -that is, there is no guarantee after training with self-chat that the models will output recognizable natural language utterances. A solution to both problems is to train most of the parameters of the model with human-human language data, and then to either disentangle or only optimize some of the parameters with model self-chat . Here, we propose a straight-forward model for that purpose. We assume an RL agent that consists of two components. The first component F c (O) = P (T c (O)) maps from an observation to a discrete variable with C possible values. It consists of a chain of two functions: a transformer T s that takes in the observation, and outputs a state representations, and a policy chooser c = P (s) \\u2208 (1, . . . , C) which takes in the state representation and outputs the value of the discrete latent variable. The second component T u (O, c) is an additional transformer that takes as input the observation as well as the output of the first component, and outputs a dialogue utterance. That is, the entire model is the chain u = T u (O, P (T s (O))). We make this explicit decomposition so that we can train only part of the model with RL; note that the \\\"action\\\" trained via RL is choosing c, not outputting the final utterance. Initial topics We first pre-train the transformer T s using the inverse model described in Section 4.1, which produces a vectorial representation of a given observation. We then run K-means over the vectorial representations of all observations from the training set to provide the mapping to one of C values, which represent dialogue topics, which we use as our initial function P (s). These two functions together give us our initialization of F c . Table 1 shows the cluster ID and the topic denoted by that cluster along with the most representative sentences (closest to the center) for that cluster for 50 topics. As we can see, the clusters learnt can be coherent about a topic. We use these 50 topics as a set of actions A for our RL setup. From c to A Given our initial choice of F c , we can also pre-train T u . We simply take our initial human-human training data, and for each observation append the topic computed by F c to it. This allows our model to be able to generate an action (utterance) conditional on both an input and a topic. We can now train a policy by RL that optimizes the topic at any given point in the episode. We keep the pre-trained portions of the model T u and T s fixed and during fine-tuning only optimize P . The cluster chooser P is redefined (from the initial K-means) to be an MLP network consisting of 2 layers. A discrete action is sampled from a categorical probability distribution over the possible topics, given by c t \\u223c Categorical(h 2 t ), where h 2 t = tanh(W 2 tanh(W 1 s t + b 1 ) + b 2 ). The state vector s t also encodes the goal g and hence, the policy is conditioned on the goal g of the agent. Hence, the policy can learn strategies that will result in picking actions at each time step t that will help the agent to achieve its goal g. As our RL agent can only choose topics, it cannnot redefine easily the meaning of words to cause language drift. The Top-K model is another approach to keeping the number of trainable parameters small. It uses the inverse model to get a context embedding v context from the observation, and a list of K candidate utterance embeddings v 1 , ...v K . These are the encodings by the inverse model of the K utterances it considers most likely given the context and goal. We then train a small (2-layer) transformer model that takes as input the set {v context , v 1 , ...v K }. We use the attention above weights of v context against the candidates at the last layer of the transformer as the distribution over the candidates for sampling an utterance. We use K = 50 in the experiments. We use the Advantage Actor-Critic implementation (A2C; Kostrikov, 2018) to train the policy and the value function for both the latent-variable and top-K models. Chit-chat dialogue There is an increasing body of work in the domain of chit-chat, where the primary approaches being currently tried are end-to-end neural approaches. They are typically large pre-trained and then fine-tuned transformers, either generative or retrieval, where currently retrieval models work best on a number of tasks (Zhang et al., 2018; Li et al., 2019) . Our work shares a commonality with these approaches in that the original LIGHT dialogue data we use has no specified goals, and humans chit-chat together (and act). Thus, the conversations cover a rich number of diverse topics. In Urbanek et al. (2019) models were trained in a similar fashion to chit-chat task models, and we adopt similar architectures here, but instead adapt them to learn to pursue goals. Goal-oriented dialogue Traditional goal-oriented dialogue has focused on narrow tasks that would typically be useful for a dialogue-based assistant, for example restaurant (Henderson et al., 2014) , taxi, train, and hotel (Budzianowski et al., 2018) or trip (El Asri et al., 2017) booking. Hence, each task typically focuses on a narrow slice of natural language and world knowledge for a specialized domain. Earlier work focused on labeled state representations, slot filling mechanisms and dialogue managers (Rieser & Lemon, 2011) , and more recent work has shifted to an end-to-end approach (Bordes et al., 2017) , in line with chit-chat models, but still the two sets of tasks are rarely considered together, or by using the same methods. RL for dialogue The classical goal-oriented dialogue literature studies RL extensively (Singh et al., 2000) . Typically, they used RL to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser & Lemon, 2011; Gasic et al., 2013; Fatemi et al., 2016) . Recent works have focused more on end-to-end learning. Some works have focused on self-play type mechanisms for end-to-end reinforcement learning, where the reward is derived from goal. A related approach to ours is the negotation tasks of ; , which require two agents to swap 3 item types (hats, balls, books) where the value of the items is different for the two agents, and derives their personal reward. In contrast, our setup encompasses a rich world of settings and characters -with 3462 object types, and a corresponding large number of actions. This is reflected in the vocabulary size itself (\\u223c32,000 versus \\u223c2,000 in the negotation tasks). Other notable uses of RL in dialogue include within visual question answering (Das et al., 2017) , in the domain of chit-chat where RL has been used to decrease repetitive and generic responses through the the use of self-play (Li et al., 2016b) , and through human-bot conversation (Sankar & Ravi, 2019) . RL for language and games RL is used extensively for learning to play games, one of the most well known examples being AlphaGo (Silver et al., 2016) . Since then, language in games has started to be more deeply explored, for example in graphical games such as Minecraft (Oh et al., 2017) , Real-time strategy war games (Hu et al., 2019) , or in text adventure games (Narasimhan et al., 2015; C\\u00f4t\\u00e9 et al., 2018) . The latter are related to our setting. However, those approaches use RL to optimize the set of actions given feedback in a single-player rather than multi-player game, so the text only refers to the environment, and there is no dialogue or actions from other agents. Our work focuses specifically on the latter. We compare our various models on the game action and emote action tasks. We experiment with differing number of steps n allowed to complete the goal, n = 1 and n = 3. Our main results for both seen and unseen test environments are given in Table 2 . We report the average reward and for n = 3 the average number of turns before completion. The results show clear improvements for our topic RL ( \\u00a74.2) and top-K RL ( \\u00a74.3) compared to the inverse model baseline for all values of n, and both types of actions (game actions and emotes). We show the training curves for topic RL in Fig. 3 , reporting rewards averaged over the batch (512 for n = 1, and 128 for n = 3). They show relatively smooth improvements over time, with clear gains over the baseline. As a sanity check we also tried, after training, to replace the topic RL policy with random topic prediction, which yielded poor results, e.g. 0.217 reward for n = 1 test seen game actions. Our model is clearly learning appropriate topic acts. We show examples of successful utterances, achieving goal actions in Fig. 3 for a diverse range of scenarios, actions and language. (n = 1) (n = 3) (n = 1) (n = 3) In this paper, we investigate agents that can interact (speak or act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. We achieve this by defining a task for an agent, where the goal is for the other player to execute a particular action. We explore two reinforcement learning based approaches to solve this task: the policy either learns to pick a topic or learns to pick an utterance given the top K utterances, and compare them against a strong baseline trained to imitate chit-chat. We show that these approaches effectively learn dialogue strategies that lead to successful completion of goals, while producing natural chat. Future work should explore further RL algorithms for agents that can act and speak in natural language at scale in our proposed rich task environment, and we expect further advancements. Constraints Outcome get object actor and object in same room actor is carrying object object is gettable drop object actor is carrying object object is in room object is gettable get object1 from object2 Actor and object2 in same room actor is carrying object1 object1 is gettable object2 is surface or container object2 is carrying object1 put object1 in/on object2 Actor and object2 in same room object2 is carrying object1 object2 is container or surface actor is carrying object1 give object to agent Actor and agent in same room agent is carrying object object is a member of actor steal object from agent actor and agent in same room actor is carrying object object is a member of agent hit agent Actor and agent in same room inform agent of attack hug agent Actor and agent in same room inform agent of hug drink object actor is carrying object inform actor of drinking successfully object is a drink eat object actor is carrying object inform actor of eating successfully object is a food wear object actor is carrying object actor is wearing object object is wearable wield object actor is carrying object actor is wielding object object is a weapon remove object actor is wearing/wielding object actor is carrying object object is wearable or a weapon Table 4 : LIGHT actions and constraints from Urbanek et al. (2019) B GAME EMOTES WITHIN LIGHT applaud, blush, cry, dance, frown, gasp, grin, groan, growl, laugh, nod, nudge, ponder, pout, scream, shrug, sigh, smile, stare, wave, wink, yawn Figure 4: Emote actions within the LIGHT platform from Urbanek et al. (2019)\",\n          \"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness. Deep neural networks achieve state-of-the-art performances on a variety of tasks (LeCun et al., 2015) . However, neural nets are known to be vulnerable to adversarial examples. Imperceptibly perturbed inputs can induce erroneous outputs in neural nets (Szegedy et al., 2013) . In image classification problems of computer vision, previous work has proposed various methods to attack deep models and induce low accuracy (Goodfellow et al., 2015; Madry et al., 2017; Papernot et al., 2016a; Carlini & Wagner, 2017a) . Whereas multiple defenses against adversarial attacks are developed, they don't ensure safety faced with strong attacking methods. There are also theories that explain the existence of adversarial examples (Ilyas et al., 2019; Shamir et al., 2019) , but they often fail to fully explain the features and behaviors of this phenomenon. This makes the study of adversarial attacks important in that it is a threat to real-life machine learning systems (Kurakin et al., 2016) . In this paper, we propose a dynamical system view on the adversarial robustness of the models, as well as new method that significantly defense adversarial attacks. Recent works have shown the connection between deep neural networks and dynamical systems (E, 2017; Haber & Ruthotto, 2017; Lu et al., 2017) . If we regard the neural net as a discretization of an ordinary differential equation (ODE), then training neural nets becomes finding an optimal control of the corresponding discrete dynamical system. Traditionally, we often treat training neural networks as an unconstrained non-convex optimization problem where \\u03b8 denotes the parameters of the model, J denotes the loss function and R denotes the regularizer term, and we solve the problem with (stochastic) gradient-descent based methods (Bottou, 2010; Ruder, 2016) . In the training process, we feed the network with a batch of training data, and compute the gradient with forward and backward propagation (E. Rumelhart et al., 1986) . The propagation process resembles solving optimal control problems that tune the parameters to make the output be close to target states. This viewpoint motivates us to bridge adversarial robustness with Lyapunov stability of a dynamical system, and to train robust networks with algorithms that find stable optimal control. We will formulate the discussion in later sections. 2 RELATED WORK 2.1 ADVERSARIAL DEFENSE Many defense methods have been proposed to improve the models' adversarial robustness. The defenses mainly fall into three types: adversarial training (Szegedy et al., 2013; Zhang et al., 2019) , modifying the networks (Gu & Rigazio, 2015; Lyu et al., 2015; Papernot et al., 2016b; Nayebi & Ganguli, 2017; Ross & Doshi-Velez, 2017) , and adding external models (Lee et al., 2017; Akhtar et al., 2017; Gebhart & Schrater, 2017; Xu et al., 2018; Sun et al., 2019) . Although various defense methods have been developed, a defended deep model is often successfully attacked by newly developed attacks or specific counter-counter measures (Carlini & Wagner, 2017b) . Therefore, it can be hoped that defenses against general attacks will be devised to make deep learning models (adversarially) robust to real-life threats. Recent works have bridged deep neural networks with ODEs and dynamical systems. On the one hand, deep residual networks (He et al., 2015) can be illustrated as forward Euler scheme approximating an ODE (E, 2017), which motivates us to design effective network structures (Lu et al., 2017) . On the other hand, regarding the network as a dynamical system allows us to set up an optimal control viewpoint of neural nets. Pontryagin's Maximum Principle (Boltyanskii et al., 1960) has been applied to train neural nets Li & Hao, 2018) . Given a T -layer neural net, we let the dynamical system {f t (x t , \\u03b8 t ) : t = 0, . . . , T } represents the network, where x t is the input of t-th layer, \\u03b8 t is the parameter, and f t : denotes the t-th layer's transformation, which is usually a non-linear function \\u03c3(\\u03b8 t x t + b t ) for fully-connected layers, convolution layers and batch normalization layers, etc. Therefore, training the neural net can be regarded as controlling the parameters to let the dynamics fit the training data. Specifically, the training optimization problem can be formulated as a typical optimal control problem as follows: . . , T \\u2212 1, where we use x i to denote the i-th input in the batch and B denote the batch size. J and L are the loss function and the regularizer, respectively. Specially, if the model is a deep residual network with structure x t+1 = x t + f t (x t , \\u03b8 t ), we can regard the problem as the forward Euler discretization of the following continuous optimal control problem: where x(t) is a continuous trajectory from the input to the output logits. Adversarial examples are usually clean images added by a small calculated perturbation \\u03b7. The model predicts correct labels fed with clean inputs x 0 , while the output is completely different when it is fed with perturbed input x 0 + \\u03b7. The dynamical system view of neural nets motivate us to characterize this sensitivity with Lyapunov stability of a system (Hirsch et al., 2004) . Definition 1 (Lyapunov Stability). For a given dynamical system\\u1e8b = f (x), x(0) = x 0 , x e is an equilibrium, then \\u2022 The system is asymptotically stable if it is Lyapunov stable and \\u2203 \\u03b4 > 0 such that if x(0) \\u2212 x e < \\u03b4, then lim t\\u2192\\u221e x(t) \\u2212 x e = 0. \\u2022 The system is exponentially stable if it is asymptotically stable and \\u2203 \\u03b1 > 0, \\u03b2 > 0, \\u03b4 > 0 such that if x(0) \\u2212 x e < \\u03b4, then x(t) \\u2212 x e \\u2264 \\u03b1 x(0) \\u2212 x e e \\u2212\\u03b2t , for all t \\u2265 0. The definitions can be easily extended to discrete-time systems. Intuitively, the Lyapunov stability states that for any small perturbation \\u03b7, the trajectory is still \\\"close enough\\\" to the original one. If we regard a neural net as a dynamical system, and ensure the network is Lyapunov stable, then the model is robust to all (adversarial) perturbations. Due to the connection between numerical ODEs and residual networks, we first consider robustness (i.e. Lyapunov stability) of continuous ODEs. , where \\u03c3 is the activation function, e.g., Sigmoid function or ReLU function, it is stable if Re(\\u03bb i (A)) \\u2264 0, \\u2200i, where Re denotes the real part, and \\u03bb i denotes the i-th eigenvalue. One can see, e.g. Hirsch et al. (2004) , for the proof of this theorem. Theorem 1 provides a set of conditions for stable ODEs. However, deep residual network is only a forward Euler discretization scheme of continuous ODE. To ensure numerical stability, we require |1 \\u2212 \\u03bb i (A)h| \\u2264 1 (Ascher & Petzold, 1998) , where the step size h = 1 in residual networks. Added by the identity mapping in residual networks, we can get the stable conditions for discrete dynamics. Theorem 2 (Stable Discrete Networks). For a discrete neural network, i.e., discrete dynamics {f t (x t , \\u03b8 t ) : t = 0, . . . , T }, where f t (x t , \\u03b8 t ) = \\u03c3(\\u03b8 t x t ) (we omit the bias term for simplicity), the network is stable if the \\u03c1(\\u03b8 t ) \\u2264 1, where \\u03c1(A) = max i (|\\u03bb i (A)|) is the spectral radius. If the conditions are added to the unconstrained optimization problem of training, we can greatly improve the adversarial robustness of neural nets. The methods will be discussed in the following section. 4.1 PMP AND MSA For deterministic systems, the Pontryagin's Maximum Principle (PMP) (Boltyanskii et al., 1960) provides a set of necessary conditions for optimal control of the system. Various algorithms have been proposed to solve the deterministic optimal control problem based on PMP. Among them, the Method of Successive Approximations (MSA) (Krylov & Chernous'ko, 1963 ) is one of the simplest algorithms. In the field of deep learning, previous work has utilized MSA to train neural networks Li & Hao, 2018) . Formally, consider the optimal control problem for training neural nets in section 3. For dynamics {f t (x t , \\u03b8 t ) : t = 0, . . . , T }, assume \\u03b8 * = \\u03b8 * 0 , . . . , \\u03b8 * T \\u22121 is a solution to the optimal control problem. Also, we define the Hamiltonian function H : , where the dot denotes the inner product. We have the following necessary conditions for \\u03b8 * . Theorem 3 (Pontryagin's Maximum Principle for Discrete Systems). Assume f t and J are sufficiently smooth. There exists co-states p * = {p * 0 , . . . , p * T } s.t. the following conditions hold: For simplicity of notations, here we assume the batch size is 1. One can easily extend the theorem to minibatch training case by summing over the batch. The theorem can be proved by KKT conditions (Boyd & Vandenberghe, 2004) , where the co-states can be seen as the Lagrangian dual variables. Consider the conditions in PMP, one can find the x equations are exactly the forward propagation of a neural net, and the p equations resemble the backward propagation process. The third condition states that the model parameters must maximize the Hamiltonian function. This motivates us to iteratively compute forward and backward propagation, and solve the Hamiltonian maximization to find the optimal control, which is exactly the Method of Successive Approximations (Algorithm 1). In practice, we usually add regularizer terms that penalize great changes in the maximization step to prevent drastic steps that cause divergence. For the connection between MSA and back-propagationbased gradient descent algorithms, see the appendix of Li & Hao (2018) . Compute the states (forward propagation): The advantages of training by MSA compared with gradient descent algorithms has been discussed in , among which the most significant feature is that the optimization steps on different layers are decoupled. Concretely, after computing the states x and co-states p, the optimization step on layer t is only searching for parameters \\u03b8 t . This not only suggests that the optimization process can be accelerated by parallelization, but also allows us to utilize the features of the problem. The parameter space is greatly reduced compared with the original intractable optimization problem, and hence the optimization is much more easier. This allows us to add constraints that ensure robustness of the model. Consider a layer in the form of f t (x) = \\u03b8 t x, where we leave the activation as an individual layer with no parameters for simplicity, we can derive the following optimization problem for Hamiltonian maximization: max where \\u03b1 \\u03b8 t 2 2 is the L 2 norm regularizer (weight decay), and \\u03b8 t is the initial parameter (i.e., \\u03b8 k t in the algorithm). The last term keeps the training process from drastic steps that cause divergence. The constraint, as illustrated in section 3, is the stable condition for discrete systems. It makes the optimization quite difficult if we directly add the constraints in gradient descent based algorithms, but the decoupled optimization in MSA allows us to do so. With regard to the constraint of parameter's spectral radius, a simple method is to apply special forms of matrices for parameters, e.g. anti-symmetric matrices. For continuous deep models, the only constraint is Theorem 1, i.e., Re(\\u03bb i (\\u03b8 t )) \\u2264 0. Anti-symmetric matrices have only imaginary eigenvalues, and hence we can replace \\u03b8 t with \\u03b8 t \\u2212 \\u03b8 (Goodfellow et al., 2015) 2.34% 77.45% 49.32% PGD-10 (Madry et al., 2017) 0.02% 46.67% 36.33% C&W (Carlini & Wagner, 2017a) Proof. Recall that \\u03c1(A) \\u2264 A 2 = \\u03bb max (A T A), we have Hence we can replace \\u03c1(\\u03b8 t ) \\u2264 1 with a positive semi-definite condition, and we turn the Hamiltonian maximization into a new optimization problem, where the target function is quadratic and the constraint is a semi-definite condition. This can be reduced to a semi-definite programming (SDP) problem (Vandenberghe & Boyd, 1998) , which is a special case of convex optimization, and thus can be solved efficiently by, e.g., interior point methods (Helmberg et al., 1970) in polynomial time. Here we summarize our method. For a given neural network, we use MSA to train the model, i.e., iteratively computing the states (forward propagation) and co-states (backward propagation), and solving the optimization for each layer. Instead of directly maximizing the Hamiltonian, we add a positive semi-definite constraint to the optimization problem, which leads to a stable control of the dynamics. To evaluate the effectiveness of our method, we conduct experiments on CIFAR10. We trained the network on clean data, with adversarial training (PGD-10) and with robust training (our method), respectively. We used FGSM (Goodfellow et al., 2015) , PGD-10 (Madry et al., 2017) and C&W (Carlini & Wagner, 2017a) to attack the network. Due to the limitation of TensorFlow, we used a simple interior point method with gradient descent to solve SDP. The network model was an 18-layer residual network (He et al., 2015) , with 8 residual blocks. We set the perturbation size as = 0.1 for both FGSM and PGD. For C&W, we used the L 0 metric. We trained the model for 150 epochs with a batch size of 200. The learning rate was set to be 10 \\u22122 initially, and was divided by 5 at epoch 30, 60 and 100. The regularizer term constant was set to be 10 \\u22123 . The results can be seen in Table 1 . The accuracy of robust models on clean data is lower than vanilla model's in that robust training and generalization is more difficult and requires more data (Schmidt et al., 2018) . Our method improves model's adversarial robustness, compared with the vanilla model. Figure 1 displays the eigenvalues of the last fully-connected layer's parameter. The complex norm of eigenvalues (spectral radius) of the model trained by our method are effectively bounded below 1, which satisfies the robust constraint on parameters in section 4.2, while eigenvalues of natural training are randomly distributed in the complex plane. Our method is not as effective as traditional adversarial training method. However, it mainly has the following advantages: (a) The training process doesn't require large numbers of gradient propagation, which consumes much time in adversarial training. In our experiment, adversarial training spends about 10 times GPU time as much as our method. (b) The decoupled training process allows us to set different hyperparameters and training methods for different layers, which is more maneuverable for large scale training. We can further control the behavior of different layers in adversarial settings. (c) Lyapunov stability provides a framework for analyzing adversarial robustness of deep models, which may lead to theoretical analysis of adversarial samples in future work. Motivated by the dynamical system view of neural networks, this work bridges adversarial robustness of deep neural models with Lyapunov stability of dynamical systems, and we also propose a method that uses a stable optimal control algorithm to train neural networks to improve the adversarial robustness of deep neural models. Though the result didn't surpass STOA defense methods, the stable control view of training neural nets points out another direction towards adversarially robust models. For future work, on the one hand, mathematical analysis on Lyapunov stability of neural models may be studied to provide theoretical understanding of adversarial robustness. On the other hand, popular platforms for deep learning, e.g., TensorFlow, PyTorch, didn't provide frameworks for optimal control. We will obtain better results if specific algorithms for SDP are applied to solve the optimization problem.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"SyxvSiCcFQ\",\n          \"BJxRrlBFwB\",\n          \"BklVA2NYvH\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training. Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network. This study proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks. Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).\",\n          \"Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This article studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This study explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.\",\n          \"An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability The authors formulate training NNs as finding an optimal controller for a discrete dynamical system, allowing them to use method of successive approximations to train a NN in a way to be more robust to adversarial attacks. This paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 202,\n        \"samples\": [\n          \"Frequency-based Search-control in Dyna\",\n          \"Neural Network Cost Landscapes as Quantum States\",\n          \"Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 40,\n        \"max\": 124,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          78,\n          80,\n          50\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"Although there is some work on how this might be done BID7 it is not known to even be possible to construct a qRAM in an efficient manner for a completely general dataset. Previous work has demonstrated and experimentally implemented the use of quantum hardware to perform binary classification, BID15 ) but this is not the same as the method proposed in this paper, as this paper is based on a different, more general gate-based form of quantum computation as opposed to the quantum annealing devices of the former. The biggest such device in existence today contains just 72 highly imperfect qubits, but it is worth noting that progress has advanced at a particularly rapid pace over the past few years and a number are available for public access on the cloud. Just as in classical computing, small sets of quantum gates are universal in that they can be combined to generate any other. Step 2: We then build a quantum circuit analogue of the neural network U QN N that takes in a given set of weights (encoded within qubits as above) and an empty register, and outputs onto the register the corresponding accuracy according to the chosen neural network i.e U QN N (w, 0) = (w, acc w ).Step 3: Since U QN N is a quantum circuit, inputting weights in superposition form allows them to be processed in parallel. This is a single quantum state representing the entire landscape of the neural network by correlating every possible set of weights with its resultant accuracy. If we restrict the problem to the special case of binary arguments only, the sign function 1 is reduced to finding whether there exist N/2 qubits out of N in state |1 . The reversibility of quantum circuits allows us to apply the QBNN for a given data point, store its output value onto its corresponding qubit on the register, perform the same QBNN in reverse order -its inverse -to refresh the other qubits, and continue for the next data point in the training set. Since both the labels and the outputs are binary, we can represent the accuracy of each of these predictions by performing a NOT gate on all the qubits corresponding to a data point with a label of 0. If the prediction qubits are all in the state |1 the training was a success and the appropriate weights can be simply read off their corresponding qubits. In practice, due to qubit number constraints, we choose to only learn the structure of the first layer of the BNN. It is particularly interesting to note that the learned structures of the two BNN solutions seem to match well with their problem definitions (equation 1 and equation 2). In fact, such an implementation is advantageous since it would allow us to use less qubits which in practical terms are limited in number in the near term. Both of these problems arise completely from our implementation of the sign function, which could either be improved or replaced entirely with a different binary activation function that could be implemented more efficiently on a quantum computer and would be compatible with non-binary input. Whilst we used the example of quantum training, the most fruitful approach in the short term is to ask whether some property of the state can be used to glean useful information for classical machine learning methods. Further work in investigating the relationship between the landscape as a quantum state and its features from a machine learning perspective would be a step forward in this direction.\",\n          \"In the literature on artificial dialogue agents, a distinction is often made between \\\"goal-oriented\\\" dialogue, where an agent is tasked with filling slots or otherwise obtaining or disseminating specified information from the user to help complete a task, and \\\"chit-chat\\\", where an agent should imitate human small talk. We compare agents that have been trained to imitate human actions given a goal (an \\\"inverse model\\\") to two different RL approaches: optimizing actions with latent discrete variables (topics), or via rewarding actions sampled from the model (via the top-K outputs). Characters (either played by humans or run by models) can speak to to each other via free text, send emote actions like applaud, nod or pout (22 emote types in total), and take actions to move to different locations and interact with objects (e.g. get cutlery, put cutlery in drawer, etc.), see Appendix A for a full list of game actions. To make the world and its textual descriptions, LIGHT consists of a large set of human-written game locations, characters, and objects, all based within a fantasy medieval setting. While players were not given specific goals, but instead asked to play the role convincingly of the character given, during play some of them effectively defined their own goals during the interactions, see Fig. 1 . Similar to Mazar\\u00e9 et al. (2018) , during training we consider the other elements of the batch as negatives. We consider an inverse model, trained to imitate human actions given a goal, as both a baseline for comparing to RL models, and for producing weights form which we can fine-tune. Optimizing all the parameters of a large transformer architecture by RL is both incredibly costly in data efficiency and computing time, and is also known to have the problem of language drift (Lee et al., 2019) -that is, there is no guarantee after training with self-chat that the models will output recognizable natural language utterances. We then run K-means over the vectorial representations of all observations from the training set to provide the mapping to one of C values, which represent dialogue topics, which we use as our initial function P (s). The cluster chooser P is redefined (from the initial K-means) to be an MLP network consisting of 2 layers. We use the attention above weights of v context against the candidates at the last layer of the transformer as the distribution over the candidates for sampling an utterance. In Urbanek et al. (2019) models were trained in a similar fashion to chit-chat task models, and we adopt similar architectures here, but instead adapt them to learn to pursue goals. Earlier work focused on labeled state representations, slot filling mechanisms and dialogue managers (Rieser & Lemon, 2011) , and more recent work has shifted to an end-to-end approach (Bordes et al., 2017) , in line with chit-chat models, but still the two sets of tasks are rarely considered together, or by using the same methods. Other notable uses of RL in dialogue include within visual question answering (Das et al., 2017) , in the domain of chit-chat where RL has been used to decrease repetitive and generic responses through the the use of self-play (Li et al., 2016b) , and through human-bot conversation (Sankar & Ravi, 2019) . However, those approaches use RL to optimize the set of actions given feedback in a single-player rather than multi-player game, so the text only refers to the environment, and there is no dialogue or actions from other agents. We achieve this by defining a task for an agent, where the goal is for the other player to execute a particular action.\",\n          \"This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. In this paper, we propose a dynamical system view on the adversarial robustness of the models, as well as new method that significantly defense adversarial attacks. On the one hand, deep residual networks (He et al., 2015) can be illustrated as forward Euler scheme approximating an ODE (E, 2017), which motivates us to design effective network structures (Lu et al., 2017) . On the other hand, regarding the network as a dynamical system allows us to set up an optimal control viewpoint of neural nets. If we regard a neural net as a dynamical system, and ensure the network is Lyapunov stable, then the model is robust to all (adversarial) perturbations. , where \\u03c3 is the activation function, e.g., Sigmoid function or ReLU function, it is stable if Re(\\u03bb i (A)) \\u2264 0, \\u2200i, where Re denotes the real part, and \\u03bb i denotes the i-th eigenvalue. In the field of deep learning, previous work has utilized MSA to train neural networks Li & Hao, 2018) . Concretely, after computing the states x and co-states p, the optimization step on layer t is only searching for parameters \\u03b8 t . Consider a layer in the form of f t (x) = \\u03b8 t x, where we leave the activation as an individual layer with no parameters for simplicity, we can derive the following optimization problem for Hamiltonian maximization: max It makes the optimization quite difficult if we directly add the constraints in gradient descent based algorithms, but the decoupled optimization in MSA allows us to do so. This can be reduced to a semi-definite programming (SDP) problem (Vandenberghe & Boyd, 1998) , which is a special case of convex optimization, and thus can be solved efficiently by, e.g., interior point methods (Helmberg et al., 1970) in polynomial time. For a given neural network, we use MSA to train the model, i.e., iteratively computing the states (forward propagation) and co-states (backward propagation), and solving the optimization for each layer. Due to the limitation of TensorFlow, we used a simple interior point method with gradient descent to solve SDP. The complex norm of eigenvalues (spectral radius) of the model trained by our method are effectively bounded below 1, which satisfies the robust constraint on parameters in section 4.2, while eigenvalues of natural training are randomly distributed in the complex plane. Motivated by the dynamical system view of neural networks, this paper bridges adversarial robustness of deep neural models with Lyapunov stability of dynamical systems, and we also propose a method that uses a stable optimal control algorithm to train neural networks to improve the adversarial robustness of deep neural models. Though the result didn't surpass STOA defense methods, the stable control view of training neural nets points out another direction towards adversarially robust models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_source\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1898,\n        \"min\": 131,\n        \"max\": 13830,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          7663,\n          5353,\n          3957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_extractive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 138,\n        \"min\": 131,\n        \"max\": 1069,\n        \"num_unique_values\": 167,\n        \"samples\": [\n          776,\n          609,\n          558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "56d28b38-6f9b-447e-e1cd-8723b3a2f507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(647, 8) (162, 8) (809, 8) (203, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'facebook/bart-base'\n",
        "max_input_length = 1024\n",
        "\n",
        "save_name = 'sampling-norep-v4/'\n",
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + save_name"
      ],
      "metadata": {
        "id": "LpOqNR1qyJGb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name, errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "1af0fa12-a887-42ec-ffc3-8c8eae3feb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "019a29f1b10145c8abc20308424e20ad",
            "87bd56f9f9db417c81d87b260e64717a",
            "e953775064244191a3245084abd7b949",
            "3e0d973080be48c3a61ca7bec49f30f0",
            "32ac86f5ff984aedbefd8393f1953c92",
            "255e5a72897049f9b2d16f5f717a7abc",
            "4edaa22c96884cf6a629cc6e8b277cbd",
            "1d41ba5b64e441caa852290dd525bf5e",
            "eb71b0e1b402499095482662879aeab5",
            "223fc245f51a45e39b4f896fab8b139a",
            "578becc7bb2146c587dca97aea75ad16",
            "1e7407be543141a5b2157b3026af7bfd",
            "fcaa9abbccee4df0bdee6114eff41c45",
            "61672ed9c6d74592a1a8b1c6a0b5fe9c",
            "a7feab89338642ec8b2cd300bf1494bd",
            "d6a8ec6ba5ad4b2cb30f24a0daa5fc94",
            "22130b81744c410093e46123bc3d58df",
            "90a902c031ac467d9b1dd1096f161a83",
            "03c6a89889cb44d98b1a9ce788088a10",
            "505a0197233e4c5aa7c6e6e98e11e245",
            "2ef4b357bbd64bdaa10197bae69d7bac",
            "844f27566bfd47b187c53cf95f6892d2",
            "6394854b894b4ed08ea6a7d7aebda655",
            "b9384ed494d04b6986b1288ef98535f3",
            "5cafb00bc5174d908ebb62d3a89b4d6a",
            "9ff9f762ab8e4d799896bd7a076dcc62",
            "c56297bd321b4449bb3417bb14056f99",
            "2b3b6d5112d849808328064301c54043",
            "b25f5c2839a441b89d956388bae8671b",
            "c8d0f972937443df86c046c5fac48036",
            "dea14f2b97b8496dbcfd30a1bde8ba1a",
            "54caed64fa214117966f4e064f8f9f50",
            "e50da815fbe040e5aa64c04aac264700"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "019a29f1b10145c8abc20308424e20ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e7407be543141a5b2157b3026af7bfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6394854b894b4ed08ea6a7d7aebda655"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BART generation config parameters\n",
        "forbidden_begin_tokens = [tokenizer.convert_tokens_to_ids('We')]\n",
        "\n",
        "forbidden_tokens = [\n",
        "     # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "     tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "     # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "     tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "     #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "     #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "\n",
        "BART_generation_parameters = {\n",
        "    'max_length' : 150,\n",
        "    'min_length' : 60,\n",
        "    'length_penalty' : 2.0,\n",
        "    'do_sample' : True,\n",
        "    'num_beams' : 4,\n",
        "    'temperature' : 0.5,\n",
        "    'begin_suppress_tokens' : forbidden_begin_tokens,\n",
        "    'suppress_tokens' : forbidden_tokens,\n",
        "    'repetition_penalty' : 1.8,\n",
        "    'no_repeat_ngram_size' : 3\n",
        "}\n",
        "\n",
        "\n",
        "# Training hyper-parameters\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "1kcRzvxBzes6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                                **BART_generation_parameters)\n",
        "\n",
        "#model.generation_config = BART_generation_parameters\n",
        "\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "ca6c02c3-72d7-492a-915a-8679e253d14a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "7770d2a7-9369-417f-cd64-f736d369caf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset,\n",
        "              validation_data=validation_dataset,\n",
        "              epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "165c4aba-fc55-49c2-d3e4-54a41d7d6583"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7d8e5695d6c0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7d8e5695d6c0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - 4104s 50s/step - loss: 3.8688 - val_loss: 3.3719 - rouge1: 39.2353 - rouge2: 10.1921 - rougeL: 22.2063 - rougeLsum: 32.0559 - gen_len: 86.5864\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3653s 45s/step - loss: 3.5389 - val_loss: 3.3010 - rouge1: 39.3726 - rouge2: 10.5648 - rougeL: 22.4130 - rougeLsum: 32.1531 - gen_len: 84.8519\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3281s 41s/step - loss: 3.3532 - val_loss: 3.2627 - rouge1: 39.5196 - rouge2: 10.8404 - rougeL: 22.6945 - rougeLsum: 32.3229 - gen_len: 83.2654\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3475s 43s/step - loss: 3.2109 - val_loss: 3.2532 - rouge1: 39.6113 - rouge2: 10.6516 - rougeL: 22.5902 - rougeLsum: 32.1820 - gen_len: 82.9136\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3333s 41s/step - loss: 3.0813 - val_loss: 3.2428 - rouge1: 40.2920 - rouge2: 10.7273 - rougeL: 22.6018 - rougeLsum: 32.7908 - gen_len: 82.3951\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3316s 41s/step - loss: 2.9701 - val_loss: 3.2385 - rouge1: 40.3328 - rouge2: 10.6940 - rougeL: 22.6790 - rougeLsum: 32.5761 - gen_len: 80.4938\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3400s 42s/step - loss: 2.8622 - val_loss: 3.2454 - rouge1: 39.8963 - rouge2: 10.7066 - rougeL: 22.3823 - rougeLsum: 32.2690 - gen_len: 82.3951\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3634s 45s/step - loss: 2.7686 - val_loss: 3.2587 - rouge1: 40.2235 - rouge2: 10.9267 - rougeL: 22.7431 - rougeLsum: 33.0138 - gen_len: 83.3951\n",
            "Epoch 9/10\n",
            "81/81 [==============================] - 3248s 40s/step - loss: 2.6739 - val_loss: 3.2882 - rouge1: 40.2523 - rouge2: 10.8509 - rougeL: 22.5084 - rougeLsum: 33.1142 - gen_len: 82.6543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "5f64ae63-3c6e-48f0-f4fe-f6bb4f01050b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c9dfef50-7ba3-4a19-f217-4b58a5c6a606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "ff899fd2-1fdb-4024-8ad2-2a7068688700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5"
      ],
      "metadata": {
        "id": "k9aB2nco5LxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google-t5/t5-small'\n",
        "max_input_length = 1024\n",
        "\n",
        "save_name = 'greedy-norep-v4/'\n",
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + save_name"
      ],
      "metadata": {
        "id": "Lfg9uUqr5ZSz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 512\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "23e860aa8bef471da405bb613c4675c3",
            "44365544de7e41fdb723bd4c9f181d8f",
            "334c86305d1543d4b851234572685c15",
            "97b8c8b1ee7547e6b92d3e3327e0abc7",
            "753f9e27f1fa4b948a4a3daabe397d83",
            "a04e2d344a7d437b98b360bde05cdc51",
            "c7f257ec5dc94a798b3a579747e596d5",
            "9d71423b9a9c48878350332599ff9b77",
            "876a5d9b630949bcb3def727d42df317",
            "131d7428db354f3ea70e83dee37beeca",
            "22c9304ad2944e37bf52f531419c67e8",
            "e88d085e848445ae9583c0769aa80b2e",
            "b4df750e01ff42ae9387fbb879b258cf",
            "d0276bc9ba9943d399f1a55e52e02510",
            "bc0bc080b16a4e19944d69538ba74bef",
            "4460338e16504d11adf3e5c0ff5899e5",
            "3ae9e8a550e84f66b78c09c42af04ec0",
            "e81fff9bdd4e48368aa021467ef05f81",
            "a9ed7aabc1bb49b5b7b0468330d3b04e",
            "0631c54c9c8349e59054664f982b635d",
            "1a4a0d39fd8040849871edddcf4e0880",
            "3c264a985d314c66a947a9f1e6a67945",
            "ea73688e2a63417899aa6a0709aabb1c",
            "25e5fdd540234be68b9c0a37887884cf",
            "80d10cbf53a0460f8780d7bd1a327d28",
            "02f79a8dbfb84580b242b6c2dd824c5f",
            "08a240a248c34fd988247c80fc2e9fb8",
            "23c5d8e187264cff8596fdffe52219c3",
            "20c531d7a2b74f2ba9ced21121ad90fc",
            "7456341ce03e491e9cf5017d8116233e",
            "d4f2160c1a8b4f9990ef1928f62afb62",
            "45bf39480e624482bef69498a446c702",
            "15093bdc7f394897b4d1eb379bc5b441"
          ]
        },
        "id": "gjHkaqLO5LXt",
        "outputId": "289cd9da-b06e-43bc-eed7-41ec8853fc2d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23e860aa8bef471da405bb613c4675c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e88d085e848445ae9583c0769aa80b2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea73688e2a63417899aa6a0709aabb1c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# T5 generation config parameters\n",
        "forbidden_begin_tokens = [tokenizer.convert_tokens_to_ids('We')]\n",
        "\n",
        "forbidden_tokens = [\n",
        "     # tokenizer.convert_tokens_to_ids('ƒ†paper'),\n",
        "     tokenizer.convert_tokens_to_ids('ƒ†Pro'),\n",
        "     # tokenizer.convert_tokens_to_ids('ƒ†authors'),\n",
        "     tokenizer.convert_tokens_to_ids('ƒ†Introdu')]\n",
        "     #  tokenizer.convert_tokens_to_ids('ƒ†work'),\n",
        "     #  tokenizer.convert_tokens_to_ids('ƒ†method')]\n",
        "\n",
        "forbidden_words = ['We', 'we', 'propose', 'Proposes']\n",
        "\n",
        "T5_generation_parameters = {\n",
        "    'max_length' : 100,\n",
        "    'min_length' : 60,\n",
        "    'length_penalty' : 2.0,\n",
        "    'num_beams' : 4,\n",
        "    'do_sample' : False,\n",
        "    'temperature' : 0.5,\n",
        "    'bad_words_ids' : tokenizer(forbidden_words,\n",
        "                                add_special_tokens=False).input_ids,\n",
        "    'repetition_penalty' : 1.8,\n",
        "    'no_repeat_ngram_size' : 3\n",
        "}\n",
        "\n",
        "\n",
        "# Training hyper-parameters\n",
        "epochs = 12\n",
        "batch_size = 8\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "L20kOSF15lS9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ0geKLy5kaT",
        "outputId": "0f1637ea-affe-4d10-d3ae-0137016d668d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"diversity_penalty\": 0.5,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True, pad_to_multiple_of=128)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True, pad_to_multiple_of=128)"
      ],
      "metadata": {
        "id": "m1ygHfFP--H6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "d8G2KNJn_A5x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHtNgWTF_B29",
        "outputId": "0ca3b7a8-6a1d-4ba5-d1fe-378fe69b0ca2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tft5_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " shared (Embedding)          multiple                  16449536  \n",
            "                                                                 \n",
            " encoder (TFT5MainLayer)     multiple                  35330816  \n",
            "                                                                 \n",
            " decoder (TFT5MainLayer)     multiple                  41625344  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 60506624 (230.81 MB)\n",
            "Trainable params: 60506624 (230.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "5ypB-dyF_JNn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "IyaSZ1H4_Ly1",
        "outputId": "5822cb2f-bf0c-4eb2-acce-40c0c6d795ad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x790372e58310> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x790372e58310> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - ETA: 0s - loss: 2.2949"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:458: UserWarning: `num_beams` is set to 1. However, `num_beam_groups` is set to `4` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `num_beam_groups`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:465: UserWarning: `num_beams` is set to 1. However, `diversity_penalty` is set to `0.5` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `diversity_penalty`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r81/81 [==============================] - 482s 5s/step - loss: 2.2949 - val_loss: 1.9815 - rouge1: 0.0000e+00 - rouge2: 0.0000e+00 - rougeL: 0.0000e+00 - rougeLsum: 0.0000e+00 - gen_len: 0.0000e+00\n",
            "Epoch 2/12\n",
            "42/81 [==============>...............] - ETA: 3:40 - loss: 1.7548"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-9b05ecf37ccf>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[INFO: fine-tuning model...]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Save the model and tokenizer to a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_batch_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1802\u001b[0m                         ):\n\u001b[1;32m   1803\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "id": "cArEDqsN_SE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              'greedy-norep-v4' : 'greedy-norep-v4/'}\n",
        "              #  'sampling-norep-v3' : 'sampling-norep-v3',\n",
        "              #  'sampling-norep-v4' : 'sampling-norep-v4'}\n",
        "\n",
        "model = 'T5/'\n",
        "\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/' + model + 'model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "id": "t9Tjj2cjL2mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'greedy-norep-v4'"
      ],
      "metadata": {
        "id": "epSxP5oRBEF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:1],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HhUafQ-DSow",
        "outputId": "a3679476-0cd9-47dd-f85b-eb40f843ef8a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet algorithm for deep neural network classification problem and a smoothing method to alleviate the over-concentration issue. This paper introduces a variational dirichlet framework for deep image classification problems, which can greatly widen the distance between in-and out-of-distribution datasets.']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'greedy-norep-v4'\n",
        "\n",
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "id": "EisXx8nuVDQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "fbbf3116-76d5-4cc2-8e55-fe5d522d04ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       3.683168\n",
              "std       19.394276\n",
              "min      -42.000000\n",
              "25%      -10.000000\n",
              "50%        4.000000\n",
              "75%       16.000000\n",
              "max       59.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['abstractive_summary'][130]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "gCZBrdBLBjv4",
        "outputId": "f3d32a4d-b069-40f2-af7c-7a88b59b8ef9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A method to generate out-distribution samples from over-generalized regions, reducing the risk of misclassifying both adversaries and samples using augmented CNNs. This paper introduces a method for learning out-department samples in a \"dustbin\" sub-manifold, which can be used to train out-destribution examples.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dw4jYBYYWZCh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOnyidQCphAZ4CP7vrkX63Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c23c62724134ade82fe013c6484da03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca9091b173674a11b509e568b4b3d132",
              "IPY_MODEL_9f0238f70c174f79bc00581e9a7d8c5f",
              "IPY_MODEL_f3d75633840c45be84b00fb9a8a9648f"
            ],
            "layout": "IPY_MODEL_7eab4d7b2def43e9857e757f4f37b101"
          }
        },
        "ca9091b173674a11b509e568b4b3d132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c2e80b91f3c4eceafbddd8999520720",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fcdfcaf0e592434181ed5b671891a82e",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "9f0238f70c174f79bc00581e9a7d8c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09eff1aeffeb41f1bcaea0b370c8afd9",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9553d2575f8544069c68e5fe2ac878d9",
            "value": 2169
          }
        },
        "f3d75633840c45be84b00fb9a8a9648f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49783bb6665941d6b38faf0ee01c9ca2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4cd66c6c840642b08397fce4698dcdf2",
            "value": "‚Äá5.65k/?‚Äá[00:00&lt;00:00,‚Äá417kB/s]"
          }
        },
        "7eab4d7b2def43e9857e757f4f37b101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c2e80b91f3c4eceafbddd8999520720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcdfcaf0e592434181ed5b671891a82e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09eff1aeffeb41f1bcaea0b370c8afd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9553d2575f8544069c68e5fe2ac878d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49783bb6665941d6b38faf0ee01c9ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cd66c6c840642b08397fce4698dcdf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "019a29f1b10145c8abc20308424e20ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87bd56f9f9db417c81d87b260e64717a",
              "IPY_MODEL_e953775064244191a3245084abd7b949",
              "IPY_MODEL_3e0d973080be48c3a61ca7bec49f30f0"
            ],
            "layout": "IPY_MODEL_32ac86f5ff984aedbefd8393f1953c92"
          }
        },
        "87bd56f9f9db417c81d87b260e64717a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255e5a72897049f9b2d16f5f717a7abc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4edaa22c96884cf6a629cc6e8b277cbd",
            "value": "Map:‚Äá100%"
          }
        },
        "e953775064244191a3245084abd7b949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d41ba5b64e441caa852290dd525bf5e",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb71b0e1b402499095482662879aeab5",
            "value": 647
          }
        },
        "3e0d973080be48c3a61ca7bec49f30f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223fc245f51a45e39b4f896fab8b139a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_578becc7bb2146c587dca97aea75ad16",
            "value": "‚Äá647/647‚Äá[00:03&lt;00:00,‚Äá163.54‚Äáexamples/s]"
          }
        },
        "32ac86f5ff984aedbefd8393f1953c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "255e5a72897049f9b2d16f5f717a7abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edaa22c96884cf6a629cc6e8b277cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d41ba5b64e441caa852290dd525bf5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb71b0e1b402499095482662879aeab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "223fc245f51a45e39b4f896fab8b139a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578becc7bb2146c587dca97aea75ad16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7407be543141a5b2157b3026af7bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcaa9abbccee4df0bdee6114eff41c45",
              "IPY_MODEL_61672ed9c6d74592a1a8b1c6a0b5fe9c",
              "IPY_MODEL_a7feab89338642ec8b2cd300bf1494bd"
            ],
            "layout": "IPY_MODEL_d6a8ec6ba5ad4b2cb30f24a0daa5fc94"
          }
        },
        "fcaa9abbccee4df0bdee6114eff41c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22130b81744c410093e46123bc3d58df",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_90a902c031ac467d9b1dd1096f161a83",
            "value": "Map:‚Äá100%"
          }
        },
        "61672ed9c6d74592a1a8b1c6a0b5fe9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c6a89889cb44d98b1a9ce788088a10",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_505a0197233e4c5aa7c6e6e98e11e245",
            "value": 162
          }
        },
        "a7feab89338642ec8b2cd300bf1494bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ef4b357bbd64bdaa10197bae69d7bac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_844f27566bfd47b187c53cf95f6892d2",
            "value": "‚Äá162/162‚Äá[00:01&lt;00:00,‚Äá141.59‚Äáexamples/s]"
          }
        },
        "d6a8ec6ba5ad4b2cb30f24a0daa5fc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22130b81744c410093e46123bc3d58df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a902c031ac467d9b1dd1096f161a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03c6a89889cb44d98b1a9ce788088a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505a0197233e4c5aa7c6e6e98e11e245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ef4b357bbd64bdaa10197bae69d7bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844f27566bfd47b187c53cf95f6892d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6394854b894b4ed08ea6a7d7aebda655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9384ed494d04b6986b1288ef98535f3",
              "IPY_MODEL_5cafb00bc5174d908ebb62d3a89b4d6a",
              "IPY_MODEL_9ff9f762ab8e4d799896bd7a076dcc62"
            ],
            "layout": "IPY_MODEL_c56297bd321b4449bb3417bb14056f99"
          }
        },
        "b9384ed494d04b6986b1288ef98535f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3b6d5112d849808328064301c54043",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b25f5c2839a441b89d956388bae8671b",
            "value": "Map:‚Äá100%"
          }
        },
        "5cafb00bc5174d908ebb62d3a89b4d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d0f972937443df86c046c5fac48036",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dea14f2b97b8496dbcfd30a1bde8ba1a",
            "value": 203
          }
        },
        "9ff9f762ab8e4d799896bd7a076dcc62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54caed64fa214117966f4e064f8f9f50",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e50da815fbe040e5aa64c04aac264700",
            "value": "‚Äá203/203‚Äá[00:01&lt;00:00,‚Äá143.46‚Äáexamples/s]"
          }
        },
        "c56297bd321b4449bb3417bb14056f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3b6d5112d849808328064301c54043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b25f5c2839a441b89d956388bae8671b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8d0f972937443df86c046c5fac48036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea14f2b97b8496dbcfd30a1bde8ba1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54caed64fa214117966f4e064f8f9f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50da815fbe040e5aa64c04aac264700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23e860aa8bef471da405bb613c4675c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44365544de7e41fdb723bd4c9f181d8f",
              "IPY_MODEL_334c86305d1543d4b851234572685c15",
              "IPY_MODEL_97b8c8b1ee7547e6b92d3e3327e0abc7"
            ],
            "layout": "IPY_MODEL_753f9e27f1fa4b948a4a3daabe397d83"
          }
        },
        "44365544de7e41fdb723bd4c9f181d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04e2d344a7d437b98b360bde05cdc51",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c7f257ec5dc94a798b3a579747e596d5",
            "value": "Map:‚Äá100%"
          }
        },
        "334c86305d1543d4b851234572685c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d71423b9a9c48878350332599ff9b77",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_876a5d9b630949bcb3def727d42df317",
            "value": 647
          }
        },
        "97b8c8b1ee7547e6b92d3e3327e0abc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131d7428db354f3ea70e83dee37beeca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22c9304ad2944e37bf52f531419c67e8",
            "value": "‚Äá647/647‚Äá[00:02&lt;00:00,‚Äá226.20‚Äáexamples/s]"
          }
        },
        "753f9e27f1fa4b948a4a3daabe397d83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a04e2d344a7d437b98b360bde05cdc51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f257ec5dc94a798b3a579747e596d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d71423b9a9c48878350332599ff9b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "876a5d9b630949bcb3def727d42df317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "131d7428db354f3ea70e83dee37beeca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22c9304ad2944e37bf52f531419c67e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e88d085e848445ae9583c0769aa80b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4df750e01ff42ae9387fbb879b258cf",
              "IPY_MODEL_d0276bc9ba9943d399f1a55e52e02510",
              "IPY_MODEL_bc0bc080b16a4e19944d69538ba74bef"
            ],
            "layout": "IPY_MODEL_4460338e16504d11adf3e5c0ff5899e5"
          }
        },
        "b4df750e01ff42ae9387fbb879b258cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ae9e8a550e84f66b78c09c42af04ec0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e81fff9bdd4e48368aa021467ef05f81",
            "value": "Map:‚Äá100%"
          }
        },
        "d0276bc9ba9943d399f1a55e52e02510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9ed7aabc1bb49b5b7b0468330d3b04e",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0631c54c9c8349e59054664f982b635d",
            "value": 162
          }
        },
        "bc0bc080b16a4e19944d69538ba74bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a4a0d39fd8040849871edddcf4e0880",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c264a985d314c66a947a9f1e6a67945",
            "value": "‚Äá162/162‚Äá[00:00&lt;00:00,‚Äá227.62‚Äáexamples/s]"
          }
        },
        "4460338e16504d11adf3e5c0ff5899e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ae9e8a550e84f66b78c09c42af04ec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e81fff9bdd4e48368aa021467ef05f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9ed7aabc1bb49b5b7b0468330d3b04e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0631c54c9c8349e59054664f982b635d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a4a0d39fd8040849871edddcf4e0880": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c264a985d314c66a947a9f1e6a67945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea73688e2a63417899aa6a0709aabb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25e5fdd540234be68b9c0a37887884cf",
              "IPY_MODEL_80d10cbf53a0460f8780d7bd1a327d28",
              "IPY_MODEL_02f79a8dbfb84580b242b6c2dd824c5f"
            ],
            "layout": "IPY_MODEL_08a240a248c34fd988247c80fc2e9fb8"
          }
        },
        "25e5fdd540234be68b9c0a37887884cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23c5d8e187264cff8596fdffe52219c3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20c531d7a2b74f2ba9ced21121ad90fc",
            "value": "Map:‚Äá100%"
          }
        },
        "80d10cbf53a0460f8780d7bd1a327d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7456341ce03e491e9cf5017d8116233e",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4f2160c1a8b4f9990ef1928f62afb62",
            "value": 203
          }
        },
        "02f79a8dbfb84580b242b6c2dd824c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45bf39480e624482bef69498a446c702",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_15093bdc7f394897b4d1eb379bc5b441",
            "value": "‚Äá203/203‚Äá[00:00&lt;00:00,‚Äá227.24‚Äáexamples/s]"
          }
        },
        "08a240a248c34fd988247c80fc2e9fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c5d8e187264cff8596fdffe52219c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c531d7a2b74f2ba9ced21121ad90fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7456341ce03e491e9cf5017d8116233e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4f2160c1a8b4f9990ef1928f62afb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45bf39480e624482bef69498a446c702": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15093bdc7f394897b4d1eb379bc5b441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}