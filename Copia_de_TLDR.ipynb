{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iguerrasevillano/TFM/blob/clean_version/Copia_de_TLDR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "FEZEiEhlnp8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install rouge_score\n",
        "!pip install huggingface_hub\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCyyzSCaF2VU",
        "outputId": "4efe70fa-a7d1-42f2-cd5f-5a7cb38e32db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.2)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=cd3ab535e94ff7a282d209a6c9cbb57176f262f9712c27ac767a51e261c188a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.6)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.62.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "Successfully installed google-auth-oauthlib-1.2.0 keras-2.15.0 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "nU-ExPlO1I2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234,
          "referenced_widgets": [
            "b3c91162751547aa84af7ee52ec3e3e4",
            "97838b1ce94544319c448dde22e9c42c",
            "301e73c3cbae4b5fbdbe4067125f4424",
            "7cb519780a024557b6956ca7c5d3f6a3",
            "b12cac50346b4f1785966aadbeb1a32c",
            "a85e44ec55aa4c83858392c5aa3c3415",
            "1aaff1b282494056a14e90402cbe5cd1",
            "27f42a728da3483b82971a6e22a8ac9d",
            "1c295f8fcb1648e3adac146d1a21c92d",
            "730e4ff0b7f54adcbb28c72c1e5c80dc",
            "13c5422277a740f298db19ce9cde93b6"
          ]
        },
        "id": "x7-Ua4kJJ-I_",
        "outputId": "a4a8e7f8-adf8-4bcf-dad1-674aee8f077d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<ipython-input-3-d54cd7d9c1fe>:37: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric('rouge', seed=42) #It is not deterministic\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3c91162751547aa84af7ee52ec3e3e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "# Transformers\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import BartTokenizer, T5Tokenizer, pipeline, TFT5ForConditionalGeneration\n",
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import PushToHubCallback\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# AST\n",
        "import ast\n",
        "\n",
        "# Metrics\n",
        "metric = load_metric('rouge', seed=42) #It is not deterministic\n",
        "\n",
        "# Current directory\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logins, Paths and Auxiliar Functions"
      ],
      "metadata": {
        "id": "PJRJURyg1Spt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlyJWKdLJ_JN",
        "outputId": "3f6a2d5c-d8c8-4698-d03d-097d4c906d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Connect w/ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "# PATHS\n",
        "BASE_PATH = \"/content/drive/MyDrive/VIU/TFM/Desarrollo/\"\n",
        "\n",
        "documents = os.listdir(BASE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rOo2flUzaIGG"
      },
      "outputs": [],
      "source": [
        "# AUXILIAR FUNCTIONS\n",
        "\n",
        "# Function to convert strings to lists\n",
        "def convert_to_list(cell):\n",
        "    try:\n",
        "        return ast.literal_eval(cell)\n",
        "    except (SyntaxError, ValueError):\n",
        "        return cell\n",
        "\n",
        "\n",
        "\n",
        "# Function to join all the sentences of input document\n",
        "def clean_data(data, column):\n",
        "  data[column] = data[column].apply(lambda x : ' '.join(x))\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the metric to use (ROUGE Scores)\n",
        "def metric_fn(eval_predictions):\n",
        "  predictions, labels = eval_predictions\n",
        "\n",
        "  for prediction in predictions:\n",
        "      prediction[prediction < 0] = tokenizer.pad_token_id  # Replace masked prediction tokens\n",
        "\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Rouge expects a newline after each sentence\n",
        "  decoded_predictions = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions\n",
        "  ]\n",
        "  decoded_labels = [\n",
        "      \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
        "  ]\n",
        "  result = metric.compute(\n",
        "      predictions=decoded_predictions, references=decoded_labels, use_stemmer=True\n",
        "  )\n",
        "\n",
        "  # Extract a few results\n",
        "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "  # Add mean generated length\n",
        "  prediction_lens = [\n",
        "      np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
        "  ]\n",
        "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "  def plot_graphics(H):\n",
        "\n",
        "    # Create a figure with 1 row and 2 columns, and set the figure size\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,5))\n",
        "\n",
        "    # Plot the training and validation loss for each epoch in the first subplot\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"]) ), H.history[\"loss\"], 'r', label=\"loss\")\n",
        "    ax[0].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"val_loss\"], 'c', label=\"val_loss\")\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    ax[0].set_ylabel('loss')\n",
        "    ax[0].legend(loc=\"upper right\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge1\"], label=\"rouge1\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rouge2\"], label=\"rouge2\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeL\"], label=\"rougeL\")\n",
        "    ax[1].plot(np.arange(0,len(H.history[\"loss\"])), H.history[\"rougeLsum\"], label=\"rougeLsum\")\n",
        "    ax[1].set_xlabel('epoch')\n",
        "    ax[1].set_ylabel('score')\n",
        "    ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "u2-4mQ872R2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(BASE_PATH+'Data/Dev/Results/Extractive/extractive_summaries.csv')\n",
        "\n",
        "data['source'] = data['source'].apply(convert_to_list)\n",
        "data = clean_data(data, 'source')\n",
        "\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "1I5H39lnQW9o",
        "outputId": "acd150b6-97ec-42a2-b41b-4b36f48aaedd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                source    paper_id  \\\n",
              "275  Gradient-based meta-learning techniques are bo...  BJgklhAcK7   \n",
              "248  Supervised machine learning models for high-va...  r1gPtjcH_N   \n",
              "428  We propose a study of the stability of several...  B1g-SnUaUN   \n",
              "350  One of the big challenges in machine learning ...   HkgNdt26Z   \n",
              "673  Using variational Bayes neural networks, we de...  S1eEmn05tQ   \n",
              "\n",
              "                                                target  \\\n",
              "275  Latent Embedding Optimization (LEO) is a novel...   \n",
              "248  We explore using passively collected eye-track...   \n",
              "428  We propose a study of the stability of several...   \n",
              "350  We propose a method of distributed fine-tuning...   \n",
              "673  A scalable method for learning an expressive p...   \n",
              "\n",
              "                                                 title  number_words_target  \\\n",
              "275   Meta-Learning with Latent Embedding Optimization                   83   \n",
              "248  Improving Sample Complexity with Observational...                   85   \n",
              "428  Reproducibility and Stability Analysis in Metr...                   33   \n",
              "350  Distributed Fine-tuning of Language Models on ...                   52   \n",
              "673         Uncertainty in Multitask Transfer Learning                   70   \n",
              "\n",
              "                                    extractive_summary  \n",
              "275  Instead of explicitly instantiating and mainta...  \n",
              "248  Supervised machine learning models for high-va...  \n",
              "428  We propose a study of the stability of several...  \n",
              "350  Two common problems arising after deployment o...  \n",
              "673  More recent tools such as deep Gaussian proces...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-387ab372-563d-4cf6-a59c-b05ecd03045a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>Gradient-based meta-learning techniques are bo...</td>\n",
              "      <td>BJgklhAcK7</td>\n",
              "      <td>Latent Embedding Optimization (LEO) is a novel...</td>\n",
              "      <td>Meta-Learning with Latent Embedding Optimization</td>\n",
              "      <td>83</td>\n",
              "      <td>Instead of explicitly instantiating and mainta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>Supervised machine learning models for high-va...</td>\n",
              "      <td>r1gPtjcH_N</td>\n",
              "      <td>We explore using passively collected eye-track...</td>\n",
              "      <td>Improving Sample Complexity with Observational...</td>\n",
              "      <td>85</td>\n",
              "      <td>Supervised machine learning models for high-va...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>We propose a study of the stability of several...</td>\n",
              "      <td>B1g-SnUaUN</td>\n",
              "      <td>We propose a study of the stability of several...</td>\n",
              "      <td>Reproducibility and Stability Analysis in Metr...</td>\n",
              "      <td>33</td>\n",
              "      <td>We propose a study of the stability of several...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>One of the big challenges in machine learning ...</td>\n",
              "      <td>HkgNdt26Z</td>\n",
              "      <td>We propose a method of distributed fine-tuning...</td>\n",
              "      <td>Distributed Fine-tuning of Language Models on ...</td>\n",
              "      <td>52</td>\n",
              "      <td>Two common problems arising after deployment o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>Using variational Bayes neural networks, we de...</td>\n",
              "      <td>S1eEmn05tQ</td>\n",
              "      <td>A scalable method for learning an expressive p...</td>\n",
              "      <td>Uncertainty in Multitask Transfer Learning</td>\n",
              "      <td>70</td>\n",
              "      <td>More recent tools such as deep Gaussian proces...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-387ab372-563d-4cf6-a59c-b05ecd03045a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-387ab372-563d-4cf6-a59c-b05ecd03045a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-387ab372-563d-4cf6-a59c-b05ecd03045a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f5572163-9b79-45b8-b0bc-e26a618b3b5e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f5572163-9b79-45b8-b0bc-e26a618b3b5e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f5572163-9b79-45b8-b0bc-e26a618b3b5e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Supervised machine learning models for high-value computer vision applications such as medical image classification often require large datasets labeled by domain experts, which are slow to collect, expensive to maintain, and static with respect to changes in the data distribution. In this context, we assess the utility of observational supervision, where we take advantage of passively-collected signals such as eye tracking or \\u201cgaze\\u201d data, to reduce the amount of hand-labeled data needed for model training. Specifically, we leverage gaze information to directly supervise a visual attention layer by penalizing disagreement between the spatial regions the human labeler looked at the longest and those that most heavily influence model output. We present evidence that constraining the model in this way can reduce the number of labeled examples required to achieve a given performance level by as much as 50%, and that gaze information is most helpful on more difficult tasks. Medical imaging is a compelling application area for supervised machine learning methods. Convolutional Neural Networks (CNNs) in particular have recently achieved promising results on applications ranging from cancer diagnosis BID5 to radiograph worklist prioritization BID4 ; however, these results rely on massive hand-labeled datasets. This requirement for large hand-labeled datasets -which are expensive, because they require physician time to create -has hampered efforts to deploy these models to improve clinical outcomes. To reduce this labeling cost, we explore rich observational signals that can be passively collected at annotation time, such as eye tracking (or \\\"gaze\\\") data, which describes where a person has looked while performing a task BID2 BID14 . This approach is possible because of recent advances in eye tracking technology, which has quickly transformed from a technique that was intrusive, inaccurate, and expensive into one that is viable for real-time gaze data collection BID6 . Inspired by the success of eye tracking techniques in NLP applications that only use gaze signal at train time BID7 , we examine a straightforward mapping of gaze data to visual attention layer activations in a way that encourages the model to draw influence from the same spatial regions most heavily utilized by the human annotator. While noisy observational signals are challenging to extract useful information from, we show that incorporating them alongside traditional declarative labels can reduce the amount of hand-labeled data required to achieve a given level of performance. We first apply our proposed technique to a simple image classification task, where we show that we can maintain model performance using as few as 50% of the training images when gaze information is incorporated at training time. We then examine how the difficulty of the task impacts this labeled data reduction, and show that observational signals appear to be more helpful for difficult tasks. While we demonstrate our approach on a non-medical dataset for which gaze data was available, similar reductions in required labeled data, particularly for difficult tasks, could improve the feasibility of training useful models for medical imaging tasks. The work presented here draws inspiration from recent research on weak supervision, the use of gaze data in machine learning systems, and attention-based models. Weak supervision has become an increasingly popular method for training machine learning models with limited hand-labeled data. Indeed, methods such as distant supervision BID0 , data programming BID13 , and crowdsourcing BID17 have shown substantial success in decreasing the amount of labeling time required to train high-performance machine learning models. Nonetheless, these approaches require supervision signals that are fundamentally declarative -that is, they do not leverage the rich information contained in user behavior that can be captured by passive sensing modalities such as eye tracking and click stream monitoring. Of particular relevance for this work are those studies that have examined using gaze data directly in the context of training computer vision models. BID9 collect eye tracking data on several different datasets and demonstrate that features derived from this data can support classdiscriminative representations that improve zero-shot fine-grained image classification; notably, this approach requires gaze data at test time. BID15 integrate gaze data directly into the optimization procedure for a modified Latent Support Vector Machine (LSVM) and demonstrate competitive performance on several image classification and localization tasks in a manner that is gaze-free at test-time. While conceptually similar, the modeling and optimization approach pursued by BID15 is substantially different than that explored here. Integration of visual attention techniques into computer vision algorithms has become a direction of significant research interest. An attention mechanism represents a learned weighting applied to different subsamples of a given input (e.g. image patches, sequence elements, etc.). Recent efforts to directly incorporate learned visual attention layers into the CNN training process have continually led to state-of-the-art performance on tasks including image classification BID14 , object recognition BID1 , visual question answering BID16 BID8 , and medical image segmentation BID10 . In our work, we leverage the same concept of visual attention that has enabled these advances as a fundamental primitive for gaze-based supervision. Figure 2: (a) Mean ROC-AUC across 10 classes for the POET image classification task versus number of training images (shaded regions are standard deviation). We observe superior performance at all training set sizes when integrating gaze data. (b) Per-class performance for models trained on 2000 examples versus ROC-AUC improvement when using gaze data. The strong negative correlation indicates that gaze data is more helpful as the classification becomes more difficult. While much initial work on the use of eye tracking signal has considered using the gaze signal as another set of features for the model, we are motivated by the prospect of using it to train standard neural network architectures that do not need gaze features at test time, and are thus practically deployable in settings where eye trackers are unavailable. To this end, we integrate the eye tracking signal into the training loss function by balancing the classification loss with a second loss term describing the deviation between model and human visual attention as shown in FIG0 , DISPLAYFORM0 with L CE the cross-entropy loss, L V A the visual attention loss, R 2 the regularization term, p i,j the probability of observation i out of N belonging to class j out of C, y i,j an indicator for the true label, \\u03bb, \\u03b1 constant hyperparameters, \\u03b8 the set of model parameters, M SE the mean squared error,M the 1 -normalized Class Activation Map (CAM, M ) of BID18 , andH the 1 -normalized human attention map H obtained by integrating the eye tracking signal in time. For a fully convolutional network wherein the field of view of any given convolutional filter can be determined via simple upsampling, M can be directly calculated as, DISPLAYFORM1 where w c * k is the weight corresponding to the true class label for each example and f k (x, y) represents the activation of unit k in the last convolutional layer at spatial location (x, y). Because CAMs generated by CNNs give signals at a resolution substantially lower than that of the original image (e.g. for an 18-layer residual network operating on a 224 x 224 input image, the CAM is 7 x 7), we downsample H such that its resolution is aligned with that of M . We hypothesize that the additional supervision signal provided by gaze data can enable model training procedures that reduce the amount of training data required to achieve a given level of performance, especially for difficult tasks. To test this hypothesis, we assess whether training a simple 3-layer CNN with gaze data improves classification performance on the public Pascal Objects Eye Tracking (POET) dataset BID11 , as measured by the average Area Under the Receiver Operating Characteristic Curve (ROC-AUC) over runs with five different random seeds. The POET dataset contains 6,270 images spanning 10 classes, along with eye tracking data from five different labelers for each image, which allows us to perform this comparison. Models are trained both with and without gaze data using the standard procedure described in Appendix A. For simplicity, we refer to networks trained with gaze data as \\\"gaze-augmented\\\" networks and those trained without gaze data as \\\"standard\\\" networks. Using this provided gaze data, we construct a heatmap by additively placing 2D Gaussians with means equal to the fixation points (i.e. locations where the labelers maintained visual gaze) and a predefined standard deviation, similar to what is done in eye-tracking toolkits BID3 . We first analyze the relative sample complexity of the gaze-augmented network and the standard network by using varying numbers of training images. If our hypothesis is correct, the gaze-augmented network will achieve equivalent performance with a smaller number of samples. As seen in Fig. 2a , integrating the gaze-based loss into our training procedure yields superior model performance at every training set size. Further, we find that the gaze-augmented models can achieve similar performance levels to the standard models while using as little as 50% the amount of training data -we observe an average value of 0.85 ROC-AUC for 2,000 images with gaze data versus an average value of 0.86 ROC-AUC for 4,000 images without gaze data. However, the gaze data do not improve performance as much in the high (8000 images) and low (1000 images) data regimes, as the model either does not have enough training examples to learn useful features in the low data regime, or has sufficient training examples such that the gaze data does not provide a performance gain in the high data regime. We also observe a substantial decrease in model variance across different random seeds when using the gaze data (Appendix B); this trend is consistent with the fact that gaze data provides additional constraints on the model parameter space during optimization. To further understand the circumstances in which observational signals may benefit model training, we examine how the difficulty of a task relates to the usefulness of gaze data at training time. We use per-class performance achieved by the standard network as a proxy for task difficulty, and evaluate the relationship between this quantity and the per-class performance gains when using the gazeaugmented model (Fig. 2b) . First, we find that some classes show substantial improvement when using gaze data (7 points ROC-AUC), while others do not (1 point ROC-AUC). Second, we observe a clear negative correlation between the task difficulty and the performance gains from using gaze data, with a Pearson correlation coefficient of \\u22120.85 (p-value 0.002). We speculate that these findings may result from the relative difficulty of identifying the different classes, with gaze data providing more benefit for more difficult classes. A more detailed analysis of performance improvements for each class as a function of sample size can be found in Appendix C.Finally, we qualitatively evaluate changes in model spatial attention when gaze data is added; we expect that gaze-augmented models will have high activations in spatial regions of the image that would be deemed relevant by a human labeler. In FIG2 we show a few random cases where the gaze-augmented network achieved the correct classification, while the standard network returned the incorrect classification. In contrast to the standard network, we observe that the gaze-augmented network is heavily influenced by qualitatively important parts of the image. These results suggest that constraining spatial activations with gaze data improves classification peformance by ensuring that the model pays attention to relevant parts of the image. In this study, we introduce a simple method for incorporating observational, passively collected gaze signals into CNN training procedures. We have demonstrated that constraining the model attention to spatial regions deemed relevant to an expert labeler can reduce the amount of labeled data needed to achieve a given level of performance. Additionally, we find that the performance gains from incorporating the observational signal are larger for more difficult tasks. Fully characterizing the circumstances in which we see such gains from gaze-augmented models is a promising avenue for future work. Going forward, we plan to assess the applicability of this technique to medical imaging tasks, and to further investigate how observational signals may improve model robustness. In this work, we train simple three-layer CNNs, where each layer has 32 kernels of size 5x5, a stride of 2, zero padding of 2, and rectified linear activation functions. The convolution layers are followed by an average pool layer with a kernel size of 3x3, and finally a fully connected layer with a softmax activation function. The dataset was divided into 70-20-10 train-dev-test splits; however, to improve performance we over-sample the training images such that each class has roughly the same number of examples, resulting in 8,000 images per epoch of training when the full dataset is used. For model training we use the Adam optimizer with \\u03b2 1 = 0.9 and \\u03b2 2 = 0.999 for 30 epochs while halving the learning rate upon a two-epoch plateau in the validation accuracy. We then cross-validate across five random train-dev splits using different random seeds and report mean results. Random hyperparameter search was performed for each combination of {\\u03b1 = 0, \\u03b1 = 0} over the appropriate subset of \\u03b1: [10 \\u22128 ,10 In Fig. 4 , we show the model improvement for each class and training set size. Taking cow versus aeroplane as an example, inspection of the dataset demonstrates many images with the label cow usually have multiple cows in them, and the cows can greatly vary in appearance; for instance, some cows have horns (i.e. are bulls), some are very skinny, and some only partially appear in the image. Conversely, most images with the label aeroplane only include one aeroplane near the center of the image, and have distinctive and consistent features, e.g. sharp edges for the wings and cone shaped nose. We suspect that for these reasons, the gaze data provided for classifying more difficult classes, such as cow, may be more helpful than those provided for identifying easier classes, such as aeroplane. Figure 4: Mean ROC-AUC improvement from training wth gaze data for each class at multiple different training set sizes. Green indicates improvement, red indicates degradation.\",\n          \"Using variational Bayes neural networks, we develop an algorithm capable of accumulating knowledge into a prior from multiple different tasks. This results in a rich prior capable of few-shot learning on new tasks. The posterior can go beyond the mean field approximation and yields good uncertainty on the performed experiments. Analysis on toy tasks show that it can learn from significantly different tasks while finding similarities among them. Experiments on Mini-Imagenet reach state of the art with 74.5% accuracy on 5 shot learning. Finally, we provide two new benchmarks, each showing a failure mode of existing meta learning algorithms such as MAML and prototypical Networks. Recently, significant progress has been made to scale Bayesian neural networks to large tasks and to provide better approximations of the posterior distribution BID4 . Recent works extend fully factorized posterior distributions to more general families BID22 BID21 . It is also possible to sample from the posterior distribution trough mini-batch updates BID23 BID36 .However, for neural networks, the prior is often chosen for convenience. This may become a problem when the number of observations is insufficient to overcome the choice of the prior. In this regime, the prior must express our current knowledge on the task and, most importantly, our lack of knowledge on it. In addition to that, a good approximation of the posterior under the small sample size regime is required, including the ability to model multiple modes. This is indeed the case for Bayesian optimization BID30 , Bayesian active learning BID12 , continual learning BID20 , safe reinforcement learning BID3 , exploration-exploitation trade-off in reinforcement learning BID16 . Gaussian processes BID27 have historically been used for these applications, but an RBF kernel constitute a prior that is unsuited for many tasks. More recent tools such as deep Gaussian processes BID6 show great potential and yet their scalability whilst learning from multiple tasks needs to be improved. Our contributions are as follow:1. We provide a simple and scalable procedure to learn an expressive prior and posterior over models from multiple tasks.2. We reach state of the art performances on mini-imagenet.3. We propose two new benchmarks, each exposing a failure mode of popular meta learning algorithms. In contrast, our method perform well on these benchmarks.\\u2022 MAML BID11 does not perform well on a collection of sinus tasks when the frequency varies.\\u2022 Prototypical Network BID29 )'s performance decrease considerably when the diversity of tasks increases. Outline: We first describe the proposed approach in Section 2. In Section 3, we extend to three level of hierarchies and obtain a model more suited for classification. Section 4 review related methods and outline the key differences. Finally, In Section 5, we conduct experiments on three different benchmarks to gain insight in the behavior of our algorithm. By leveraging the variational Bayesian approach, we show how we can learn a prior over models with neural networks. We start our analysis with the goal of learning a prior p(w|\\u03b1) over the weights w of neural networks across multiple tasks. We then provide a reduction of the Evidence Lower BOund (ELBO) showing that it is not necessary to explicitly model a distribution in the very high dimension of the weight space of neural networks. Instead the algorithm learns a subspace suitable for expressing model uncertainty within the distributions of tasks considered in the multi-task environment. This simplification results in a scalable algorithm which we refer to as deep prior. To learn a probability distribution p(w|\\u03b1) over the weights w of a network parameterized by \\u03b1, we use a hierarchical Bayes approach across N tasks, with hyper-prior p(\\u03b1). Each task has its own parameters w j , with DISPLAYFORM0 , we have the following posterior: DISPLAYFORM1 The term p(y ij |x ij , w j ) corresponds to the likelihood of sample i of task j given a model parameterized by w j e.g. the probability of class y ij from the softmax of a neural network parameterized by w j with input x ij . For the posterior p(\\u03b1|D), we assume that the large amount of data available across multiple tasks will be enough to overcome a generic prior p(\\u03b1), such as an isotropic Normal distribution. Hence, we consider a point estimate of the posterior p(\\u03b1|D) using maximum a posteriori 2 .We can now focus on the remaining term: p(w j |\\u03b1). Since w j is potentially high dimensional with intricate correlations among the different dimensions, we cannot use a simple Gaussian distribution. Following inspiration from generative models such as GANs BID13 and VAE BID18 , we use an auxiliary variable z \\u223c N (0, I dz ) and a deterministic function projecting the noise z to the space of w i.e. w = h \\u03b1 (z). Marginalizing z, we have: p(w|\\u03b1) = z p(z)p(w|z, \\u03b1)dz = z p(z)\\u03b4 h\\u03b1(z)\\u2212w dz, where \\u03b4 is the Dirac delta function. Unfortunately, directly marginalizing z is intractable for general h \\u03b1 . To overcome this issue, we add z to the joint inference and marginalize it at inference time. Considering the point estimation of \\u03b1, the full posterior is factorized as follows: DISPLAYFORM2 where p(y ij |x ij , w j ) is the conventional likelihood function of a neural network with weight matrices generated from the function h \\u03b1 i.e.: w j = h \\u03b1 (z j ). Similar architecture has been used in BID21 and BID22 , but we will soon show that it can be reduced to a simpler architecture in the context of multi-task learning. The other terms are defined as follows: DISPLAYFORM3 The task will consist of jointly learning a function h \\u03b1 common to all tasks and a posterior distribution p(z j |\\u03b1, S j ) for each task. At inference time, predictions are performed by marginalizing z i.e.: DISPLAYFORM4 In the previous section, we described the different components for expressing the posterior distribution of Equation 4. While all these components are tractable, the normalization factor is still intractable. To address this issue, we follow the Variational Bayes approach BID4 .Conditioning on \\u03b1, we saw in Equation 1 that the posterior factorizes independently for all tasks. This reduces the joint Evidence Lower BOund (ELBO) to a sum of individual ELBO for each task. Given a family of distributions q \\u03b8j (z j |S j , \\u03b1), parameterized by {\\u03b8 j } N j=1 and \\u03b1, the Evidence Lower Bound for task j is: DISPLAYFORM0 where, DISPLAYFORM1 Notice that after simplification 3 , KL j is no longer over the space of w j but only over the space z j . Namely, the posterior distribution is factored into two components, one that is task specific and one that is task agnostic and can be shared with the prior. This amounts to finding a low dimensional manifold in the parameter space where the different tasks can be distinguished. Then, the posterior p(z j |S j , \\u03b1) only has to model which of the possible tasks are likely, given observations S j instead of modeling the high dimensional p(w j |S j , \\u03b1).But, most importantly, any explicit reference to w has now vanished from both Equation 5 and Equation 6. This simplification has an important positive impact on the scalability of the proposed approach. Since we no longer need to explicitly calculate the KL on the space of w, we can simplify the likelihood function to p(y ij |x ij , z j , \\u03b1), which can be a deep network parameterized by \\u03b1, taking both x ij and z j as inputs. This contrasts with the previous formulation, where h \\u03b1 (z j ) produces all the weights of a network, yielding an extremely high dimensional representation and slow training. For modeling q \\u03b8j (z j |S j , \\u03b1), we can use N (\\u00b5 j , \\u03c3 j ), where \\u00b5 j and \\u03c3 j can be learned individually for each task. This, however limits the posterior family to express a single mode. For more flexibility, we also explore the usage of more expressive posterior, such as Inverse Autoregressive Flow (IAF) BID19 or Neural Autoregressive Flow BID17 . This gives a flexible tool for learning a rich variety of multivariate distributions. In principle, we can use a different IAF for each task, but for memory and computational reasons, we use a single IAF for all tasks and we condition 4 on an additional task specific context c j .Note that with IAF, we cannot evaluate q \\u03b8j (z j |S j , \\u03b1) for any values of z efficiently, only for these which we just sampled, but this is sufficient for estimating the KL term with a Monte-Carlo approxi-mation i.e.: DISPLAYFORM0 It is common to approximate KL j with a single sample and let the mini-batch average the noise incurred on the gradient. We experimented with n mc = 10, but this did not significantly improve the rate of convergence. In order to compute the loss proposed in Equation 5, we would need to evaluate every sample of every task. To accelerate the training, we use a Monte-Carlo approximation as is commonly done through the mini-batch procedure. First we replace summations with expectations: DISPLAYFORM0 Now it suffices to approximate the gradient with n mb samples across all tasks. Thus, we simply concatenate all datasets into a meta-dataset and added j as an extra field. Then, we sample uniformly 5 n mb times with replacement from the meta-dataset. Notice the term n j appearing in front of the likelihood in Equation 7, this indicates that individually for each task it finds the appropriate trade-off between the prior and the observations. Refer to Algorithm 1 for more details on the procedure.1: for i in 1 .. n mb :2: sample x, y and j uniformly from the meta dataset 3: DISPLAYFORM1 5: DISPLAYFORM2 Calculating the loss for a mini-batch 3 EXTENDING TO 3 LEVEL OF HIERARCHIESDeep prior gives rise to a very flexible way to transfer knowledge from multiple tasks. However, there is still an important assumption at the heart of deep prior (and other VAE-based approach such as BID10 ): the task information must be encoded in a low dimensional variable z. In Section 5, we show that it is appropriate for regression, but for image classification, it is not the most natural assumption. Hence, we propose to extend to a third level of hierarchy by introducing a latent classifier on the obtained representation. This provides a simple way to enhance existing algorithm such as Prototypical Networks (Proto Net) BID29 . , for a given 6 task j, we decomposed the likelihood p(S|z) into n i=1 p(y i |x i , z) by assuming that the neural network is directly predicting p(y i |x i , z). Here, we introduce a latent variable v to make the prediction p(y i |x i , v). This can be, for example, a Gaussian linear regression on the representation \\u03c6 \\u03b1 (x, z) produced by the neural network. The general form now factorizes as follow: DISPLAYFORM0 , which is commonly called the marginal likelihood. To compute ELBO j in 5 and update the parameters \\u03b1, the only requirement is to be able to compute the marginal likelihood p(S|z). There are closed form solutions for, e.g., linear regression with Gaussian prior, but our aim is to compare with algorithms such as Prototypical Networks on a 5 We also explored a sampling scheme that always make sure to have at least k samples from the same task. The aim was to reduce gradient variance on task specific parameters but, we did not observed any benefits. 6 We removed j from equations to alleviate the notation.classification benchmark. Alternatively, we can factor the marginal likelihood as follow p(S|z) = n i=1 p(y i |x i , S 0..i\\u22121 , z). If a well calibrated task uncertainty is not required, one can also use a leave-one-out procedure n i=1 p(y i |x i , S \\\\ {x i , y i }, z). Both of these factorizations correspond to training n times the latent classifier on a subset of the training set and evaluating on a sample left out. We refer the reader to Rasmussen (2004, Chapter 5) for a discussion on the difference between leave-one-out cross-validation and marginal likelihood. For a practical algorithm, we propose a closed form solution for leave-one-out in prototypical networks. In its standard form, the prototypical network produces a prototype c k by averaging all representations \\u03b3 i = \\u03c6 \\u03b1 (x i , z) of class k i.e. c k = 1 |K| i\\u2208K \\u03b3 i , where K = { i : y i = k}. Then, predictions are made using p(y = k|x, \\u03b1, z) \\u221d exp (\\u2212 c k \\u2212 \\u03b3 i 2 ). k \\u2200k be the prototypes computed without example x i , y i in the training set. Then, DISPLAYFORM0 We defer the proof to supplementary materials. Hence, we only need to compute prototypes once and rescale the Euclidean distance when comparing with a sample that was used for computing the current prototype. This gives an efficient algorithm with the same complexity as the original one and a good proxy for the marginal likelihood. Hierarchical Bayes algorithms for multitask learning has a long history BID7 BID35 BID1 . However most of the literature focuses on simple statistical models and does not consider transferring on new tasks. More recently, BID10 and BID5 explore hierarchical Bayesian inference with neural networks and evaluate on new tasks. Both papers use a two-level Hierarchical VAE for modeling the observations. While similar, our approach differs in a few different ways. We use a discriminative approach and focus on model uncertainty. We show that we can obtain a posterior on z without having to explicitly encode S j . We also explore the usage of more complex posterior family such as IAF. these differences make our algorithm simpler to implement, and easier to scale to larger datasets. Other works consider neural networks with latent variables BID32 BID9 BID33 but does not explore the ability to learn across multiple tasks. Some recent works on meta-learning are also targeting transfer learning from multiple tasks. ModelAgnostic Meta-Learning (MAML) BID11 ) finds a shared parameter \\u03b8 such that for a given task, one gradient step on \\u03b8 using the training set will yield a model with good predictions on the test set. Then, a meta-gradient update is performed from the test error through the one gradient step in the training set, to update \\u03b8. This yields a simple and scalable procedure which learns to generalize. Recently BID14 considers a Bayesian version of MAML. Additionally, BID28 ) also consider a meta-learning approach where an encoding network reads the training set and generates the parameters of a model, which is trained to perform well on the test set. Finally, some recent interest in few-shot learning give rise to various algorithms capable of transferring from multiple tasks. Many of these approaches BID34 BID29 find a representation where a simple algorithm can produce a classifier from a small training set. BID2 use a neural network pre-trained on a standard multi-class dataset to obtain a good representation and use classes statistics to transfer prior knowledge to new classes. Through experiments, we want to answer i) Can deep prior learn a meaningful prior on tasks? ii) Can it compete against state of the art on a strong benchmark? iii) In which situations does deep prior and other approaches fail? To gain a good insight into the behavior of the prior and posterior, we choose a collection of one dimensional regression tasks. We also want to test the ability of the method to learn the task and not just match the observed points. For this, we will use periodic functions and test the ability of the regressor to extrapolate outside of its domain. Specifically, each dataset consists of (x, y) pairs (noisily) sampled from a sum of two sine waves with different phase and amplitude and a frequency ratio of 2: f (x) = a 1 sin(\\u03c9\\u00b7x+b 1 )+a 2 sin(2\\u00b7\\u03c9\\u00b7x+b 2 ), where y \\u223c N (f (x), \\u03c3 2 y ). We construct a meta-training set of 5000 tasks, sampling \\u03c9 \\u223c U(5, 7), (b 1 , b 2 ) \\u223c U(0, 2\\u03c0) 2 and (a 1 , a 2 ) \\u223c N (0, 1) 2 independently for each task. To evaluate the ability to extrapolate outside of the task's domain, we make sure that each task has a different domain. Specifically, x values are sampled according to N (\\u00b5 x , 1), where \\u00b5 x is sample from the meta-domain U (\\u22124, 4) . The number of training samples ranges from 4 to 50 for each task and, evaluation is performed on 100 samples from tasks never seen during training. Model Once z is sampled from IAF, we simply concatenate it with x and use 12 densely connected layers of 128 neurons with residual connections between every other layer. The final layer linearly projects to 2 outputs \\u00b5 y and s, where s is used to produce a heteroskedastic noise, \\u03c3 y = sigmoid(s) \\u00b7 0.1 + 0.001. Finally, we use p(y|x, z) = N (\\u00b5 y (x, z), \\u03c3 y (x, z)2 ) to express the likelihood of the training set. To help gradient flow, we use ReLU activation functions and Layer Normalization 7 BID0 .Results Figure 1a depicts examples of tasks with 1, 2, 8, and 64 samples. The true underlying function is in blue while 10 samples from the posterior distributions are faded in the background. The thickness of the line represent 2 standard deviations. The first plot has only one single data point and mostly represents samples from the prior, passing near this observed point. Interestingly, all samples are close to some parametrization of Equation 5.1. Next with only 2 points, the posterior is starting to predict curves highly correlated with the true function. However, note that the uncertainty is over optimistic and that the posterior failed to fully represent all possible harmonics fitting these two points. We discuss this issue more in depth in supplementary materials. Next, with 8 points, it managed to mostly capture the task, with reasonable uncertainty. Finally, with 64 points the model is certain of the task. To add a strong baseline, we experimented with MAML BID11 . After exploring a variety of values for hyper-parameter and architecture design we couldn't make it work for our two harmonics meta-task. We thus reduced the meta-task to a single harmonic and reduced the base frequency range by a factor of two. With these simplifications, we managed to make it converge, but the results are far behind that of deep prior even in this simplified setup. Figure 1b shows some form of adaptation with 16 samples per task but the result is jittery and the extrapolation capacity is very limited. these results were obtained with a densely connected network of 8 hidden layers of 64 units 8 , with residual connections every other layer. The training is performed with two gradient steps and the evaluation with 5 steps. To make sure our implementation is valid, we first replicated their regression result with a fixed frequency as reported in BID11 .Finally, to provide a stronger baseline, we remove the KL regularizer of deep prior and reduced the posterior q \\u03b8j (z j |S j , \\u03b1) to a deterministic distribution centered on \\u00b5 j . The mean square error is reported in Figure 2 for an increasing dataset size. This highlights how the uncertainty provided by deep prior yields a systematic improvement. BID34 proposed to use a subset of Imagenet to generate a benchmark for few-shot learning. Each task is generated by sampling 5 classes uniformly and 5 training samples per class, the remaining images from the 5 classes are used as query images to compute accuracy. The number of unique classes sums to 100, each having 600 examples of 84 \\u00d7 84 images. To perform meta-validation and meta-test on unseen tasks (and classes), we isolate 16 and 20 classes respectively from the original The baseline corresponds to the same model without the KL regularizer. Each value is averaged over 100 tasks and 10 different restart. right: 4 sample tasks from the Synbols dataset. Each row is a class and each column is a sample from the classes. In the 2 left tasks, the symbol have to be predicted while in the two right tasks, the font has to be predicted. set of 100, leaving 64 classes for the training tasks. This follows the procedure suggested in BID28 . The training procedure proposed in Section 2 requires training on a fixed set of tasks. We found that 1000 tasks yields enough diversity and that over 9000 tasks, the embeddings are not being visited often enough over the course of the training. To increase diversity during training, the 5 \\u00d7 5 training and test sets are re-sampled every time from a fixed train-test split of the given task 9 .We first experimented with the vanilla version of deep prior (2). In this formulation, we use a ResNet BID15 network, where we inserted FILM layers BID26 between each residual block to condition on the task. Then, after flattening the output of the final convolution layer and reducing to 64 hidden units, we apply a 64 \\u00d7 5 matrix generated from a transformation of z. Finally, predictions are made through a softmax layer. We found this architecture to be slow to train as the generated last layer is noisy for a long time and prevent the rest of the Matching Networks BID34 60.0 % Meta-Learner BID28 60.6 % MAML BID11 63.2% Prototypical Networks BID29 68.2 % SNAIL BID24 68.9 % Discriminative k-shot BID2 73.9 % adaResNet BID25 71.9 % Deep Prior (Ours) 62.7 % Deep Prior + Proto Net (Ours) 74.5 % 68.6 \\u00b1 0.5% 69.6 \\u00b1 0.8% + ResNet (12) 72.4 \\u00b1 1.0% 76.8 \\u00b1 0.4% + Conditioning 72.3 \\u00b1 0.6% 80.1 \\u00b1 0.9% + Leave-One-Out 73.9 \\u00b1 0.4% 82.7 \\u00b1 0.2% + KL 74.5 \\u00b1 0.5% 83.5 \\u00b1 0.4% Table 2 : Ablation Study of our model. Accuracy is shown with 90% confidence interval over bootstrap of the validation set.network to learn. Nevertheless, we obtained 62.6% accuracy on Mini-Imagenet, on par with many strong baselines. To enhance the model, we combine task conditioning with prototypical networks as proposed in Section 3. This approach alleviates the need to generate the final layer of the network, thus accelerating training and increasing generalization performances. While we no longer have a well calibrated task uncertainty, the KL term still acts as an effective regularizer and prevents overfitting on small datasets 10 . With this improvement, we are now the new state of the art with 74.5% TAB0 . In Table 2 , we perform an ablation study to highlight the contributions of the different components of the model. In sum, a deeper network with residual connections yields major improvements. Also, task conditioning does not yield improvement if the leave-one-out procedure is not used. Finally, the KL regularizer is the final touch to obtain state of the art. In Section 5.2, we saw that conditioning helps, but only yields a minor improvement. This is due to the fact that Mini-Imagenet is a very homogeneous collection of tasks where a single representation is sufficient to obtain good results. To support this claim, we provide a new benchmark 11 of synthetic symbols which we refer to as Synbols. Images are generated using various font family on different alphabets (Latin, Greek, Cyrillic, Chinese) and background noise (Figure 2, right) . For each task we have to predict either a subset of 4 font families or 4 symbols with only 4 examples. Predicting either fonts or symbols with two separate Prototypical Networks, yields 84.2% and 92.3% accuracy respectively, with an average of 88.3%. However, blending the two collections of tasks in a single benchmark, brings prototypical network down to 76.8%. Now, conditioning on the task with deep prior brings back the accuracy to 83.5%. While there is still room for improvement, this supports the claim that a single representation will only work on homogeneous collection of tasks and that task conditioning helps learning a family of representations suitable for heterogeneous benchmarks. Using a variational Bayes framework, we developed a scalable algorithm for hierarchical Bayesian learning of neural networks, called deep prior. This algorithm is capable of transferring information from tasks that are potentially remarkably different. Results on the Harmonics dataset shows that the learned manifold across tasks exhibits the properties of a meaningful prior. Finally, we found that MAML, while very general, will have a hard time adapting when tasks are too different. Also, we found that algorithms based on a single image representation only works well when all tasks can succeed with a very similar set of features. Together, these findings allowed us to reach the state of the art on Mini-Imagenet. 7.1 PROOF OF LEAVE-ONE-OUT Theorem 1. Let c \\u2212i k \\u2200k be the prototypes computed without example x i , y i in the training set. Then, DISPLAYFORM0 Proof. Let \\u03b3 i = \\u03c6 \\u03b1 (x i ), n = |K| and assume y i = k then, DISPLAYFORM1 When y i = k, the result is trivially \\u03b3 i \\u2212 c When experimenting with the Harmonics toy dataset in Section 5.1, we observed issues with repeatability, most likely due to local minima. We decided to investigate further on the multimodality of posterior distributions with small sample size and the capacity of IAF to model them. For this purpose we simplified the problem to a single sine function and removed the burden of learning the prior. The likelihood of the observations is defined as follows: DISPLAYFORM2 f (x) = sin(5(\\u03c9 \\u00b7 x + b)); y \\u223c N (f (x), \\u03c3 2 y ), where \\u03c3 y = 0.1 is given and p(\\u03c9) = p(b) = N (0, 1). Only the frequency \\u03c9 and the bias b are unknown 12 , yielding a bi-dimensional problem that is easy to visualize and quick to train. We use a dataset of 2 points at x = 1.5 and x = 3 and the corresponding posterior distribution is depicted in FIG1 -middle, with an orange point at the location of the true underlying function. Some samples from the posterior distribution can be observed in FIG1 -top. We observe a high amount of multi-modality on the posterior distribution FIG1 . Some of the modes are just the mirror of another mode and correspond to the same functions e.g. b + 2\\u03c0 or \\u2212f ; b + \\u03c0. But most of the time they correspond to different functions and modeling them is crucial for some application. The number of modes varies a lot with the choice of observed dataset, ranging from a few to several dozens. Now, the question is: \\\"How many of those modes can IAF model?\\\". Unfortunately, FIG1 -bottom reveals poor capability for this particular case. After carefully adjusting the hyperparameters 13 of IAF, exploring different initialization schemes and running multiple restarts, we rarely capture more than two modes (sometimes 4). Moreover, it will not be able to fully separate the two modes. There is systematically a thin path of density connecting each modes as a chain. With longer training, the path becomes thinner but never vanishes and the magnitude stays significant.\",\n          \"We propose a study of the stability of several few-shot learning algorithms subject to variations in the hyper-parameters and optimization schemes while controlling the random seed.   We propose a methodology for testing for statistical differences in model performances under several replications. To study this specific design, we attempt to reproduce results from three prominent papers: Matching Nets, Prototypical Networks, and TADAM. We analyze on the miniImagenet dataset on the standard classification task in the 5-ways, 5-shots learning setting at test time. We find that the selected implementations exhibit stability across random seed, and repeats. b \\u223c Normal(0, \\u03c3 2 I) (1) y = X\\u03b2 + Zb + \\u03b1 + .(2)Where \\u03b2 \\u2208 R P is our slope vector, \\u03b1 \\u2208 R is the intercept, and \\u223c Normal(0, I) is the random 127 noise vector. To model the clusters, we introduce Zb,where Z is the n \\u00d7 q model matrix for the q- and the dependent variable mean is captured by X\\u03b2 + \\u03b1 when we marginalize over all the samples. The random effects component Zb captures variations in the data, it can be interpreted as an individual 134 deviation from the group-level fixed effect. In our context, we can write the model as follows:136 metric ijk = (A + \\u03b1 0j + \\u03b1 1k ) + \\u03b2Experiment i + i (3) metric ijk = A + \\u03b2Experiment i + (\\u03b1 0j + \\u03b1 1k + i ) (4) DISPLAYFORM0 Where A is the intercept, \\u03b2 is a vector of parameters and Experiment i is a one hot vector of 137 experiments for the observation i. We can regroup all the random effects, where alpha 0j is a 138 random effect associated with an observation from a random seed j, and alpha 1k is associated to an observation from a repeat k. Finally, it is possible to regroup all the nuisance parameters in seeds for a given implementation, except for inherent differences due to parallelism on CPU and 250 GPU. DISPLAYFORM0 Some implementations: \\u2022 call a random number generator at an execution point placed before the episodes data 253 generation, hence changing the state of the random number generator, \\u2022 generate the episodes data in advance, others generate it for each episode on the fly: different states of random number generator are involved in the data generation process, \\u2022 start training at different states of the random number generator: random number sequences The trend of large-scale compute-intensive ML experiments has caused concern in the community\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"r1gPtjcH_N\",\n          \"S1eEmn05tQ\",\n          \"B1g-SnUaUN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"We explore using passively collected eye-tracking data to reduce the amount of labeled data needed during training. A method to use gaze information to reduce the sample complexity of a model and the needed labeling effort to get a target performance, with improved results in middle-sized samples and harder tasks. A method to incorporate gaze signals into standard CNNs for image classification, adding a loss function term based in the difference between the model's Class Activation Map and the map constructed from eye tracking information.\",\n          \"A scalable method for learning an expressive prior over neural networks across multiple tasks. The paper presents a method for training a probabilistic model for Multitasks Transfer Learning by introducing a latent variable per task to capture the commonality in the task instances. The work proposes a variational approach to meta-learning that employs latent variables corresponding to task-specific datasets. Aims to learn a prior over neural networks for multiple tasks. \",\n          \"We propose a study of the stability of several few-shot learning algorithms subject to variations in the hyper-parameters and optimization schemes while controlling the random seed. This paper studies reproducibility for few-shot learning.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Improving Sample Complexity with Observational Supervision\",\n          \"Uncertainty in Multitask Transfer Learning\",\n          \"Reproducibility and Stability Analysis in Metric-Based Few-Shot Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22,\n        \"min\": 33,\n        \"max\": 85,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          85,\n          70,\n          33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Supervised machine learning models for high-value computer vision applications such as medical image classification often require large datasets labeled by domain experts, which are slow to collect, expensive to maintain, and static with respect to changes in the data distribution. In this context, we assess the utility of observational supervision, where we take advantage of passively-collected signals such as eye tracking or \\u201cgaze\\u201d data, to reduce the amount of hand-labeled data needed for model training. We present evidence that constraining the model in this way can reduce the number of labeled examples required to achieve a given performance level by as much as 50%, and that gaze information is most helpful on more difficult tasks. Inspired by the success of eye tracking techniques in NLP applications that only use gaze signal at train time BID7 , we examine a straightforward mapping of gaze data to visual attention layer activations in a way that encourages the model to draw influence from the same spatial regions most heavily utilized by the human annotator. Nonetheless, these approaches require supervision signals that are fundamentally declarative -that is, they do not leverage the rich information contained in user behavior that can be captured by passive sensing modalities such as eye tracking and click stream monitoring. Of particular relevance for this work are those studies that have examined using gaze data directly in the context of training computer vision models. BID9 collect eye tracking data on several different datasets and demonstrate that features derived from this data can support classdiscriminative representations that improve zero-shot fine-grained image classification; notably, this approach requires gaze data at test time. BID15 integrate gaze data directly into the optimization procedure for a modified Latent Support Vector Machine (LSVM) and demonstrate competitive performance on several image classification and localization tasks in a manner that is gaze-free at test-time. While much initial work on the use of eye tracking signal has considered using the gaze signal as another set of features for the model, we are motivated by the prospect of using it to train standard neural network architectures that do not need gaze features at test time, and are thus practically deployable in settings where eye trackers are unavailable. We hypothesize that the additional supervision signal provided by gaze data can enable model training procedures that reduce the amount of training data required to achieve a given level of performance, especially for difficult tasks. We also observe a substantial decrease in model variance across different random seeds when using the gaze data (Appendix B); this trend is consistent with the fact that gaze data provides additional constraints on the model parameter space during optimization. To further understand the circumstances in which observational signals may benefit model training, we examine how the difficulty of a task relates to the usefulness of gaze data at training time. We use per-class performance achieved by the standard network as a proxy for task difficulty, and evaluate the relationship between this quantity and the per-class performance gains when using the gazeaugmented model (Fig. 2b) . In contrast to the standard network, we observe that the gaze-augmented network is heavily influenced by qualitatively important parts of the image. These results suggest that constraining spatial activations with gaze data improves classification peformance by ensuring that the model pays attention to relevant parts of the image. Taking cow versus aeroplane as an example, inspection of the dataset demonstrates many images with the label cow usually have multiple cows in them, and the cows can greatly vary in appearance; for instance, some cows have horns (i.e. are bulls), some are very skinny, and some only partially appear in the image.\",\n          \"More recent tools such as deep Gaussian processes BID6 show great potential and yet their scalability whilst learning from multiple tasks needs to be improved. In contrast, our method perform well on these benchmarks.\\u2022 MAML BID11 does not perform well on a collection of sinus tasks when the frequency varies.\\u2022 Prototypical Network BID29 )'s performance decrease considerably when the diversity of tasks increases. We then provide a reduction of the Evidence Lower BOund (ELBO) showing that it is not necessary to explicitly model a distribution in the very high dimension of the weight space of neural networks. To address this issue, we follow the Variational Bayes approach BID4 .Conditioning on \\u03b1, we saw in Equation 1 that the posterior factorizes independently for all tasks. In principle, we can use a different IAF for each task, but for memory and computational reasons, we use a single IAF for all tasks and we condition 4 on an additional task specific context c j .Note that with IAF, we cannot evaluate q \\u03b8j (z j |S j , \\u03b1) for any values of z efficiently, only for these which we just sampled, but this is sufficient for estimating the KL term with a Monte-Carlo approxi-mation i.e.: DISPLAYFORM0 It is common to approximate KL j with a single sample and let the mini-batch average the noise incurred on the gradient. Notice the term n j appearing in front of the likelihood in Equation 7, this indicates that individually for each task it finds the appropriate trade-off between the prior and the observations. However, there is still an important assumption at the heart of deep prior (and other VAE-based approach such as BID10 ): the task information must be encoded in a low dimensional variable z. In Section 5, we show that it is appropriate for regression, but for image classification, it is not the most natural assumption. However most of the literature focuses on simple statistical models and does not consider transferring on new tasks. Additionally, BID28 ) also consider a meta-learning approach where an encoding network reads the training set and generates the parameters of a model, which is trained to perform well on the test set. Many of these approaches BID34 BID29 find a representation where a simple algorithm can produce a classifier from a small training set. To gain a good insight into the behavior of the prior and posterior, we choose a collection of one dimensional regression tasks. For this, we will use periodic functions and test the ability of the regressor to extrapolate outside of its domain. However, note that the uncertainty is over optimistic and that the posterior failed to fully represent all possible harmonics fitting these two points. This is due to the fact that Mini-Imagenet is a very homogeneous collection of tasks where a single representation is sufficient to obtain good results. Also, we found that algorithms based on a single image representation only works well when all tasks can succeed with a very similar set of features. For this purpose we simplified the problem to a single sine function and removed the burden of learning the prior.\",\n          \"We propose a study of the stability of several few-shot learning algorithms subject to variations in the hyper-parameters and optimization schemes while controlling the random seed.   We propose a methodology for testing for statistical differences in model performances under several replications. To study this specific design, we attempt to reproduce results from three prominent papers: Matching Nets, Prototypical Networks, and TADAM. We analyze on the miniImagenet dataset on the standard classification task in the 5-ways, 5-shots learning setting at test time. We find that the selected implementations exhibit stability across random seed, and repeats. b \\u223c Normal(0, \\u03c3 2 I) (1) y = X\\u03b2 + Zb + \\u03b1 + .(2)Where \\u03b2 \\u2208 R P is our slope vector, \\u03b1 \\u2208 R is the intercept, and \\u223c Normal(0, I) is the random 127 noise vector. To model the clusters, we introduce Zb,where Z is the n \\u00d7 q model matrix for the q- and the dependent variable mean is captured by X\\u03b2 + \\u03b1 when we marginalize over all the samples. The random effects component Zb captures variations in the data, it can be interpreted as an individual 134 deviation from the group-level fixed effect. In our context, we can write the model as follows:136 metric ijk = (A + \\u03b1 0j + \\u03b1 1k ) + \\u03b2Experiment i + i (3) metric ijk = A + \\u03b2Experiment i + (\\u03b1 0j + \\u03b1 1k + i ) (4) DISPLAYFORM0 Where A is the intercept, \\u03b2 is a vector of parameters and Experiment i is a one hot vector of 137 experiments for the observation i. We can regroup all the random effects, where alpha 0j is a 138 random effect associated with an observation from a random seed j, and alpha 1k is associated to an observation from a repeat k. Finally, it is possible to regroup all the nuisance parameters in seeds for a given implementation, except for inherent differences due to parallelism on CPU and 250 GPU. Some implementations: \\u2022 call a random number generator at an execution point placed before the episodes data 253 generation, hence changing the state of the random number generator, \\u2022 generate the episodes data in advance, others generate it for each episode on the fly: different states of random number generator are involved in the data generation process, \\u2022 start training at different states of the random number generator: random number sequences The trend of large-scale compute-intensive ML experiments has caused concern in the community\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random.seed(42)\n",
        "\n",
        "# def shuffle_list(text):\n",
        "#     lst = text.split('. ')\n",
        "#     lst[:-1] = [sentence + '.' for sentence in lst[:-1]]\n",
        "#     random.shuffle(lst)\n",
        "#     return ' '.join(lst)\n",
        "\n",
        "# data['target'] = data['target'].apply(shuffle_list)"
      ],
      "metadata": {
        "id": "03zjLD42tXLy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of words of source and target\n",
        "def count_words(data, column):\n",
        "  return data[column].apply(lambda x : len(x.split()))\n",
        "\n",
        "data['number_words_target'] = count_words(data, 'target')\n",
        "data['number_words_source'] = count_words(data, 'source')\n",
        "data['number_words_extractive'] = count_words(data, 'extractive_summary')"
      ],
      "metadata": {
        "id": "eoYG245eurut"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['number_words_target'].describe())\n",
        "print(data['number_words_source'].describe())\n",
        "print(data['number_words_extractive'].describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjsgp2YRUwZ1",
        "outputId": "3523ad6d-7346-4601-84a0-43a6379978ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    1312.000000\n",
            "mean       58.935213\n",
            "std        20.402388\n",
            "min        30.000000\n",
            "25%        41.000000\n",
            "50%        58.000000\n",
            "75%        73.000000\n",
            "max       149.000000\n",
            "Name: number_words_target, dtype: float64\n",
            "count     1312.000000\n",
            "mean      5177.437500\n",
            "std       2107.094236\n",
            "min        126.000000\n",
            "25%       4069.750000\n",
            "50%       5001.500000\n",
            "75%       6187.750000\n",
            "max      24589.000000\n",
            "Name: number_words_source, dtype: float64\n",
            "count    1312.000000\n",
            "mean      621.176067\n",
            "std       143.951058\n",
            "min       126.000000\n",
            "25%       534.000000\n",
            "50%       611.000000\n",
            "75%       693.000000\n",
            "max      1199.000000\n",
            "Name: number_words_extractive, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogramas\n",
        "data.hist(bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "ip8aBXD3ZCuc",
        "outputId": "116123f9-dbfe-4dd3-e2d4-28ef4b34ebb4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABat0lEQVR4nO3deVgUV/o24KfZGhCalh0UEFcU16AioxgVBJG4Jy5xIhrUxEgySmKMTlRwTHAbNTpuk0VNJibRZKITd1wxikRRNC4hYjBGBVQIIKDQ0uf7w4/62Ta7QFfrc19XX1qnTle9dag+/XZVnSqFEEKAiIiISEZMDB0AERER0eOYoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkhwkKERERyQ4TFCIiIpIdJihEREQkO0xQiIiISHaYoMjQ4cOHoVAo8O233xo6FNlq1qwZxo8fb+gwiIwG+5WqsV+RFyYoRP/fzZs3ERMTg5SUFEOHUi3GFi8RUU0wQSH6/27evInY2Fij+cI3tniJiGqCCcozrLCw0NAhVEjOsdXU07QtRFWR8/4u59jqmxAC9+7dM3QYNcIEBUBMTAwUCgXS0tIwfvx4qNVq2NnZYcKECSgqKgIAXL16FQqFAhs3btR7v0KhQExMjN7yfv31V/z1r3+FnZ0dnJycMGfOHAgh8Mcff2DIkCFQqVRwdXXFP//5z3LjKi0txezZs+Hq6opGjRph8ODB+OOPP/TqJSUlYcCAAbCzs4O1tTWef/55HDt2rNxtvHjxIl5++WU0btwYvXr1qrJthBBwdHREdHS0VKbVaqFWq2Fqaorc3FypfNGiRTAzM0NBQYFUdvDgQQQGBqJRo0ZQq9UYMmQILl26VO3YhBBYsGABmjZtCmtra/Tt2xcXLlzQi1Oj0SA2NhatWrWCpaUlHBwc0KtXL8THx1e5jcDD8/PdunUDAEyYMAEKhULn73306FG89NJL8PT0hFKphIeHB6ZPn673gR8/fjxsbGxw5coVDBw4ELa2thg7diwA4N69e3jrrbfg6OgIW1tbDB48GDdu3NDbfwDgxo0bePXVV+Hi4gKlUglfX1989tln1Y6XDI/9SsWelX6lzKpVq+Dr6wtra2s0btwYXbt2xebNm3XqnDlzBmFhYVCpVLCxsUFQUBBOnDhR7jY9buPGjVAoFLh69apU1qxZM7zwwgvYu3cvunbtCisrK6xfvx4AkJubi+nTp6NZs2ZQKpVo2rQpxo0bhzt37kjvLy4uxrx589CyZUupz3v33XdRXFxco21/EmYNtiYjMHLkSHh7eyMuLg6nT5/GJ598AmdnZyxatKhWyxs1ahTatm2LhQsXYufOnViwYAHs7e2xfv169OvXD4sWLcKXX36Jd955B926dUPv3r113v/BBx9AoVBg5syZuHXrFlasWIHg4GCkpKTAysoKwMMPalhYGPz8/DBv3jyYmJhgw4YN6NevH44ePYru3bvrLPOll15Cq1at8OGHH0IIUeU2KBQK9OzZEwkJCVLZuXPnkJeXBxMTExw7dgzh4eEAHn6Jd+nSBTY2NgCA/fv3IywsDM2bN0dMTAzu3buHVatWoWfPnjh9+jSaNWtWZWxz587FggULMHDgQAwcOBCnT59GSEgISkpKdN4bExODuLg4TJw4Ed27d0d+fj5OnTqF06dPo3///lVuZ9u2bTF//nzMnTsXkydPRmBgIADgL3/5CwBg69atKCoqwpQpU+Dg4ICffvoJq1atwvXr17F161adZT148AChoaHo1asXli5dCmtrawAPk5ctW7bglVdeQY8ePXDkyBGp7R6VlZWFHj16QKFQICoqCk5OTti9ezciIyORn5+PadOmVRkvyQf7FX3PSr8CAB9//DHeeustvPjii/jb3/6G+/fv49y5c0hKSsLLL78MALhw4QICAwOhUqnw7rvvwtzcHOvXr0efPn1w5MgR+Pv7V2tdj0tNTcWYMWPw2muvYdKkSWjTpg0KCgoQGBiIS5cu4dVXX8Vzzz2HO3fu4H//+x+uX78OR0dHaLVaDB48GD/++CMmT56Mtm3b4ueff8by5cvx66+/Ytu2bbWKp8YEiXnz5gkA4tVXX9UpHzZsmHBwcBBCCJGeni4AiA0bNui9H4CYN2+e3vImT54slT148EA0bdpUKBQKsXDhQqn8zz//FFZWViIiIkIqO3TokAAgmjRpIvLz86XyLVu2CADio48+EkIIodVqRatWrURoaKjQarVSvaKiIuHt7S369++vF9OYMWNq1jhCiCVLlghTU1MplpUrVwovLy/RvXt3MXPmTCGEEKWlpUKtVovp06dL7+vcubNwdnYW2dnZUtnZs2eFiYmJGDduXJWx3bp1S1hYWIjw8HCd7Zs9e7YAoNNmnTp1EuHh4TXetkedPHmywr9xUVGRXllcXJxQKBTi999/l8oiIiIEAPHee+/p1E1OThYAxLRp03TKx48fr7f/REZGCjc3N3Hnzh2duqNHjxZ2dnZSLJXFS4bHfqVyz0q/MmTIEOHr61tpnaFDhwoLCwtx5coVqezmzZvC1tZW9O7dW2+bHrdhwwYBQKSnp0tlXl5eAoDYs2ePTt25c+cKAOK///2v3nLK2uOLL74QJiYm4ujRozrz161bJwCIY8eOVbo9dYWneB7x+uuv60wHBgYiOzsb+fn5tVrexIkTpf+bmpqia9euEEIgMjJSKler1WjTpg1+++03vfePGzcOtra20vSLL74INzc37Nq1CwCQkpKCy5cv4+WXX0Z2djbu3LmDO3fuoLCwEEFBQUhISIBWq610G6sjMDAQpaWlOH78OICHv2gCAwMRGBiIo0ePAgDOnz+P3Nxc6Zd8RkYGUlJSMH78eNjb20vL6tixI/r37y9tQ2Wx7d+/HyUlJXjzzTd1DmtOmzZN771qtRoXLlzA5cuXa7x91VH2yxJ4eB77zp07+Mtf/gIhBM6cOaNXf8qUKTrTe/bsAQC88cYbOuVvvvmmzrQQAt999x0GDRoEIYT0N71z5w5CQ0ORl5eH06dP19VmUQNgv1K+Z6VfUavVuH79Ok6ePFnu/NLSUuzbtw9Dhw5F8+bNpXI3Nze8/PLL+PHHH2u9r3h7eyM0NFSn7LvvvkOnTp0wbNgwvfpl7bF161a0bdsWPj4+On1Qv379AACHDh2qVTw1xQTlEZ6enjrTjRs3BgD8+eefdbI8Ozs7WFpawtHRUa+8vHW0atVKZ1qhUKBly5bSecayD01ERAScnJx0Xp988gmKi4uRl5enswxvb+8ab8dzzz0Ha2trqdMo60h69+6NU6dO4f79+9K8snO8v//+OwCgTZs2estr27at1OFVFlvZMh5vBycnJ+lvU2b+/PnIzc1F69at0aFDB8yYMQPnzp2r8bZW5Nq1a1KnaGNjAycnJzz//PMAoNfGZmZmaNq0qd62mJiY6G1jy5YtdaZv376N3Nxc/Pvf/9b7m06YMAEAcOvWrTrbLqp/7FfK96z0KzNnzoSNjQ26d++OVq1aYerUqTrX8ty+fRtFRUUVbpNWqy33GqHqKO/vcuXKFbRv377S912+fBkXLlzQ+/u3bt0aQMP1QbwG5RGmpqbllgshyr0wCXiY/dZkeZWto6bKfsUsWbIEnTt3LrdO2XnbMo8eCaguc3Nz+Pv7IyEhAWlpacjMzERgYCBcXFyg0WiQlJSEo0ePwsfHB05OTjVe/pPEVqZ37964cuUKtm/fjn379uGTTz7B8uXLsW7dOp1fnLVRWlqK/v37IycnBzNnzoSPjw8aNWqEGzduYPz48Xq/JpVKJUxMapf7ly3rr3/9KyIiIsqt07Fjx1otmwyD/Ur5npV+pW3btkhNTcWOHTuwZ88efPfdd1izZg3mzp2L2NjYGsVT0/2lttuu1WrRoUMHLFu2rNz5Hh4etVpuTTFBqaayzPrRq8uB/8vG68PjhxWFEEhLS5O+oFq0aAEAUKlUCA4Orrc4gIeHYxctWoT9+/fD0dERPj4+UCgU8PX1xdGjR3H06FG88MILUn0vLy8ADy/Setwvv/wCR0dHNGrUqNJ1li3j8uXLOoc+b9++Xe4vQ3t7e0yYMAETJkxAQUEBevfujZiYmGp3JBV9+H/++Wf8+uuv2LRpE8aNGyeV1+RKfi8vL2i1WqSnp+v8cktLS9Op5+TkBFtbW5SWllb5N60oXjIe7Fee/n4FABo1aoRRo0Zh1KhRKCkpwfDhw/HBBx9g1qxZcHJygrW1dYXbZGJiIiUEj+4varVaqleT/aVFixY4f/58lXXOnj2LoKAgg/YzPMVTTSqVCo6OjjpXnQPAmjVr6m2dn3/+Oe7evStNf/vtt8jIyEBYWBgAwM/PDy1atMDSpUt1huCVuX37dp3FEhgYiOLiYqxYsQK9evWSdtrAwEB88cUXuHnzpnSeGHh4/rRz587YtGmTTud7/vx57Nu3DwMHDqxyncHBwTA3N8eqVat0fgmuWLFCr252drbOtI2NDVq2bFmjIXFlHdvjXxZlv04fjUEIgY8++qjayy47D/z4/rJq1Sq9dY0YMQLfffdduZ3Io3/TiuIl48F+5envVx5fhoWFBdq1awchBDQaDUxNTRESEoLt27frDBPOysrC5s2b0atXL6hUKgD/lzw+ur8UFhZi06ZN1Y5nxIgROHv2LL7//nu9eWXtMXLkSNy4cQMff/yxXp179+412P1keASlBiZOnIiFCxdi4sSJ6Nq1KxISEvDrr7/W2/rs7e3Rq1cvTJgwAVlZWVixYgVatmyJSZMmAQBMTEzwySefICwsDL6+vpgwYQKaNGmCGzdu4NChQ1CpVPjhhx/qJJaAgACYmZkhNTUVkydPlsp79+6NtWvXAoBORwI8PEQcFhaGgIAAREZGSsMB7ezs9O77UR4nJye88847iIuLwwsvvICBAwfizJkz2L17t9759nbt2qFPnz7w8/ODvb09Tp06hW+//RZRUVHV3sYWLVpArVZj3bp1sLW1RaNGjeDv7w8fHx+0aNEC77zzDm7cuAGVSoXvvvuuRtcQ+Pn5YcSIEVixYgWys7OlYcZl+8+jv1IWLlyIQ4cOwd/fH5MmTUK7du2Qk5OD06dPY//+/cjJyak03tpcD0CGw37l6e5XQkJC4Orqip49e8LFxQWXLl3Cv/71L4SHh0sXKy9YsADx8fHo1asX3njjDZiZmWH9+vUoLi7G4sWLdZbl6emJyMhIzJgxA6ampvjss8/g5OSEa9euVSueGTNm4Ntvv8VLL72EV199FX5+fsjJycH//vc/rFu3Dp06dcIrr7yCLVu24PXXX8ehQ4fQs2dPlJaW4pdffsGWLVuke6vUuwYZKyRzZUO3bt++rVP++NCtoqIiERkZKezs7IStra0YOXKkuHXrVoXDAR9fXkREhGjUqJHe+p9//nmdYWhlwwG/+uorMWvWLOHs7CysrKxEeHi4zpDWMmfOnBHDhw8XDg4OQqlUCi8vLzFy5Ehx4MCBKmOqiW7dugkAIikpSSq7fv26ACA8PDzKfc/+/ftFz549hZWVlVCpVGLQoEHi4sWLOnUqi620tFTExsYKNzc3YWVlJfr06SPOnz8vvLy8dIYDLliwQHTv3l2o1WphZWUlfHx8xAcffCBKSkpqtI3bt28X7dq1E2ZmZjrDPy9evCiCg4OFjY2NcHR0FJMmTRJnz57VGyJa0d9YCCEKCwvF1KlThb29vbCxsRFDhw4VqampAoDOEFEhhMjKyhJTp04VHh4ewtzcXLi6uoqgoCDx73//u1rxkuGxX6mep71fWb9+vejdu7fUji1atBAzZswQeXl5OvVOnz4tQkNDhY2NjbC2thZ9+/YVx48f11tecnKy8Pf3FxYWFsLT01MsW7aswmHGFQ2Rzs7OFlFRUaJJkybCwsJCNG3aVEREROjc2qCkpEQsWrRI+Pr6CqVSKRo3biz8/PxEbGysXuz1RSFELa6iIqI6kZKSgi5duuA///mPdMdZIiLiNShEDaa852CsWLECJiYmenf7JCJ61vEalGdYSUmJdD1DRezs7J5omJ4cyGU7Fy9ejOTkZPTt2xdmZmbYvXs3du/ejcmTJzfYsD2i+iaXz1t9e1a206Aa5EQSyVLZOenKXk/DNQ1y2c59+/aJnj17isaNGwtzc3PRokULERMTIzQaTb2vm6ihyOXzVt+ele00pBpfg5KQkIAlS5YgOTkZGRkZ+P777zF06FBp/vjx4/WGPIWGhkq3+gaAnJwcvPnmm/jhhx9gYmKCESNG4KOPPtK7+Q/Vrz///BPJycmV1vH19YWbm1sDRVQ/npXtJJKDZ+Xz9qxspyHVOEHZvXs3jh07Bj8/PwwfPrzcBCUrKwsbNmyQypRKpc4thMPCwpCRkYH169dDo9FgwoQJ6Natm97jp4no2bJ27VqsXbtWuh+Er68v5s6dK92j4/79+3j77bfx9ddfo7i4GKGhoVizZg1cXFykZVy7dg1TpkzBoUOHYGNjg4iICMTFxcHMjGe0iYxJjT+xYWFhUmdREaVSCVdX13LnXbp0CXv27MHJkyelcdSrVq3CwIEDsXTpUri7u9c0JCJ6SjRt2hQLFy5Eq1atIITApk2bMGTIEJw5cwa+vr6YPn06du7cia1bt8LOzg5RUVEYPny49GyT0tJShIeHw9XVFcePH0dGRgbGjRsHc3NzfPjhhwbeOiKqiScaZqxQKMo9grJt2zZYWFigcePG6NevHxYsWAAHBwcAwGeffYa3335b5yZXDx48gKWlJbZu3VruExaLi4t17tyn1WqRk5MDBwcH3u6bqI4JIXD37l24u7vX+plCdcne3h5LlizBiy++CCcnJ2zevBkvvvgigIe3Am/bti0SExPRo0cP7N69Gy+88AJu3rwpHVVZt24dZs6cidu3b8PCwqLK9Wm1Wty8eRO2trbsX4jqWE36lzo/5jlgwAAMHz4c3t7euHLlCmbPno2wsDAkJibC1NQUmZmZcHZ21g3CzAz29vbIzMwsd5lxcXE1fqgSET2ZP/74Q++pzA2ptLQUW7duRWFhIQICApCcnAyNRqPzfBgfHx94enpKCUpiYiI6dOigc8onNDQUU6ZMwYULF9ClSxe99Tz+A+jGjRto165d/W4c0TOuOv1LnScoo0ePlv7foUMHdOzYES1atMDhw4cRFBRUq2XOmjUL0dHR0nReXh48PT2Rnp4u3SpYrjQaDQ4dOoS+ffvC3Nzc0OEYLbZj3amqLe/evQtvb2+DfbZ+/vlnBAQE4P79+7CxscH333+Pdu3aISUlBRYWFjoPSQMAFxcX6cdNZmamTnJSNr9sXnkq+gH0ySefwNraug62iIjKFBUVYeLEidXqX+r9qrHmzZvD0dERaWlpCAoKgqurK27duqVT58GDB8jJyanwuhWlUgmlUqlXbm9vLz1ESa40Gg2sra3h4ODAL9YnwHasO1W1ZVmZoU5vtGnTBikpKcjLy8O3336LiIgIHDlypN7W9/gPoPz8fHh4eGDo0KEV9i8ajQbx8fHo378/98daYPs9OWNtw/z8fEycOLFa/Uu9JyjXr19Hdna2NNQqICAAubm5SE5Ohp+fHwDg4MGD0Gq18Pf3r+9wiEjmLCws0LJlSwAPH7J48uRJfPTRR9Kj6h9/1HxWVpb048bV1RU//fSTzvKysrKkeeWp6AeQubl5lR1/depQxdh+T87Y2rAmsdb4CriCggKkpKQgJSUFAJCeno6UlBRcu3YNBQUFmDFjBk6cOIGrV6/iwIEDGDJkCFq2bCk9br5t27YYMGAAJk2ahJ9++gnHjh1DVFQURo8ezRE8RKRHq9WiuLgYfn5+MDc3x4EDB6R5qampuHbtGgICAgA8/AH0888/6xyljY+Ph0ql4nUlREamxkdQTp06hb59+0rTZYdGIyIisHbtWpw7dw6bNm1Cbm4u3N3dERISgn/84x86v1C+/PJLREVFISgoSLpR28qVK+tgc4jImM2aNQthYWHw9PTE3bt3sXnzZhw+fBh79+6FnZ0dIiMjER0dLZ3effPNNxEQEIAePXoAePg4+nbt2uGVV17B4sWLkZmZiffffx9Tp04t9ygJEclXjROUPn36oLKRyXv37q1yGfb29rwpGxHpuXXrFsaNG4eMjAzY2dmhY8eO2Lt3L/r37w8AWL58ufSj5tEbtZUxNTXFjh07MGXKFAQEBKBRo0aIiIjA/PnzDbVJRFRLT/2tFZu9t7Pc8qsLwxs4EiKqyqefflrpfEtLS6xevRqrV6+usI6Xlxd27dpV16HJFvs4eloZ/i5MRERERI9hgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHTNDB0BERHWv2Xs7K5x3dWF4A0ZCVDs8gkJERESywwSFiIiIZIcJChEREckOExQiIiKSnWf2IlleQEZERCRfPIJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZeWZv1CZ3Fd1IjjeRIyKiZwGPoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkhwkKERERyQ4TFCIiIpKdGicoCQkJGDRoENzd3aFQKLBt2zad+UIIzJ07F25ubrCyskJwcDAuX76sUycnJwdjx46FSqWCWq1GZGQkCgoKnmhDiMi4xcXFoVu3brC1tYWzszOGDh2K1NRUnTr379/H1KlT4eDgABsbG4wYMQJZWVk6da5du4bw8HBYW1vD2dkZM2bMwIMHDxpyU+pcs/d2VvgielrVOEEpLCxEp06dsHr16nLnL168GCtXrsS6deuQlJSERo0aITQ0FPfv35fqjB07FhcuXEB8fDx27NiBhIQETJ48ufZbQURG78iRI5g6dSpOnDiB+Ph4aDQahISEoLCwUKozffp0/PDDD9i6dSuOHDmCmzdvYvjw4dL80tJShIeHo6SkBMePH8emTZuwceNGzJ071xCbRERPoMY3agsLC0NYWFi584QQWLFiBd5//30MGTIEAPD555/DxcUF27Ztw+jRo3Hp0iXs2bMHJ0+eRNeuXQEAq1atwsCBA7F06VK4u7vrLbe4uBjFxcXSdH5+PgBAo9FAo9FUGq/SVNR0E6tcZm2WVdNlVhR3XcZmTGrbjqSvqrY0VBvv2bNHZ3rjxo1wdnZGcnIyevfujby8PHz66afYvHkz+vXrBwDYsGED2rZtixMnTqBHjx7Yt28fLl68iP3798PFxQWdO3fGP/7xD8ycORMxMTGwsLAwxKYRUS3U6Z1k09PTkZmZieDgYKnMzs4O/v7+SExMxOjRo5GYmAi1Wi0lJwAQHBwMExMTJCUlYdiwYXrLjYuLQ2xsrF75vn37YG1tXWlMi7vXfDt27dpV8zdVIT4+vkb1K4q7PmIzJjVtR6pYRW1ZVFTUwJGULy8vDwBgb28PAEhOToZGo9HpX3x8fODp6YnExET06NEDiYmJ6NChA1xcXKQ6oaGhmDJlCi5cuIAuXbrorac2P4AaOmGuzQ+tyhg60ecPjidnrG1Yk3jrNEHJzMwEAJ3OoWy6bF5mZiacnZ11gzAzg729vVTncbNmzUJ0dLQ0nZ+fDw8PD4SEhEClUlUaU/uYvTXejvMxoTV+T0U0Gg3i4+PRv39/mJubV/t9FcVdl7EZk9q2I+mrqi3LvqANSavVYtq0aejZsyfat28P4GHfYWFhAbVarVP38f6lvP6nbF55nuQHUEMlzLX5oVUZufzQ4Q+OJ2dsbViTH0BG8SwepVIJpVKpV25ubl7ll1VxqaLG66uPL8DqxPqoiuJ+1r+ca9qOVLGK2lIO7Tt16lScP38eP/74Y72vqzY/gBo6Ya7ND63KGPqHDn9wPDljbcOa/ACq0wTF1dUVAJCVlQU3NzepPCsrC507d5bq3Lp1S+d9Dx48QE5OjvR+Inp2RUVFSRfPN23aVCp3dXVFSUkJcnNzdY6iZGVlSX2Hq6srfvrpJ53llY3yqah/eZIfQA2VMNfmh1Zl5PKFxh8cT87Y2rAmsdbpfVC8vb3h6uqKAwcOSGX5+flISkpCQEAAACAgIAC5ublITk6W6hw8eBBarRb+/v51GQ4RGREhBKKiovD999/j4MGD8Pb21pnv5+cHc3Nznf4lNTUV165d0+lffv75Z50fQfHx8VCpVGjXrl3DbAgR1YkaH0EpKChAWlqaNJ2eno6UlBTY29vD09MT06ZNw4IFC9CqVSt4e3tjzpw5cHd3x9ChQwEAbdu2xYABAzBp0iSsW7cOGo0GUVFRGD16dLkjeIjo2TB16lRs3rwZ27dvh62trXTNiJ2dHaysrGBnZ4fIyEhER0fD3t4eKpUKb775JgICAtCjRw8AQEhICNq1a4dXXnkFixcvRmZmJt5//31MnTq13KMkRCRfNU5QTp06hb59+0rTZeduIyIisHHjRrz77rsoLCzE5MmTkZubi169emHPnj2wtLSU3vPll18iKioKQUFBMDExwYgRI7By5co62BwiMlZr164FAPTp00enfMOGDRg/fjwAYPny5VKfUVxcjNDQUKxZs0aqa2pqih07dmDKlCkICAhAo0aNEBERgfnz5zfUZhBRHalxgtKnTx8IUfGQN4VCgfnz51faIdjb22Pz5s01XTURPcUq61fKWFpaYvXq1RXeKBIAvLy8ZDNKhYhqj8/iISIiItlhgkJERESywwSFiIiIZMcobtT2tOKTSImIiMrHIyhEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2eFFskamsgtrry4Mr7P3ENHTi30CGQMeQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkezwRm3l4E2MiIiIDIsJSh2pKKlRmgos7t7AwRARERk5nuIhIiIi2WGCQkRERLLDUzxPkcqunSEiIjImTFAaSPuYvSguVRg6DD0VJTW8GJiIiAyJp3iIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDocZ1xDvNUJERFT/eASFiIiIZIcJChEREckOT/FQuSo7lcW7zBIRUX3jERQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSnTpPUGJiYqBQKHRePj4+0vz79+9j6tSpcHBwgI2NDUaMGIGsrKy6DoOIjFBCQgIGDRoEd3d3KBQKbNu2TWe+EAJz586Fm5sbrKysEBwcjMuXL+vUycnJwdixY6FSqaBWqxEZGYmCgoIG3Aoiqgv1cgTF19cXGRkZ0uvHH3+U5k2fPh0//PADtm7diiNHjuDmzZsYPnx4fYRBREamsLAQnTp1wurVq8udv3jxYqxcuRLr1q1DUlISGjVqhNDQUNy/f1+qM3bsWFy4cAHx8fHYsWMHEhISMHny5IbaBCKqI/XyLB4zMzO4urrqlefl5eHTTz/F5s2b0a9fPwDAhg0b0LZtW5w4cQI9evSoj3CIyEiEhYUhLCys3HlCCKxYsQLvv/8+hgwZAgD4/PPP4eLigm3btmH06NG4dOkS9uzZg5MnT6Jr164AgFWrVmHgwIFYunQp3N3dG2xbiOjJ1EuCcvnyZbi7u8PS0hIBAQGIi4uDp6cnkpOTodFoEBwcLNX18fGBp6cnEhMTK0xQiouLUVxcLE3n5+cDADQaDTQaTaWxKE1FHWxR7SlNhM6/T4Oq2rw+12mIdT9tqmpLubZxeno6MjMzdfoPOzs7+Pv7IzExEaNHj0ZiYiLUarWUnABAcHAwTExMkJSUhGHDhukttzb9S0Pvjw3ZjzXENvHz/OSMtQ1rEm+dJyj+/v7YuHEj2rRpg4yMDMTGxiIwMBDnz59HZmYmLCwsoFardd7j4uKCzMzMCpcZFxeH2NhYvfJ9+/bB2tq60ngWd6/VZtS5f3TVGjqEOrNr1y6DrTs+Pt5g637aVNSWRUVFDRxJ9ZT1ES4uLjrlj/YfmZmZcHZ21plvZmYGe3v7CvuYJ+lfGmp/bMh+rCE/3/w8Pzlja8Oa9C91nqA8eni2Y8eO8Pf3h5eXF7Zs2QIrK6taLXPWrFmIjo6WpvPz8+Hh4YGQkBCoVKpK39s+Zm+t1llXlCYC/+iqxZxTJijWKgwaS105HxPa4OvUaDSIj49H//79YW5u3uDrf5pU1ZZlRxCeFbXpXxp6fzR0PwbU7eeen+cnZ6xtWJP+pV5O8TxKrVajdevWSEtLQ//+/VFSUoLc3FydoyhZWVnlXrNSRqlUQqlU6pWbm5tX+YcpLpVHUlCsVcgmlidlyA9Ddf7mVD0VtaVc27esj8jKyoKbm5tUnpWVhc6dO0t1bt26pfO+Bw8eICcnp8I+5kn6l4baH+XQd9THdvLz/OSMrQ1rEmu93weloKAAV65cgZubG/z8/GBubo4DBw5I81NTU3Ht2jUEBATUdyhEZMS8vb3h6uqq03/k5+cjKSlJ6j8CAgKQm5uL5ORkqc7Bgweh1Wrh7+/f4DETUe3V+RGUd955B4MGDYKXlxdu3ryJefPmwdTUFGPGjIGdnR0iIyMRHR0Ne3t7qFQqvPnmmwgICOAIHiJCQUEB0tLSpOn09HSkpKTA3t4enp6emDZtGhYsWIBWrVrB29sbc+bMgbu7O4YOHQoAaNu2LQYMGIBJkyZh3bp10Gg0iIqKwujRozmCh8jI1HmCcv36dYwZMwbZ2dlwcnJCr169cOLECTg5OQEAli9fDhMTE4wYMQLFxcUIDQ3FmjVr6joMIjJCp06dQt++faXpsmtDIiIisHHjRrz77rsoLCzE5MmTkZubi169emHPnj2wtLSU3vPll18iKioKQUFBUl+zcuXKBt8WInoydZ6gfP3115XOt7S0xOrVqyu8ERMRPbv69OkDISoeUqtQKDB//nzMnz+/wjr29vbYvHlzfYRHRA2Iz+IhIiIi2an3UTz07Gj23s4K511dGN6AkRARkbHjERQiIiKSHSYoREREJDs8xUM1VtmpHCIiorrAIyhEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2eFFskREMsEL0In+D4+gEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHF8mSQfEBg0REVB4eQSEiIiLZYYJCREREssNTPNQgeH8HIiKqCR5BISIiItlhgkJERESyw1M8RERUJY64o4bGIyhEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2eFFskRE9EQquoCWF8/Sk2CCQkRUS+1j9qK4VFHuPH45Ez0ZnuIhIiIi2eERFCKiBsTHPhBVD4+gEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0OMybZenQ4ptJUYHH3/7sxFm+CRXLH4cRET4YJCj1VKvtSYFJDRGQ8eIqHiIiIZIdHUMgo8fA5kfEr71lGPNJJZZigEBFRvajoh0TZNWVEleEpHiIiIpIdHkGhZx4vrCUikh8eQSEiIiLZMegRlNWrV2PJkiXIzMxEp06dsGrVKnTvzhOTJH886iJ/7F+MEz9bVMZgCco333yD6OhorFu3Dv7+/lixYgVCQ0ORmpoKZ2dnQ4VFpKMuRwux42047F+IjJ/BEpRly5Zh0qRJmDBhAgBg3bp12LlzJz777DO89957OnWLi4tRXFwsTefl5QEAcnJyoNFoKl2P2YPCOo68Zsy0AkVFWphpTFCqVVT9BipXXbRjy3e2lL/sJwmsHNnZ2eWvp5J9saL31IZ/3IEK5yXNCoJGo0FRURGys7Nhbm6uV+fu3bsAACFEncXU0Oq7fylrQ36ua6e2n+eKPieV7fMVSZoVVOP3yElVn2O5qlH/IgyguLhYmJqaiu+//16nfNy4cWLw4MF69efNmycA8MUXXw34+uOPPxqoR6hb7F/44kv+r+r0LwY5gnLnzh2UlpbCxcVFp9zFxQW//PKLXv1Zs2YhOjpamtZqtcjJyYGDgwMUCnn/esnPz4eHhwf++OMPqFQqQ4djtNiOdaeqthRC4O7du3B3dzdAdE+uIfoX7o9Phu335Iy1DWvSvxjFMGOlUgmlUqlTplarDRNMLalUKqPaieSK7Vh3KmtLOzu7Bo7GcJ6kf+H++GTYfk/OGNuwuv2LQYYZOzo6wtTUFFlZWTrlWVlZcHV1NURIRPSUYP9C9HQwSIJiYWEBPz8/HDjwfxc2abVaHDhwAAEBAYYIiYieEuxfiJ4OBjvFEx0djYiICHTt2hXdu3fHihUrUFhYKF11/7RQKpWYN2+e3iFkqhm2Y915FtqyvvuXZ6EN6xPb78k9C22oEMJwYwn/9a9/STdS6ty5M1auXAl/f39DhUNETxH2L0TGzaAJChEREVF5+CweIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQakDMTExUCgUOi8fHx9p/v379zF16lQ4ODjAxsYGI0aM0LuJ1LMqISEBgwYNgru7OxQKBbZt26YzXwiBuXPnws3NDVZWVggODsbly5d16uTk5GDs2LFQqVRQq9WIjIxEQUFBA26F4VXVjuPHj9fbRwcMGKBTh+1YfatXr0azZs1gaWkJf39//PTTT4YOqd411Gf13LlzCAwMhKWlJTw8PLB48WK9WLZu3QofHx9YWlqiQ4cO2LVrV51vb12Li4tDt27dYGtrC2dnZwwdOhSpqak6darzXXHt2jWEh4fD2toazs7OmDFjBh48eKBT5/Dhw3juueegVCrRsmVLbNy4US8eo9iHn/TBXPTwYWO+vr4iIyNDet2+fVua//rrrwsPDw9x4MABcerUKdGjRw/xl7/8xYARy8euXbvE3//+d/Hf//5XANB7wNvChQuFnZ2d2LZtmzh79qwYPHiw8Pb2Fvfu3ZPqDBgwQHTq1EmcOHFCHD16VLRs2VKMGTOmgbfEsKpqx4iICDFgwACdfTQnJ0enDtuxer7++mthYWEhPvvsM3HhwgUxadIkoVarRVZWlqFDq1cN8VnNy8sTLi4uYuzYseL8+fPiq6++ElZWVmL9+vVSnWPHjglTU1OxePFicfHiRfH+++8Lc3Nz8fPPP9d7GzyJ0NBQsWHDBnH+/HmRkpIiBg4cKDw9PUVBQYFUp6rvigcPHoj27duL4OBgcebMGbFr1y7h6OgoZs2aJdX57bffhLW1tYiOjhYXL14Uq1atEqampmLPnj1SHWPZh5mg1IF58+aJTp06lTsvNzdXmJubi61bt0plly5dEgBEYmJiA0VoHB7v9LRarXB1dRVLliyRynJzc4VSqRRfffWVEEKIixcvCgDi5MmTUp3du3cLhUIhbty40WCxy0lFCcqQIUMqfA/bsfq6d+8upk6dKk2XlpYKd3d3ERcXZ8CoGlZ9fVbXrFkjGjduLIqLi6U6M2fOFG3atJGmR44cKcLDw3Xi8ff3F6+99lqdbmN9u3XrlgAgjhw5IoSo3nfFrl27hImJicjMzJTqrF27VqhUKqnN3n33XeHr66uzrlGjRonQ0FBp2lj2YZ7iqSOXL1+Gu7s7mjdvjrFjx+LatWsAgOTkZGg0GgQHB0t1fXx84OnpicTEREOFaxTS09ORmZmp03Z2dnbw9/eX2i4xMRFqtRpdu3aV6gQHB8PExARJSUkNHrOcHT58GM7OzmjTpg2mTJmC7OxsaR7bsXpKSkqQnJyss0+amJggODj4mf4819VnNTExEb1794aFhYVUJzQ0FKmpqfjzzz+lOo+up6yOsbV/Xl4eAMDe3h5A9b4rEhMT0aFDB50ndYeGhiI/Px8XLlyQ6lTWPsa0DzNBqQP+/v7YuHEj9uzZg7Vr1yI9PR2BgYG4e/cuMjMzYWFhofd0VBcXF2RmZhomYCNR1j6PfhjLpsvmZWZmwtnZWWe+mZkZ7O3t2b6PGDBgAD7//HMcOHAAixYtwpEjRxAWFobS0lIAbMfqunPnDkpLSyvdJ59FdfVZzczMLHcZj66jojrG1P5arRbTpk1Dz5490b59ewCo1nfFk7RPfn4+7t27Z1T7sMGexfM0CQsLk/7fsWNH+Pv7w8vLC1u2bIGVlZUBIyN6aPTo0dL/O3TogI4dO6JFixY4fPgwgoKCDBgZ0bNn6tSpOH/+PH788UdDhyJrPIJSD9RqNVq3bo20tDS4urqipKQEubm5OnX46PeqlbXP41exP9p2rq6uuHXrls78Bw8eICcnh+1biebNm8PR0RFpaWkA2I7V5ejoCFNT00r3yWdRXX1WXV1dy13Go+uoqI6xtH9UVBR27NiBQ4cOoWnTplJ5db4rnqR9VCoVrKysjGofZoJSDwoKCnDlyhW4ubnBz88P5ubmOo9+T01NxbVr1/jo9yp4e3vD1dVVp+3y8/ORlJQktV1AQAByc3ORnJws1Tl48CC0Wi0fDFeJ69evIzs7G25ubgDYjtVlYWEBPz8/nX1Sq9XiwIEDz/Tnua4+qwEBAUhISIBGo5HqxMfHo02bNmjcuLFU59H1lNWRe/sLIRAVFYXvv/8eBw8ehLe3t8786nxXBAQE4Oeff9ZJ9OLj46FSqdCuXTupTmXtY1T7sKGv0n0avP322+Lw4cMiPT1dHDt2TAQHBwtHR0dx69YtIcTDoWOenp7i4MGD4tSpUyIgIEAEBAQYOGp5uHv3rjhz5ow4c+aMACCWLVsmzpw5I37//XchxMOhi2q1Wmzfvl2cO3dODBkypNyhi126dBFJSUnixx9/FK1atXrmhsdW1o53794V77zzjkhMTBTp6eli//794rnnnhOtWrUS9+/fl5bBdqyer7/+WiiVSrFx40Zx8eJFMXnyZKFWq3VGVjyNGuKzmpubK1xcXMQrr7wizp8/L77++mthbW2tN8zYzMxMLF26VFy6dEnMmzfPKIYZT5kyRdjZ2YnDhw/rDPcvKiqS6lT1XVE2zDgkJESkpKSIPXv2CCcnp3KHGc+YMUNcunRJrF69utxhxsawDzNBqQOjRo0Sbm5uwsLCQjRp0kSMGjVKpKWlSfPv3bsn3njjDdG4cWNhbW0thg0bJjIyMgwYsXwcOnRIANB7RURECCEeDl+cM2eOcHFxEUqlUgQFBYnU1FSdZWRnZ4sxY8YIGxsboVKpxIQJE8Tdu3cNsDWGU1k7FhUViZCQEOHk5CTMzc2Fl5eXmDRpkl5nxHasvlWrVglPT09hYWEhunfvLk6cOGHokOpdQ31Wz549K3r16iWUSqVo0qSJWLhwoV4sW7ZsEa1btxYWFhbC19dX7Ny5s962u66U13YAxIYNG6Q61fmuuHr1qggLCxNWVlbC0dFRvP3220Kj0ejUOXTokOjcubOwsLAQzZs311lHGWPYhxVCCNFwx2uIiIiIqsZrUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSljh0+fBgKhQLffvutoUORrWbNmmH8+PGGDsNoXb16FQqFotxHqBMRPS2YoBCV4+LFi4iJicHVq1cNFsPmzZuxYsUKg62fiMiQmKAQlePixYuIjY2VZYLi5eWFe/fu4ZVXXmn4oIiIGggTFCNVWFho6BAqJOfY6oMQAvfu3Wuw9SkUClhaWsLU1LTB1klE1NCMPkGJiYmBQqFAWloaxo8fD7VaDTs7O0yYMAFFRUUAKj9nr1AoEBMTo7e8X3/9FX/9619hZ2cHJycnzJkzB0II/PHHHxgyZAhUKhVcXV3xz3/+s9y4SktLMXv2bLi6uqJRo0YYPHgw/vjjD716SUlJGDBgAOzs7GBtbY3nn38ex44dK3cbL168iJdffhmNGzdGr169qmwbIQQcHR0RHR0tlWm1WqjVapiamuo8lGrRokUwMzNDQUGBVHbw4EEEBgaiUaNGUKvVGDJkCC5dulTt2IQQWLBgAZo2bQpra2v07dsXFy5c0ItTo9EgNjYWrVq1gqWlJRwcHNCrVy/Ex8dXuY2PKi4uxrx589CyZUsolUp4eHjg3XffRXFxsVQnIiIClpaWetsRGhqKxo0b4+bNm9i4cSNeeuklAEDfvn2hUCigUChw+PBhAA+voXnhhRewd+9edO3aFVZWVli/fj0AYMOGDejXrx+cnZ2hVCrRrl07rF27ttx4d+/ejeeffx62trZQqVTo1q0bNm/eDADo06cPdu7cid9//11af7NmzQDo789Lly6FQqHA77//rreOWbNmwcLCAn/++adUVp19jojI0Iw+QSkzcuRI3L17F3FxcRg5ciQ2btyI2NjYWi9v1KhR0Gq1WLhwIfz9/bFgwQKsWLEC/fv3R5MmTbBo0SK0bNkS77zzDhISEvTe/8EHH2Dnzp2YOXMm3nrrLcTHxyM4OFjnl/bBgwfRu3dv5OfnY968efjwww+Rm5uLfv364aefftJb5ksvvYSioiJ8+OGHmDRpUpXboFAo0LNnT534zp07h7y8PADQ+VI6evQounTpAhsbGwDA/v37ERoailu3biEmJgbR0dE4fvw4evbsWe5pj/Jimzt3LubMmYNOnTphyZIlaN68OUJCQvSOsMTExCA2NhZ9+/bFv/71L/z973+Hp6cnTp8+XeU2ltFqtRg8eDCWLl2KQYMGYdWqVRg6dCiWL1+OUaNGSfU++ugjODk5ISIiAqWlpQCA9evXY9++fVi1ahXc3d3Ru3dvvPXWWwCA2bNn44svvsAXX3yBtm3bSstJTU3FmDFj0L9/f3z00Ufo3LkzAGDt2rXw8vLC7Nmz8c9//hMeHh544403sHr1ap14N27ciPDwcOTk5GDWrFlYuHAhOnfujD179gAA/v73v6Nz585wdHSU1l/R9SgjR46EQqHAli1b9OZt2bIFISEh0oPWarrPEREZjCHvs18X5s2bJwCIV199Vad82LBhwsHBQQghRHp6ut4zD8oAEPPmzdNb3uTJk6WyBw8eiKZNmwqFQqHzXIg///xTWFlZSc+iEOL/nlfRpEkTkZ+fL5Vv2bJFABAfffSREOLhcytatWolQkNDhVarleoVFRUJb29v0b9/f72YavPgtiVLlghTU1MplpUrVwovLy/RvXt3MXPmTCGEEKWlpUKtVovp06dL7+vcubNwdnYW2dnZUtnZs2eFiYmJGDduXJWx3bp1S1hYWIjw8HCd7Zs9e7bO8zuEEKJTp04iPDy8xtv2qC+++EKYmJiIo0eP6pSvW7dOABDHjh2Tyvbu3SsAiAULFojffvtN2NjYiKFDh+q8b+vWrQKAOHTokN66vLy8BACdh2+VefTBX2VCQ0NF8+bNpenc3Fxha2sr/P39dR6kJoTQaavw8HDh5eWlt7zy9ueAgADh5+enU++nn34SAMTnn38uLbu6+xwRkaE9NUdQXn/9dZ3pwMBAZGdnIz8/v1bLmzhxovR/U1NTdO3aFUIIREZGSuVqtRpt2rTBb7/9pvf+cePGwdbWVpp+8cUX4ebmhl27dgEAUlJScPnyZbz88svIzs7GnTt3cOfOHRQWFiIoKAgJCQnQarWVbmN1BAYGorS0FMePHwfw8EhJYGAgAgMDcfToUQDA+fPnkZubi8DAQABARkYGUlJSMH78eNjb20vL6tixI/r37y9tQ2Wx7d+/HyUlJXjzzTehUCik8mnTpum9V61W48KFC7h8+XKNt6/M1q1b0bZtW/j4+EhteefOHfTr1w8AcOjQIaluSEgIXnvtNcyfPx/Dhw+HpaWldIqmury9vREaGqpXbmVlJf0/Ly8Pd+7cwfPPP4/ffvtNOnIVHx+Pu3fv4r333oOlpaXO+x9tq5oYNWoUkpOTceXKFansm2++gVKpxJAhQwDUbp8jIjKUpyZB8fT01JkuO6T96Ln3J1menZ0dLC0t4ejoqFde3jpatWqlM61QKNCyZUvp9EjZl3FERAScnJx0Xp988gmKi4ulL7Qy3t7eNd6O5557DtbW1lIyUpag9O7dG6dOncL9+/eleWXXjpRdy9CmTRu95bVt21b6UqsstrJlPN4OTk5O0t+mzPz585Gbm4vWrVujQ4cOmDFjBs6dO1ej7bx8+TIuXLig15atW7cGANy6dUun/tKlS2Fvb4+UlBSsXLkSzs7ONVpfRX+LY8eOITg4WLpux8nJCbNnzwYA6e9ZlkS0b9++RuuszEsvvQQTExN88803AB5e/7N161aEhYVBpVIBqN0+R0RkKGaGDqCuVDSiQQhR4a/SsmsQqru8ytZRU2W/VJcsWSJdv/C4sutByjz667y6zM3N4e/vj4SEBKSlpSEzMxOBgYFwcXGBRqNBUlISjh49Ch8fHzg5OdV4+U8SW5nevXvjypUr2L59O/bt24dPPvkEy5cvx7p163SOZFVGq9WiQ4cOWLZsWbnzPTw8dKbPnDkjJS0///wzxowZU6OYy9veK1euICgoCD4+Pli2bBk8PDxgYWGBXbt2Yfny5fV6dMLd3R2BgYHYsmULZs+ejRMnTuDatWtYtGiRVKc2+xwRkaE8NQlKZcp+sT86agVAuaMe6srjpyuEEEhLS0PHjh0BAC1atAAAqFQqBAcH11scwMPTPIsWLcL+/fvh6OgIHx8fKBQK+Pr64ujRozh69CheeOEFqb6XlxeAhxeCPu6XX36Bo6MjGjVqVOk6y5Zx+fJlNG/eXCq/fft2uUec7O3tMWHCBEyYMAEFBQXo3bs3YmJiqp2gtGjRAmfPnkVQUFCVp0kKCwsxYcIEtGvXDn/5y1+wePFiDBs2DN26dZPq1OZUyw8//IDi4mL873//0zkC9+jppbJYgYen1lq2bFnh8moaw6hRo/DGG28gNTUV33zzDaytrTFo0CC99TbEPkdE9KSemlM8lVGpVHB0dNQbbbNmzZp6W+fnn3+Ou3fvStPffvstMjIyEBYWBgDw8/NDixYtsHTpUp2hvWVu375dZ7EEBgaiuLgYK1asQK9evaQvvsDAQHzxxRe4efOmdP0JALi5uaFz587YtGmTTlJ3/vx57Nu3DwMHDqxyncHBwTA3N8eqVat0jjCVNxIlOztbZ9rGxgYtW7bUGR5clZEjR+LGjRv4+OOP9ebdu3dP55TUzJkzce3aNWzatAnLli1Ds2bNEBERobO+sgTs8aS2MmVH2B7d3ry8PGzYsEGnXkhICGxtbREXF4f79+/rzHv0vY0aNarRKZcRI0bA1NQUX331FbZu3YoXXnhBJ5FsyH2OiOhJPRNHUICHF70uXLgQEydORNeuXZGQkIBff/213tZnb2+PXr16YcKECcjKysKKFSvQsmVLaQiuiYkJPvnkE4SFhcHX1xcTJkxAkyZNcOPGDRw6dAgqlQo//PBDncQSEBAAMzMzpKamYvLkyVJ57969pXt0PJqgAA9PA4SFhSEgIACRkZG4d+8eVq1aBTs7O537xlTEyckJ77zzDuLi4vDCCy9g4MCBOHPmDHbv3q13HU+7du3Qp08f+Pn5wd7eHqdOncK3336LqKioam/jK6+8gi1btuD111/HoUOH0LNnT5SWluKXX37Bli1bpHuWHDx4EGvWrMG8efPw3HPPAXh475I+ffpgzpw5WLx4MQCgc+fOMDU1xaJFi5CXlwelUind36QiISEhsLCwwKBBg/Daa6+hoKAAH3/8MZydnZGRkSHVU6lUWL58OSZOnIhu3bpJ9485e/YsioqKsGnTJgAPE4pvvvkG0dHR6NatG2xsbHSOiDzO2dkZffv2xbJly3D37l2d4dVAw+5zRERPzHADiOpG2TDX27dv65Rv2LBBABDp6elCiIdDKSMjI4WdnZ2wtbUVI0eOFLdu3apwmPHjy4uIiBCNGjXSW//zzz8vfH19pemyYcZfffWVmDVrlnB2dhZWVlYiPDxc/P7773rvP3PmjBg+fLhwcHAQSqVSeHl5iZEjR4oDBw5UGVNNdOvWTQAQSUlJUtn169cFAOHh4VHue/bv3y969uwprKyshEqlEoMGDRIXL17UqVNZbKWlpSI2Nla4ubkJKysr0adPH3H+/Hnh5eWlM8x4wYIFonv37kKtVgsrKyvh4+MjPvjgA1FSUlKjbSwpKRGLFi0Svr6+QqlUisaNGws/Pz8RGxsr8vLyRH5+vvDy8hLPPfec0Gg0Ou+dPn26MDExEYmJiVLZxx9/LJo3by5MTU11hhx7eXlVOCz6f//7n+jYsaOwtLQUzZo1E4sWLRKfffaZzr74aN2//OUvUvt2795dfPXVV9L8goIC8fLLLwu1Wi0ASEOOKxs2//HHHwsAwtbWVm8Ic5nq7HNERIamEKIWV3gSERER1aNn4hoUIiIiMi7PzDUoT5uSkhLk5ORUWsfOzu6Jhv/KwbOynUREpIsJipE6fvw4+vbtW2mdDRs2YPz48Q0TUD15VraTiIh08RoUI/Xnn38iOTm50jq+vr5wc3NroIjqx7OynUREpIsJChEREckOL5IlIiIi2THKa1C0Wi1u3rwJW1vbWj/9lYjKJ4TA3bt34e7uDhMT/oYhIsMwygTl5s2beg9/I6K69ccff6Bp06aGDoOInlFGmaDY2toCeNiBlj1KvqFpNBrs27cPISEhMDc3N0gMNWWMMQOMuyFpNBps27YNEydOlD5nRESGYJQJStlpHZVKZdAExdraGiqVyqi+fIwtZoBxN6SymIHaPdGZiKiu8AQzERERyQ4TFCIiIpIdJihEREQkO0xQiIiISHaM8iJZMqxm7+0st/zqwvAGjoSIiJ5WPIJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkezUOEFJSEjAoEGD4O7uDoVCgW3btunMHz9+PBQKhc5rwIABOnVycnIwduxYqFQqqNVqREZGoqCg4Ik2hIiIiJ4eNU5QCgsL0alTJ6xevbrCOgMGDEBGRob0+uqrr3Tmjx07FhcuXEB8fDx27NiBhIQETJ48uebRExER0VPJrKZvCAsLQ1hYWKV1lEolXF1dy5136dIl7NmzBydPnkTXrl0BAKtWrcLAgQOxdOlSuLu71zQkIiIiesrUOEGpjsOHD8PZ2RmNGzdGv379sGDBAjg4OAAAEhMToVarpeQEAIKDg2FiYoKkpCQMGzZMb3nFxcUoLi6WpvPz8wEAGo0GGo2mPjahSmXrNdT6a6OuYlaaikqXX9eMsa0B44zbmGIloqdbnScoAwYMwPDhw+Ht7Y0rV65g9uzZCAsLQ2JiIkxNTZGZmQlnZ2fdIMzMYG9vj8zMzHKXGRcXh9jYWL3yffv2wdrauq43oUbi4+MNuv7aeNKYF3cvv3zXrl1PtNyqGGNbA8YbNxGRIdV5gjJ69Gjp/x06dEDHjh3RokULHD58GEFBQbVa5qxZsxAdHS1N5+fnw8PDAyEhIVCpVE8cc21oNBrEx8ejf//+MDc3N0gMNVVezO1j9tbZ8s/HhNbZsh5ljG0NGGfcGo0G27dvN3QYRET1c4rnUc2bN4ejoyPS0tIQFBQEV1dX3Lp1S6fOgwcPkJOTU+F1K0qlEkqlUq/c3Nzc4B2/HGKoqUdjLi5V1Oly65MxtjVgvHETERlSvd8H5fr168jOzoabmxsAICAgALm5uUhOTpbqHDx4EFqtFv7+/vUdDhERERmBGh9BKSgoQFpamjSdnp6OlJQU2Nvbw97eHrGxsRgxYgRcXV1x5coVvPvuu2jZsiVCQx8e/m/bti0GDBiASZMmYd26ddBoNIiKisLo0aM5goeIiIgA1OIIyqlTp9ClSxd06dIFABAdHY0uXbpg7ty5MDU1xblz5zB48GC0bt0akZGR8PPzw9GjR3VO0Xz55Zfw8fFBUFAQBg4ciF69euHf//533W0VERERGbUaH0Hp06cPhCh/mCkA7N1b9UWX9vb22Lx5c01XTURERM8IPouHiIiIZIcJChEREclOvQ8zpmdHs/d2Vjjv6sLwBoyEiIiMHY+gEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7ZoYOgJ4Nzd7bWW751YXhDRwJEREZAx5BISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOHxb4DCh7UJ/SVGBxd6B9zF4UlyoMHBUREVHFeASFiIiIZKfGCUpCQgIGDRoEd3d3KBQKbNu2TWe+EAJz586Fm5sbrKysEBwcjMuXL+vUycnJwdixY6FSqaBWqxEZGYmCgoIn2hAiIiJ6etQ4QSksLESnTp2wevXqcucvXrwYK1euxLp165CUlIRGjRohNDQU9+/fl+qMHTsWFy5cQHx8PHbs2IGEhARMnjy59ltBRERET5UaX4MSFhaGsLCwcucJIbBixQq8//77GDJkCADg888/h4uLC7Zt24bRo0fj0qVL2LNnD06ePImuXbsCAFatWoWBAwdi6dKlcHd3f4LNISIioqdBnV4km56ejszMTAQHB0tldnZ28Pf3R2JiIkaPHo3ExESo1WopOQGA4OBgmJiYICkpCcOGDdNbbnFxMYqLi6Xp/Px8AIBGo4FGo6nLTai2svUaav01oTQVD/810f1XDqrTfsbU1o8yxriNKVYierrVaYKSmZkJAHBxcdEpd3FxkeZlZmbC2dlZNwgzM9jb20t1HhcXF4fY2Fi98n379sHa2rouQq+1+Ph4g66/OhZ3153+R1etYQIpx65du6pd1xjaujzGGjcRkSEZxTDjWbNmITo6WprOz8+Hh4cHQkJCoFKpDBKTRqNBfHw8+vfvD3Nzc4PEUF3tY/YCeHjk5B9dtZhzygTFWnkMMz4fE1plHWNq60cZY9wajQbbt283dBhERHWboLi6ugIAsrKy4ObmJpVnZWWhc+fOUp1bt27pvO/BgwfIycmR3v84pVIJpVKpV25ubm7wjl8OMVTl8XueFGsVsrkPSk3azhjaujzGGjcRkSHV6X1QvL294erqigMHDkhl+fn5SEpKQkBAAAAgICAAubm5SE5OluocPHgQWq0W/v7+dRkOERERGakaH0EpKChAWlqaNJ2eno6UlBTY29vD09MT06ZNw4IFC9CqVSt4e3tjzpw5cHd3x9ChQwEAbdu2xYABAzBp0iSsW7cOGo0GUVFRGD16NEfwEBEREYBaJCinTp1C3759pemya0MiIiKwceNGvPvuuygsLMTkyZORm5uLXr16Yc+ePbC0tJTe8+WXXyIqKgpBQUEwMTHBiBEjsHLlyjrYHCIiInoa1DhB6dOnD4SoeJiqQqHA/PnzMX/+/Arr2NvbY/PmzTVdNRERET0j+CweIiIikh0mKERERCQ7TFCIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7Nb7VPVFdavbezgrnXV0Y3oCREBGRnPAIChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHd7q/ilS2W3jiYiIjAmPoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkhwkKERERyQ4TFCIiIpIdJihEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2WGCQkRERLLDBIWIiIhkp84TlJiYGCgUCp2Xj4+PNP/+/fuYOnUqHBwcYGNjgxEjRiArK6uuwyAiIiIjVi9HUHx9fZGRkSG9fvzxR2ne9OnT8cMPP2Dr1q04cuQIbt68ieHDh9dHGERERGSkzOploWZmcHV11SvPy8vDp59+is2bN6Nfv34AgA0bNqBt27Y4ceIEevToUR/hkJFq9t5OAIDSVGBxd6B9zF4UlyoAAFcXhhsyNCIiqmf1kqBcvnwZ7u7usLS0REBAAOLi4uDp6Ynk5GRoNBoEBwdLdX18fODp6YnExMQKE5Ti4mIUFxdL0/n5+QAAjUYDjUZTH5tQpbL1Gmr95VGaisrnmwidf41FeXHLqd0rIsd9pCrGFCsRPd0UQog6/bbavXs3CgoK0KZNG2RkZCA2NhY3btzA+fPn8cMPP2DChAk6yQYAdO/eHX379sWiRYvKXWZMTAxiY2P1yjdv3gxra+u6DJ/omVdUVISXX34ZeXl5UKlUhg6HiJ5RdX4EJSwsTPp/x44d4e/vDy8vL2zZsgVWVla1WuasWbMQHR0tTefn58PDwwMhISEG60A1Gg3i4+PRv39/mJubGySGx7WP2VvpfKWJwD+6ajHnlAmKtYoGiurJ1STu8zGhDRRV1eS4j1RFo9Fg+/bthg6DiKh+TvE8Sq1Wo3Xr1khLS0P//v1RUlKC3NxcqNVqqU5WVla516yUUSqVUCqVeuXm5uYG7/jlEEOZsuszqqynVVS7rpxUJ265/C0eJad9hIjIWNT7fVAKCgpw5coVuLm5wc/PD+bm5jhw4IA0PzU1FdeuXUNAQEB9h0JERERGos6PoLzzzjsYNGgQvLy8cPPmTcybNw+mpqYYM2YM7OzsEBkZiejoaNjb20OlUuHNN99EQEAAR/AQERGRpM4TlOvXr2PMmDHIzs6Gk5MTevXqhRMnTsDJyQkAsHz5cpiYmGDEiBEoLi5GaGgo1qxZU9dhEBERkRGr8wTl66+/rnS+paUlVq9ejdWrV9f1qomIiOgpwWfxEBERkewwQSEiIiLZYYJCREREslPv90GhulX2fBoiIqKnGY+gEBERkezwCAo9VSo7wsQnIBMRGQ8eQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJDhMUIiIikh0mKERERCQ7TFCIiIhIdngfFHrm8d4pRETywyMoREREJDtMUIiIiEh2eIqHnhl80CIRkfHgERQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7PAiWaJKVHRhLe+PQkRUv5igyBRHnBAR0bOMp3iIiIhIdpigEBERkewwQSEiIiLZYYJCREREssMEhYiIiGSHCQoRERHJzlM/zLg297GobIgv739BAPcRIqL69tQnKEQNrSx5UZoKLO4OtI/Zi+JSBRMXIqIa4CkeIiIikp1n9ggK79RKREQkXwZNUFavXo0lS5YgMzMTnTp1wqpVq9C9e3dDhtSgmCQRERGVz2AJyjfffIPo6GisW7cO/v7+WLFiBUJDQ5GamgpnZ2dDhUVUb3hhLRFR9RksQVm2bBkmTZqECRMmAADWrVuHnTt34rPPPsN7771nqLCqxAsgqT7U5mhaRfscEyEiehoYJEEpKSlBcnIyZs2aJZWZmJggODgYiYmJevWLi4tRXFwsTefl5QEAcnJyoNFoKl2X2YPCOor6seVqBYqKtDDTmKBUq0B2dnaFdf3jDpS/jHqJrGKPx2wsGHf5KtrnKtvnK9tPAUCj0aCoqAgAIISofXBERE/IIAnKnTt3UFpaChcXF51yFxcX/PLLL3r14+LiEBsbq1fu7e1dbzFWx8uP/N/xnwYLo0ZerrqKLDFufbXZ52rynrt378LOzq7mKyEiqgNGMYpn1qxZiI6Olqa1Wi1ycnLg4OAAhcIwv6jz8/Ph4eGBP/74AyqVyiAx1JQxxgww7oZUFvPFixfh7u5u6HCI6BlmkATF0dERpqamyMrK0inPysqCq6urXn2lUgmlUqlTplar6zPEalOpVEbz5VPGGGMGGHdDatKkCUxMeJskIjIcg/RAFhYW8PPzw4ED/3dthlarxYEDBxAQEGCIkIiIiEhGDHaKJzo6GhEREejatSu6d++OFStWoLCwUBrVQ0RERM8ugyUoo0aNwu3btzF37lxkZmaic+fO2LNnj96Fs3KlVCoxb948vVNPcmaMMQOMuyEZY8xE9HRSCI4lJCIiIpnhVXBEREQkO0xQiIiISHaYoBAREZHsMEEhIiIi2WGCQkRERLLDBOX/i4uLQ7du3WBrawtnZ2cMHToUqampOnX69OkDhUKh83r99dd16ly7dg3h4eGwtraGs7MzZsyYgQcPHtRb3DExMXox+fj4SPPv37+PqVOnwsHBATY2NhgxYoTeHXwbOmYAaNasmV7cCoUCU6dOBSCftk5ISMCgQYPg7u4OhUKBbdu26cwXQmDu3Llwc3ODlZUVgoODcfnyZZ06OTk5GDt2LFQqFdRqNSIjI1FQUKBT59y5cwgMDISlpSU8PDywePHieolZo9Fg5syZ6NChAxo1agR3d3eMGzcON2/e1FlGeX+fhQsX1lvMRESPY4Ly/x05cgRTp07FiRMnEB8fD41Gg5CQEBQW6j4ZdtKkScjIyJBej3bKpaWlCA8PR0lJCY4fP45NmzZh48aNmDt3br3G7uvrqxPTjz/+KM2bPn06fvjhB2zduhVHjhzBzZs3MXz4cIPHfPLkSZ2Y4+PjAQAvvfSSVEcObV1YWIhOnTph9erV5c5fvHgxVq5ciXXr1iEpKQmNGjVCaGgo7t+/L9UZO3YsLly4gPj4eOzYsQMJCQmYPHmyND8/Px8hISHw8vJCcnIylixZgpiYGPz73/+u85iLiopw+vRpzJkzB6dPn8Z///tfpKamYvDgwXp158+fr9P+b775Zr3FTESkR1C5bt26JQCII0eOSGXPP/+8+Nvf/lbhe3bt2iVMTExEZmamVLZ27VqhUqlEcXFxvcQ5b9480alTp3Ln5ebmCnNzc7F161ap7NKlSwKASExMNFjM5fnb3/4mWrRoIbRarRBCnm0NQHz//ffStFarFa6urmLJkiVSWW5urlAqleKrr74SQghx8eJFAUCcPHlSqrN7926hUCjEjRs3hBBCrFmzRjRu3Fgn7pkzZ4o2bdrUeczl+emnnwQA8fvvv0tlXl5eYvny5RW+pz5jJiISQggeQalAXl4eAMDe3l6n/Msvv4SjoyPat2+PWbNmoaioSJqXmJiIDh066NwNNzQ0FPn5+bhw4UK9xXr58mW4u7ujefPmGDt2LK5duwYASE5OhkajQXBwsFTXx8cHnp6eSExMNGjMjyopKcF//vMfvPrqqzpPp5ZjWz8qPT0dmZmZOu1rZ2cHf39/nfZVq9Xo2rWrVCc4OBgmJiZISkqS6vTu3RsWFhY625Kamoo///yz3rcjLy8PCoVC7wGcCxcuhIODA7p06YIlS5bonD4zdMxE9PQz2K3u5Uyr1WLatGno2bMn2rdvL5W//PLL8PLygru7O86dO4eZM2ciNTUV//3vfwEAmZmZerfqL5vOzMysl1j9/f2xceNGtGnTBhkZGYiNjUVgYCDOnz+PzMxMWFhY6H3xuLi4SPEYIubHbdu2Dbm5uRg/frxUJse2flzZesqL49H2dXZ21plvZmYGe3t7nTre3t56yyib17hx43qJH3h4jdLMmTMxZswYnScuv/XWW3juuedgb2+P48ePY9asWcjIyMCyZcsMHjMRPRuYoJRj6tSpOH/+vM61HAB0rhvo0KED3NzcEBQUhCtXrqBFixYNHSYAICwsTPp/x44d4e/vDy8vL2zZsgVWVlYGiammPv30U4SFhcHd3V0qk2NbP200Gg1GjhwJIQTWrl2rMy86Olr6f8eOHWFhYYHXXnsNcXFxfE4PETUInuJ5TFRUFHbs2IFDhw6hadOmldb19/cHAKSlpQEAXF1d9UbIlE27urrWQ7T61Go1WrdujbS0NLi6uqKkpAS5ubl6MZXFY+iYf//9d+zfvx8TJ06stJ4c27psPeXF8Wj73rp1S2f+gwcPkJOTY9C/QVly8vvvvyM+Pl7n6El5/P398eDBA1y9etVgMRPRs4UJyv8nhEBUVBS+//57HDx4UO/wdXlSUlIAAG5ubgCAgIAA/PzzzzpfSGWdf7t27eol7scVFBTgypUrcHNzg5+fH8zNzXHgwAFpfmpqKq5du4aAgABZxLxhwwY4OzsjPDy80npybGtvb2+4urrqtG9+fj6SkpJ02jc3NxfJyclSnYMHD0Kr1UpJV0BAABISEqDRaHS2pU2bNvVyqqQsObl8+TL2798PBweHKt+TkpICExMT6XRVQ8dMRM8gQ1+lKxdTpkwRdnZ24vDhwyIjI0N6FRUVCSGESEtLE/PnzxenTp0S6enpYvv27aJ58+aid+/e0jIePHgg2rdvL0JCQkRKSorYs2ePcHJyErNmzaq3uN9++21x+PBhkZ6eLo4dOyaCg4OFo6OjuHXrlhBCiNdff114enqKgwcPilOnTomAgAAREBBg0JjLlJaWCk9PTzFz5kydcjm19d27d8WZM2fEmTNnBACxbNkycebMGWnEy8KFC4VarRbbt28X586dE0OGDBHe3t7i3r170jIGDBggunTpIpKSksSPP/4oWrVqJcaMGSPNz83NFS4uLuKVV14R58+fF19//bWwtrYW69evr/OYS0pKxODBg0XTpk1FSkqKzr5eNiLn+PHjYvny5SIlJUVcuXJF/Oc//xFOTk5i3Lhx9RYzEdHjmKD8fwDKfW3YsEEIIcS1a9dE7969hb29vVAqlaJly5ZixowZIi8vT2c5V69eFWFhYcLKyko4OjqKt99+W2g0mnqLe9SoUcLNzU1YWFiIJk2aiFGjRom0tDRp/r1798Qbb7whGjduLKytrcWwYcNERkaGQWMus3fvXgFApKam6pTLqa0PHTpU7n4REREhhHg41HjOnDnCxcVFKJVKERQUpLc92dnZYsyYMcLGxkaoVCoxYcIEcffuXZ06Z8+eFb169RJKpVI0adJELFy4sF5iTk9Pr3BfP3TokBBCiOTkZOHv7y/s7OyEpaWlaNu2rfjwww/F/fv36y1mIqLHKYQQosEO1xARERFVA69BISIiItlhgkJERESywwSFiIiIZIcJChEREckOExQiIiKSHSYoREREJDtMUIiIiEh2mKAQERGR7DBBISIiItlhgkJERESywwSFiIiIZOf/Ado/z71f+SF/AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['number_words_target']>=40].reset_index(drop=True)\n",
        "data['number_words_target'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCCUFXpg772",
        "outputId": "2cc810ed-f4c2-42b2-f03d-cbbeb52727ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1012.000000\n",
              "mean       66.381423\n",
              "std        17.169049\n",
              "min        40.000000\n",
              "25%        54.000000\n",
              "50%        64.000000\n",
              "75%        77.000000\n",
              "max       149.000000\n",
              "Name: number_words_target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms_paper = ['paper', 'study', 'work', 'article']\n",
        "articles = ['The', 'the', 'This', 'this', 'A', 'a', 'An', 'an']\n",
        "\n",
        "def substitution(text, syn, articles):\n",
        "  list_words = text.split()\n",
        "  new_list_words = []\n",
        "  for index, word in enumerate(list_words):\n",
        "    if word in articles and list_words[index+1] in syn:\n",
        "      new_list_words.append(word)\n",
        "      new_list_words.append(random.choice(syn))\n",
        "    elif word in syn and list_words[index-1] in articles:\n",
        "      continue\n",
        "    else:\n",
        "      new_list_words.append(word)\n",
        "  return ' '.join(new_list_words)\n",
        "\n",
        "data['extractive_summary'] = data['extractive_summary'].apply(lambda x : substitution(x, synonyms_paper, articles))\n",
        "data['target'] = data['target'].apply(lambda x : substitution(x, synonyms_paper, articles))"
      ],
      "metadata": {
        "id": "WVce8pJfdwFn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data in different sets\n",
        "data_training, data_test = train_test_split(data,\n",
        "                                      test_size=0.20,\n",
        "                                      shuffle=True,\n",
        "                                      random_state=42)\n",
        "\n",
        "data_train, data_val = train_test_split(data_training,\n",
        "                                        test_size=0.20,\n",
        "                                        shuffle=True,\n",
        "                                        random_state=42)"
      ],
      "metadata": {
        "id": "2cBrIvVfpgqG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "o46lFUVw5taI",
        "outputId": "4d925699-0484-4ff4-9d9f-646b9386541f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                source    paper_id  \\\n",
              "631  With the recently rapid development in deep le...  ByxmXnA9FQ   \n",
              "634  Neural machine translation (NMT) models learn ...  H1z-PsR5KX   \n",
              "963  Recent results from linear algebra stating tha...  SkeUG30cFQ   \n",
              "625  Analogical reasoning has been a principal focu...  SylLYsCcFm   \n",
              "365  Recent advances in computing technology and se...   ByJbJwxCW   \n",
              "\n",
              "                                                target  \\\n",
              "631  A new framework based variational inference fo...   \n",
              "634  Unsupervised methods for finding, analyzing, a...   \n",
              "963  We provide a theoretical study of the properti...   \n",
              "625  The most robust capacity for analogical reason...   \n",
              "365  We propose a deep Multi Instance Learning fram...   \n",
              "\n",
              "                                                 title  number_words_target  \\\n",
              "631  A Variational Dirichlet Framework for Out-of-D...                   78   \n",
              "634  Identifying and Controlling Important Neurons ...                   54   \n",
              "963  The Expressive Power of Deep Neural Networks w...                   54   \n",
              "625  Learning to Make Analogies by Contrasting Abst...                   67   \n",
              "365  Relational Multi-Instance Learning for Concept...                   97   \n",
              "\n",
              "                                    extractive_summary  number_words_source  \\\n",
              "631  Therefore, it is very essential to design a ro...                 3289   \n",
              "634  First, it targets the whole vector representat...                 4933   \n",
              "963  Recent results from linear algebra stating tha...                 4143   \n",
              "625  It is natural to consider, however, whether th...                 6473   \n",
              "365  Most of the medical time series lack annotatio...                 4819   \n",
              "\n",
              "     number_words_extractive  \n",
              "631                      695  \n",
              "634                      456  \n",
              "963                      594  \n",
              "625                      673  \n",
              "365                      513  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-139bb00b-0357-4e06-9b52-843d3ba33743\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "      <th>title</th>\n",
              "      <th>number_words_target</th>\n",
              "      <th>extractive_summary</th>\n",
              "      <th>number_words_source</th>\n",
              "      <th>number_words_extractive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>With the recently rapid development in deep le...</td>\n",
              "      <td>ByxmXnA9FQ</td>\n",
              "      <td>A new framework based variational inference fo...</td>\n",
              "      <td>A Variational Dirichlet Framework for Out-of-D...</td>\n",
              "      <td>78</td>\n",
              "      <td>Therefore, it is very essential to design a ro...</td>\n",
              "      <td>3289</td>\n",
              "      <td>695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>Neural machine translation (NMT) models learn ...</td>\n",
              "      <td>H1z-PsR5KX</td>\n",
              "      <td>Unsupervised methods for finding, analyzing, a...</td>\n",
              "      <td>Identifying and Controlling Important Neurons ...</td>\n",
              "      <td>54</td>\n",
              "      <td>First, it targets the whole vector representat...</td>\n",
              "      <td>4933</td>\n",
              "      <td>456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>Recent results from linear algebra stating tha...</td>\n",
              "      <td>SkeUG30cFQ</td>\n",
              "      <td>We provide a theoretical study of the properti...</td>\n",
              "      <td>The Expressive Power of Deep Neural Networks w...</td>\n",
              "      <td>54</td>\n",
              "      <td>Recent results from linear algebra stating tha...</td>\n",
              "      <td>4143</td>\n",
              "      <td>594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>Analogical reasoning has been a principal focu...</td>\n",
              "      <td>SylLYsCcFm</td>\n",
              "      <td>The most robust capacity for analogical reason...</td>\n",
              "      <td>Learning to Make Analogies by Contrasting Abst...</td>\n",
              "      <td>67</td>\n",
              "      <td>It is natural to consider, however, whether th...</td>\n",
              "      <td>6473</td>\n",
              "      <td>673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>Recent advances in computing technology and se...</td>\n",
              "      <td>ByJbJwxCW</td>\n",
              "      <td>We propose a deep Multi Instance Learning fram...</td>\n",
              "      <td>Relational Multi-Instance Learning for Concept...</td>\n",
              "      <td>97</td>\n",
              "      <td>Most of the medical time series lack annotatio...</td>\n",
              "      <td>4819</td>\n",
              "      <td>513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-139bb00b-0357-4e06-9b52-843d3ba33743')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-139bb00b-0357-4e06-9b52-843d3ba33743 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-139bb00b-0357-4e06-9b52-843d3ba33743');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7249dd61-3d0d-4a2c-aa37-51709b241ac3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7249dd61-3d0d-4a2c-aa37-51709b241ac3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7249dd61-3d0d-4a2c-aa37-51709b241ac3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_test",
              "summary": "{\n  \"name\": \"data_test\",\n  \"rows\": 203,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"Quantum computers promise significant advantages over classical computers for a number of different applications. We show that the complete loss function landscape of a neural network can be represented as the quantum state output by a quantum computer. We demonstrate this explicitly for a binary neural network and, further, show how a quantum computer can train the network by manipulating this state using a well-known algorithm known as quantum amplitude amplification. We further show that with minor adaptation, this method can also represent the meta-loss landscape of a number of neural network architectures simultaneously. We search this meta-loss landscape with the same method to simultaneously train and design a binary neural network. Finding a suitable set of weights for a neural network has become one of the most studied problems of modern machine learning. It has presented a significant challenge to computer scientists for whom few successful alternatives to back-propagation are available. It can be difficult to explore very large search spaces efficiently and, worse, optimization may converge to a local minima far from global optimum BID2 . Understanding the cost function landscape is also hard, and choosing hyper-parameters and designing neural networks remains mostly a manual process. As Moore's law approaches its end, two new computing paradigms have been explored, neuromorphic and quantum computers. Quantum computing is based on quantum bits (or qbits) obeying the laws of quantum physics as opposed to the classical bits of today that are based on classical physics. Note that in physics the term classical is used to mean non-quantum and we use this terminology throughout. Quantum machine learning aims to find an advantage in applying quantum computing to machine learning. Current research into quantum machine learning falls into one of two catgeories. Some quantum algorithms promise a revolution in machine learning in theory, but contain many gaps in their implementation in practice. In contrast, others are more realistic in their method, but struggle to justify a place amongst the well-established methods of machine learning. In this paper, it is shown that a quantum computer can output a quantum state that represents the entire cost landscape for a given neural network. The method is shown to be versatile and even able to represent a meta-cost landscape of all possible hyperparameters and parameters. Applying it to the connectivities and weights of a binary neural network and simulating the quantum algorithm on a classical computer, we further show that this landscape state can be used for training and metatraining the binary neural network for a small toy problem using quantum amplitude amplification, a standard quantum algorithm. Binary Neural Networks (BNNs) are neural networks with weights and activations restricted to taking only binary values, usually \\u00b11. The greatest advantage of BNNs is in their deployment as using binary provides great advantages in compression and inference time, as well as computational efficiency through the use of bitwise operations. On the other hand they are relatively tricky to train as the sign function has a derivative of zero nearly everywhere, the search space is discrete, and alternative training methods take significantly longer than non-binarized neural networks. Nonetheless, BNNs have achieved state-of-the-art performance on smaller datasets such as MNIST and CIFAR10 BID4 but initially suffered when applied to larger datasets such as ImageNet. A popular approach to solving this issue has been to relax the binarisation constraints. This has been achieved by using multiple binary activations BID13 or by introducing scale factors BID16 , both of which result in improvements in accuracy. On the other hand, it has been argued that a better training strategy for BNNs is sufficient to achieve high accuracy on large datasets without compromising on the pure binary nature BID22 . After investigating the accuracy failures of the previous methods, a number of improvements to the BNN training process have been suggested such as changing the activation function, lowering the learning rate and using a different regularization term. These changes helped achieve both high accuracy and high compression rates on ImageNet. Again, this solution is not entirely ideal, as training BNNs is already relatively slow, and a lower learning rate exacerbates this issue. Between the efficient deployment, discrete search space, slow training and relatively small problem size (near-term quantum computers favor problems that require fewer bits), training a binary neural network represents an ideal test case for a quantum computer. Finally, BNNs have been suggested as a candidate for efficient hybrid architectures through transfer learning. The idea is that a BNN pretrained on ImageNet may be used as a feature extractor for other datasets by retraining a final non-binarised layer. In this way, a hybrid hardware-software architecture can implement the binary part using efficient hardware and the non-binary final layer in software BID12 . Quantum computers use quantum bits, manipulated with quantum gates in quantum circuits according to quantum algorithms. The advantage of quantum computers over classical computers is that certain quantum algorithms show significantly improved computational complexity compared to the best known classical algorithms. Such improved scaling, combined with the exponentially growing computational power of qubits suggests that (large, error-free) quantum computers would be able to easily handle and process very large amounts of data. Most relevant to this paper is the quantum search algorithm known as Grover's algorithm BID8 , itself a specific case of another algorithm known as quantum amplitude amplification BID0 . These algorithms can search for an element of an unstructured dataset of size N in O( \\u221a N ) operations, over the classical O(N ). It is important to keep in mind that these are compared to the best-known classical algorithms, and not that they are better than all possible classical algorithms. A recent paper BID21 has challenged the presumed superiority of a quantum recommendation algorithm with a new classical algorithm inspired by the quantum method that shows similar scaling. In our case, the optimality of Grover's algorithm has been proven BID24 and so the assumption of its inherent advantage is robust. Some quantum algorithms are able to efficiently perform k-means clustering BID14 and solve linear systems of equations BID9 , among other such achievements (see BID3 for a review). All of these algorithms require the classical data to be encoded into an accessible quantum form of RAM known as a qRAM. Although there is some work on how this might be done BID7 it is not known to even be possible to construct a qRAM in an efficient manner for a completely general dataset. To many, this is a significant drawback that cannot be ignored, and places a heavy burden on the feasibility of these methods. An alternative approach has been to mimic the progress of classical machine learning by using methods classically known to work. Many have taken to using classical computers to train parametrized quantum circuits to perform classification BID19 or to learn generative models BID6 . Some, but not all, of these circuits mimic neural networks in that they are layered and try to utilize non-linearities . The biggest issue with this approach is the lack of an efficient algorithm for training quantum circuits and so current methods are akin to black box optimization. The motivation is that the output of quantum circuits are known to be impossible to efficiently simulate with classical computers and could therefore provide superior performance on that basis. A slightly different approach to training a perceptron using quantum amplitude amplification has been explored before and its complexity studied compared to classical methods BID10 . Previous work has demonstrated and experimentally implemented the use of quantum hardware to perform binary classification, BID15 ) but this is not the same as the method proposed in this paper, as this work is based on a different, more general gate-based form of quantum computation as opposed to the quantum annealing devices of the former. Quantum computing follows the structure of classical computing very closely. Quantum bits, or qubits, are the fundamental unit of quantum information. Their values are manipulated by applying quantum (logic) gates to them in the form of quantum circuits. Qubits are challenging to manufacture in practice due to the noise-sensitive nature of quantum properties. The biggest such device in existence today contains just 72 highly imperfect qubits, but it is worth noting that progress has advanced at a particularly rapid pace over the past few years and a number are available for public access on the cloud. In addition, simulating the behaviour of qubits using classical computers is difficult, requiring exponentially increasing resources as the number of qubits increases -with an upper limit of 50 (perfect) qubits often cited for the most powerful supercomputers. Therefore, quantum algorithms are almost always defined in terms of their circuit implementation, as opposed to the higher level abstraction of classical algorithms. Qubits are the unit of quantum information and are fundamentally different to classical bits. Whilst classical bits are completely described as being in one of two states, either 0 or 1, the state of a qubit cannot be fully described by just a single number. It can be in the 0 state, the 1 state or a quantum superposition of both. Mathematically the state of a qubit is a two dimensional vector with complex elements and a unit norm. We can write a general form for this vector as DISPLAYFORM0 Here \\u03b1 and \\u03b2 are the probability amplitudes of the zero state |0 and the one state |1 respectively. Qubits cannot be simply read out as classical bits are, but are instead measured. Measurement is a unique feature of quantum mechanics. If the qubit given above is measured, it will be found in the zero state with probability |\\u03b1| 2 , outputting a value of 0, and the one state with probability |\\u03b2| 2 outputting a value of 1. Therefore measurement of a qubit state always produces a binary outcome, no matter the actual state itself. Measurement is fundamentally indeterministic, probabilistic and irreversible. Upon measurement, the original state is lost along with the values of \\u03b1 and \\u03b2 as the qubit collapses to the state |0 or |1 corresponding to the measurement outcome. As a result, the values \\u03b1 and \\u03b2 cannot be obtained without repeated measurements of many identical copies of the state. Here \\u03c6 is a phase that does not affect measurement outcome, but can be manipulated with quantum gates and play a role in quantum algorithms. Part of the power of quantum computing is the ability to harness superposition to parallelize certain computations and processes. An important feature of qubits is the way in which they are combined. N qubits are collectively described by a complex vector of unit norm in a similar way as the above, but the length of this vector is given by 2 N . It is this exponential scaling that makes even modest numbers of qubits unfeasible to simulate on a classical computer. In both classical and quantum computing, gates manipulate the states of bits and qubits. As complex vectors, qubit states are transformed into one another by applying complex matrices called operators or simply, quantum gates. This transformation follows the rules of linear algebra and a state |\\u03c8 is transformed into a different state |\\u03c6 by a gate U according to the matrix transformation |\\u03c6 = U |\\u03c8 . In order to maintain the stringent requirement of a unit norm, these matrices are restricted to being unitary. A unitary matrix is defined as any square matrix who's inverse is its complex conjugate transpose. Unitarity implies that every quantum gate is reversible, in a manner similar to reversible computing. This fundamental difference in the kinds of operations that can be performed on qubits compared to classical bits is part of the power of quantum computing, but can make analogies to classical computing difficult. Many quantum operations have no classical analogue and conversely, certain simple classical operations (e.g copying the state of a general qubit) are impossible in quantum computing. Just as in classical computing, small sets of quantum gates are universal in that they can be combined to generate any other. It transpires that a small set of quantum gates are sufficient to our work and we choose to list them here, both in terms of their actions and their matrix forms. The X (NOT) gate flips the state of a qubit from |1 to |0 and vice versa. For qubits in superposition, it swaps the amplitudes of the |1 and |0 states. Its matrix form is DISPLAYFORM0 The Z gate has no classical analogue and takes the matrix form DISPLAYFORM1 It transforms an arbitrary state \\u03b1 |0 + \\u03b2 |1 into the state \\u03b1 |0 \\u2212 \\u03b2 |1 . The probability amplitude of the |1 component has changed sign, but the probabilities associated with measurement outcome, as squares of the probability amplitudes, remain unchanged. Note that this still represents a completely different state. The Hadamard (H) gate also has no classical analogue. It is used to transform qubits from their initial state |0 into the state DISPLAYFORM2 |1 -an equal quantum superposition of 0 and 1. As a matrix it is DISPLAYFORM3 The controlled-not (CNOT) gate can be thought of as a generalisation of the classical XOR gate. It performs a NOT gate on a target qubit if a control qubit is in the state |1 . We write this as DISPLAYFORM4 Note that controlled gates can be extended both to arbitrary gates (e.g. CZ) and to arbitrary numbers of control qubits (e.g. CCCNOT). The main advantage of qubits over classical bits is their ability to be placed and processed in quantum superpositions of states. The key to our method is to use superposition to parallelize the processing of weights in a way not possible classically. Our scheme proceeds as follows: Step 1: The weights are represented in some way by the quantum state of a set of qubits. Setting those qubits into a state that represents an equal superposition of every possible set of weights allows them to define the domain. Step 2: We then build a quantum circuit analogue of the neural network U QN N that takes in a given set of weights (encoded within qubits as above) and an empty register, and outputs onto the register the corresponding accuracy according to the chosen neural network i.e U QN N (w, 0) = (w, acc w ).Step 3: Since U QN N is a quantum circuit, inputting weights in superposition form allows them to be processed in parallel. Thus by using the domain-defining qubits as the weights input to U QN N the output will be a superposition correlating all possible weights to their corresponding accuracies. This is what we refer to as the landscape state. We can write this as DISPLAYFORM0 where W is the set of all possible weights, W is its size and O w the accuracy of the neural network given the set of weights w. This is a single quantum state representing the entire landscape of the neural network by correlating every possible set of weights with its resultant accuracy. In the language of quantum physics the weights and the accuracies are entangled. This method can be adapted in many ways. For example, if just a single weight is set it to superposition and the rest kept to a given value, then the output is the cost landscape of just that one weight conditional on the value of the others. We are not limited to only setting weights in superposition. We note that a meta-neural network with the presence/absence of the connections within the neural network themselves represented by binary parameters can also be created. These meta-parameters can also be encoded in qubits, formed into a quantum circuit and set to superposition. If we set both the weights and the connection meta-parameters to superposition then the output state of the quantum circuit contains an entire meta-cost landscape of every possible weight with every possible connectivity of a neural network simultaneously correlated with the respective accuracy. We demonstrate our method by generating the landscape state for a small binary neural network on simple toy problems and use it to train the network. The advantage of binary neural networks is that each weight can be naturally represented by just one qubit and so are therefore a suitable demonstration given the fundamentally small number of qubits that can be simulated on a nonquantum device. We construct two toy problems, both of which are a binary classification on three binary features x i \\u2208 {\\u22121, 1} of eight data points corresponding to every 2 3 arrangement of those features. In problem 1, the label is given by the function DISPLAYFORM0 and for problem 2 the label is given by DISPLAYFORM1 In both cases we define the sign function as: DISPLAYFORM2 We choose to implement the BNN given in figure 1 meaning that we are aiming to find eight binary weights. To construct a quantum circuit equivalent to the BNN, henceforth known as the Quantum Binary Neural Network (QBNN), every operation in the implementation of a BNN must be mapped to a quantum equivalent. Below we detail each of these and their quantum implementation. Representing numerical values with qubits is already well established in the literature BID20 . Other parts of our construction are, however, incompatible with non-binary input and so we restrict ourselves to the simple case of a binary data input. In this case, the qubit states |1 and |0 represent the values +1, \\u22121 respectively. In a quantum circuit, all qubits begin in the |0 state and need only an application of a single NOT gate to be set to |1 where appropriate. Given two qubits representing binary values \\u00b11 as described above, we can multiply them using an anti-CNOT gate. An anti-CNOT gate applies a NOT gate to a target qubit if the control qubit is in the state |0 instead of |1 . Its truth table is identical to an XNOR gate and outputs |1 if both input values are equal, and |0 otherwise. This truth table matches the truth table of multiplying two binary values and thus performs the same function. It can be constructed using two NOT gates and a CNOT gate. Qubits that encode weights must always be used as control qubits to preserve the values they encode. Since the sign function is highly non-linear, it poses the greatest challenge to translate to the linear algebra-based language of quantum mechanics. Generally, the problem can be overcome by the addition of extra helper or 'ancilla' qubits. If we restrict the problem to the special case of binary arguments only, the sign function 1 is reduced to finding whether there exist N/2 qubits out of N in state |1 . This can be achieved by constructing a quantum analogue of a classical majority function by replacing AND gates with CCNOT gates and constructing OR gates out of CNOT and NOT gates. The number of gates needed scales as the binomial coefficient N choose N/2. As an example, figure 2 shows a three input neuron and its quantum circuit implementation. Note that this is just a single neuron, and not our entire network. In practice, it works in the same manner as a classical neural network. The activations of each neuron in one layer are then weighted by their own weight qubits and used as input to the next layer and so on. This whole circuit is what we refer to as the QBNN. For each data point on the training set we must compare the prediction to the label in order to find the accuracy. We initialise a register of qubits to store the predictions. The reversibility of quantum circuits allows us to apply the QBNN for a given data point, store its output value onto its corresponding qubit on the register, perform the same QBNN in reverse order -its inverse -to refresh the other qubits, and continue for the next data point in the training set. This resetting is a common, necessary workaround for small quantum computers and is easily avoided by parallelization given more qubits. For a training set of size N , we obtain a register of N qubits containing the predictions of the QBNN for each of them. Since both the labels and the outputs are binary, we can represent the accuracy of each of these predictions by performing a NOT gate on all the qubits corresponding to a data point with a label of 0. Each qubit in this register will then be in the state |1 if it corresponds to a correctly classified data point and |0 if it does not. By applying the QBNN over the entire training set with the weights initialized in superposition, our circuit output is the cost landscape state. Training the BNN can be seen as a search for a single state within the cost function landscape, for which we use a quantum algorithm known as quantum amplitude amplification. It is not the first time that quantum amplitude amplification has been suggested as a means to train quantum neural networks BID17 ), but they did not construct the actual details of an implementation such as the method of generating a nonlinearity. Quantum amplitude amplification is a technique to amplify the probability amplitudes that correspond to desired state(s) within the superposition and therefore increase the probability of measuring one of these. It works by splitting the space of all states into a 'good' and a 'bad' subspace and rotating their relative probabilities when measured. In this case the 'good' subspace is defined as that which has all the qubits in the prediction register in the state |1 implying that all data points have been correctly classified. It is known that quantum amplitude amplification requires just O(1/ \\u221a a) to search for an entry with an occurrence probability of a BID0 . Quantum amplitude amplification works by first constructing the amplifying operator, Q. DISPLAYFORM0 The composite operation, Q, is interpreted as a sequence of operations applied from right to left as read in the equation above. U QBN N is our entire QBNN circuit (for all data points), and U \\u22121 QBN N is its (matrix) inverse. Since quantum gates are reversible, and every gate we have used is self-inverse, we obtain this by applying all of the gates of U QBN N in reverse order. The operations S 0 and S \\u03c7 reverse the sign of the probability amplitudes of the initial state and the target state(s) respectively. In this case, our target states correspond to those with an accuracy of 100% and S \\u03c7 is a controlled-Z gate performed on each of the target qubits. Similarly, the initial state of any quantum computer is defined as having all the qubits in the state |0 , and thus we can implement S 0 by first applying a NOT gate to each qubit and then applying the same controlled-Z gates as for S \\u03c7 . FIG2 is a pictorial representation of how quantum amplitude amplification changes the probability distribution of the measured weights. If we write the initial probability of obtaining the correct weights by random as p and the number of successive applications of operator Q to be k, it can be shown that the probability of obtaining the optimal weights when measuring the circuit after k amplifications is sin 2 (2k + 1)\\u03b8where p and \\u03b8 obey the relation p = sin 2 \\u03b8 BID0 . The probability of success is therefore highly periodic in k. The problem of training the BNN essentially reduces to a probabilistic search on this one hyper-parameter and its regular periodic landscape. The location of the first maximum, i.e of k * , is inversely proportional to \\u03b8 and hence to the probability of obtaining the weights by random. In other words, a harder problem with more weights to search requires a greater number of quantum amplifications to find. In practical terms the landscape state is a set of 8 weight qubits and 8 prediction qubits. After the search, at the end of the entire process, all the qubits are measured. If the prediction qubits are all in the state |1 the training was a success and the appropriate weights can be simply read off their corresponding qubits. We constructed and simulated the QBNN and quantum amplitude amplification circuits on the projectQ framework BID18 . The use of an actual quantum computer was not possible as the number of gates used during the computation (called circuit depth) exceeds the maximum possible circuit depth for the current generation of imperfect noisy qubits. Furthermore, we use more qubits than are available on current publicly accessible quantum hardware. For each of the two problems defined, we plotted the probability of obtaining an optimal set of weights against the number of iterations of the quantum amplitude amplification and obtained results, shown in FIG3 , that match well with the expected periodic behavior described in equation 3. This confirms that a quantum search of the landscape state can indeed be used to train a BNN in exactly the manner as predicted theoretically. We emphasize here that every reference to finding optimal weights means that the BNN has been trained to an accuracy of 100% on the training data. In order to demonstrate the performance of this method in actual training, we follow the simple algorithm described in BID0 for probing this landscape. This simple algorithm begins with n = 0 and chooses a random integer k of quantum amplifications between 0 and n. n increases by 1 until the training succeeds. In our experiment, we perform 100 runs of this algorithm and present in figure 5 a cumulative plot of the proportion of these runs that were successful against the number of iterations this algorithm required. We find that training succeeds with a probability over 90% after just 5 steps for the first problem and 6 steps for the second. In order to compare this to a classical search, we search the entire space of 2 8 = 256 possible sets of weights and find that there are eight and four correct sets of weights (giving 100% accuracy) for the first and second problem respectively. Statistically, if these weights were to be searched through the analogous classical brute data is the cumulative probability of success over 100 runs of the algorithm. Classical results are analytically derived from the known probability of obtaining a solution by random search. The superior scaling of the quantum algorithm becomes more prominent for harder problems.force search, one would find that it requires 28 and 57 steps respectively to succeed with a confidence over 90%. This matches our expectation of a quadratic speedup of the quantum search over the classical. We then construct a more complex QBNN which can incorporate meta-training by introducing a set of binary indicators that correspond to the presence or absence of a set of connections within the BNN and encode these within qubits in the exact same way as was done with the weights. With the weights and connection parameters both set to superpositions, the output of this circuit is the meta-cost landscape, where weights, connections and accuracy are all entangled with one another. As before quantum amplitude amplification is used to search for the state with all points correctly classified. Again this has been suggested before, but we present a full circuit implementation of this idea (da BID5 . In practice, due to qubit number constraints, we choose to only learn the structure of the first layer of the BNN. The second layer remains fixed. Due to the increased size of the circuit, and the significant increase in computational cost, we did not perform a complete classical search of the space as before but it is clear to see that the space of parameters we are searching has increased and therefore the number of amplifications required has similarly increased. Between 16 and 20 amplifications were found to be sufficient to produce results with a reasonable probability. FIG5 (a) shows the meta-BNN that was used, and (b) and (c) show two solutions to problems 1 and 2 respectively learnt by our meta-QBNN. It is particularly interesting to note that the learned structures of the two BNN solutions seem to match well with their problem definitions (equation 1 and equation 2). Note that due to our circuit construction a neuron that receives no input will always output \\u22121. We show that quantum superposition can be used to represent many parameters of a neural network at once and efficiently encode entire loss landscapes in a quantum state using just a single run of a quantum circuit. We demonstrate this explicitly for both parameters and hyper-parameters of a BNN, and show that further processing of this state can lead to quantum advantage in training and metatraining. As a training method it possesses significant advantages as it is landscape-independent, has a quadratic speedup over a classical search of the same kind, and would be able to solve statistically neutral problems such as parity problems BID23 . It is not, however, without shortcomings. One potential criticism is the issue of over-fitting. Since our problem is so small, we chose to define a target state as one where the accuracy is 100% on the training set but this is rarely desirable in real machine learning. One solution may be to simply run the quantum algorithm and, upon finding a particular set of weights that represents an overfit, run the algorithm again but with a deselection of that particular set of weights. This can be done by simply changing the sign of the probability amplitude corresponding to that state during each iteration of the quantum amplitude amplification. A similar issue is that regular machine learning typically uses batch learning, whilst our method incorporates the entire dataset at once. This too can be fixed by altering our method to use a different batch of the data for each quantum amplitude amplification iteration. This works since no matter what batch we use, a good set of weights should still be amplified by the circuit. In fact, such an implementation is advantageous since it would allow us to use less qubits which in practical terms are limited in number in the near term. A significant limitation in our method is the requirement that the input is binary, and the poor scaling of the activation function. Both of these problems arise completely from our implementation of the sign function, which could either be improved or replaced entirely with a different binary activation function that could be implemented more efficiently on a quantum computer and would be compatible with non-binary input. There has been progress on creating effective non-linear activation functions by so-called repeat-until-success circuits BID1 ). An alternative approach would be to use floating point representations as in classical computing and the quantum equivalent of full-adders, but this would require an overhead in the number of qubits that would take us beyond the limit of classical simulation. Finally, we note that this method scales poorly compared to backpropagation and that the advantage only appears in like for like comparisons of unstructured classical/quantum searches. The cost function landscape is not unstructured and algorithms such as backpropagation take advantage of this. We conjecture that a quantum search method that applies quantum advantage to structured searches, if it exists, can be applied to the cost landscape in place of quantum amplitude amplification. Finding ways to harness quantum computers to aid classical machine learning methods in a meaningful way remains an open problem and we present the loss landscape state as a plausible candidate towards this goal. Whilst we used the example of quantum training, the most fruitful approach in the short term is to ask whether some property of the state can be used to glean useful information for classical machine learning methods. This might take the form of understanding the roughness of the landscape, identifying certain features, or even choosing an appropriate learning rate. Further work in investigating the relationship between the landscape as a quantum state and its features from a machine learning perspective would be a step forward in this direction.\",\n          \"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a more straightforward learning signal. Humans effortlessly combine the two, and engage in chit-chat for example with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning via self-play against an imitation-learned chit-chat model with two new approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-k utterances. We show that both models outperform a strong inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals. In the literature on artificial dialogue agents, a distinction is often made between \\\"goal-oriented\\\" dialogue, where an agent is tasked with filling slots or otherwise obtaining or disseminating specified information from the user to help complete a task, and \\\"chit-chat\\\", where an agent should imitate human small talk. Modeling goal-oriented dialogue can have advantages over chit-chat imitation as it gives clearer metrics of success and perhaps more meaningful learning signals; but goal-oriented dialogue data is often more specialized, covering only a narrow slice of natural language. Current goal-oriented datasets study setting like booking restaurants or airline tickets, or obtaining weather information, as standalone tasks (Raux et al., 2005; Henderson et al., 2014; Bordes et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) . Chit-chat agents, by contrast, might focus on coarse statistical regularities of dialogue data without accurately modeling the underlying \\\"meaning\\\"; but the data often covers a much wider space of natural language. For example, Twitter or Reddit chitchat tasks (Li et al., 2016a; Yang et al., 2018; Mazar\\u00e9 et al., 2018 ) cover a huge spectrum of language and diverse topics. Chit-chat and goal-oriented dialogue are not mutually exclusive: when humans engage in chit-chat, their aim is to exchange information, or to elicit specific responses from their partners. Modeling such goals, however, is made difficult by the fact that it requires large amounts of world knowledge, and that goals in real life are implicit. In this work, we study goal-oriented dialogue agents in the setting of a multi-player text-based fantasy environment (Urbanek et al., 2019) . The environment is built on top of a game engine that grounds actions and reference objects, and thus codifies a body of world-knowledge. Although the interactions between objects and characters are simulated, the choice and types of interactions, the text used to describe them, and the dialogues between characters, are \\\"natural\\\" and wide-ranging, having been collected from human crowdworkers. We define the general task of, given a particular character in a particular scenario (location, set of objects and other characters to interact with) to conduct open-ended dialogue such that a given action is executed in the future by their dialogue partner. The given action could be an emote action (smile, laugh, ponder, . . . ), or a game action (wear chain mail, drink mead, put glass on table, . . . ). The richness of the environment means that there are a huge set of possible tasks and scenarios in which to achieve a wide range of actions. Thus, this task is ideally suited for bridging the divide between goal-oriented and chit-chat dialogue, combining clearer metrics and learning signals on the one hand, with the richness and complexity of situated but open-domain natural language on the other. Figure 1 : Example episode from the LIGHT dataset, consisting of an environment (location setting, characters with given personas, objects), utterances and game actions. There are 10,777 such humanhuman gameplay episodes, and a rich world of 663 locations, 1755 characters and 3462 objects. We train models to achieve these tasks using reinforcement learning (RL) and a type of self-play between two agents. The first agent, which we call the environment agent, is trained with imitation learning on human-human interactions (game actions, utterances and emotes) and subsequently kept fixed. The second agent, the RL agent, is trained to conduct dialogue given the goal, and the two agents interact within a given environment until the goal is either reached or a given number of turns has expired. At that point, rewards are given, and the RL agent is updated. We compare agents that have been trained to imitate human actions given a goal (an \\\"inverse model\\\") to two different RL approaches: optimizing actions with latent discrete variables (topics), or via rewarding actions sampled from the model (via the top-K outputs). We show that both types of RL agent are able to learn effectively, outperforming the inverse model approach or a vanilla chit-chat imitation baseline, and can converse naturally with their dialogue partner to achieve goals. We work in the LIGHT game environment (Urbanek et al., 2019) , which is a multi-user text-based game, involving many characters playing the game at once. Characters (either played by humans or run by models) can speak to to each other via free text, send emote actions like applaud, nod or pout (22 emote types in total), and take actions to move to different locations and interact with objects (e.g. get cutlery, put cutlery in drawer, etc.), see Appendix A for a full list of game actions. LIGHT at its core has a game engine which can formally be defined as a graph, where each location, object and character is a node, and they are connected by labeled edges representing relationships, for example contained-in, path-to or has-property. Actions in the game result in changes in state of the graph. To a player (agent) a local view of the graph can be seen and this is expressed in text, as are the game actions and changes of state. This text then naturally interleaves with the dialogue utterances of the speakers as well to form a input context sequence from which a character can base their subsequent actions. See Fig. 1 for an example. To make the world and its textual descriptions, LIGHT consists of a large set of human-written game locations, characters, and objects, all based within a fantasy medieval setting. Their names, descriptions and properties were crowd-sourced, yielding a total of 663 locations, 1755 characters, and 3462 objects. They range from beaches with crabs and seaweed to crypts with archaeologists and coffins, yielding an extremely rich environment for agents to learn within. An additional set of crowdworkers were then asked to play the role of characters (randomly picked from the set of 1755) within the created world as rendered by the game engine. This involved them making utterances, game actions and emotes, while interacting with each other (in pairs). The resulting gameplay data consists of 10,777 episodes with an average of 18.3 actions each (game actions, emotes and utterances) of rich human play. These are split into train (8538), validation (500) and test (1739) portions, the latter being split into new episodes in existing settings (test seen, 1000) and completely new settings (test unseen, 739) . This gameplay data can be used for training models using imitation learning, as well as for obtaining \\\"common sense\\\" knowledge about how the world works, i.e., what kinds of things certain characters say; what actions they use with certain objects; what they say and how they act in certain environments or while interacting with certain other characters. The whole environment is thus intended as a proxy for learning about the world within a rich simulation, while avoiding the complexities and bandwidth of rendering (3D) computer graphics. While players were not given specific goals, but instead asked to play the role convincingly of the character given, during play some of them effectively defined their own goals during the interactions, see Fig. 1 . The tasks we consider in this work involve interaction between two agents in a given LIGHT scenario. One of the agents, which we will call M env , together with the game engine, effectively functions as an environment for the other agent, which we will call M RL . Because we will formulate our tasks as a reinforcement learning problem, we will also refer to M env as the \\\"environment agent\\\" and M RL as the \\\"RL agent\\\". We assume that the environment agent is fixed; in this work it will be a model trained via behavioral cloning from human-human interaction data. The RL agent must conduct open-ended dialogue such that a given goal action is executed in the future by the environment agent. Our task is formally defined as follows. The two agents M env and M RL are given their views of the scenario (D env and D RL respectively). These consist of the setting name, scenario description, character names, and their own persona, all described as a sequence of text (see Fig 2) . Note that each agent can only access their own persona but not the persona of the partner with whom they are conversing, but they do know the name of their partner. Denote by t the time-step of the environment, U RL t and U env t the utterances of the agents M RL and M env respectively, and denote by A env t the environment actions by M env . Hence the interaction sequence looks like Note that there is an inversion from the usual reinforcement literature language, as the \\\"actions\\\" of the RL agent are its utterances U RL t ; the actions A env t of the environment agent should be considered as internal mechanics of the environment. The agent M RL is additionally given a goal g to achieve, which consists of an action which must be executed by the other agent. That is, the objective of M RL is for M env to take the action g. An episode ends when A env t == g or when n becomes larger than a set number of turns. The RL agent only speaks, but does not perform game or emote actions. This was chosen for simplicity, but also to guarantee that the RL agent cannot help force the goal to be reached by performing actions itself -it has to pick the appropriate utterances U RL such that M env eventually takes the action g. Goals We experiment separately with two different types of goals: game actions and emote actions. We use the same train, valid, test (seen and unseen) split of the original human-human LIGHT episodes, assign roles M RL and M env randomly, and randomly pick an action by M env that occurs in the episode as the goal. We can then present the corresponding setting to our agents in order to form a new interaction, but within the same scenario and with a goal that was naturally desirable and achievable within that setting. Observations The state observation O t = (D RL , S t\\u22121 , g) at time t given to an RL model consists of the RL agent's setting description (D RL ), the utterance and action history up to that time step (S t\\u22121 ), and the agent's goal (g). Our RL agent models consume O t as a flattened sequence of tokens, and return a dialogue utterance U RL t . Each structured component is represented in the flattened sequenced separated by a special token denoting the types, e.g. names, settings, etc., see Fig. 2 . Note that because the entire history and goal is given to the RL agent, the environment is Markovian. Reward We have a terminal reward of +1 only if the goal g is achieved and 0 otherwise, i.e, it is +1 if the environment agent takes the goal action g. The episode ends after n steps. In our experiments we consider n = 1 and n = 3. In this section we describe the models for M env and M RL . In this work these are retrieval models, using the LIGHT dialogue corpus as candidates. We leave generative models to future work. Base Agent Architecture For all our models we adopt the same base architecture, which is a 12-layer bidirectional transformer (Vaswani et al., 2017) pre-trained on a large dialogue corpus (Reddit, 174M examples), and then fine-tuned on our task 1 . To score retrieval candidates, we use a biencoder as in Urbanek et al., 2019) . That is, two transformers are used, one to encode the context, and another to encoder a candidate dialogue, and a dot product between the first output vector of each scores the match. To produce a dialogue utterance one then takes the utterance with the largest output from the training set candidates (111k in this case). For emotes and actions, the same procedure is used, but with those candidate sets instead. For actions, the candidates are the set of admissible actions at that game state, which are provided by the game engine, for example get apple is only available in the candidate set if it is a valid action (an apple is present in the room). For emotes, all 22 candidates are always available. To train the model, a cross entropy loss is used. Similar to Mazar\\u00e9 et al. (2018) , during training we consider the other elements of the batch as negatives. Environment agent The environment agent is the base agent described above, and stays fixed during episodes where the RL agent is trained. This helps guarantee that our RL models stick to using the semantics of natural language (English) rather than so-called language drift, of learning a new emergent language with the same tokens (Lee et al., 2019) . RL agents We design two RL approaches for our tasks -learn to pick the right latent discrete variables (topics) that lead to the correct U RL i ; and learn to pick the correct U RL i from the top K candidates. These are described in more detail in Sections 4.2 and 4.3. We also discuss a baseline \\\"inverse\\\" model trained via behavioral cloning on the human-human data. We consider an inverse model, trained to imitate human actions given a goal, as both a baseline for comparing to RL models, and for producing weights form which we can fine-tune. The inverse model consists of a Bi-encoder, as described above, which takes as input an observation O t similar to our RL models, and outputs an utterance. We train it by extracting from the human-human game logs training set (which does not have goals) every instance where a game action occurs at time t in S t , that is where for 0 < i < t might be null). We then construct a training example for the inverse model with observation (D RL , g = A env t , S t\\u22121 ). i.e. setting the goal g to be A env t , and with the desired action to be taken by the agent as U RL t . Here we use the subscripts \\\"RL\\\" and \\\"env\\\" just to mark the relative positions in the sequence, as all actions and utterances come from the human logs. Note also that unlike the RL agents we train, the human in the RL agent \\\"position\\\" can take game actions. We can thus train this model in a supervised manner using a cross entropy loss as described before. This model does not learn a policy interactively, and hence might not learn to plan or strategize optimally for goal completion. The data distribution it is trained on is different than the data distribution seen by the RL agents. Nevertheless, it can serve as a strong baseline. Further, when training our RL agents, we initialize their weights to the weights of this model, and then fine-tune from that point. Optimizing all the parameters of a large transformer architecture by RL is both incredibly costly in data efficiency and computing time, and is also known to have the problem of language drift (Lee et al., 2019) -that is, there is no guarantee after training with self-chat that the models will output recognizable natural language utterances. A solution to both problems is to train most of the parameters of the model with human-human language data, and then to either disentangle or only optimize some of the parameters with model self-chat . Here, we propose a straight-forward model for that purpose. We assume an RL agent that consists of two components. The first component F c (O) = P (T c (O)) maps from an observation to a discrete variable with C possible values. It consists of a chain of two functions: a transformer T s that takes in the observation, and outputs a state representations, and a policy chooser c = P (s) \\u2208 (1, . . . , C) which takes in the state representation and outputs the value of the discrete latent variable. The second component T u (O, c) is an additional transformer that takes as input the observation as well as the output of the first component, and outputs a dialogue utterance. That is, the entire model is the chain u = T u (O, P (T s (O))). We make this explicit decomposition so that we can train only part of the model with RL; note that the \\\"action\\\" trained via RL is choosing c, not outputting the final utterance. Initial topics We first pre-train the transformer T s using the inverse model described in Section 4.1, which produces a vectorial representation of a given observation. We then run K-means over the vectorial representations of all observations from the training set to provide the mapping to one of C values, which represent dialogue topics, which we use as our initial function P (s). These two functions together give us our initialization of F c . Table 1 shows the cluster ID and the topic denoted by that cluster along with the most representative sentences (closest to the center) for that cluster for 50 topics. As we can see, the clusters learnt can be coherent about a topic. We use these 50 topics as a set of actions A for our RL setup. From c to A Given our initial choice of F c , we can also pre-train T u . We simply take our initial human-human training data, and for each observation append the topic computed by F c to it. This allows our model to be able to generate an action (utterance) conditional on both an input and a topic. We can now train a policy by RL that optimizes the topic at any given point in the episode. We keep the pre-trained portions of the model T u and T s fixed and during fine-tuning only optimize P . The cluster chooser P is redefined (from the initial K-means) to be an MLP network consisting of 2 layers. A discrete action is sampled from a categorical probability distribution over the possible topics, given by c t \\u223c Categorical(h 2 t ), where h 2 t = tanh(W 2 tanh(W 1 s t + b 1 ) + b 2 ). The state vector s t also encodes the goal g and hence, the policy is conditioned on the goal g of the agent. Hence, the policy can learn strategies that will result in picking actions at each time step t that will help the agent to achieve its goal g. As our RL agent can only choose topics, it cannnot redefine easily the meaning of words to cause language drift. The Top-K model is another approach to keeping the number of trainable parameters small. It uses the inverse model to get a context embedding v context from the observation, and a list of K candidate utterance embeddings v 1 , ...v K . These are the encodings by the inverse model of the K utterances it considers most likely given the context and goal. We then train a small (2-layer) transformer model that takes as input the set {v context , v 1 , ...v K }. We use the attention above weights of v context against the candidates at the last layer of the transformer as the distribution over the candidates for sampling an utterance. We use K = 50 in the experiments. We use the Advantage Actor-Critic implementation (A2C; Kostrikov, 2018) to train the policy and the value function for both the latent-variable and top-K models. Chit-chat dialogue There is an increasing body of work in the domain of chit-chat, where the primary approaches being currently tried are end-to-end neural approaches. They are typically large pre-trained and then fine-tuned transformers, either generative or retrieval, where currently retrieval models work best on a number of tasks (Zhang et al., 2018; Li et al., 2019) . Our work shares a commonality with these approaches in that the original LIGHT dialogue data we use has no specified goals, and humans chit-chat together (and act). Thus, the conversations cover a rich number of diverse topics. In Urbanek et al. (2019) models were trained in a similar fashion to chit-chat task models, and we adopt similar architectures here, but instead adapt them to learn to pursue goals. Goal-oriented dialogue Traditional goal-oriented dialogue has focused on narrow tasks that would typically be useful for a dialogue-based assistant, for example restaurant (Henderson et al., 2014) , taxi, train, and hotel (Budzianowski et al., 2018) or trip (El Asri et al., 2017) booking. Hence, each task typically focuses on a narrow slice of natural language and world knowledge for a specialized domain. Earlier work focused on labeled state representations, slot filling mechanisms and dialogue managers (Rieser & Lemon, 2011) , and more recent work has shifted to an end-to-end approach (Bordes et al., 2017) , in line with chit-chat models, but still the two sets of tasks are rarely considered together, or by using the same methods. RL for dialogue The classical goal-oriented dialogue literature studies RL extensively (Singh et al., 2000) . Typically, they used RL to improve dialogue managers, which manage transitions between dialogue states (Singh et al., 2002; Pietquin et al., 2011; Rieser & Lemon, 2011; Gasic et al., 2013; Fatemi et al., 2016) . Recent works have focused more on end-to-end learning. Some works have focused on self-play type mechanisms for end-to-end reinforcement learning, where the reward is derived from goal. A related approach to ours is the negotation tasks of ; , which require two agents to swap 3 item types (hats, balls, books) where the value of the items is different for the two agents, and derives their personal reward. In contrast, our setup encompasses a rich world of settings and characters -with 3462 object types, and a corresponding large number of actions. This is reflected in the vocabulary size itself (\\u223c32,000 versus \\u223c2,000 in the negotation tasks). Other notable uses of RL in dialogue include within visual question answering (Das et al., 2017) , in the domain of chit-chat where RL has been used to decrease repetitive and generic responses through the the use of self-play (Li et al., 2016b) , and through human-bot conversation (Sankar & Ravi, 2019) . RL for language and games RL is used extensively for learning to play games, one of the most well known examples being AlphaGo (Silver et al., 2016) . Since then, language in games has started to be more deeply explored, for example in graphical games such as Minecraft (Oh et al., 2017) , Real-time strategy war games (Hu et al., 2019) , or in text adventure games (Narasimhan et al., 2015; C\\u00f4t\\u00e9 et al., 2018) . The latter are related to our setting. However, those approaches use RL to optimize the set of actions given feedback in a single-player rather than multi-player game, so the text only refers to the environment, and there is no dialogue or actions from other agents. Our work focuses specifically on the latter. We compare our various models on the game action and emote action tasks. We experiment with differing number of steps n allowed to complete the goal, n = 1 and n = 3. Our main results for both seen and unseen test environments are given in Table 2 . We report the average reward and for n = 3 the average number of turns before completion. The results show clear improvements for our topic RL ( \\u00a74.2) and top-K RL ( \\u00a74.3) compared to the inverse model baseline for all values of n, and both types of actions (game actions and emotes). We show the training curves for topic RL in Fig. 3 , reporting rewards averaged over the batch (512 for n = 1, and 128 for n = 3). They show relatively smooth improvements over time, with clear gains over the baseline. As a sanity check we also tried, after training, to replace the topic RL policy with random topic prediction, which yielded poor results, e.g. 0.217 reward for n = 1 test seen game actions. Our model is clearly learning appropriate topic acts. We show examples of successful utterances, achieving goal actions in Fig. 3 for a diverse range of scenarios, actions and language. (n = 1) (n = 3) (n = 1) (n = 3) In this paper, we investigate agents that can interact (speak or act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. We achieve this by defining a task for an agent, where the goal is for the other player to execute a particular action. We explore two reinforcement learning based approaches to solve this task: the policy either learns to pick a topic or learns to pick an utterance given the top K utterances, and compare them against a strong baseline trained to imitate chit-chat. We show that these approaches effectively learn dialogue strategies that lead to successful completion of goals, while producing natural chat. Future work should explore further RL algorithms for agents that can act and speak in natural language at scale in our proposed rich task environment, and we expect further advancements. Constraints Outcome get object actor and object in same room actor is carrying object object is gettable drop object actor is carrying object object is in room object is gettable get object1 from object2 Actor and object2 in same room actor is carrying object1 object1 is gettable object2 is surface or container object2 is carrying object1 put object1 in/on object2 Actor and object2 in same room object2 is carrying object1 object2 is container or surface actor is carrying object1 give object to agent Actor and agent in same room agent is carrying object object is a member of actor steal object from agent actor and agent in same room actor is carrying object object is a member of agent hit agent Actor and agent in same room inform agent of attack hug agent Actor and agent in same room inform agent of hug drink object actor is carrying object inform actor of drinking successfully object is a drink eat object actor is carrying object inform actor of eating successfully object is a food wear object actor is carrying object actor is wearing object object is wearable wield object actor is carrying object actor is wielding object object is a weapon remove object actor is wearing/wielding object actor is carrying object object is wearable or a weapon Table 4 : LIGHT actions and constraints from Urbanek et al. (2019) B GAME EMOTES WITHIN LIGHT applaud, blush, cry, dance, frown, gasp, grin, groan, growl, laugh, nod, nudge, ponder, pout, scream, shrug, sigh, smile, stare, wave, wink, yawn Figure 4: Emote actions within the LIGHT platform from Urbanek et al. (2019)\",\n          \"Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness. Deep neural networks achieve state-of-the-art performances on a variety of tasks (LeCun et al., 2015) . However, neural nets are known to be vulnerable to adversarial examples. Imperceptibly perturbed inputs can induce erroneous outputs in neural nets (Szegedy et al., 2013) . In image classification problems of computer vision, previous work has proposed various methods to attack deep models and induce low accuracy (Goodfellow et al., 2015; Madry et al., 2017; Papernot et al., 2016a; Carlini & Wagner, 2017a) . Whereas multiple defenses against adversarial attacks are developed, they don't ensure safety faced with strong attacking methods. There are also theories that explain the existence of adversarial examples (Ilyas et al., 2019; Shamir et al., 2019) , but they often fail to fully explain the features and behaviors of this phenomenon. This makes the study of adversarial attacks important in that it is a threat to real-life machine learning systems (Kurakin et al., 2016) . In this paper, we propose a dynamical system view on the adversarial robustness of the models, as well as new method that significantly defense adversarial attacks. Recent works have shown the connection between deep neural networks and dynamical systems (E, 2017; Haber & Ruthotto, 2017; Lu et al., 2017) . If we regard the neural net as a discretization of an ordinary differential equation (ODE), then training neural nets becomes finding an optimal control of the corresponding discrete dynamical system. Traditionally, we often treat training neural networks as an unconstrained non-convex optimization problem where \\u03b8 denotes the parameters of the model, J denotes the loss function and R denotes the regularizer term, and we solve the problem with (stochastic) gradient-descent based methods (Bottou, 2010; Ruder, 2016) . In the training process, we feed the network with a batch of training data, and compute the gradient with forward and backward propagation (E. Rumelhart et al., 1986) . The propagation process resembles solving optimal control problems that tune the parameters to make the output be close to target states. This viewpoint motivates us to bridge adversarial robustness with Lyapunov stability of a dynamical system, and to train robust networks with algorithms that find stable optimal control. We will formulate the discussion in later sections. 2 RELATED WORK 2.1 ADVERSARIAL DEFENSE Many defense methods have been proposed to improve the models' adversarial robustness. The defenses mainly fall into three types: adversarial training (Szegedy et al., 2013; Zhang et al., 2019) , modifying the networks (Gu & Rigazio, 2015; Lyu et al., 2015; Papernot et al., 2016b; Nayebi & Ganguli, 2017; Ross & Doshi-Velez, 2017) , and adding external models (Lee et al., 2017; Akhtar et al., 2017; Gebhart & Schrater, 2017; Xu et al., 2018; Sun et al., 2019) . Although various defense methods have been developed, a defended deep model is often successfully attacked by newly developed attacks or specific counter-counter measures (Carlini & Wagner, 2017b) . Therefore, it can be hoped that defenses against general attacks will be devised to make deep learning models (adversarially) robust to real-life threats. Recent works have bridged deep neural networks with ODEs and dynamical systems. On the one hand, deep residual networks (He et al., 2015) can be illustrated as forward Euler scheme approximating an ODE (E, 2017), which motivates us to design effective network structures (Lu et al., 2017) . On the other hand, regarding the network as a dynamical system allows us to set up an optimal control viewpoint of neural nets. Pontryagin's Maximum Principle (Boltyanskii et al., 1960) has been applied to train neural nets Li & Hao, 2018) . Given a T -layer neural net, we let the dynamical system {f t (x t , \\u03b8 t ) : t = 0, . . . , T } represents the network, where x t is the input of t-th layer, \\u03b8 t is the parameter, and f t : denotes the t-th layer's transformation, which is usually a non-linear function \\u03c3(\\u03b8 t x t + b t ) for fully-connected layers, convolution layers and batch normalization layers, etc. Therefore, training the neural net can be regarded as controlling the parameters to let the dynamics fit the training data. Specifically, the training optimization problem can be formulated as a typical optimal control problem as follows: . . , T \\u2212 1, where we use x i to denote the i-th input in the batch and B denote the batch size. J and L are the loss function and the regularizer, respectively. Specially, if the model is a deep residual network with structure x t+1 = x t + f t (x t , \\u03b8 t ), we can regard the problem as the forward Euler discretization of the following continuous optimal control problem: where x(t) is a continuous trajectory from the input to the output logits. Adversarial examples are usually clean images added by a small calculated perturbation \\u03b7. The model predicts correct labels fed with clean inputs x 0 , while the output is completely different when it is fed with perturbed input x 0 + \\u03b7. The dynamical system view of neural nets motivate us to characterize this sensitivity with Lyapunov stability of a system (Hirsch et al., 2004) . Definition 1 (Lyapunov Stability). For a given dynamical system\\u1e8b = f (x), x(0) = x 0 , x e is an equilibrium, then \\u2022 The system is asymptotically stable if it is Lyapunov stable and \\u2203 \\u03b4 > 0 such that if x(0) \\u2212 x e < \\u03b4, then lim t\\u2192\\u221e x(t) \\u2212 x e = 0. \\u2022 The system is exponentially stable if it is asymptotically stable and \\u2203 \\u03b1 > 0, \\u03b2 > 0, \\u03b4 > 0 such that if x(0) \\u2212 x e < \\u03b4, then x(t) \\u2212 x e \\u2264 \\u03b1 x(0) \\u2212 x e e \\u2212\\u03b2t , for all t \\u2265 0. The definitions can be easily extended to discrete-time systems. Intuitively, the Lyapunov stability states that for any small perturbation \\u03b7, the trajectory is still \\\"close enough\\\" to the original one. If we regard a neural net as a dynamical system, and ensure the network is Lyapunov stable, then the model is robust to all (adversarial) perturbations. Due to the connection between numerical ODEs and residual networks, we first consider robustness (i.e. Lyapunov stability) of continuous ODEs. , where \\u03c3 is the activation function, e.g., Sigmoid function or ReLU function, it is stable if Re(\\u03bb i (A)) \\u2264 0, \\u2200i, where Re denotes the real part, and \\u03bb i denotes the i-th eigenvalue. One can see, e.g. Hirsch et al. (2004) , for the proof of this theorem. Theorem 1 provides a set of conditions for stable ODEs. However, deep residual network is only a forward Euler discretization scheme of continuous ODE. To ensure numerical stability, we require |1 \\u2212 \\u03bb i (A)h| \\u2264 1 (Ascher & Petzold, 1998) , where the step size h = 1 in residual networks. Added by the identity mapping in residual networks, we can get the stable conditions for discrete dynamics. Theorem 2 (Stable Discrete Networks). For a discrete neural network, i.e., discrete dynamics {f t (x t , \\u03b8 t ) : t = 0, . . . , T }, where f t (x t , \\u03b8 t ) = \\u03c3(\\u03b8 t x t ) (we omit the bias term for simplicity), the network is stable if the \\u03c1(\\u03b8 t ) \\u2264 1, where \\u03c1(A) = max i (|\\u03bb i (A)|) is the spectral radius. If the conditions are added to the unconstrained optimization problem of training, we can greatly improve the adversarial robustness of neural nets. The methods will be discussed in the following section. 4.1 PMP AND MSA For deterministic systems, the Pontryagin's Maximum Principle (PMP) (Boltyanskii et al., 1960) provides a set of necessary conditions for optimal control of the system. Various algorithms have been proposed to solve the deterministic optimal control problem based on PMP. Among them, the Method of Successive Approximations (MSA) (Krylov & Chernous'ko, 1963 ) is one of the simplest algorithms. In the field of deep learning, previous work has utilized MSA to train neural networks Li & Hao, 2018) . Formally, consider the optimal control problem for training neural nets in section 3. For dynamics {f t (x t , \\u03b8 t ) : t = 0, . . . , T }, assume \\u03b8 * = \\u03b8 * 0 , . . . , \\u03b8 * T \\u22121 is a solution to the optimal control problem. Also, we define the Hamiltonian function H : , where the dot denotes the inner product. We have the following necessary conditions for \\u03b8 * . Theorem 3 (Pontryagin's Maximum Principle for Discrete Systems). Assume f t and J are sufficiently smooth. There exists co-states p * = {p * 0 , . . . , p * T } s.t. the following conditions hold: For simplicity of notations, here we assume the batch size is 1. One can easily extend the theorem to minibatch training case by summing over the batch. The theorem can be proved by KKT conditions (Boyd & Vandenberghe, 2004) , where the co-states can be seen as the Lagrangian dual variables. Consider the conditions in PMP, one can find the x equations are exactly the forward propagation of a neural net, and the p equations resemble the backward propagation process. The third condition states that the model parameters must maximize the Hamiltonian function. This motivates us to iteratively compute forward and backward propagation, and solve the Hamiltonian maximization to find the optimal control, which is exactly the Method of Successive Approximations (Algorithm 1). In practice, we usually add regularizer terms that penalize great changes in the maximization step to prevent drastic steps that cause divergence. For the connection between MSA and back-propagationbased gradient descent algorithms, see the appendix of Li & Hao (2018) . Compute the states (forward propagation): The advantages of training by MSA compared with gradient descent algorithms has been discussed in , among which the most significant feature is that the optimization steps on different layers are decoupled. Concretely, after computing the states x and co-states p, the optimization step on layer t is only searching for parameters \\u03b8 t . This not only suggests that the optimization process can be accelerated by parallelization, but also allows us to utilize the features of the problem. The parameter space is greatly reduced compared with the original intractable optimization problem, and hence the optimization is much more easier. This allows us to add constraints that ensure robustness of the model. Consider a layer in the form of f t (x) = \\u03b8 t x, where we leave the activation as an individual layer with no parameters for simplicity, we can derive the following optimization problem for Hamiltonian maximization: max where \\u03b1 \\u03b8 t 2 2 is the L 2 norm regularizer (weight decay), and \\u03b8 t is the initial parameter (i.e., \\u03b8 k t in the algorithm). The last term keeps the training process from drastic steps that cause divergence. The constraint, as illustrated in section 3, is the stable condition for discrete systems. It makes the optimization quite difficult if we directly add the constraints in gradient descent based algorithms, but the decoupled optimization in MSA allows us to do so. With regard to the constraint of parameter's spectral radius, a simple method is to apply special forms of matrices for parameters, e.g. anti-symmetric matrices. For continuous deep models, the only constraint is Theorem 1, i.e., Re(\\u03bb i (\\u03b8 t )) \\u2264 0. Anti-symmetric matrices have only imaginary eigenvalues, and hence we can replace \\u03b8 t with \\u03b8 t \\u2212 \\u03b8 (Goodfellow et al., 2015) 2.34% 77.45% 49.32% PGD-10 (Madry et al., 2017) 0.02% 46.67% 36.33% C&W (Carlini & Wagner, 2017a) Proof. Recall that \\u03c1(A) \\u2264 A 2 = \\u03bb max (A T A), we have Hence we can replace \\u03c1(\\u03b8 t ) \\u2264 1 with a positive semi-definite condition, and we turn the Hamiltonian maximization into a new optimization problem, where the target function is quadratic and the constraint is a semi-definite condition. This can be reduced to a semi-definite programming (SDP) problem (Vandenberghe & Boyd, 1998) , which is a special case of convex optimization, and thus can be solved efficiently by, e.g., interior point methods (Helmberg et al., 1970) in polynomial time. Here we summarize our method. For a given neural network, we use MSA to train the model, i.e., iteratively computing the states (forward propagation) and co-states (backward propagation), and solving the optimization for each layer. Instead of directly maximizing the Hamiltonian, we add a positive semi-definite constraint to the optimization problem, which leads to a stable control of the dynamics. To evaluate the effectiveness of our method, we conduct experiments on CIFAR10. We trained the network on clean data, with adversarial training (PGD-10) and with robust training (our method), respectively. We used FGSM (Goodfellow et al., 2015) , PGD-10 (Madry et al., 2017) and C&W (Carlini & Wagner, 2017a) to attack the network. Due to the limitation of TensorFlow, we used a simple interior point method with gradient descent to solve SDP. The network model was an 18-layer residual network (He et al., 2015) , with 8 residual blocks. We set the perturbation size as = 0.1 for both FGSM and PGD. For C&W, we used the L 0 metric. We trained the model for 150 epochs with a batch size of 200. The learning rate was set to be 10 \\u22122 initially, and was divided by 5 at epoch 30, 60 and 100. The regularizer term constant was set to be 10 \\u22123 . The results can be seen in Table 1 . The accuracy of robust models on clean data is lower than vanilla model's in that robust training and generalization is more difficult and requires more data (Schmidt et al., 2018) . Our method improves model's adversarial robustness, compared with the vanilla model. Figure 1 displays the eigenvalues of the last fully-connected layer's parameter. The complex norm of eigenvalues (spectral radius) of the model trained by our method are effectively bounded below 1, which satisfies the robust constraint on parameters in section 4.2, while eigenvalues of natural training are randomly distributed in the complex plane. Our method is not as effective as traditional adversarial training method. However, it mainly has the following advantages: (a) The training process doesn't require large numbers of gradient propagation, which consumes much time in adversarial training. In our experiment, adversarial training spends about 10 times GPU time as much as our method. (b) The decoupled training process allows us to set different hyperparameters and training methods for different layers, which is more maneuverable for large scale training. We can further control the behavior of different layers in adversarial settings. (c) Lyapunov stability provides a framework for analyzing adversarial robustness of deep models, which may lead to theoretical analysis of adversarial samples in future work. Motivated by the dynamical system view of neural networks, this work bridges adversarial robustness of deep neural models with Lyapunov stability of dynamical systems, and we also propose a method that uses a stable optimal control algorithm to train neural networks to improve the adversarial robustness of deep neural models. Though the result didn't surpass STOA defense methods, the stable control view of training neural nets points out another direction towards adversarially robust models. For future work, on the one hand, mathematical analysis on Lyapunov stability of neural models may be studied to provide theoretical understanding of adversarial robustness. On the other hand, popular platforms for deep learning, e.g., TensorFlow, PyTorch, didn't provide frameworks for optimal control. We will obtain better results if specific algorithms for SDP are applied to solve the optimization problem.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"paper_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"SyxvSiCcFQ\",\n          \"BJxRrlBFwB\",\n          \"BklVA2NYvH\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training. Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network. This study proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks. Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).\",\n          \"Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This article studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This article explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.\",\n          \"An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability The authors formulate training NNs as finding an optimal controller for a discrete dynamical system, allowing them to use method of successive approximations to train a NN in a way to be more robust to adversarial attacks. This paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 202,\n        \"samples\": [\n          \"Frequency-based Search-control in Dyna\",\n          \"Neural Network Cost Landscapes as Quantum States\",\n          \"Model Distillation with Knowledge Transfer from Face Classification to Alignment and Verification\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16,\n        \"min\": 40,\n        \"max\": 124,\n        \"num_unique_values\": 61,\n        \"samples\": [\n          78,\n          80,\n          50\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"extractive_summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 203,\n        \"samples\": [\n          \"Although there is some work on how this might be done BID7 it is not known to even be possible to construct a qRAM in an efficient manner for a completely general dataset. Previous work has demonstrated and experimentally implemented the use of quantum hardware to perform binary classification, BID15 ) but this is not the same as the method proposed in this paper, as this paper is based on a different, more general gate-based form of quantum computation as opposed to the quantum annealing devices of the former. The biggest such device in existence today contains just 72 highly imperfect qubits, but it is worth noting that progress has advanced at a particularly rapid pace over the past few years and a number are available for public access on the cloud. Just as in classical computing, small sets of quantum gates are universal in that they can be combined to generate any other. Step 2: We then build a quantum circuit analogue of the neural network U QN N that takes in a given set of weights (encoded within qubits as above) and an empty register, and outputs onto the register the corresponding accuracy according to the chosen neural network i.e U QN N (w, 0) = (w, acc w ).Step 3: Since U QN N is a quantum circuit, inputting weights in superposition form allows them to be processed in parallel. This is a single quantum state representing the entire landscape of the neural network by correlating every possible set of weights with its resultant accuracy. If we restrict the problem to the special case of binary arguments only, the sign function 1 is reduced to finding whether there exist N/2 qubits out of N in state |1 . The reversibility of quantum circuits allows us to apply the QBNN for a given data point, store its output value onto its corresponding qubit on the register, perform the same QBNN in reverse order -its inverse -to refresh the other qubits, and continue for the next data point in the training set. Since both the labels and the outputs are binary, we can represent the accuracy of each of these predictions by performing a NOT gate on all the qubits corresponding to a data point with a label of 0. If the prediction qubits are all in the state |1 the training was a success and the appropriate weights can be simply read off their corresponding qubits. In practice, due to qubit number constraints, we choose to only learn the structure of the first layer of the BNN. It is particularly interesting to note that the learned structures of the two BNN solutions seem to match well with their problem definitions (equation 1 and equation 2). In fact, such an implementation is advantageous since it would allow us to use less qubits which in practical terms are limited in number in the near term. Both of these problems arise completely from our implementation of the sign function, which could either be improved or replaced entirely with a different binary activation function that could be implemented more efficiently on a quantum computer and would be compatible with non-binary input. Whilst we used the example of quantum training, the most fruitful approach in the short term is to ask whether some property of the state can be used to glean useful information for classical machine learning methods. Further work in investigating the relationship between the landscape as a quantum state and its features from a machine learning perspective would be a step forward in this direction.\",\n          \"In the literature on artificial dialogue agents, a distinction is often made between \\\"goal-oriented\\\" dialogue, where an agent is tasked with filling slots or otherwise obtaining or disseminating specified information from the user to help complete a task, and \\\"chit-chat\\\", where an agent should imitate human small talk. We compare agents that have been trained to imitate human actions given a goal (an \\\"inverse model\\\") to two different RL approaches: optimizing actions with latent discrete variables (topics), or via rewarding actions sampled from the model (via the top-K outputs). Characters (either played by humans or run by models) can speak to to each other via free text, send emote actions like applaud, nod or pout (22 emote types in total), and take actions to move to different locations and interact with objects (e.g. get cutlery, put cutlery in drawer, etc.), see Appendix A for a full list of game actions. To make the world and its textual descriptions, LIGHT consists of a large set of human-written game locations, characters, and objects, all based within a fantasy medieval setting. While players were not given specific goals, but instead asked to play the role convincingly of the character given, during play some of them effectively defined their own goals during the interactions, see Fig. 1 . Similar to Mazar\\u00e9 et al. (2018) , during training we consider the other elements of the batch as negatives. We consider an inverse model, trained to imitate human actions given a goal, as both a baseline for comparing to RL models, and for producing weights form which we can fine-tune. Optimizing all the parameters of a large transformer architecture by RL is both incredibly costly in data efficiency and computing time, and is also known to have the problem of language drift (Lee et al., 2019) -that is, there is no guarantee after training with self-chat that the models will output recognizable natural language utterances. We then run K-means over the vectorial representations of all observations from the training set to provide the mapping to one of C values, which represent dialogue topics, which we use as our initial function P (s). The cluster chooser P is redefined (from the initial K-means) to be an MLP network consisting of 2 layers. We use the attention above weights of v context against the candidates at the last layer of the transformer as the distribution over the candidates for sampling an utterance. In Urbanek et al. (2019) models were trained in a similar fashion to chit-chat task models, and we adopt similar architectures here, but instead adapt them to learn to pursue goals. Earlier work focused on labeled state representations, slot filling mechanisms and dialogue managers (Rieser & Lemon, 2011) , and more recent work has shifted to an end-to-end approach (Bordes et al., 2017) , in line with chit-chat models, but still the two sets of tasks are rarely considered together, or by using the same methods. Other notable uses of RL in dialogue include within visual question answering (Das et al., 2017) , in the domain of chit-chat where RL has been used to decrease repetitive and generic responses through the the use of self-play (Li et al., 2016b) , and through human-bot conversation (Sankar & Ravi, 2019) . However, those approaches use RL to optimize the set of actions given feedback in a single-player rather than multi-player game, so the text only refers to the environment, and there is no dialogue or actions from other agents. We achieve this by defining a task for an agent, where the goal is for the other player to execute a particular action.\",\n          \"This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. In this paper, we propose a dynamical system view on the adversarial robustness of the models, as well as new method that significantly defense adversarial attacks. On the one hand, deep residual networks (He et al., 2015) can be illustrated as forward Euler scheme approximating an ODE (E, 2017), which motivates us to design effective network structures (Lu et al., 2017) . On the other hand, regarding the network as a dynamical system allows us to set up an optimal control viewpoint of neural nets. If we regard a neural net as a dynamical system, and ensure the network is Lyapunov stable, then the model is robust to all (adversarial) perturbations. , where \\u03c3 is the activation function, e.g., Sigmoid function or ReLU function, it is stable if Re(\\u03bb i (A)) \\u2264 0, \\u2200i, where Re denotes the real part, and \\u03bb i denotes the i-th eigenvalue. In the field of deep learning, previous work has utilized MSA to train neural networks Li & Hao, 2018) . Concretely, after computing the states x and co-states p, the optimization step on layer t is only searching for parameters \\u03b8 t . Consider a layer in the form of f t (x) = \\u03b8 t x, where we leave the activation as an individual layer with no parameters for simplicity, we can derive the following optimization problem for Hamiltonian maximization: max It makes the optimization quite difficult if we directly add the constraints in gradient descent based algorithms, but the decoupled optimization in MSA allows us to do so. This can be reduced to a semi-definite programming (SDP) problem (Vandenberghe & Boyd, 1998) , which is a special case of convex optimization, and thus can be solved efficiently by, e.g., interior point methods (Helmberg et al., 1970) in polynomial time. For a given neural network, we use MSA to train the model, i.e., iteratively computing the states (forward propagation) and co-states (backward propagation), and solving the optimization for each layer. Due to the limitation of TensorFlow, we used a simple interior point method with gradient descent to solve SDP. The complex norm of eigenvalues (spectral radius) of the model trained by our method are effectively bounded below 1, which satisfies the robust constraint on parameters in section 4.2, while eigenvalues of natural training are randomly distributed in the complex plane. Motivated by the dynamical system view of neural networks, this article bridges adversarial robustness of deep neural models with Lyapunov stability of dynamical systems, and we also propose a method that uses a stable optimal control algorithm to train neural networks to improve the adversarial robustness of deep neural models. Though the result didn't surpass STOA defense methods, the stable control view of training neural nets points out another direction towards adversarially robust models.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_source\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1898,\n        \"min\": 131,\n        \"max\": 13830,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          7663,\n          5353,\n          3957\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"number_words_extractive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 138,\n        \"min\": 131,\n        \"max\": 1069,\n        \"num_unique_values\": 167,\n        \"samples\": [\n          776,\n          609,\n          558\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train.shape, data_val.shape, data_training.shape, data_test.shape)"
      ],
      "metadata": {
        "id": "UrY4AEOhphm9",
        "outputId": "017593a0-7426-4d02-afdd-ac013a776669",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(647, 8) (162, 8) (809, 8) (203, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all data to HF DatasetDict\n",
        "tf_data_train = Dataset.from_pandas(data_train)\n",
        "tf_data_test = Dataset.from_pandas(data_test)\n",
        "tf_data_val = Dataset.from_pandas(data_val)\n",
        "\n",
        "raw_data = DatasetDict({'train': tf_data_train,\n",
        "                           'validation': tf_data_val,\n",
        "                           'test': tf_data_test})"
      ],
      "metadata": {
        "id": "mDeGyFjUpkVv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BART\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwSHOSdoAHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'facebook/bart-base'\n",
        "max_input_length = 1024\n",
        "\n",
        "save_name = 'sampling-norep-v4/'\n",
        "save_path = BASE_PATH + '/Results/TLDR/BART/model_save/' + save_name"
      ],
      "metadata": {
        "id": "LpOqNR1qyJGb"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prepare Data for Tokenization"
      ],
      "metadata": {
        "id": "t4eTk7kNwH5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name, errors='ignore')\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "\n",
        "def tokenize_function(data):\n",
        "  model_inputs = tokenizer(data['extractive_summary'], max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "9kNP88WHh5pZ",
        "outputId": "1af0fa12-a887-42ec-ffc3-8c8eae3feb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "019a29f1b10145c8abc20308424e20ad",
            "87bd56f9f9db417c81d87b260e64717a",
            "e953775064244191a3245084abd7b949",
            "3e0d973080be48c3a61ca7bec49f30f0",
            "32ac86f5ff984aedbefd8393f1953c92",
            "255e5a72897049f9b2d16f5f717a7abc",
            "4edaa22c96884cf6a629cc6e8b277cbd",
            "1d41ba5b64e441caa852290dd525bf5e",
            "eb71b0e1b402499095482662879aeab5",
            "223fc245f51a45e39b4f896fab8b139a",
            "578becc7bb2146c587dca97aea75ad16",
            "1e7407be543141a5b2157b3026af7bfd",
            "fcaa9abbccee4df0bdee6114eff41c45",
            "61672ed9c6d74592a1a8b1c6a0b5fe9c",
            "a7feab89338642ec8b2cd300bf1494bd",
            "d6a8ec6ba5ad4b2cb30f24a0daa5fc94",
            "22130b81744c410093e46123bc3d58df",
            "90a902c031ac467d9b1dd1096f161a83",
            "03c6a89889cb44d98b1a9ce788088a10",
            "505a0197233e4c5aa7c6e6e98e11e245",
            "2ef4b357bbd64bdaa10197bae69d7bac",
            "844f27566bfd47b187c53cf95f6892d2",
            "6394854b894b4ed08ea6a7d7aebda655",
            "b9384ed494d04b6986b1288ef98535f3",
            "5cafb00bc5174d908ebb62d3a89b4d6a",
            "9ff9f762ab8e4d799896bd7a076dcc62",
            "c56297bd321b4449bb3417bb14056f99",
            "2b3b6d5112d849808328064301c54043",
            "b25f5c2839a441b89d956388bae8671b",
            "c8d0f972937443df86c046c5fac48036",
            "dea14f2b97b8496dbcfd30a1bde8ba1a",
            "54caed64fa214117966f4e064f8f9f50",
            "e50da815fbe040e5aa64c04aac264700"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "019a29f1b10145c8abc20308424e20ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e7407be543141a5b2157b3026af7bfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6394854b894b4ed08ea6a7d7aebda655"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "JZ1r9jZrKK54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BART generation config parameters\n",
        "forbidden_begin_tokens = [tokenizer.convert_tokens_to_ids('We')]\n",
        "\n",
        "forbidden_tokens = [\n",
        "     # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "     tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "     # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "     tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "     #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "     #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "\n",
        "BART_generation_parameters = {\n",
        "    'max_length' : 150,\n",
        "    'min_length' : 60,\n",
        "    'length_penalty' : 2.0,\n",
        "    'do_sample' : True,\n",
        "    'num_beams' : 4,\n",
        "    'temperature' : 0.5,\n",
        "    'begin_suppress_tokens' : forbidden_begin_tokens,\n",
        "    'suppress_tokens' : forbidden_tokens,\n",
        "    'repetition_penalty' : 1.8,\n",
        "    'no_repeat_ngram_size' : 3\n",
        "}\n",
        "\n",
        "\n",
        "# Training hyper-parameters\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "1kcRzvxBzes6"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BART Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                                **BART_generation_parameters)\n",
        "\n",
        "#model.generation_config = BART_generation_parameters\n",
        "\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "use_XLA = False\n",
        "if use_XLA:\n",
        "  model.generation_config.no_repeat_ngram_size = 0 # In order to use XLA Generation\n",
        "\n",
        "print(model.generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcl-QF2twE6j",
        "outputId": "ca6c02c3-72d7-492a-915a-8679e253d14a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"begin_suppress_tokens\": [\n",
            "    170\n",
            "  ],\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"do_sample\": true,\n",
            "  \"early_stopping\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_bos_token_id\": 0,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 150,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"suppress_tokens\": [\n",
            "    1698,\n",
            "    32687\n",
            "  ],\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True)"
      ],
      "metadata": {
        "id": "I5X0CdWU3Z2n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "OQiVB8may_xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftwsFHUAEfUT",
        "outputId": "7770d2a7-9369-417f-cd64-f736d369caf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"tf_bart_for_conditional_generation\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " model (TFBartMainLayer)     multiple                  139420416 \n",
            "                                                                 \n",
            " final_logits_bias (BiasLay  multiple                  50265     \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 139470681 (532.04 MB)\n",
            "Trainable params: 139420416 (531.85 MB)\n",
            "Non-trainable params: 50265 (196.35 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tensorboard_callback = TensorBoard(log_dir=BART_PATH+'/BART_model_save/logs')\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "# push_to_hub_callback = PushToHubCallback(\n",
        "#     output_dir=\"./model_save\",\n",
        "#     tokenizer=tokenizer,\n",
        "#     hub_model_id=\"iguerrasevillano/BART-summ-v1\",\n",
        "#     save_strategy='epoch',\n",
        "#     checkpoint=True\n",
        "# )\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "YLOqj07QtPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset,\n",
        "              validation_data=validation_dataset,\n",
        "              epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-fa8UHNoaZn",
        "outputId": "165c4aba-fc55-49c2-d3e4-54a41d7d6583"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO: fine-tuning model...]\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7d8e5695d6c0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7d8e5695d6c0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "81/81 [==============================] - 4104s 50s/step - loss: 3.8688 - val_loss: 3.3719 - rouge1: 39.2353 - rouge2: 10.1921 - rougeL: 22.2063 - rougeLsum: 32.0559 - gen_len: 86.5864\n",
            "Epoch 2/10\n",
            "81/81 [==============================] - 3653s 45s/step - loss: 3.5389 - val_loss: 3.3010 - rouge1: 39.3726 - rouge2: 10.5648 - rougeL: 22.4130 - rougeLsum: 32.1531 - gen_len: 84.8519\n",
            "Epoch 3/10\n",
            "81/81 [==============================] - 3281s 41s/step - loss: 3.3532 - val_loss: 3.2627 - rouge1: 39.5196 - rouge2: 10.8404 - rougeL: 22.6945 - rougeLsum: 32.3229 - gen_len: 83.2654\n",
            "Epoch 4/10\n",
            "81/81 [==============================] - 3475s 43s/step - loss: 3.2109 - val_loss: 3.2532 - rouge1: 39.6113 - rouge2: 10.6516 - rougeL: 22.5902 - rougeLsum: 32.1820 - gen_len: 82.9136\n",
            "Epoch 5/10\n",
            "81/81 [==============================] - 3333s 41s/step - loss: 3.0813 - val_loss: 3.2428 - rouge1: 40.2920 - rouge2: 10.7273 - rougeL: 22.6018 - rougeLsum: 32.7908 - gen_len: 82.3951\n",
            "Epoch 6/10\n",
            "81/81 [==============================] - 3316s 41s/step - loss: 2.9701 - val_loss: 3.2385 - rouge1: 40.3328 - rouge2: 10.6940 - rougeL: 22.6790 - rougeLsum: 32.5761 - gen_len: 80.4938\n",
            "Epoch 7/10\n",
            "81/81 [==============================] - 3400s 42s/step - loss: 2.8622 - val_loss: 3.2454 - rouge1: 39.8963 - rouge2: 10.7066 - rougeL: 22.3823 - rougeLsum: 32.2690 - gen_len: 82.3951\n",
            "Epoch 8/10\n",
            "81/81 [==============================] - 3634s 45s/step - loss: 2.7686 - val_loss: 3.2587 - rouge1: 40.2235 - rouge2: 10.9267 - rougeL: 22.7431 - rougeLsum: 33.0138 - gen_len: 83.3951\n",
            "Epoch 9/10\n",
            "81/81 [==============================] - 3248s 40s/step - loss: 2.6739 - val_loss: 3.2882 - rouge1: 40.2523 - rouge2: 10.8509 - rougeL: 22.5084 - rougeLsum: 33.1142 - gen_len: 82.6543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/vocab.json',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/merges.txt',\n",
              " '/content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_path + 'training_history.json', 'w') as file:\n",
        "    json.dump(H.history, file)"
      ],
      "metadata": {
        "id": "9HMOzV0tMAdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the source folder\n",
        "weights_last_epoch = 'tf_model.h5'\n",
        "\n",
        "source_file_path = os.path.join(save_path, weights_last_epoch)\n",
        "os.makedirs(save_path + 'last_epoch/')\n",
        "destination_file_path = os.path.join(save_path, 'last_epoch/', weights_last_epoch)\n",
        "shutil.move(source_file_path, destination_file_path)\n",
        "\n",
        "print(f\"File {weights_last_epoch} moved from {source_file_path} to {destination_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB0o_XV-BPLe",
        "outputId": "5f64ae63-3c6e-48f0-f4fe-f6bb4f01050b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File tf_model.h5 moved from /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/tf_model.h5 to /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/BART/model_save/sampling-norep-v4/last_epoch/tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change name\n",
        "original_path = save_path + '/weights.h5'\n",
        "new_name = 'tf_model.h5'\n",
        "\n",
        "# Create the new file path\n",
        "new_path = os.path.join(os.path.dirname(original_path), new_name)\n",
        "\n",
        "# Rename the file\n",
        "os.rename(original_path, new_path)\n",
        "\n",
        "print(f\"File weights.h5 renamed as {new_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26nxeyrWC6RN",
        "outputId": "c9dfef50-7ba3-4a19-f217-4b58a5c6a606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File weights.h5 renamed as tf_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "N1nzU8JqwmtZ",
        "outputId": "ff899fd2-1fdb-4024-8ad2-2a7068688700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5"
      ],
      "metadata": {
        "id": "k9aB2nco5LxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google-t5/t5-small'\n",
        "max_input_length = 1024\n",
        "\n",
        "save_name = 'sampling-rep-v0/'\n",
        "save_path = BASE_PATH + '/Results/TLDR/T5/model_save/' + save_name"
      ],
      "metadata": {
        "id": "Lfg9uUqr5ZSz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name, errors='ignore')\n",
        "prefix = 'summarize: '\n",
        "\n",
        "# Function in order to tokenize source and target\n",
        "max_input_length = 512\n",
        "\n",
        "def tokenize_function(data):\n",
        "  inputs = [prefix + input for input in data['extractive_summary']]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(data['target'], padding=True)\n",
        "\n",
        "  # The target input_ids tokens are added to the model inputs\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "  return model_inputs\n",
        "\n",
        "tokenized_data = raw_data.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302,
          "referenced_widgets": [
            "f8b64d71d26f49439e34d0834ce5455a",
            "9e3da80d01794ee094242f0d0c6a3e51",
            "ab18d94056104dacacd3c4971d71d1b4",
            "6442ea4f67fe453dacdcbd069928e805",
            "d20b33bf127347c8aa72dc0b2759d520",
            "9dd6c163acdd48ecaae7fee02da727a5",
            "a85736c8a99e480896488e786c57f9f0",
            "df6a13b2ba104274b539287dabebee81",
            "dade38ff1ebe4518bf9432dfa9cbc780",
            "d990e0f96f5642a3ad479da59cc4adf7",
            "75f8025ad3f54cdb9dfa08576603ad21",
            "c61abf6fac8444b187b3a130dd6c82e7",
            "bc99e7edeead481f8f5b16e18a1e0806",
            "7ab2295468924a5295f2777776646d7a",
            "63a64226267e43cab9a4d8266a641897",
            "613e9c182dd74bcc9857e75b67de926f",
            "44d2e75a08c449138ec0d253037bfa91",
            "0f9d8c544609469ab650c3629a0d68ea",
            "55649441611b48159ca212f3da912fad",
            "551f56d439574690895d00d58900777d",
            "b2e905f790064ea1906ef31fbc060bda",
            "43e6e835f71544ee8693ed7a7aa36717",
            "5a90b4b26d69417abc0a1b957727348c",
            "e444e326124b4d35862c94e38bf83ee0",
            "a21dbb7162e24ed3a8e04cb051d3d977",
            "0e2b82d5ccd54f4b9a8bab7428baa5c0",
            "2a09e73599e64012a8633d1ca67e46c4",
            "81d2569f18cf45c184909c221baa7aa2",
            "9ade6d2a01a443348a63a87274846432",
            "e445aecb13db42379714f36ab945c191",
            "df3d79b4970e4a3cbd904cb3d04145c1",
            "f7f4bb1c0114471dbecdbda9c109f84c",
            "93b59f58a1054f6088b8e69edc1a880d",
            "688c6430c55e4bad9fcb386ca5e0cde7",
            "261a9df27765480491d8cfa743a84a86",
            "f1d29f6ca4a24d1ca526784b66a6f54a",
            "44d3d16a8b814f8aaca47728c1c340c7",
            "109a4844b7394207aa3a9d541392ca18",
            "02115bc0b3094725bff10a434b1c3fbd",
            "caeaa027d95749f6a99770901f31efbe",
            "17ceb5c2f0254012aa2b65e805111751",
            "3b5c501b840542b78f5bdc7949795d94",
            "cd0e7c62cb324efd8accf204058195ad",
            "fe57cbe0bcf84f2880865aa2d9406fb1",
            "b0d0c713d5a24be5bd0dc57a5649d929",
            "105f2271e03043088aa1e3fee66a0881",
            "3246b04e24804dadb9ef1748bbe9289c",
            "658b18be49134550b53fad7d7c8ee136",
            "0240778de93549348b3ad4ae2e48c4c7",
            "f23fef37cb7e46aaabc31d04f25eb562",
            "d2ebf189194b4818a6277c683db2dcaa",
            "74a8ac202558410a8c9eeeb863ecaf3a",
            "4a041b2b55ff42a5bdb7e858c9dd7b31",
            "f600546d1618429aa5d5369dd06b48c7",
            "c5b07a4c6c0b4545950c314d91be3ffc",
            "fb7c2b50e11b48a98c3564521c5a8f51",
            "fcf37b5520724f25b3bfc06f7d77b296",
            "007b6e25316d4666aa39153269a12c31",
            "875b3332785d4f81aca59746fdeac3ce",
            "723f9c8fbeea47be8813607cc991cd1a",
            "aa438f2f3a544159811bdb327e66b199",
            "a32235c1469b4962a7432ac744fba288",
            "b7ff5f8f247c498893f61e2d8c533197",
            "e0427a261d95469cbf56113e79bd2edc",
            "6be304ae878e461491bc7f474ea692f4",
            "558f9e1a04dc48158a6e8313d6c05fa4"
          ]
        },
        "id": "gjHkaqLO5LXt",
        "outputId": "a1a2a133-7666-48f3-da9b-1f572b52231c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8b64d71d26f49439e34d0834ce5455a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c61abf6fac8444b187b3a130dd6c82e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a90b4b26d69417abc0a1b957727348c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "688c6430c55e4bad9fcb386ca5e0cde7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0d0c713d5a24be5bd0dc57a5649d929"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb7c2b50e11b48a98c3564521c5a8f51"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# T5 generation config parameters\n",
        "forbidden_begin_tokens = [tokenizer.convert_tokens_to_ids('We')]\n",
        "\n",
        "forbidden_tokens = [\n",
        "     # tokenizer.convert_tokens_to_ids('Ġpaper'),\n",
        "     tokenizer.convert_tokens_to_ids('ĠPro'),\n",
        "     # tokenizer.convert_tokens_to_ids('Ġauthors'),\n",
        "     tokenizer.convert_tokens_to_ids('ĠIntrodu')]\n",
        "     #  tokenizer.convert_tokens_to_ids('Ġwork'),\n",
        "     #  tokenizer.convert_tokens_to_ids('Ġmethod')]\n",
        "\n",
        "forbidden_words = ['We', 'we', 'propose', 'Proposes']\n",
        "\n",
        "T5_generation_parameters = {\n",
        "    'max_length' : 100,\n",
        "    'min_length' : 60,\n",
        "    'length_penalty' : 2.0,\n",
        "    'num_beams' : 4,\n",
        "    'do_sample' : True,\n",
        "    'temperature' : 0.5,\n",
        "    'bad_words_ids' : tokenizer(forbidden_words,\n",
        "                                add_special_tokens=False).input_ids,\n",
        "    'repetition_penalty' : 1.8,\n",
        "    'no_repeat_ngram_size' : 3\n",
        "}\n",
        "\n",
        "\n",
        "# Training hyper-parameters\n",
        "epochs = 12\n",
        "batch_size = 8\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "id": "L20kOSF15lS9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 Base-Model\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "if not model.generation_config.do_sample:\n",
        "  model.generation_config.num_beam_groups = 4\n",
        "  model.generation_config.diversity_penalty = 0.5\n",
        "\n",
        "\n",
        "print(model.generation_config)\n",
        "\n",
        "use_XLA = False\n",
        "\n",
        "batch_size = 8\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZ0geKLy5kaT",
        "outputId": "8fb85952-b96e-45d4-e29c-42e4dd0e738b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"diversity_penalty\": 0.5,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"num_beam_groups\": 4,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True, pad_to_multiple_of=128)\n",
        "\n",
        "if use_XLA:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", pad_to_multiple_of=128)\n",
        "else:\n",
        "  generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", padding=True, pad_to_multiple_of=128)"
      ],
      "metadata": {
        "id": "m1ygHfFP--H6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['train'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    drop_remainder=False,\n",
        ")\n",
        "\n",
        "validation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "generation_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_data['validation'],\n",
        "    batch_size=2*batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "d8G2KNJn_A5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "optimizer = AdamWeightDecay(\n",
        "    learning_rate=learning_rate, weight_decay_rate=weight_decay\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pHtNgWTF_B29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn,\n",
        "    eval_dataset=generation_dataset,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=use_XLA\n",
        ")\n",
        "\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=save_path+\"/weights.h5\",\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=True,\n",
        "    mode='min',\n",
        ")\n",
        "\n",
        "stop_early = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "5ypB-dyF_JNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [metric_callback,\n",
        "            #  push_to_hub_callback,\n",
        "             stop_early,\n",
        "             checkpoint_callback]\n",
        "\n",
        "# Train\n",
        "print('[INFO: fine-tuning model...]')\n",
        "H = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=callbacks)\n",
        "\n",
        "# Save the model and tokenizer to a directory\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ],
      "metadata": {
        "id": "IyaSZ1H4_Ly1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graphics(H)\n",
        "plt.savefig(save_path + '/history.png')"
      ],
      "metadata": {
        "id": "cArEDqsN_SE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "gHCDrFwvNMKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_models = {'sampling-norep-v0' : 'sampling-norep-v0/'}\n",
        "              #  'sampling-norep-v1' : 'sampling-norep-v1/',\n",
        "              #  'greedy-norep-v0' : 'greedy-norep-v0/',\n",
        "              #  'greedy-norep-v1' : 'greedy-norep-v1/',\n",
        "              #  'greedy-norep-v2' : 'greedy-norep-v2/',\n",
        "              #  'greedy-norep-v3' : 'greedy-norep-v3/',\n",
        "              #  'sampling-norep-v3' : 'sampling-norep-v3',\n",
        "              #  'sampling-norep-v4' : 'sampling-norep-v4'}\n",
        "\n",
        "model = 'T5/'\n",
        "\n",
        "\n",
        "save_paths = {}\n",
        "models = {}\n",
        "# tokenizers = {}\n",
        "\n",
        "for name in name_models:\n",
        "  save_paths[name] = BASE_PATH + '/Results/TLDR/' + model + 'model_save/' + name_models[name]\n",
        "\n",
        "  models[name] = TFAutoModelForSeq2SeqLM.from_pretrained(save_paths[name])\n",
        "  # tokenizers[name] = BartTokenizer.from_pretrained(save_path[name])\n",
        "  print(models[name].generation_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Tjj2cjL2mE",
        "outputId": "5f1c1cb6-57e1-4f34-da23-a7ea7072354b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/VIU/TFM/Desarrollo//Results/TLDR/T5/model_save/sampling-norep-v0/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GenerationConfig {\n",
            "  \"bad_words_ids\": [\n",
            "    [\n",
            "      101\n",
            "    ],\n",
            "    [\n",
            "      62\n",
            "    ],\n",
            "    [\n",
            "      4230\n",
            "    ],\n",
            "    [\n",
            "      13543,\n",
            "      32,\n",
            "      2260\n",
            "    ]\n",
            "  ],\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 2.0,\n",
            "  \"max_length\": 100,\n",
            "  \"min_length\": 60,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 1.8,\n",
            "  \"temperature\": 0.5\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v0'"
      ],
      "metadata": {
        "id": "epSxP5oRBEF4"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = models[name].generate(tokenized_data['test']['input_ids'][:1],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)"
      ],
      "metadata": {
        "id": "xRDHYfXcJQvS"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HhUafQ-DSow",
        "outputId": "a3679476-0cd9-47dd-f85b-eb40f843ef8a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A variational Dirichlet algorithm for deep neural network classification problem and a smoothing method to alleviate the over-concentration issue. This paper introduces a variational dirichlet framework for deep image classification problems, which can greatly widen the distance between in-and out-of-distribution datasets.']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'sampling-norep-v0'\n",
        "\n",
        "generated_summaries = []\n",
        "rouge_scores = []\n",
        "accumulated_metrics = {}\n",
        "\n",
        "n = len(tokenized_data['test']['input_ids'])\n",
        "batch_size = 2 # Must be greater than 1\n",
        "\n",
        "# Last batch is dropped (just one sample if batch_size=2) to average ROUGE scores properly\n",
        "for i in range(0, n-1, batch_size):\n",
        "  if i % 10 == 0:\n",
        "    print(f'[INFO]: {i}/{n-1}')\n",
        "\n",
        "\n",
        "  generated_ids = models[name].generate(tokenized_data['test']['input_ids'][i:i+batch_size],\n",
        "                                        generation_config=models[name].generation_config,\n",
        "                                        seed=42)\n",
        "  reference_labels = np.array(tokenized_data['test']['labels'][i:i+batch_size])\n",
        "\n",
        "  predicted_labels = np.array(generated_ids)\n",
        "\n",
        "  current_metrics = metric_fn((predicted_labels, reference_labels))\n",
        "  accumulated_metrics = {k : current_metrics.get(k, 0) + accumulated_metrics.get(k, 0) for k in current_metrics.keys()}\n",
        "\n",
        "  # Predicted summaries, keep the rouge scores\n",
        "  generated_summaries.append(tokenizer.batch_decode(generated_ids, skip_special_tokens=True))\n",
        "  rouge_scores.append(current_metrics)\n",
        "\n",
        "# Undo nested list\n",
        "generated_summaries = [summary for summaries in generated_summaries for summary in summaries]\n",
        "average_rouge_scores = {k : accumulated_metrics.get(k, 0)/len(range(0, n-1, batch_size)) for k in accumulated_metrics.keys()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisXx8nuVDQ4",
        "outputId": "5e4adc93-4dda-46c5-8319-222393e49148"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: 0/202\n",
            "[INFO]: 10/202\n",
            "[INFO]: 20/202\n",
            "[INFO]: 30/202\n",
            "[INFO]: 40/202\n",
            "[INFO]: 50/202\n",
            "[INFO]: 60/202\n",
            "[INFO]: 70/202\n",
            "[INFO]: 80/202\n",
            "[INFO]: 90/202\n",
            "[INFO]: 100/202\n",
            "[INFO]: 110/202\n",
            "[INFO]: 120/202\n",
            "[INFO]: 130/202\n",
            "[INFO]: 140/202\n",
            "[INFO]: 150/202\n",
            "[INFO]: 160/202\n",
            "[INFO]: 170/202\n",
            "[INFO]: 180/202\n",
            "[INFO]: 190/202\n",
            "[INFO]: 200/202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test = data_test[:-1].reset_index(drop=True)\n",
        "updated_data_test['abstractive_summary'] = pd.Series(generated_summaries)\n",
        "\n",
        "updated_data_test['number_words_abstractive'] = count_words(updated_data_test, 'abstractive_summary')\n",
        "\n",
        "updated_data_test['length_difference'] = updated_data_test['number_words_target'] - updated_data_test['number_words_abstractive']\n",
        "\n",
        "updated_data_test.to_csv(save_path + '/updated_data_test.csv', index=False)\n",
        "\n",
        "summaries = updated_data_test[['target', 'abstractive_summary']]\n",
        "summaries.to_csv(save_path + '/summaries.csv', index=False)\n",
        "\n",
        "with open(save_path + '/average_rouge_scores.json', 'w') as json_scores:\n",
        "    json.dump(average_rouge_scores, json_scores, indent=4)"
      ],
      "metadata": {
        "id": "ms1HilZokIQF"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['length_difference'].describe()"
      ],
      "metadata": {
        "id": "Ul2eFAgQkNEI",
        "outputId": "fbbf3116-76d5-4cc2-8e55-fe5d522d04ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    202.000000\n",
              "mean       3.683168\n",
              "std       19.394276\n",
              "min      -42.000000\n",
              "25%      -10.000000\n",
              "50%        4.000000\n",
              "75%       16.000000\n",
              "max       59.000000\n",
              "Name: length_difference, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data_test['abstractive_summary'][130]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gCZBrdBLBjv4",
        "outputId": "f3d32a4d-b069-40f2-af7c-7a88b59b8ef9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A method to generate out-distribution samples from over-generalized regions, reducing the risk of misclassifying both adversaries and samples using augmented CNNs. This paper introduces a method for learning out-department samples in a \"dustbin\" sub-manifold, which can be used to train out-destribution examples.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dw4jYBYYWZCh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYEsFGRMOTkVJVk28MWwZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3c91162751547aa84af7ee52ec3e3e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97838b1ce94544319c448dde22e9c42c",
              "IPY_MODEL_301e73c3cbae4b5fbdbe4067125f4424",
              "IPY_MODEL_7cb519780a024557b6956ca7c5d3f6a3"
            ],
            "layout": "IPY_MODEL_b12cac50346b4f1785966aadbeb1a32c"
          }
        },
        "97838b1ce94544319c448dde22e9c42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85e44ec55aa4c83858392c5aa3c3415",
            "placeholder": "​",
            "style": "IPY_MODEL_1aaff1b282494056a14e90402cbe5cd1",
            "value": "Downloading builder script: "
          }
        },
        "301e73c3cbae4b5fbdbe4067125f4424": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27f42a728da3483b82971a6e22a8ac9d",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c295f8fcb1648e3adac146d1a21c92d",
            "value": 2169
          }
        },
        "7cb519780a024557b6956ca7c5d3f6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_730e4ff0b7f54adcbb28c72c1e5c80dc",
            "placeholder": "​",
            "style": "IPY_MODEL_13c5422277a740f298db19ce9cde93b6",
            "value": " 5.65k/? [00:00&lt;00:00, 448kB/s]"
          }
        },
        "b12cac50346b4f1785966aadbeb1a32c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85e44ec55aa4c83858392c5aa3c3415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aaff1b282494056a14e90402cbe5cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27f42a728da3483b82971a6e22a8ac9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c295f8fcb1648e3adac146d1a21c92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "730e4ff0b7f54adcbb28c72c1e5c80dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c5422277a740f298db19ce9cde93b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "019a29f1b10145c8abc20308424e20ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87bd56f9f9db417c81d87b260e64717a",
              "IPY_MODEL_e953775064244191a3245084abd7b949",
              "IPY_MODEL_3e0d973080be48c3a61ca7bec49f30f0"
            ],
            "layout": "IPY_MODEL_32ac86f5ff984aedbefd8393f1953c92"
          }
        },
        "87bd56f9f9db417c81d87b260e64717a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255e5a72897049f9b2d16f5f717a7abc",
            "placeholder": "​",
            "style": "IPY_MODEL_4edaa22c96884cf6a629cc6e8b277cbd",
            "value": "Map: 100%"
          }
        },
        "e953775064244191a3245084abd7b949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d41ba5b64e441caa852290dd525bf5e",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb71b0e1b402499095482662879aeab5",
            "value": 647
          }
        },
        "3e0d973080be48c3a61ca7bec49f30f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_223fc245f51a45e39b4f896fab8b139a",
            "placeholder": "​",
            "style": "IPY_MODEL_578becc7bb2146c587dca97aea75ad16",
            "value": " 647/647 [00:03&lt;00:00, 163.54 examples/s]"
          }
        },
        "32ac86f5ff984aedbefd8393f1953c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "255e5a72897049f9b2d16f5f717a7abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edaa22c96884cf6a629cc6e8b277cbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d41ba5b64e441caa852290dd525bf5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb71b0e1b402499095482662879aeab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "223fc245f51a45e39b4f896fab8b139a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578becc7bb2146c587dca97aea75ad16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7407be543141a5b2157b3026af7bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcaa9abbccee4df0bdee6114eff41c45",
              "IPY_MODEL_61672ed9c6d74592a1a8b1c6a0b5fe9c",
              "IPY_MODEL_a7feab89338642ec8b2cd300bf1494bd"
            ],
            "layout": "IPY_MODEL_d6a8ec6ba5ad4b2cb30f24a0daa5fc94"
          }
        },
        "fcaa9abbccee4df0bdee6114eff41c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22130b81744c410093e46123bc3d58df",
            "placeholder": "​",
            "style": "IPY_MODEL_90a902c031ac467d9b1dd1096f161a83",
            "value": "Map: 100%"
          }
        },
        "61672ed9c6d74592a1a8b1c6a0b5fe9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03c6a89889cb44d98b1a9ce788088a10",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_505a0197233e4c5aa7c6e6e98e11e245",
            "value": 162
          }
        },
        "a7feab89338642ec8b2cd300bf1494bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ef4b357bbd64bdaa10197bae69d7bac",
            "placeholder": "​",
            "style": "IPY_MODEL_844f27566bfd47b187c53cf95f6892d2",
            "value": " 162/162 [00:01&lt;00:00, 141.59 examples/s]"
          }
        },
        "d6a8ec6ba5ad4b2cb30f24a0daa5fc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22130b81744c410093e46123bc3d58df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a902c031ac467d9b1dd1096f161a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03c6a89889cb44d98b1a9ce788088a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "505a0197233e4c5aa7c6e6e98e11e245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ef4b357bbd64bdaa10197bae69d7bac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844f27566bfd47b187c53cf95f6892d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6394854b894b4ed08ea6a7d7aebda655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9384ed494d04b6986b1288ef98535f3",
              "IPY_MODEL_5cafb00bc5174d908ebb62d3a89b4d6a",
              "IPY_MODEL_9ff9f762ab8e4d799896bd7a076dcc62"
            ],
            "layout": "IPY_MODEL_c56297bd321b4449bb3417bb14056f99"
          }
        },
        "b9384ed494d04b6986b1288ef98535f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3b6d5112d849808328064301c54043",
            "placeholder": "​",
            "style": "IPY_MODEL_b25f5c2839a441b89d956388bae8671b",
            "value": "Map: 100%"
          }
        },
        "5cafb00bc5174d908ebb62d3a89b4d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d0f972937443df86c046c5fac48036",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dea14f2b97b8496dbcfd30a1bde8ba1a",
            "value": 203
          }
        },
        "9ff9f762ab8e4d799896bd7a076dcc62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54caed64fa214117966f4e064f8f9f50",
            "placeholder": "​",
            "style": "IPY_MODEL_e50da815fbe040e5aa64c04aac264700",
            "value": " 203/203 [00:01&lt;00:00, 143.46 examples/s]"
          }
        },
        "c56297bd321b4449bb3417bb14056f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b3b6d5112d849808328064301c54043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b25f5c2839a441b89d956388bae8671b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8d0f972937443df86c046c5fac48036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea14f2b97b8496dbcfd30a1bde8ba1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54caed64fa214117966f4e064f8f9f50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50da815fbe040e5aa64c04aac264700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8b64d71d26f49439e34d0834ce5455a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e3da80d01794ee094242f0d0c6a3e51",
              "IPY_MODEL_ab18d94056104dacacd3c4971d71d1b4",
              "IPY_MODEL_6442ea4f67fe453dacdcbd069928e805"
            ],
            "layout": "IPY_MODEL_d20b33bf127347c8aa72dc0b2759d520"
          }
        },
        "9e3da80d01794ee094242f0d0c6a3e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dd6c163acdd48ecaae7fee02da727a5",
            "placeholder": "​",
            "style": "IPY_MODEL_a85736c8a99e480896488e786c57f9f0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ab18d94056104dacacd3c4971d71d1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6a13b2ba104274b539287dabebee81",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dade38ff1ebe4518bf9432dfa9cbc780",
            "value": 2324
          }
        },
        "6442ea4f67fe453dacdcbd069928e805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d990e0f96f5642a3ad479da59cc4adf7",
            "placeholder": "​",
            "style": "IPY_MODEL_75f8025ad3f54cdb9dfa08576603ad21",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 197kB/s]"
          }
        },
        "d20b33bf127347c8aa72dc0b2759d520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd6c163acdd48ecaae7fee02da727a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85736c8a99e480896488e786c57f9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df6a13b2ba104274b539287dabebee81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dade38ff1ebe4518bf9432dfa9cbc780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d990e0f96f5642a3ad479da59cc4adf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f8025ad3f54cdb9dfa08576603ad21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c61abf6fac8444b187b3a130dd6c82e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc99e7edeead481f8f5b16e18a1e0806",
              "IPY_MODEL_7ab2295468924a5295f2777776646d7a",
              "IPY_MODEL_63a64226267e43cab9a4d8266a641897"
            ],
            "layout": "IPY_MODEL_613e9c182dd74bcc9857e75b67de926f"
          }
        },
        "bc99e7edeead481f8f5b16e18a1e0806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44d2e75a08c449138ec0d253037bfa91",
            "placeholder": "​",
            "style": "IPY_MODEL_0f9d8c544609469ab650c3629a0d68ea",
            "value": "spiece.model: 100%"
          }
        },
        "7ab2295468924a5295f2777776646d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55649441611b48159ca212f3da912fad",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_551f56d439574690895d00d58900777d",
            "value": 791656
          }
        },
        "63a64226267e43cab9a4d8266a641897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2e905f790064ea1906ef31fbc060bda",
            "placeholder": "​",
            "style": "IPY_MODEL_43e6e835f71544ee8693ed7a7aa36717",
            "value": " 792k/792k [00:00&lt;00:00, 13.5MB/s]"
          }
        },
        "613e9c182dd74bcc9857e75b67de926f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d2e75a08c449138ec0d253037bfa91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f9d8c544609469ab650c3629a0d68ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55649441611b48159ca212f3da912fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "551f56d439574690895d00d58900777d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2e905f790064ea1906ef31fbc060bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e6e835f71544ee8693ed7a7aa36717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a90b4b26d69417abc0a1b957727348c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e444e326124b4d35862c94e38bf83ee0",
              "IPY_MODEL_a21dbb7162e24ed3a8e04cb051d3d977",
              "IPY_MODEL_0e2b82d5ccd54f4b9a8bab7428baa5c0"
            ],
            "layout": "IPY_MODEL_2a09e73599e64012a8633d1ca67e46c4"
          }
        },
        "e444e326124b4d35862c94e38bf83ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d2569f18cf45c184909c221baa7aa2",
            "placeholder": "​",
            "style": "IPY_MODEL_9ade6d2a01a443348a63a87274846432",
            "value": "tokenizer.json: 100%"
          }
        },
        "a21dbb7162e24ed3a8e04cb051d3d977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e445aecb13db42379714f36ab945c191",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df3d79b4970e4a3cbd904cb3d04145c1",
            "value": 1389353
          }
        },
        "0e2b82d5ccd54f4b9a8bab7428baa5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f4bb1c0114471dbecdbda9c109f84c",
            "placeholder": "​",
            "style": "IPY_MODEL_93b59f58a1054f6088b8e69edc1a880d",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 17.1MB/s]"
          }
        },
        "2a09e73599e64012a8633d1ca67e46c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81d2569f18cf45c184909c221baa7aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ade6d2a01a443348a63a87274846432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e445aecb13db42379714f36ab945c191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3d79b4970e4a3cbd904cb3d04145c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7f4bb1c0114471dbecdbda9c109f84c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93b59f58a1054f6088b8e69edc1a880d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688c6430c55e4bad9fcb386ca5e0cde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_261a9df27765480491d8cfa743a84a86",
              "IPY_MODEL_f1d29f6ca4a24d1ca526784b66a6f54a",
              "IPY_MODEL_44d3d16a8b814f8aaca47728c1c340c7"
            ],
            "layout": "IPY_MODEL_109a4844b7394207aa3a9d541392ca18"
          }
        },
        "261a9df27765480491d8cfa743a84a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02115bc0b3094725bff10a434b1c3fbd",
            "placeholder": "​",
            "style": "IPY_MODEL_caeaa027d95749f6a99770901f31efbe",
            "value": "Map: 100%"
          }
        },
        "f1d29f6ca4a24d1ca526784b66a6f54a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17ceb5c2f0254012aa2b65e805111751",
            "max": 647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b5c501b840542b78f5bdc7949795d94",
            "value": 647
          }
        },
        "44d3d16a8b814f8aaca47728c1c340c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd0e7c62cb324efd8accf204058195ad",
            "placeholder": "​",
            "style": "IPY_MODEL_fe57cbe0bcf84f2880865aa2d9406fb1",
            "value": " 647/647 [00:02&lt;00:00, 232.87 examples/s]"
          }
        },
        "109a4844b7394207aa3a9d541392ca18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02115bc0b3094725bff10a434b1c3fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caeaa027d95749f6a99770901f31efbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17ceb5c2f0254012aa2b65e805111751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b5c501b840542b78f5bdc7949795d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd0e7c62cb324efd8accf204058195ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe57cbe0bcf84f2880865aa2d9406fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0d0c713d5a24be5bd0dc57a5649d929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_105f2271e03043088aa1e3fee66a0881",
              "IPY_MODEL_3246b04e24804dadb9ef1748bbe9289c",
              "IPY_MODEL_658b18be49134550b53fad7d7c8ee136"
            ],
            "layout": "IPY_MODEL_0240778de93549348b3ad4ae2e48c4c7"
          }
        },
        "105f2271e03043088aa1e3fee66a0881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f23fef37cb7e46aaabc31d04f25eb562",
            "placeholder": "​",
            "style": "IPY_MODEL_d2ebf189194b4818a6277c683db2dcaa",
            "value": "Map: 100%"
          }
        },
        "3246b04e24804dadb9ef1748bbe9289c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74a8ac202558410a8c9eeeb863ecaf3a",
            "max": 162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a041b2b55ff42a5bdb7e858c9dd7b31",
            "value": 162
          }
        },
        "658b18be49134550b53fad7d7c8ee136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f600546d1618429aa5d5369dd06b48c7",
            "placeholder": "​",
            "style": "IPY_MODEL_c5b07a4c6c0b4545950c314d91be3ffc",
            "value": " 162/162 [00:00&lt;00:00, 236.78 examples/s]"
          }
        },
        "0240778de93549348b3ad4ae2e48c4c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f23fef37cb7e46aaabc31d04f25eb562": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ebf189194b4818a6277c683db2dcaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74a8ac202558410a8c9eeeb863ecaf3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a041b2b55ff42a5bdb7e858c9dd7b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f600546d1618429aa5d5369dd06b48c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b07a4c6c0b4545950c314d91be3ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7c2b50e11b48a98c3564521c5a8f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcf37b5520724f25b3bfc06f7d77b296",
              "IPY_MODEL_007b6e25316d4666aa39153269a12c31",
              "IPY_MODEL_875b3332785d4f81aca59746fdeac3ce"
            ],
            "layout": "IPY_MODEL_723f9c8fbeea47be8813607cc991cd1a"
          }
        },
        "fcf37b5520724f25b3bfc06f7d77b296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa438f2f3a544159811bdb327e66b199",
            "placeholder": "​",
            "style": "IPY_MODEL_a32235c1469b4962a7432ac744fba288",
            "value": "Map: 100%"
          }
        },
        "007b6e25316d4666aa39153269a12c31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7ff5f8f247c498893f61e2d8c533197",
            "max": 203,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0427a261d95469cbf56113e79bd2edc",
            "value": 203
          }
        },
        "875b3332785d4f81aca59746fdeac3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be304ae878e461491bc7f474ea692f4",
            "placeholder": "​",
            "style": "IPY_MODEL_558f9e1a04dc48158a6e8313d6c05fa4",
            "value": " 203/203 [00:00&lt;00:00, 231.78 examples/s]"
          }
        },
        "723f9c8fbeea47be8813607cc991cd1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa438f2f3a544159811bdb327e66b199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32235c1469b4962a7432ac744fba288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7ff5f8f247c498893f61e2d8c533197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0427a261d95469cbf56113e79bd2edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6be304ae878e461491bc7f474ea692f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558f9e1a04dc48158a6e8313d6c05fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}