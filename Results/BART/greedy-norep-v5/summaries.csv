target,abstractive_summary
"A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.","A variational Dirichlet framework for deep neural network classification. This work introduces a new uncertainty metric for image classification that is more accurate than existing uncertainty measures. The study presents a new dataset-based uncertainty measure for deep learning, which can be used to detect out-of-distribution examples. The authors investigate the problem of over-concentration in deep neural networks in order to address the problem."
"Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This paper presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.","An unsupervised method for analyzing the contribution of individual neurons to language translation. This paper presents a novel translation control method for language translation that uses a combination of neural networks and a hierarchical model. The authors present a novel approach to interpret language translation using a hierarchical network architecture, and show that neural networks are more interpretable than non-linear models."
We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The paper proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This work proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.,"A deep diagonal-circulant ReLU network that can be decomposed into 4k + 1 diagonal and circulant matrices. This work presents a new method for training deep neural networks that can approximate deep ReLU networks with low rank approximators. This article presents a method to train deep neural nets with high rank decomposition of the matrix A, which can be used to approximate deep RecLU networks."
"The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This study describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.","A novel neural network architecture that can learn to make analogies with visual and symbolic inputs, but only if they are contrasted with alternative incorrect answers that are plausible at the level of relations rather than simple perceptual attributes. This work presents a novel approach to analogical reasoning in neural networks by proposing a new training regime for neural networks that is more naturalistic than human-like."
"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The work addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.",A novel concept annotation task for medical time series data. This work introduces a novel task of predicting and localizing medical concepts by modeling the medical time-series data as input. The study presents a novel method to predict and localize medical concepts based on a combination of machine learning and deep learning models.
"We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This work proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines","A novel multi-way encoding method for binary classification that relaxes and improves the encoding dimensionality. This paper introduces a multi-direction encoding method to reduce the number of possible gradients in binary classification. The authors introduce a new encoding method, called 1of K, which reduces the likelihood of adversarial attacks on binary classification models by reducing the complexity of the encoding."
"We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The paper proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts This article focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up","A spherical CNN that can detect patterns regardless of how they are rotated over the sphere. This article presents a new CNN for spherical signals that is equivariant to the rotation invariant properties of the sphere, and shows that it is more efficient than other CNNs. This work introduces a new class of CNNs, called spherical CNNs (S 2-CNNs), that can be used to detect patterns on the sphere and vice versa."
"Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations","A deep reinforcement learning framework for understanding the dynamics of multi-agent systems. This work presents a novel approach to understanding the behavior of agents in multi-player games. The authors present an RFM module that learns to predict the actions of each agent in a game, and provide insights into the relational and social structures of the agents."
"We introduce a transparent middleware for neural network acceleration, with own compiler engine, achieving up to 11.8x speed up on CPUs and 2.3x on GPUs. This article proposes a transparent middleware layer for neural network acceleration and obtains some acceleration results on basic CPU and GPU architectures",Introduces a transparent middleware layer for neural network acceleration. This article introduces a new framework for deep neural networks acceleration that combines hardware-specific optimizations with deep learning frameworks. This work addresses the problem of optimizing neural networks on multiple hardware platforms by combining hardware and deep learning libraries. The authors introduce a new middleware framework for the optimization of both prediction and training of neural networks. This paper presents a method for optimizing neural network architectures on several hardware platforms
"Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This article studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This work explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.","Real-time dialogue agents are trained to imitate human actions given a goal (a ""goal-oriented"" model). This work presents a novel approach to solving the problem of language drift in video game dialogue by using a large transformer architecture. The authors present a novel model of dialogue agents that can be used to learn to communicate with other agents in a multi-player setting."
"CharNMT is brittle This paper investigates the impact of character-level noise on 4 different neural machine translation systems This study empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. This study investigates the impact of noisy input on Machine Translation and tests ways to make NMT models more robust","A novel character-based CNN model that learns to address synthetic noise by training on noisy texts. This study investigates the problem of synthetic noise generation in NLP and shows that it is more robust to natural human errors than the meanChar model. The authors investigate the issue of natural human noise generation, and show that a character based CNN can learn to address multiple types of noise in a sentence."
"Feedforward neural networks that can have weights pruned after training could have had the same weights pruned before training Shows that there exists sparse subnetworks that can be trained from scratch with good generalization performance and proposes a unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The study examines the hypothesis that randomly initialized neural networks contain sub-networks that converge equally fast or faster and can reach the same or better classification accuracy","An iterative pruning strategy for improving the accuracy and validation accuracy of a fully-connected neural network with a small number of parameters. This study studies the problem of pruning neural networks using stochastic gradient descent, and shows that it is more difficult to find winning tickets than prior pruning strategies. This article presents a method for finding winning tickets for neural networks with a large number of parameter sets."
"A new regularization term can improve your training of wasserstein gans The work proposes a regularization scheme for Wasserstein GAN based on relaxation of the constraints on the Lipschitz constant of 1. The work deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric.","A novel GAN-GP that penalizes the deviation of the gradient norm of the critic function (as a function of the network's input) from one, was proposed as an alternative that improves training. This work introduces a new GAN algorithm for generating real-looking data points by using alternating gradient descent updates. The authors introduce a new algorithm called WGAN-GP which penalizes deviations of the gradients of the discriminator function with respect to its input."
"DeFINE uses a deep, hierarchical, sparse network with new skip connections to learn better word embeddings efficiently. This article describes a new method for learning deep word-level representations efficiently by using a hierarchical structure with skip-connections for the use of low dimensional input and output layers.","A hierarchical group embedding method for deep neural networks. This work introduces a hierarchical group-based embedding framework that enables deep neural network representations to be learned jointly while reducing the number of parameters. The authors introduce a new embedding matrix, called DeFINE, to improve the performance of deep neural models by reducing the computational overhead."
"We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces. Paper's concepts work in the discrete-time formalism, use the master equation, and remove reliance on a locally quadratic approximation of the loss function or on any Gaussian asumptions of the SGD noise. The authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD and use the relations to set training schedule adaptively and analyze the loss-function landscape.",This study studies the fluctuation-dissipation relations of a stationary-state distribution in SGD and shows that they can be used to determine the shape of the loss-function landscape. This work studies the effect of noise on the learning rate of a stochastic gradient on the training rate of an SGD model. The authors investigate the existence of a non-Gaussian distribution in the loss function landscape of a Gaussian model
"We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training. Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network. This work proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks. Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).","Quantum neural network architecture based on a quantum gate-based form of quantum computation as opposed to the classical annealing devices of the past. This work presents a novel method for learning the state of a neural network using binary activation functions, which can be used to train neural networks in parallel. The authors present a new approach to learning a representation of the landscape of a deep neural network by using a binary activation function."
"Transfer learning for estimating causal effects using neural networks. Develops algorithms to estimate conditional average treatment effect by auxiliary dataset in different environments, both with and without base learner. The authors propose methods to address a novel task of transfer learning for estimating the CATE function, and evaluate them using a synthetic setting and a real-world experimental dataset. Using neural network regression and comparing transfer learning frameworks to estimate a conditional average treatment effect under string ignorability assumptions","A meta-learning method for transfer learning that can speed up the transfer process. This paper presents a method for transferring data from one dataset to another in order to obtain a more accurate estimate of the true CATE for each sample. The study presents a meta learning method to transfer data from a dataset to a new dataset, which is then used to generate a CATE estimator."
"We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization. Studies the forgetting behavior of training examples during SGD, and shows there exist ""support examples"" in neural network training across different network architectures. This study analyzes the extent to which networks learn to correctly classify specific examples and then forget these examples over the course of training. The study studies whether some examples in training neural networks are harder to learn than others. Such examples are forgotten and relearned multiple times through learning.","Unforgettable examples can be learned during training, while others are forgotten during training. This study investigates the phenomenon of forgetting events in deep neural networks and investigates the correlation between forgetting statistics and the intrinsic dimension of the learning problem. The authors investigate the relationship between forgetting events and generalization performance of deep neural architectures, and show that forgetting events can be used to identify ""important"" samples."
"A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability. The authors claim that the previous art directly integrate neural networks into the graphical models as components, which renders the models uninterpretable. Proposal for a combination of neural nets and graphical models by using a deep neural net to predict the parameters of a graphical model.",CENs are competitive with the state-of-the-art for image and text classification and survival analysis tasks. This work introduces a new class of probabilistic models that can be used to generate explanations for complex data. The study presents a new framework for understanding the explanations generated by a deep neural network in order to improve interpretability.
"We proposed ""Difference-Seeking Generative Adversarial Network"" (DSGAN) model to learn the target distribution which is hard to collect training data. This study presents DS-GAN, which aims to learn the difference between any two distributions whose samples are difficult or impossible to collect, and shows its effectiveness on semi-supervised learning and adversarial training tasks. This paper considers the problem of learning a GAN to capture a target distribution with only very few training samples from that distribution available.","Generative approaches are developed for learning data distribution from its samples and thereafter produce novel and high-dimensional samples from learned distributions, such as image and speech synthesis BID18 ). This work introduces a new GAN, DSGAN, which learns a mixture distribution from the data generated by the generator and discriminator. The work presents a new method for generating samples from the generated data that is suitable for semi-supervised learning."
"We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document to perform disease entity normalization. Uses a GRU autoencoder to represent the ""context"" (related enitities of a given disease within the span of a sentence), solving the BioNLP task with significant improvements over the best-known methods.","Improving disease normalization with a combination of semantic features and topical coherence. This work presents a novel method for improving the accuracy and predictive quality of biomedicine by combining two different approaches. The authors present a new approach to disease normalizing that combines semantic features with coherence, which improves the accuracy of the previous methods."
"We present a single shot analysis of a trained neural network to remove redundancy and identify optimal network structure This article proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset This work presents a framework for optimising neural networks architectures through the identification of redundant filters across layers","This article analyzes all layers of neural networks in a single shot and identifies an optimal structure in terms of both width and depth, without any iterative searches for thresholds. This article presents a method for segmenting neural networks based on the number of significant filters in each layer of the network. The authors present a method to reduce the dimensionality of a neural network by reducing its number of 'principal filters'."
"The first deep learning approach to MFSR to solve registration, fusion, up-sampling in an end-to-end manner. This study proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. This article proposes a framework including recursive fusion to co-registration loss to solve the problem of super-resolution results and high-resolution labels not being pixel aligned.","A deep learning framework that solves the co-registration, fusion and registration-at-loss problems in an end-to-end learning framework. This work introduces a deep learning approach to solve the problem of single image super-resolution (MFSR) by combining two differentiable registration and registration components. The authors present a deep-learning framework for multi-image super-resolution that combines multiple low-resolution images into one image."
We introduce a technique that allows for gradient based training of quantized neural networks. Proposes a unified and general way of training neural networks with reduced precision quantized synaptic weights and activations. A new approach to quantizing activations which is state of the art or competitive on several real image problems. A method for learning neural networks with quantized weights and activations by stochastically quantizing values and replacing the resulting categotical distribution with a continuous relaxation,Stochastic rounding can be used to reduce the precision of neural network quantization. This paper introduces a new method for quantizing neural networks with reduced numerical precision. The authors present a method for reducing numerical precision of a neural network by using a stochastic rounding procedure. This work presents a method to reduce numerical precision in neural networks by reducing the number of quantization operations in the network.
"Interpretation by Identifying model-learned features that serve as indicators for the task of interest. Explain model decisions by highlighting the response of these features in test data. Evaluate explanations objectively with a controlled dataset. This paper proposes a method for producing visual explanations for deep neural network outputs and releases a new synthetic dataset. A method for Deep Neural Networks that identifies automatically relevant features of the set of the classes, supporting interpretation and explanation without relying on additional annotations.","An automatic neural network prediction method based on feature selection This paper introduces a method to predict the class of interest of a given neural network in a given dataset. The authors present a method for predicting a class using a network-encoded feature label, which provides a visual explanation of the predicted class of an image."
"A method that build representations of sequential data and its dynamics through generative models with an active process Combines neural networks and Gaussian distributions to create an architecture and generative model for images and video which minimizes the error between generated and supplied images. The paper proposes a Bayesian network model, realized as a neural network, that learns different data in the form of a linear dynamical system",A novel machine learning architecture that learns to adapt to changing dynamics in the environment. This work presents a novel model of machine learning that is able to learn to adapt dynamically to changing environments. The authors present a method for learning a generative model of the environment by learning a latent representation of the inputs and the dynamics of the data.
"Obtains state-of-the-art accuracy for quantized, shallow nets by leveraging distillation. Proposes small and low-cost models by combining distillation and quantization for vision and neural machine translation experiments This article presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression.","Quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. This work introduces a new method for distillation of deep neural networks, where the weight is quantized in order to improve the accuracy of the model."
"We use question-answering to evaluate how much knowledge about the environment can agents learn by self-supervised prediction. Proposes QA as a tool to investigate what agents learn about in the world, arguing this as an intuitive method for humans which allows for arbitrary complexity. The authors propose a framework to assess representations built by predictive models that contain sufficient information to answer questions about the environment they are trained on, showing those by SimCore contained sufficient information for the LSTM to answer questions accurately.","A question-answering evaluation paradigm for agents trained to answer questions about the environment. The study presents a method for answering questions from an environment-based exploration objective by using a QA decoder trained on the agent's internal memory. This work investigates the problem of answering questions in a context where the agent is learning to understand the environment, and shows that QA can be used to train agents to answer more complex questions than previously thought."
"We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space. This study addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This study proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.","A deep reinforcement learning algorithm that approximates the quantile functions of the action dimensions in a d-dimensional hypercube. This article introduces a new reinforcement learning method for training agents on vector rewards using quantile regression. This work introduces a deep neural network architecture to train agents on vectors with a quantile function, and shows that it can be trained faster than other methods."
"Learn representations for images that factor out a single attribute. This work builds on Conditional VAE GANs to allow attribute manipulation in the synthesis process. This paper proposes a generative model to learn the representation which can separate the identity of an object from an attribute, and extends the autoencoder adversarial by adding an auxiliary network.","A novel convolutional GAN architecture that learns to factor attributes out of the latent space in order to achieve competitive scores on a facial attribute classification task. This article introduces a new convolutionAL GAN, IFcVAE-GAN, which learns a latent space representation that separates an object category from its attributes. This work presents a novel Convolutional Generative Adversarial Ecosystem (VAE) model that learns an auxiliary classifier to factorize the representation. The authors present a convolutionial GAN based on the VAE, which can be used for image attribute synthesis."
We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification This work proposes to transfer the classifier from the model for face classification to the task of alignment and verification. The manuscript presents experiments on distilling knowledge from a face classification model to student models for face alignment and verification.,"A method for distillation of knowledge in face alignment and verification by transferring the distilled knowledge from the teacher network to the student network. This work introduces a new distillation method for face alignment, which uses a common initialization trick to further boost the distillation performance of the teacher networks. The study presents a method to distill knowledge from a teacher network into a deep ResNet-50/8 model."
"New state-of-the-art framework for image restoration The paper proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications. This paper proposes a residual non-local attention network for image restoration","Local and non-local attention to better guide feature extraction in deep neural networks. This work presents a deep neural network architecture that learns to extract features from hierarchical layers of the image. The authors study the problem of image generation in deep networks, and show that local attention is beneficial for image generation."
"We present an influence-directed approach to constructing explanations for the behavior of deep convolutional networks, and show how it can be used to answer a broad set of questions that could not be addressed by prior work. A way to measure influence that satisfies certain axioms, and a notion of influence that can be used to identify what input part is most influential for the output of a neuron in a deep neural network. This work proposes to measure the influence of single neurons with regard to a quantity of interest represented by another neuron.",Localizing relevance of neurons in convolutional neural networks This work presents a new approach to interpreting the influence of neural networks on the output of images. The authors present a method for learning the influence on the outputs of convolutionally neural networks by using an axiomatic way of explaining the structure of a neural network.
"In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning. A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction. Reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead is trained to generate frames that happen at any point in the future.","A novel method to train time-agnostic CVAE-GANs with a fixed minimum over time loss. This paper presents a novel method for training video prediction models that can be applied to multiple tasks at once. The authors present a new method for learning video prediction, called TAP, which is based on a generalized minimum-over-time loss, and shows that it is more efficient than deterministic methods."
We perform functional variational inference on the stochastic processes defined by Bayesian neural networks. Fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples. Presents a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors features in the literature. Presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally rather than via a prior over weights.,"An implicit variational inference algorithm that approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. This article presents a method for training neural networks with a generative prior, which can be used to estimate the log density derivative function of a function. The authors present a novel approach for learning generative inference based on the log-ML distribution of functions in a neural network."
"Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation. A method for learning deep latent-variable MRF with an optimization objective that utilizes Bethe free energy, that also solves the underlying constraints of Bethe free energy optimizations. An objective for learning latent variable MRFs based on Bethe free energy and amortized inference, different from optimizing the standard ELBO.","A saddlepoint objective for learning deep, undirected graphical models with latent variables variational inference. This article presents a saddlepoint approach to learning deep neural HMMs using the Bethe free energy approximation to the model's partition functions. This work introduces a saddle-point objective to learn deep, unsupervised graphical models using latent variables, and shows that it outperforms other approaches."
"In this work, we point to a new connection between DNNs expressivity and Sharkovsky’s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks Shows how the expressive power of NN depends on its depth and width, furthering the understanding of the benefit of deep nets for representing certain function classes. The authors derive depth-width tradeoff conditions for when relu networks are able to represent periodic functions using dynamical systems analysis.","A deep neural network that can be approximated by a shallow neural network. This article studies the composition of periodic functions in deep neural networks and shows that they can be easily approximated with shallow neural networks. The authors show that deep neural nets can be trained to represent periodic functions as a function of the depth, and show that such deep networks are more stable than shallow networks."
"We propose a method that infers the time-varying data quality level for spatiotemporal forecasting without explicitly assigned labels. Introduces a new definition of data quality that relies on the notion of local variation defined in (Zhou and Scholkopf) and extends it to multiple heterogenous data sources. This article proposed a new way to evaluate the quality of different data sources with the time-vary graph model, with the quality level used as a regularization term in the objective function",Graph convolutional neural networks can be used to improve the predictive performance of neural networks via experiments on urban heat island prediction in Los Angeles. This paper presents a graph convolutionAL neural network with graph connectivity that is more robust to local variation than other networks. The authors present a graph-based neural network architecture that is able to predict temperature in the summer and winter seasons using data from different nodes.
"We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty in conventional networks. This study proposes using batch normalisation at test time to get the predictive uncertainty, and shows Monte Carlo prediction at test time using batch norm is better than dropout. Proposes that the regularization procedure called batch normalization can be understood as performing approximate Bayesian inference, which performs similarly to MC dropout in terms of the estimates of uncertainty that it produces.","A Bayesian approach for estimating model uncertainty in deep neural networks. This paper presents a Bayesian framework for estimating the uncertainty of deep neural network predictions. The authors provide a Bayes-based approach to estimating uncertainty in neural networks based on batch normalization, and show that it performs on par with MCDO. This work studies the predictive uncertainty of neural networks using batch normalized models"
"Understanding the neural network Hessian eigenvalues under the data generating distribution. This article analyzes the spectrum of the Hessian matrix of large neural networks, with an analysis of max/min eigenvalues and visualization of spectra using a Lanczos quadrature approach. This work uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposes an efficient spectrum visualization methods.","A method to analyze the spectral differences between the Empirical Hessian and the True Hessian. This work studies the spectral perturbations between the two distributions, and presents a method for studying the spectral difference between the true and false Hessian distributions. The authors investigate the relationship between the values of the two different distributions and show that the true Hessian is more robust than the False Hessian on a large dataset."
"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time This study proposes the Recurrent Discounted Attention (RDA), an extension to Recurrent Weighted Average (RWA) by adding a discount factor. Extends the recurrent weight average to overcome the limitation of the original method while maintaining its advantage and proposes the method of using Elman nets as the base RNN","A new recurrent unit for the LSTM that is able to discount the attention applied to previous timesteps. This article introduces a new unit for LSTMs that can be used to compress information in a sequence. The authors introduce a new RWA, which extends the Recurrent Discounted Attention (RWA) unit by allowing it to discount its attention on previous inputs based on later information."
"How you should evaluate adversarial attacks on seq2seq The authors investigate ways of generating adversarial examples, showing that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting. The paper is about meaning-preserving adversarial perturbations in the context of Seq2Seq models","A new method to evaluate the effectiveness of adversarial attacks on word-based MT systems and show, via human and automatic evaluation, that they produce more semantically similar adversarial inputs than unconstrained attacks. This article presents a novel method for evaluating the efficacy of adversarially inspired word substitution attacks on machine learning systems."
Imposing graph structure on neural network layers for improved visual interpretability. A novel regularizer to impose graph structure upon hidden layers of a Neural Network to improve the interpretability of hidden representations. Highlights the contribution of graph spectral regularizer to the interpretability of neural networks.,"A graph-structured neural network with spectral regularization that can be used to enforce spectral properties of the activations generated by them. This work introduces a new class of convolutional regularizations for the activation of neural networks, which is based on graph-translated Gaussians as dictionary atoms. The authors introduce a graph-based regularization method for generating spectral patterns in neural networks."
A framework for learning high-quality sentence representations efficiently. Proposes a faster algorithm for learning SkipThought-style sentence representations from corpora of ordered sentences that swaps the word-level decoder for a contrastive classification loss. This article proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences,A novel approach for learning sentence encoders with skip-thought vectors. This work introduces a new model for sentence embedding that is more efficient than existing models. The authors introduce a new approach to sentence encoder learning which uses skip-grams as a representation of the input and the output of the decoder. This article presents a novel way of learning sentences using skip-words
Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly. Proposes stochastic determination methods for truncation points in backpropagation through time. A new approximation to backpropagation through time to overcome the computational and memory loads that arise when having to learn from long sequences.,"Stochastic gradient descent with biased estimates, such as the one provided by truncated BPTT, can lead to divergence even in simple situations and even with large truncation lengths FIG3. This article presents a novel gradient descent algorithm for recurrent learning that is unbiased and provably unbiased. The authors introduce a new algorithm for training deep neural networks, called Fixed-Truncated Probabilistic Gradient Extraction (ARTBP), which is based on a deterministic gradient descent scheme."
"Exploration using Distributional RL and truncagted variance. Presents an RL method to manage exploration-explotation trade-offs via UCB techniques. A method to use the distribution learned by Quantile Regression DQN for exploration, in place of the usual epsilon-greedy strategy. Proposes new algorithsms (QUCB and QUCB+) to handle the exploration tradeoff in Multi-Armed Bendits and more generally in Reinforcement Learning",An exploration algorithm for multi-armed bandits using symmetric probability density functions. This article studies the asymmetry of the distributions estimated by random random variables in a multi-arm bandits setting. The study presents a method to estimate the upper confidence bound of the distribution of the return for a single arm based on the empirical distribution of each arm.
"Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more. Describes the conditioned GAN model to generate speaker conditioned Mel spectra by augmenting the z-space corresponding to the identification This article proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitating fine-grained control over various attributes This paper proposes a model that can control non-annotated attributes such as speaking style, accent, background noise, etc.",A new generative model to synthesize speech that resembles the prosody of a reference utterance. This work introduces a novel generative language model (GMVAE) that combines autoencoder and conditional auto-encoding with a latent variable. The study presents a novel approach to generating generative languages by combining autoencoders with a fixed set of latent variables.
"In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model as a surrogate to the abnormal distribution. Describes a novel approach to optimising the choice of kernel towards increased testing power and shown to offer improvements over alternatives.","A new kernel parametrization framework for the kernel two-sample test. This work introduces a new kernel selection objective, KL-CPD, which aims at optimizing test power via an auxiliary generative model. The authors present a method to optimize the test power of a kernel with limited samples from the abnormal distribution Q. This study presents a new approach to kernel two sample testing that achieves better performance than previous approaches."
"A bottom-up algorithm that expands CNNs starting with one feature per layer to architectures with sufficient representational capacity. Proposes to dynamically adjust the feature map depth of a fully convolutional neural network, formulating a measure of self-resemblance and boosting performance. Introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. Aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network.",A new algorithm to train deep neural networks with one feature per layer and widen them until a task depending suitable representational capacity is achieved. This article introduces a new method for training deep neural network architectures with one or more features per layer. The authors introduce a new approach to training deep convolutional neural networks using a top-down pruning procedure that adds features at each layer and expands the representation capacity.
"A noval GAN framework that utilizes transformation-invariant features to learn rich representations and strong generators. Proposes a modified GAN objective consisting of a classic GAN term and an invariant encoding term. This work presents the IVE-GAN, a model that introduces en encoder to the Generative Adversarial Network framework.","A novel GAN framework for learning rich and transformation invariant representation of the data in their latent space. This work presents a novel GANA framework for training generative Adversarial Networks (IVE-GAN) that learns a rich representation of data in the latent space without mode collapsing issues. This study introduces a new GAN architecture for generating samples from latent space, which can be trained on a few modes of the true data distribution."
"Multi-view learning improves unsupervised sentence representation learning Approach uses different, complementary encoders of the input sentence and consensus maximization. The paper presents a multi-view framework for improving sentence representation in NLP tasks using generative and discriminative objective architectures. This paper shows that multi-view frameworks are more effective than using individual encoders for learning sentence representations.","A multi-view framework for unsupervised transfer of sentences from one view to another. This work presents a multi-task framework which combines RNN-based encoders with an RNN encoder to improve the accuracy of sentence decoding on supervised tasks. The authors present a new framework for learning sentence representations from two different views, with the aim of improving the accuracy and scalability of sentence decoder."
"Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This work proposes finding ""meaningful"" neurons in Neural Machine Translation models by ranking based on correlation between pairs of models, different epochs, or different datasets, and proposes a controlling mechanism for the models.","A method for ranking neurons in neural language representations based on correlations between pairs of models. This article presents a new way of ranking neural language models based on the correlation between neuron activations. This study studies the effect of neural language learning on language translation quality, and finds that many of the neurons are correlated with specific language properties."
"We use graph co-attention in a paired graph training system for graph classification and regression. This article injects a multi-head co-attention mechanism in GCN that allows one drug to attend to another drug during drug side effect prediction. A method to extend graph-based learning with a co-attentional layer, which outperforms other previous ones on a pairwise graph classification task.","A novel neural network architecture for pairwise graph classification. This work introduces a new model for molecule prediction, Equation 5, which combines a graph neural network with a co-attentional layer. The study presents a novel graph-based model for molecular prediction that combines information from multiple graphs into a single prediction task. The authors present a novel algorithm for molecule-to-drug prediction based on a pairwise representation of two graphs."
Devising unsupervised defense mechanisms against adversarial attacks is crucial to ensure the generalizability of the defense. This work presents a method for detecting adversarial examples in a deep learning classification setting This study presents an unsupervised method for detecting adversarial examples of neural networks.,An automated end-to-end framework for unsupervised model assurance and defending against adversarial samples. This work presents a novel approach to unsupervising data abstractions by using parallel checkpointing modules in each intermediate DL layer. The authors present a method for training adversarial models in which the defender module is trained to detect non-overlapping data points within each class of data.
"We show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the DSP literature. The authors propose incorporating Residual, Highway and Masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task The authors interpret highway, residual and masking connections. The authors generate their own noisy speech by artificially adding noise from a well established noise data-set to a less know clean speech data-set.","A new method for learning the spectral domain of a deep neural network for speech enhancement. This article introduces skip connections in deep neural networks, and shows that they can improve the performance of models trained for speech denoising. The authors introduce skip connections, which are based on predicting time-frequency cells dominated by noise, and show that skipping connections can help to improve speech structure."
We propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. Discusses a core failing and need for I2I translation models. The article explores the idea that an image has two components and applies an attention model where the feature masks that steer the translation process do not require semantic labels,"A new translation method for image generation that combines multimodal and domain-specific style information. This article introduces a new translation technique, EGSC-IT, which combines the two existing methods of combining style information and domain specific style information in a shared latent space. The authors introduce a new approach to image translation by combining the style information of two different domains into a single representation."
A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models. A fast high performance paraphrasing based data augmentation method and a non-recurrent reading comprehension model using only convolutions and attention. This paper proposes applying CNNs+self-attention modules instead of LSTMs and enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model in order to improve RC performance. This work presents a reading comprehension model using convolutions and attention and propose to augment additional training data by paraphrasing based on off-the-shelf neural machine translation,"A novel reading comprehension model that consists of only convolutional and self-attention, a combination that is empirically effective for both training and inference. This study presents a novel reading recognition model for the reading comprehension problem, which consists of two layers: a neural network embedding layer, and a context-query attention layer."
"We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition. This paper discusses zero shot generalization into new environments, and proposes an approach with results on Grid-World, Super Mario Bros, and 3D Robotics. A method aiming to learn task-agnostic priors for zero-shot generalization, with the idea to employ a modeling approach on top of the model-based RL framework.","A novel reinforcement learning algorithm that learns to select trajectories for a task in a grid-world environment. This work introduces a new model for reinforcement learning that learns the best trajectory for a given task in an environment. The study presents a method for training a reinforcement learning agent to select different trajectories from a set of parameters, and shows that it outperforms other approaches."
An autoregressive deep learning model for generating diverse point clouds. An approach for generating 3D shapes as point clouds which considers the lexicographic ordering of points according to coordinates and trains a model to predict points in order. The paper introduces a generative model for point clouds using a pixel RNN-like auto-regressive model and an attention model to handle longer-range interactions.,"Conditional PointGrow, a generative model for 3D point cloud generation, learns a smooth manifold of given images where 3D shape interpolation and arithmetic calculation can be performed inside. This work introduces a self-attention module to generate point clouds that can be used to generate 3D objects."
"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies. The authors show that elimination singularities and overlap singularities impede learning in deep neural networks, and demonstrate that skip connections can reduce the prevalence of these singularities, speeding up learning. Paper examines the use of skip connections in deep networks as a way of alleviating singularities in the Hessian matrix during training.","Overlap singularities are caused by the permutation symmetry of the hidden units at a given layer and they arise when two units become identical, e.g. when their incoming weights become identical to each other. This study introduces skip connections between adjacent layers of nonlinear linear residual networks, and shows that skip connections can be explained by stochastic gradient descent and random initialization."
"We formulated SGD as a Bayesian filtering problem, and show that this gives rise to RMSprop, Adam, AdamW, NAG and other features of state-of-the-art adaptive methods The paper analyzes stochastic gradient descent through Bayesian filtering as a framework for analyzing adaptive methods. The authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior",An approximate inference between adaptive SGD algorithms and natural gradient variational inference. This article presents a method to approximate gradient updates in neural networks using Bayesian (Kalman) filtering. The authors provide a method for approximating gradients in deep neural networks by incorporating momentum and momentum into the updates. This work presents a new way to approximate gradients of neural networks with Bayesian filtering
"We show that, in continual learning settings, catastrophic forgetting can be avoided by applying off-policy RL to a mixture of new and replay experience, with a behavioral cloning loss. Proposes a particular variant of experience replay with behavior cloning as a method for continual learning.","Continuous learning is a method for reducing catastrophic forgetting in continuous learning. This work presents a new approach to the problem of catastrophic forgetting that combines continuous learning with sequential learning. The authors present a novel method for preventing catastrophic forgetting, called continuous learning, in which a single learning network is fed experiences from different points in the training sequence. This study presents a novel way to reduce catastrophic forgetting on continuous learning tasks."
"Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates. A missing data imputation network to incorporate correlation, temporal relationships, and data uncertainty for the problem of data sparsity in EHRs, which yields higher AUC on mortality rate classification tasks. The paper presented a method that combines VAE and uncertainty aware GRU for sequential missing data imputation and outcome prediction.","A deep generative model to estimate the missing values based on features correlations and temporal relations in the latent space of a GRU cell. This work presents a novel method for estimating missing values in an in-hospital mortality prediction framework. This study presents a deep generational model for estimating the missing value estimates based on the features correlations, temporal relations, and the uncertainty."
"In a deep convolutional neural network trained with sufficient level of data augmentation, optimized by SGD, explicit regularizers (weight decay and dropout) might not provide any additional generalization improvement. This work proposes data augmentation as an alternative to commonly used regularisation techniques, and shows that for a few reference models/tasks that the same generalization performance can be achived using only data augmentation. This paper presents a systematic study of data augmentation in image classification with deep neural networks, suggesting that data augmentation can replicit some common regularizers like weight decay and dropout.","A systematic analysis of the impact of data augmentation on deep neural networks compared to popular techniques of explicit regularization. This article presents a systematic study of the role of deep neural network regularization in improving the generalization of the training data. The authors present a systematic review of the effect of deep learning on the training performance of neural networks, focusing on the use of implicit regularization as a substitute for explicit regularized methods."
"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. Introduces a method for creating mini batches for a student network by using a second learned representation space to dynamically select examples by their 'easiness and true diverseness'. Experiments the classification accuracy on MNIST, FashionMNIST, and CIFAR-10 datasets to learn a representation with curriculum learning style minibatch selection in an end-to-end framework.","A self-paced sample selection framework that learns a dynamic representation space for the training data and learns when to introduce certain samples to the DNN during training. This work introduces a new dataset selection framework called Learning Embeddings for Adap- tive Pace (LEAP) that is independent of model architecture or objective, and learns which samples to introduce during training in order to improve performance on classification tasks."
"We introduce an embedding space approach to constrain neural network output probability distribution. This work introduces a method to perform semi-supervised learning with deep neural networks, and the model achieves relatively high accuracy, given a small training size. This study incorporates label distribution into model learning when a limited number of training instances is available, and proposes two techniques for handling the problem of output label distribution being wrongly biased.","A novel semi-supervised learning algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries. This work introduces a new method for training deep neural networks with low dimensional feature space, where labels can be trained to draw decision boundaries correctly from a limited number of examples. The study presents a new approach to train deep neural network models with high dimensional feature spaces."
A comparison of five deep neural network architectures for detection of malicious domain names shows surprisingly little difference. Authors propose using five deep architectures for the cybersecurity task of domain generation algorithm detection. Applies several NN architectures to classify url's between begign and malware related URLs. This paper proposes to automatically recognize domain names as malicious or benign by deep networks trained to directly classify the character sequence as such.,"A new deep neural network classifier that can detect domain names as benign or malicious based solely on the domain name string. This study presents a new deep learning framework for detecting domain names that are not explicitly told to be labeled as malicious. The authors present an end-to-end approach to detecting domain name strings in a deep learning context, focusing on the problem of identifying malicious domain names and proposing an endgame model."
A novel approach to maintain orthogonal recurrent weight matrices in a RNN. Introduces a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This study suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. Novel parametrization of RNNs allows representing orthogonal weight matrices relatively easily.,"A new deep learning architecture for orthogonal and unitary RNNs. This article introduces a deep learning framework for deep learning that is able to solve the vanishing/exploding gradient problem in deep neural networks. The authors introduce a new algorithm for learning deep learning, called scoRNN, which can be used to train deep learning neural networks with low memory usage. This work introduces a new architecture of deep learning based on the Cayley transform"
"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization. The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates The authors analyze training of residual networks using large cyclic learning rates, and demonstrate fast convergence with cyclic learning rates and evidence of large learning rates acting as regularization.","A new adaptive learning rate method for SGD that can speed up training by an order of magnitude. This article studies the problem of super-convergence in SGD using a cyclical learning rate, and shows that using large learning rates is beneficial for training SGD with very large data sets. This work investigates the issue of hyper-parameterized learning rates in deep learning, and suggests that using very large learning rate methods can help train SGD efficiently."
"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks. This article constructs an infinite Topic Model with Variational Auto-Encoders by combining Nalisnick & Smith's stick-breaking variational auto-encoder with latent Dirichlet allocation and several inference techniques used in Miao.","A Bayesian nonparametric topic model with variational auto-Encoders for a countably infinite set of topics. This work introduces a new class of topic models that can be used to approximate the variational posterior of a given dataset. The authors introduce a new type of topic model, called iTM-VAE-Prod, which is an infinite number of topics and can be approximated using variational inference."
"Finetuning after quantization matches or exceeds full-precision state-of-the-art networks at both 8- and 4-bit quantization. This paper proposes to improve the performance of low-precision models by doing quantization on pre-trained models, using large batches size, and using proper learning rate annealing with longer training time. A method for low bit quantization to enable inference on efficient hardware that achieves full accuracy on ResNet50 with 4-bit weights and activations, based on observations that fine-tuning at low precision introduces noise in the gradient.","A new quantization technique for training low-precision neural networks with integer constraints. This article introduces a method to reduce the gradient noise in deep neural networks by quantizing the weights and activations of the weights. The study presents a method for training neural networks at 8-bit precision, which outperforms the current state-of-the-art methods."
"Translating portions of the input during training can improve cross-lingual performance. The study proposes a cross-lingual data augmentation method to improve the language inference and question answering tasks. This paper proposes to augment crosslingual data with heuristic swaps using aligned translations, like bilingual humans do in code-switching.","XNLI data augmentation is a method that replaces a segment of the input text with its translation in another language. This article introduces a new neural machine translation method for cross-lingual word embeddings that uses XNLI, a dataset of 15 different languages. The authors introduce a new method for translating text from one language to another, and show that it can be used with both pretrained and randomly initialized models."
new GNN formalism + extensive experiments; showing differences between GGNN/GCN/GAT are smaller than thought The work proposes a new Graph Neural Network architecture that uses Feature-wise Linear Modulation to condition the source-to-target node message-passing based on the target node representation.,"GNN-FiLM outperforms baseline methods on a regression task on molecular graphs and performs competitively on other tasks. This article presents a novel convolutional neural network architecture for molecular graphs that uses graph edges instead of linear layers. The study presents a new Convolutional Neural Network architecture for the molecular graphs, which is based on graph edges and shows that it is more efficient than existing convolutionals."
"We propose Hierarchical Complement Objective Training, a novel training paradigm to effectively leverage category hierarchy in the labeling space on both image classification and semantic segmentation. A method that regularizes the entropy of the posterior distribution over classes which can be useful for image classsification and segmentation tasks","A new training objective for deep neural models that leverages information from a label hierarchy. This work introduces a new training paradigm for deep learning that leveraged information from labels to improve model performance. This study presents a new approach to training deep neural networks using a hierarchical structure in the label space. The authors present a new method for training a deep neural network with hierarchical information, which leads to significant performance improvement over previous approaches."
We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. The study proposes a new model to use deep models for detecting logical entailment as a product of continuous functions over possible worlds. Proposes a new model designed for machine learning with predicting logical entailment.,"A new generative framework for the entailment of convolutional neural networks. This article presents a novel approach to entailment in neural networks, which is based on the notion that propositional logic is invariant to the structure of a given word. The authors present a novel generative process for the problem of entailment, and show that it can be used to train neural networks to capture structure in sequences of images."
"We show that training feedforward relu networks with a weak regularizer results in a maximum margin and analyze the implications of this result. Studies margin theory for neural sets and shows that max margin is monotonically increasing in size of the network This study studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations, obtaining a generalization upper bound which does not increase with the network size.","A novel algorithm for optimizing weaklyregularized logistic loss over two-layer relu networks. The authors prove that the regularization of deep neural networks can be achieved with a linear SVM, and show that it is possible to achieve the same result. This article introduces a new algorithm for solving the problem of regularizing logistic losses over two layers of neural networks, and shows that the algorithm can be used to train deep neural nets."
"From an incomplete RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and infer their complete object geometry. Proposes an end-to-end 3D CNN structure which combines color features and 3D features to predict the missing 3D structure of a scene from RGB-D scans. The authors propose a novel end-to-end 3D convolutional network which predicts 3D semantic instance completion as object bounding boxes, class labels and complete object geometry.","An end-to-end approach for 3D semantic instance completion based on image segmentation and instance segmentation. This paper presents a novel method for predicting object segmentation in real-world 3D scans, which uses a deep neural network to predict object segmentations. This work presents a new method for 2D semantic segmentation of 3D 3D images, which is based on a deep Neural Network (NAN) architecture."
"We propose Convolutional CRFs a fast, powerful and trainable alternative to Fully Connected CRFs. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel and show that inference is more efficient and training is easier. Proposes to perform message passing on a truncated Gaussian kernel CRF using a defined kernel and parallelized message passing on GPU.","A new framework for fully-connected convolution based segmentation that can be implemented on GPUs. This work introduces conditional independence to convolutional neural networks (CNNs) and introduces a conditional independence assumption for the conditional independence of convolution layers in CNNs. The authors introduce a new framework of fully connected CNNs with conditional independence, which allows convolutions to be trained as convolutions with truncated Gaussian kernel"
"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features. This work extends SGNS with an architectural change from a bag-of-words model to a feedforward model, and contributes a new form of regularization by tying a subset of layers between different associated networks. A method to use non-linear combination of context vectors for learning vector representation of words, where the main idea is to replace each word embedding by a neural network.","A new embedding method based on parameter sharing in word2net that captures latent semantic properties of the word. This article introduces a new embeddings method for embedding word/tag pairs in a context of other words. The authors present word2nets, a neural network-based embedding technique that combines conditional probability and parameter sharing to capture latent semantics of words."
Comparison of psychophysical and CNN-encoded texture representations in a one-class neural network novelty detection application. This work focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task This article considers detecting anomalies in textures and proposes original loss function. Proposes training two anomaly detectors from three different models to detect perceptual anomalies in visual textures.,A novel objective function to train a neural network based on features of a pretrained deep convolutional neural network (CNN) for novelty detection. This work introduces a new objective function that explicitly learns a hyperplane separating reference data and image statistics in order to detect novelty detection in texture representations. The study presents a novel objective method for training neural networks based on the features of CNN-encoded textures.
Human-like Clustering with CNNs The paper validates the idea that deep convolutional neural networks could learn to cluster input data better than other clustering methods by noting their ability to interpret the context of every input point due to a large field of view. This study combines deep learning for feature representation with the task of human-like unsupervised grouping.,"A hierarchical clustering framework for image segmentation. This article presents a new method for clustering neural networks, based on the idea that clustering is easier for humans than it is for humans. The study presents a hierarchical cluster framework for deep CNNs, which can be used to classify objects in a given image."
"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance. This article proposes new layer architectures of neural networks using a low-rank representation of tensors This paper incorporates tensor decomposition and tensor regression into CNN by using a new tensor regression layer.","A new deep convolutional neural network based on tensor decompositions, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer. This article introduces a new method for learning tensor regression layers in deep neural networks, and shows that it is more robust to changes in rank constraints than traditional linear regression methods."
"A novel network architecture to perform Deep 3D Zoom or close-ups. A method for creating a ""zoomed image"" for a given input image,and a novel back re-projection reconstruction loss that allows the network to learn underlying 3D structure and maintain a natural appearance. An algorithm for synthesizing 3D-zoom behavior when the camera is moving forward, a network structure incorporating disparity estimation in a GANs framework to synthesize novel views, and a proposed new computer vision task.","Deep 3D-Zoom Net is a novel view generation framework for the novel view synthesis problem. This work introduces a deep neural network architecture that learns to estimate the disparity and normals of the input image in a single pass. The authors introduce a new class of deep neural networks, Deep3DNet, which can be used to generate images for both the KITTI and Cityscapes datasets."
"We introduce the universal deep neural network compression scheme, which is applicable universally for compression of any models and can perform near-optimally regardless of their weight distribution. Introduces a pipeline for network compression that is similar to deep compression and uses randomized lattice quantization instead of the classical vector quantization, and uses universal source coding (bzip2) instead of Huffman coding.","A universal DNN quantization framework consisting of universal quantization and universal lossless source coding. This work introduces a universal quantizing framework for vector quantization in high resolution images, and shows that it is more efficient than the classical lossy compression framework. The study presents a universal method for quantizing vector quantized DNNs by adding uniform random dithers to the quantized weights."
"A quantitative refinement of the universal approximation theorem via an algebraic approach. The authors derive the universal approximation property proofs algebraically and assert that the results are general to other kinds of neural networks and similar learners. A new proof of Leshno's version of the universal approximation property for neural networks, and new insights into the universal approximation property.","A neural network with two hidden layers of sizes 2n + 1 and 4n + 3 respectively is able to approximate any function f ∈ P ≤d (X, R m) to any desired approximation error threshold ε. The authors prove that the UAP can be used to approximate all non-bias functions in the first layer of a neural network."
"We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling Introducting an importance sampling distribution and using samples from distribution to compute importance-weighted estimate of the gradient This article proposes to use important sampling to optimize VAE with discrete latent variables.","An easy latent variable model that can be trained with stochastic gradient descent on a training data set. This article presents a novel latent variable learning algorithm which can be used to train VAEs with SGVB. The authors introduce a new variant of the FCVAE, where the latent variables are parametrized and the decoder is trained to generate images that fit well to all the training data."
"Linking Wasserstein-trust region entropic policy gradients, and the heat equation. The paper explores the connections between reinforcement learning and the theory of quadratic optimal transport The authors studied policy gradient with change of policies limited by a trust region of Wasserstein distance in the multi-armed bandit setting, showing that in the small steps limit, the policy dynamics are governed by the heat equation (Fokker-Planck equation).","A stochastic gradient ascent for entropy-regularized rewards in the context of on-policy rewards maximization. This article introduces a new algorithm for policy regularization, which is based on the entropy gradient ascent of the policy distance between policies. The work presents a novel algorithm for randomization of policy distances between policies, where entropy is a regularization term and the reward is a free energy function."
We introduce a scale-invariant neural network architecture for changepoint detection in multivariate time series. The work leverages the concept of wavelet transform within a deep architecture to solve change point detection. This article proposes a pyramid based neural net and applies it to 1D signals with underlying processes occurring at different time scales where the task is change point detection,"A new deep neural network architecture that can detect both abrupt and gradual changes at multiple scales. This work presents a novel deep neural architecture for the detection of abrupt, gradual, and abrupt changepoints in multimodality time series. The study presents a deep neural model for detecting abrupt, abrupt, and gradual changepoints at different scales."
"A novel graph signal processing framework for quantifying the effects of experimental perturbations in single cell biomedical data. This work introduces several methods to process experimental results on biological cells and proposes a MELD algorithm mapping hard group assignments to soft assignments, allowing relevant groups of cells to be clustered.","MELD learns the EES by filtering the noisy categorical experimental label in the graph frequency domain to recover a smooth signal with continuous values. This work introduces a novel and flexible filter on graph frequency domains to identify cell states that are most or least affected by an experimental perturbation. The study presents a novel algorithm for identifying cell states with different responses to perturbations, which can be used to determine whether a cell is more or less affected than a control condition."
"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning. Provided a convergence analysis of Sign SGD algorithm for non-covex cases The work explores an algorithm that uses the sign of the gradients instead of actual gradients for training deep models","A new algorithm for deep neural network optimisation that can avoid saddle points. This article introduces a new algorithm called signGD, which is a gradient descent algorithm that avoids saddle points in order to avoid the curvature-induced error. The authors investigate the problem of avoiding saddle points and show that the algorithm can avoid them by avoiding them."
"Existing pruning methods fail when applied to GANs tackling complex tasks, so we present a simple and robust method to prune generators that works well for a wide variety of networks and tasks. The authors propose a modification to the classic distillation method for the task of compressing a network to address the failure of previous solutions when applied to generative adversarial networks.","A novel compression method for generating images from compressed data that can be used to improve the quality of the compressed generator. This work introduces a new compression technique for image distillation, where the discriminator is trained alongside the teacher and the generator to generate images that are more similar to the original generator."
We propose to solve a problem of simultaneous classification and novelty detection within the GAN framework. Proposes a GAN to unify classification and novelty detection. The work presents a method for novelty detection based on a multi-class GAN which is trained to output images generated from a mixture of the nominal and novel distributions. The paper proposes a GAN for novelty detection using a mixture generator with feature matching loss,"A mixture generator that is trained with a Feature Matching loss and which generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. This work introduces a GAN framework for simultaneous classification and novelty detection based on a mixture generator. The authors introduce a new discriminator classifier, called a mixture-generator, that is capable of generating samples from different distributions of data."
"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task. This article proposes a new method for classifying nodes of a graph, which can be used in semi-supervised scenarios and on a completely new graph. The study introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks. Provides a fair and almost comprehensive discussion of the state of art approaches to learning vector representations for the nodes of a graph.","A graph convolutional model that leverages sparse matrix operations, reducing the storage complexity to linear in the number of nodes and edges and enabling the execution of inductive learning on large graphs. This article introduces a new neural network architecture that combines multi-head attention with nonlinearity in the graph structure."
A formal method's approach to skill composition in reinforcement learning tasks The work combines RL and constraints expressed by logical formulas by setting up an automation from scTLTL formulas. Proposes a method that helps to construct policy from learned subtasks on the topic of combining RL tasks with linear temporal logic formulas.,Combine individual Q-functions of existing policies with an energy-based model. This paper presents a new approach to skill composition and multi-task learning/meta-learning where the goal is to obtain a policy that satisfies each of the constituent tasks in the shortest amount of time. The authors present a new method for skill composition that combines individual Qfunctions from existing policies into a new policy that maximizes the reward of individual tasks.
"We created a new dataset for data interpretation over plots and also propose a baseline for the same. The authors propose a pipeline to solve the DIP problem involving learning from datasets containing triplets of the form {plot, question, answer} Proposes an algorithm that can interpret data shown in scientific plots.",A multi-staged modular framework for plot question answering. This work introduces a new dataset which contains plots generated from synthetic data and asks a wide range of annotators to answer questions. The study presents a novel dataset with a large number of plot templates that can be used to solve the problem of plot question solving.
Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue This article proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations with sizable gains over the baselines. Proposes combining external pretrained word embeddings and pretrained word embeddings on training data by keeping them as two views. Proposes method to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to conversational datasets and applies new variants of LSTM-based model to the task of response-selection in dialogue modeling.,Combine pre-trained word embedding vectors with character-composed word vectors on Ubuntu Dialogue Corpus and obtained state-of-the-art results on the Stanford Natural Language Inference (SNLI) Corpus. Presents a new language model based on vector concatenation that combines character-level embedding and character-based word vectors. This work presents a novel language model for dialog and dialogue that combines pre-training word vectors with LSTM neural networks.
Acquire states from high frequency region for search-control in Dyna. The authors propose to do sampling in the high-frequency domain to increase the sample efficiency This article proposes a new way to select states from which do do transitions in dyna algorithm.,"A simple method to locally measure the frequency of a function by gradient norm, and provide theoretical justification for this approach. This work introduces a new algorithm for learning a function with as few samples as possible, which is more difficult to approximate and requires more training data. The authors introduce a new deep learning algorithm called Dyna-Value, which uses gradient ascent to obtain samples from high-frequency regions of the state space."
"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines. Proposes a method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent Proposal for an online distillation method called co-distillation, applied at scale, where two different models are trained to match predictions of the other model in addition to minimizing its own loss. Online distillation technique is introduced to accelerate traditional algorithms for large-scaled distributed neural network training","An ensemble distillation algorithm for asynchronous and synchronous SGD. This work introduces a new distillation method for asynchronous SGD, which is more efficient than traditional distillation methods. The authors present a method for distillation of neural networks to reduce prediction churn and improve performance. This article presents a method to distill the predictions of two different neural networks into a single distillation term."
"Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark. A sequential latent variable model for knowledge selection in dialogue generation that extends the posterior attention model to the latent knowledge selection problem and achieves higher performances than previous state-of-the-art models. A novel architecture for selecting knowledge-grounded multi-turn dialogue that yields state of the art on relevant benchmarks datasets, and scores higher in human evaluations.","A sequential latent variable model for knowledge selection in multi-turn knowledge-grounded dialogue. This paper presents a novel method for learning from a large pool of knowledge candidates in open-domain knowledge-based dialogue. The authors present a novel latent model for the task of knowledge selection, which can be used to improve the accuracy of the knowledge selection and subsequent utterances."
"Neural Network Verification for Temporal Properties and Sequence Generation Models This work extends interval bound propagation to recurrent computation and auto-regressive models, introduces and extends Signal Temporal Logic for specifying temporal contraints, and provides proof that STL with bound propagation can ensure neural models conform to temporal specification. A way to train time-series regressors verifiably with respect to a set of rules defined by signal temporal logic, and work in deriving bound propagation rules for the STL language.","A novel verification procedure for deep neural networks that leads to sequential outputs. This work presents a novel verification method for deep reinforcement learning, which is based on the principle of temporal specifications. The authors present a new method for training deep RL agents with temporal specifications, and show that it can be used to train models that are provably consistent with specifications."
We propose to use explicit vector algebraic formulae projection as an alternative way to visualize embedding spaces specifically tailored for goal-oriented analysis tasks and it outperforms t-SNE in our user study. Analysis of embedding psaces in a non-parametric (example-based_ way,"A method for visualizing the different dimensions of an embedding space and comparing them to each other with respect to interpretable dimensions of variability. This work presents a new way of visualizing a representation of a deep learning dataset, where the goal is to obtain a better understanding of the distribution of information between different embeddings. The authors present a new approach to visualizing two different embedding spaces in order to provide a more general view of the problem."
"a joint model and gradient sparsification method for federated learning Applies variational dropout to reduce the communication cost of distributed training of neural networks, and does experiments on mnist, cifar10 and svhn datasets. The authors propose an algorithm that reduces communication costs in federated learning by sending sparse gradients from device to server and back. Combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.",A federated learning framework for Bayesian neural networks that achieves convergence and communication cost while reducing the amount of gradients exchanged during the iterative training process. This work introduces a method to jointly and iteratively sparsify the parameters of the shared model to be learned as well as the gradients exchanging between the server and the participating devices during the distributed SGD training process This article presents a method for jointly learning a deep neural network using variational dropout
"Discrete transformer which uses hard attention to ensure that each step only depends on a fixed context. This paper presents modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. This study proposes three Discrete Transformers: a discrete and stochastic Gumbel-softmax based attention module, a two-stream syntactic and semantic transformer, and sparsity regularization.","A novel transformer architecture for machine translation with sparse attention. This work introduces a new model architecture that uses sparse attention in the context of prediction. The authors introduce a new transformer architecture that combines sparse attention with a multi-headed attention mechanism. The study presents a novel model architecture for translating sentences from soft to soft attention, where the attention layer is composed of multiple layers and each layer receives a different input."
We propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization. Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'Differentiable Greedy Network' (DGN). Proposes a neural network that aims to select a subset of elements (e.g. selecting k sentences that are mostly related to a claim from a set of retrieved docs),"Deep unfolding BID9 is a method to derive novel neural networks that are interpretable as inference algorithms by turning iterations of the inference algorithms into layers of a network. This work introduces a new deep learning algorithm called Deep Gaining Networks (DGN), which combines the advantages of deep learning and generative models with the benefits of inference."
"We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs. The work proposes a regularization term for the conditional GAN objective in order to promote diverse multimodal generation and prevent mode collapse. The article proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction, which can be applied to various conditional synthesis frameworks for various tasks.","A conditional Generative Adversarial Network that learns a multi-modal mapping from input to output, and the generated distribution is getting closer to the actual distribution. This paper presents a novel conditional GAN which learns a map from latent code to output in order to resolve the mode-collapse problem in conditional generative tasks."
"A method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost A method to train a network with large capacity, only parts of which are used at inference time dependent on input, using fine-grained conditional selection and a new method of regularization, ""batch shaping.""","A batch-shaping loss for convolutional neural networks that is conditionally activated at the start of training and improves performance significantly. This work introduces a new gating method to train deep neural networks with high sparsity, and shows that it can be used to train a deep neural network with very large sparsity. The study presents a new Gating method for training deep neural nets with high levels of sparsification."
"We diagnose deep neural networks for 3D point cloud processing to explore the utility of different network architectures. The paper investigates different neural network architectures for 3D point cloud processing and proposes metrics for adversarial robustness, rotational robustness, and neighborhood consistency.","A new deep neural network architecture for 3D point cloud processing. This study presents a deep neural net architecture that learns to represent 3D objects in 3D. The authors present a new neural network based on PointConv, which is designed to be used as a reinforcement learning framework for point cloud computing. This work presents a novel approach to the problem of learning 3D object representations by using a single layer of neural networks"
"Optimized gated deep learning architectures for sensor fusion is proposed. The authors improve upon several limitations of the baseline negated architecture by proposing a coarser-grained gated fusion architecture and a two-stage gated fusion architecture Proposes two gated deep learning architectures for sensor fusion and by having the grouped features, demonstrates improved performance, especially in the presence of random sensor noise and failures.","A new netgated architecture which learns robustly a set of fusion weights at the (feature) group level. This work introduces a new gating architecture for neural network architectures, FG-GFA, which combines feature-level and group-level fusion weights. This study presents a new net-gated framework for neural networks that is more robust to noisy or corrupted features than the baseline netgate architecture."
We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization The article presents a combination of evolutionary computation and variational EM for models with binary latent variables represented via a particle-based approximation The study makes an attempt to tightly integrate expectation-maximization training algorithms with evolutionary algorithms.,Probabilistic generative models with binary hidden variables can be used for variational optimization in the variational loop of EM. This study presents a probabilistic model that is optimized according to a variational expectation maximization objective. The authors present a probababilistic approach to variational parameter optimization of probabilized generative model with binary latent variables.
"We closely analyze the VAE objective function and draw novel conclusions that lead to simple enhancements. Proposes a two-stage VAE method to generate high-quality samples and avoid blurriness. This work analyzes the Gaussian VAEs. The study provides a number of theoretical results on ""vanilla"" Gaussian Variational Auto-Encoders, which are then used to build a new algorithm called ""2 stage VAEs"".","A Gaussian-based VAE pipeline that achieves the global optimum and yet does not assign the ground-truth probability measure. The authors present a Gaussian architecture for the first-stage VAE, which is based on the assumption that the latent dimension is equal to the ambient dimension. This paper presents a Gauss-based approach to the problem of finding the ground truth probability measure for continuous data, and shows that Gaussian methods can be used to achieve the objective."
"We describe an end-to-end differentiable model for QA that learns to represent spans of text in the question as denotations in knowledge graph, by learning both neural modules for composition and the syntactic structure of the sentence. This article presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser. Proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words","A neural network framework for answering compositional questions using a Knowledge Graph (KG). This work presents a model for answering question answering that combines phrases with a knowledge graph, resulting in a deep latent tree that answers the question. The study presents a neural network architecture for answer-to-question answering that is grounded in the Knowledge Graph, and shows that it can be used to solve compositional question answering tasks."
"Couple the GAN based image restoration framework with another task-specific network to generate realistic image while preserving task-specific features. A novel method of Task-GAN of image coupling that couples GAN and a task-specific network, which alleviates to avoid hallucination or mode collapse. The authors propose to augment GAN-based image restoration with another task-specific branch, such as classification tasks, for further improvement.","Task Generative Adversarial Network (GAN) for image restoration. This work introduces a task-driven GAN framework to improve the accuracy of image restoration on medical imaging and natural image applications. The study presents a new approach to image restoration based on task-based GANs that includes a task network, a generator and a discriminator network."
"Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies. A method to coordinate agent behaviour by using policies that have shared latent structure, a variational policy optimization method to optimize the coordinated policies, and a derivation of the authors' variational, hierarchical update. This article suggests an algorithmic innovation consisting of hierarchical latent variables for coordinated exploration in multi-agent settings","A structured probabilistic policy class that uses a hierarchy of stochastic latent variables to train agents in multi-agent environments. This work introduces a new deep learning framework for the multi agent reinforcement learning problem with a centralized controller. The authors introduce a hierarchical model of a multi agent policy class, and show that it improves sample complexity on cooperative games with large number of agents."
"Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness This article presents an adaptation of the algorithmic robustness of Xu&Mannor'12 and presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error. Proposes a article of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness and gives bounds on generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice. The work studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context","An ensemble robustness approach for deep neural networks based on their generalization performance. This study studies the generalization ability of deep neural nets based on the ensemble randomness of the training examples. The study presents a method for predicting the robustness of a deep neural network based on its ensemble Randomness, which is correlated with its generalization."
"An architecture for tabular data, which emulates branches of decision trees and uses dense residual connectivity This article proposes deep neural forest, an algorithm which targets tabular data and integrates strong points of gradient boosting of decision trees. A novel neural network architecture mimicking how decision forests work to tackle the general problem of training deep models for tabular data and showcasing effectiveness on par with GBDT.","A deep neural tree architecture that combines elements from decision trees as well as dense residual connections in a hybrid deep architecture. This work presents a novel architecture for tabular and multi-modal data, which combines elements of decision trees and sparse residual connections. The study presents a new deep neural architecture, called Deep Neural Trees (DNF), that combines the elements from a decision tree and a sparse residual connection."
"An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability The authors formulate training NNs as finding an optimal controller for a discrete dynamical system, allowing them to use method of successive approximations to train a NN in a way to be more robust to adversarial attacks. This article uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness.","Decoupled training of deep neural networks with Lyapunov stability of dynamical systems This paper presents a new method for training neural networks to improve the adversarial robustness of the models. This work introduces a new approach to training deep neural nets, which is based on a dynamical system view of neural networks."
"Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice. Presents a distributed implementation of signSGD with majority vote as aggregation.","SIGNSGD is a mini-batch algorithm for distributed SGD. This work presents a mini batch algorithm for distributing SGD in a low SNR regime, and shows that it can be used to train SGD at a reasonable speed. The authors show that the algorithm can be trained at a high SNR setting, and show that it performs better than other deep learning algorithms."
"Few-shot learning PixelCNN The paper proposes on using density estimation when the availability of training data is low by using a meta-learning model. This study considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning The work focuses on few shot learning with autoregressive density estimation and improves PixelCNN with neural attention and meta learning techniques.","A meta-learning method for few-shot autoregressive models that combines attention and meta learning. This article presents a meta learning method for neural density estimation, which combines attention with meta learning to achieve state-of-the-art results in terms of likelihood on Omniglot. This work studies the connection between attention and semi-attention in deep neural networks, and shows that it can be used to improve performance in the few shot setting."
"we proposed a new self-driving model which is composed of perception module for see and think and driving module for behave to acquire better generalization and accident explanation ability. Presents a multitask learning architecture for depth and segmentation map estimation and the driving prediction using a perception module and a driving decision module. A method for a modified end-to-end architecture that has better generalization and explanation ability, is more robust to a different testing setting, and has decoder output that can help with debugging the model. The authors present a multi-task convolutional neural network for end-to-end driving and provide evaluations with the CARLA open source simulator showing better generalization performance in new driving conditions than baselines",A new deep learning driving module based on image input that can be used to train more robust models. The study presents a new deep-learning driving module which is based on a combination of image input and the driving module weights. This study studies the generalization ability of deep learning autonomous driving models by using image input instead of driving module.
We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating. The authors introduce a gradient-based approach to minimize an objective function with an L0 sparse penalty to help learn sparse neural networks,"A method to smooth the expected L 0 regularized objective with continuous distributions in a way that can maintain the exact zeros in the parameters while still allowing for efficient gradient based optimization of parametric models. This article introduces a new algorithm for training parametric neural networks with continuous random variables, and shows that it can be used to speed up gradient based learning."
We develop meta-learning methods for adversarially robust few-shot learning. This paper presents a method that enhances the robustness of few-shot learning by introducing adversarial query data attack in the inner-task fine-tuning phase of a meta-learning algorithm. The authors of this article propose a novel approach for training a robust few-shot model.,"A new meta-learning routine for adversarial querying that outperforms other robustness techniques by a wide margin in terms of both clean accuracy and adversarial robustness. This article introduces a new meta learning method for adversarially querying, which combines deep learning with deep learning to improve the robustness of existing deep learning methods."
This work demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set. This article presents a deep autoencoder model for rating prediction that outperforms other state-of-the-art approahces on the Netflix prize dataset. Proposes to use a deep AE to do rating prediction tasks in recommender systems. The authors present a model for more accurate Netflix recommendations demonstrating that a deep autoencoder can out-perform more complex RNN-based models that have temporal information.,"A new deep autoencoder model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the art models on a time-split Netflix data set. This article introduces a new autoencoders architecture for the task of rating prediction, and shows that it performs better than previous deep learning architectures."
"We propose that training with growing sets stage-by-stage provides an optimization for neural networks. The authors compare curriculum learning to learning in a random order with stages that add a new sample of examples to the previously, randomly constructed set This work studies the influence of ordering in the Curriculum and Self paced learning, and shows that to some extent the ordering of training instances is not important.",A new method to train with random samples and add new samples according to difficulty levels improves the learning performance in both curriculum and anti-curriculum learning. This study studies the effectiveness of random samples in curriculum learning and shows that random samples are more effective than random samples. The study presents a new method for learning from random samples by adding new samples randomly without a meaningful order.
"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. In the paper the authors suggest to use MERA tensorization technique for compressing neural networks. A new parameterization of linear maps for neural network use, using a hierarchical factorization of the linear map that reduces the number of parameters while still allowing for relatively complex interactions to be modelled. Studies compressing feed forward layers using low rank tensor decompositions and explore a tree like decomposition",A new deep neural network architecture that can capture correlations on different length scales using tensor trains. This work introduces a deep neural networks architecture that combines tree-like connectivity with tensor-trained neural networks. The authors introduce a deep learning architecture based on the idea of embedding a tree into a neural network to capture correlations between inputs.
"A new method for gradient-descent inference of permutations, with applications to latent matching inference and supervised learning of permutations with neural networks The paper utilizes finite approximation of the Sinkhorn operator to describe how one can construct a neural network for learning from permutation valued training data. The study proposes a new method that approximates the discrete max-weight for learning latent permutations","A deep neural network that learns to find the right permutation for a given sequence of sequences. This work presents a deep neural networks that can learn to solve entropy-regularized linear assignment problems by randomly permuting pieces of random matrices. The work presents an algorithm for learning the correct permutation of a sequence of sequence of sequential sequences of sequences, and shows that it is possible to learn the correct number of sequences in a sequence."
Compressing trained DNN models by minimizing their complexity while constraining their loss. This paper proposes a method for deep neural network compression under accuracy constraints. This article presents a loss value constrained k-means encoding method for network compression and develops an iterative algorithm for model optimization.,"A method for compressing trained neural network models that directly minimizes its complexity while maintaining its accuracy. This paper presents a method for reducing the complexity of trained neural networks by reducing the number of centroids and reducing the computational complexity. This work introduces a method to compress neural networks in order to reduce their computational complexity, and shows that it achieves similar results for SVHN and CIFAR10 datasets."
"Imitation from pixels, with sparse or no reward, using off-policy RL and a tiny adversarially-learned reward function. The study proposes to use a ""minimal adversary"" in generative adversarial imitation learning under high-dimensional visual spaces. This paper aims at solving the problem of estimating sparse rewards in a high-dimensional input setting.","A deep reinforcement learning agent that learns to stack faster than a dense staged reward baseline agent with only demonstrations and a sparse binary reward indicating whether or not the stack has completed. This work introduces a new policy gradient for reinforcement learning, which is a state-of-the-art off-policy method for control, that can take advantage of a replay buffer to store past experiences."
"Improvements to adversarial robustness, as well as provable robustness guarantees, are obtained by augmenting adversarial training with a tractable Lipschitz regularization Explores augmenting the training loss with an additional gradient regularization term to improve robustness of models against adversarial examples Uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.",Adversarial training improves robustness to adversarial attacks by using Lipschitz regularization of the loss gradients. This article presents a novel method for learning adversarial defences based on the loss gradient of a pre-trained neural network. The authors present a novel approach for training a deep neural network that is more tractable and efficient than adversarial training methods. This work introduces a new method for training an adversarial model with a loss gradient in order to improve robustness.
"A new state-of-the-art approach for knowledge graph embedding. Presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base. This work proposes an approach to knowledge graph embedding by modeling relations as rotations in the complex vector space. Proposes a method for graph embedding to be used for link prediction","A probabilistic model that can model and infer all the three types of relation patterns for link prediction. This work introduces a new knowledge graph embedding model, called RotatE, which is able to model and learn all the relation patterns. The study presents a new approach to link prediction based on the idea of modeling and inferring relation patterns in a knowledge graph. The authors introduce a new framework for learning relation patterns that can be used to predict links between entities."
"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks. This work extends Neural Architecture Search to the multi-task learning problem where a task conditioned model search controller is learned to handle multiple tasks simultaneously. In this paper, authors summarize their work on building a framework, called Multitask Neural Model Search controller, for automated neural network construction across multiple tasks simultaneously.","Multi-task Neural Model Search (MNMS), an automated model construction framework that finds the best performing models in the search space for multiple tasks simultaneously. This work introduces a multi-task neural model search framework that learns to search for previously seen tasks simultaneously, reducing search time on previously unseen tasks. The authors introduce a new neural model architecture for ImageNet classification that is pre-trained on several different tasks simultaneously"
Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples. This work proposes adding an additional label for detecting OOD samples and adversarial examples in CNN models. The paper proposes an additional class that incorporates natural out-distribution images and interpolated images for adversarial and out-distribution samples in CNNs,An augmented CNN for black-box adversarial examples can significantly reduce the misclassification rates for both naive and robust out-distribution sets. This study presents a method for training effective CNNs that can be used to generate adversarial samples from a wide range of out-of-distributary sets.
"Permutation-invariant loss function for point set prediction. Proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. This article introduces a novel distance function between point sets, applies two other permutation distances in an end-to-end object detection task, and shows that in two dimensions all local minima of the holographic loss are global minima. Proposes permutation invariant loss functions which depend on the distance of sets.","An analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function as well as a regularizer for machine learning applications. This article introduces a new loss function for small point sets with known cardinalities that can be used as a deep learning loss function. This work presents a new method for learning deep learning points by using a simple permutation-invariant loss function"
"Genetic algorithms based approach for optimizing deep neural network policies The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together. This work proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks.","A new policy gradient algorithm for black-box policy optimization that efficiently combines two parent policies and generates offspring that takes advantage of both the parents. This work introduces a new variant of GAs which combines state-space crossover with a novel mutation operator. The authors introduce a new GAs algorithm for deep reinforcement learning, called GAs, which combines two policies into one policy and generates a hybrid policy."
"We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints. Proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. This study introduces an approach to compressing neural networks by looking at the correlation of filter responses in each layer via two strategies. This study proposes a compression method based on spectral analysis","A closed-form pruning algorithm based on spectral energy analysis for suggesting the number of filters to remove in a layer. This paper introduces a new pruning technique, called PFA-En, which uses spectral energy estimation to determine the amount of filters that should be removed at each layer. The authors present a method for optimizing the performance of structured pruning algorithms by using spectral energy analyses to determine how many filters should be eliminated."
This study studies the discrimination and generalization properties of GANs when the discriminator set is a restricted function class like neural networks. Balances capacities of generator and discriminator classes in GANs by guaranteeing that induced IPMs are metrics and not pseudo metrics This paper provides a mathematical analysis of the role of the size of the adversary/discriminator set in GANs,"A generalization bound for neural f -divergence in GANs with non-parametric discriminators. The authors prove that neural discriminators are discriminative when they are bounded by a bounded Lipschitz distance, and show that the discriminator set can be used to achieve strong convergence. This study studies the discriminators' discriminator-to- discriminator relationship between different parameters of a GAN."
"Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. A domain generalization approach to reveal semantic information based on a linear projection scheme from CNN and NGLCM output layers. The paper proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks","A new neural network building block that is differentiable and tunable through backpropagation. This work introduces a new neural model for image recognition that is more tunable than previous models. The authors introduce a new Neural Network building block, GLCM, which can be used to extract textural information from images of training domains."
"Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior The authors suggest that statistical mechanics ideas will help to understand generalization properties of deep neural networks, and give an approach that provides strong qualitative descriptions of empirical results regarding deep neural networks and learning algorithms. A set of ideas related to theoretical understanding generalization properties of multilayer neural networks, and a qualitative analogy between behaviours in deep learning and results from quantitative statistical physics analysis of single and two-layer neural networks.","A theoretical analysis of the NN/DNN learning process. This article presents a theoretical analysis on the generalization performance of NNs and shows that it is possible to generalize to a large number of control parameters. The study presents a method for generalizing NNs to large amounts of noise in the data, showing that generalization can be achieved on a small amount of noise."
"This article introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative evaluation of representation learning. This work discusses the problem of evaluating and diagnosing the represenatations learnt using a generative model. Authors present a set of criteria to categorize MNISt digists and a set of interesting perturbations to modify MNIST dataset.","A new quantitative framework for assessing the generative direction of generative models of images. This work introduces a quantitative framework to evaluate generative representations of images, based on morphometric data. The study presents a method for evaluating generative representation of images by using morphometric analysis to determine whether a model has learned to represent specific factors of variation."
"We show that modular structured models are the best in terms of systematic generalization and that their end-to-end versions don't generalize as well. This paper evaluates systemic generalization between modular neural networks and otherwise generic models via introduction of a new, spatial reasoning dataset A targeted empirical evaluation of generalization in models for visual reasoning, focused on the problem of recognizing (object, relation, object) triples in synthetic scenes featuring letters and numbers.","A new synthetic dataset for the purpose of generalization of deep neural networks. This study presents a new dataset for deep neural network generalization, called SQOOP, where models are required to learn a layout that matches the task. The authors investigate the generalization capabilities of deep-learning neural networks and show that they are capable of doing systematic generalization on an end-to-end basis."
"We propose a self-monitoring agent for the Vision-and-Language Navigation task. A method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module, and performs well on standard benchmarks. This study describes a model for vision-and-language navigation with a panoramic visual attention and an auxillary progress monitoring loss, giving state-of-the-art results.","A novel LSTM based on textually grounded instructions that can be used to track progress towards the goal. The authors present a novel approach for tracking the progress of an action selection agent by using textual grounding. This work introduces a novel method for measuring the completeness of instruction-following tasks, which uses textual grounding to measure the progress made."
"We show that ENAS with ES-optimization in RL is highly scalable, and use it to compactify neural network policies by weight sharing. The authors construct reinforcement learning policies with very few parameters by compressing a feed-forward neural network, forcing it to share weights, and using a reinforcement learning method to learn the mapping of shared weights. This article combines ideas from ENAS and ES methods for optimisation, and introduces the chromatic network architecture, which partitions weights of the RL network into tied sub-groups.","Combine ENAS and ES for learning compact representations that learn effective policies with over 92% reduction of the number of parameters. This paper combines ENAS, ES, and random partitioning to learn efficient policies for RL tasks. The authors combine ENAS with ES to train a large population of CPU workers in order to improve the performance of RL networks."
"Gaggle, an interactive visual analytic system to help users interactively navigate model space for classification and ranking tasks. A new visual analytic system which aims to enable non-expert users to interactively navigate a model space by using a demonstration-based approach. A visual analytics system that helps novice analysts navigate model space in performing classification and ranking tasks.","A user-defined model space for classification and ranking models. This article presents a novel approach to the problem of overfitting ML models with user feedback by using a combination of a learning algorithm and associated hyperparameters. The authors introduce Gaggle, a multi-modal ML system that learns to classify data from multiple datasets and then uses user feedback to select a new model."
"We propose to generate adversarial example based on generative adversarial networks in a semi-whitebox and black-box settings. Describes AdvGAN, a conditional GAN plus adversarial loss, and evaluates AdvGAN on semi-white box and black box setting, reporting state-of-art results. This article proposes a way of generating adversarial examples that fool classification systems and wins MadryLab's mnist challenge.","An adversarial perturbation strategy for black-box attacks based on the assumption that adversaries have no prior knowledge of training data or the model itself. This paper presents a method for generating adversarial examples using a dynamic distillation technique, where the adversary assumes that the adversarial instance is close to the original one in terms of L 2."
We introduce a novel measure of flatness at local minima of the loss surface of deep neural networks which is invariant with respect to layer-wise reparameterizations and we connect flatness to feature robustness and generalization. The authors propose a notion of feature robustness which is invariant with respect to rescaling the weight and discuss the notion's relationship to generalization. This article defines a notion of feature-robustness and combines it with epsilon representativeness of a function to describe a connection between flatness of minima and generalization in deep neural networks.,"Feature robustness is related to generalization in neural networks. This study studies the robustness of a neural network with ReLU activations, and shows that feature robustness can be related to the generalization properties of the neural network. This work considers the generalisation properties of neural networks and introduces a new measure of flatness, which is based on the fact that the loss curve of a given function is not randomly modified."
"We introduce DPFRL, a framework for reinforcement learning under partial and complex observations with a fully differentiable discriminative particle filter Introduces ideas for training DLR agents with latent state variables, modeled as a belief distribution, so they can handle partially observed environments. This paper introduces a principled method for POMDP RL: Discriminative Particle Filter Reinforcement Learning that allows for reasoning with partial observations over multiple time steps, achieving state-of-the-art on benchmarks.","A discriminative observation model that learns to explicitly track a latent belief, while circumventing the difficulty of generative observation modeling. This article introduces a new deep learning method for deep learning that learns a latent knowledge representation from particle observations. The study presents a deep learning approach to deep learning by using a discriminatively parameterized observation function to learn a latent information representation."
"We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time. Studies the problem of learning a single convolutional filter using SGD and shows that under certain conditions, SGD learns a single convolutional filter. This work extends the Gaussian distribution assumption to a more general angular smoothness assumption, which covers a wider family of input distributions","This article studies the dynamics of gradient descent and stochastic gradient descent in deep neural networks. This study studies the learning dynamics of deep neural network and shows that gradient descent with random initialization leads to faster convergence than gradient descent. The study presents a method for learning the weights of a neural network using a Gaussian distribution, which leads to higher convergence rates."
"We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique. An algorithm called value iteration with negative sampling to address the covariate shift problem in imitation learning.","A novel reinforcement learning algorithm that learns a value function that extrapolates to unseen states more conservatively, as an approach to attack the optimistic extrapolation problem. This work introduces a new model of reinforcement learning based on values functions that extrapolate from unseen states to unseen ones. The study presents a reinforcement learning method for reinforcement learning that learns values functions with conservative extrapolation and achieves near-optimal performance."
"We investigate the merits of employing neural networks in the match prediction problem where one seeks to estimate the likelihood of a group of M items preferred over another, based on partial group comparison data. This work proposes a deep neural network solution to the set ranking problem and designs a architecture for this task inspired by previous manually designed algorithms. This article provides a technique to solve the match prediction problem using a deep learning architecture.","A novel neural network architecture for match prediction that can be easily adapted for a variety of real-world online game datasets. This work introduces a new neural network framework for the match prediction problem, based on a BTL model. The authors introduce two new neural networks, MM-sum and MM-prod, to address the problem of cross entropy loss in match prediction."
"We investigate ReLU networks in the Fourier domain and demonstrate peculiar behaviour. Fourier analysis of ReLU network, finding that they are biased towards learning low frequency This article has theoretical and empirical contributions on topic of Fourier coefficients of neural networks","This study studies the spectral bias of deep neural networks in the context of random parameter perturbations. This study investigates the problem of learning a deep neural network with low frequencies, and shows that it is more robust to random parameters than deep neural nets with high frequencies. The study presents a method for learning deep neural architectures with low-frequency components that can be used to train models with low frequency."
"We refine the over-approximation results from incomplete verifiers using MILP solvers to prove more robustness properties than state-of-the-art. Introduces a verifier that obtains improvement on precision of incomplete verifiers and scalability of the complete verifiers using over-parameterization, mixed integer linear programming and linear programming relaxation. A mixed strategy to obtain better precision on robustness verifications of feed-forward neural networks with piecewise linear activation functions, achieving better precision than incomplete verifiers and more scalability than complete verifiers.","A combination of state-of-the-art overapproximation techniques used by incomplete methods for neural network robustness. This work presents a method to certify neural networks robustness in the context of adversarial examples. The authors present a method for certifying neural networks that is asymptotically complete as non-complete methods, and show that it is possible to verify robustness properties."
Using Wasserstein-GANs to generate realistic neural activity and to detect the most relevant features present in neural population patterns. A method for simulating spike trains from populations of neurons which match empirical data using a semi-convolutional GAN. The study proposes to use GANs for synthesizing realistic neural activity patterns,Spike-GAN is able to learn the structure of packets generated by a neural network. This study presents a novel neural network architecture that learns to encode information about a given set of stimuli in a spatio-temporal manner. The authors investigate the applicability of Spike-GAN to deep neural networks and show that it can be used to detect the most relevant aspects of a given stimulus.
This work describe a 3D authoring tool for providing AR in assembly lines of industry 4.0 The article addresses how AR authoring tools support training of assembly line systems and proposes an approach An AR guidance system for industrial assembly lines that allows for on-site authoring of AR content. Presents a system that allows factory workers to be trained more efficiently using augmented reality system.,A novel 3D authoring tool that allows users to create 3D scenes of workstations by placing 3D objects on the assembly line. This article presents a novel 3-D authorization tool that enables users to make 3D 3D models of the workstation. The authors present a novel method for authoring 3D images of the assembly lines of an industrial machine.
Learning Priors for Adversarial Autoencoders Proposes a simple extension of adversarial auto-encoders for conditional image generation. Focuses on adversarial autoencoders and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution,"A deep generative framework for generating images with a simple prior that is best suited for explaining the data distribution. This work introduces a new approach to generating images by replacing the manually-specified prior with a fully connected code generator. The authors introduce a new model of generative models, AAE, which uses a variational learning technique to transform the manually selected prior into a better representation of the data."
learn hierarchal sub-policies through end-to-end training over a distribution of tasks The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. This study proposes a novel method for inducing temporal hierarchical structure in a specialized multi-task setting.,A method for fast learning in complex physics environments with long time horizons. This work presents a method to iteratively learn sub-policies that allow agents to achieve maximum reward over sparse-reward tasks. The authors present a method for learning sub-policy structures that can be used to speed up the learning process on complex physics tasks. This study presents an approach to fast learning of complex physics problems using a policy gradient update.
"Represent each entity as a probability distribution over contexts embedded in a ground space. Proposes to construct word embeddings from a histogram over context words, instead of as point vectors, which allows for measuring distances between two words in terms of optimal transport between the histograms through a method that augments representation of an entity from standard ""point in a vector space"" to a histogram with bins located at some points in that vector space.","A new metric for word entailment between entities and contexts. This work introduces a new metric to measure the distance between words between entities, which can be used to improve the efficiency of existing point-wise embedding methods. The study presents a method for measuring distance between two words in NLP, and shows that it is more efficient than point-based methods."
"precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization Investigates the problem of neural network quantization by employing an end-to-end precision highway to reduce the accumulated quantization error and enable ultra-low precision in deep neural networks. This work studies methods to improve the performance of quantized neural networks This paper proposes to keep a high activation/gradient flow in two kinds of networks structures, ResNet and LSTM.","A novel network-level method for quantization of high-precision information flow. This work introduces a new method for training convolutional neural networks, called precision highway, which can be applied to both pre-activation and post-activation neural networks. The study presents a novel approach to quantization in the context of deep neural networks where the activation tensor is directly connected to the residual path."
"Many graph classification data sets have duplicates, thus raising questions about generalization abilities and fair comparison of the models. The authors discuss isomorphism bias in graph datasets, the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model, theoretically analogous to data leakage effects.","A method for graph classification based on isomorphic graphs in training data sets. This study presents a new method for learning to classify graphs in the training data set, which is based on graph isomorphism bias. The authors introduce a new algorithm for graph isomorphic graph classification, called WL algorithm, which can be used to improve the generalization ability of the models."
"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders. Alternating minimization framework for training autoencoder and encoder-decoder networks The authors explore an alternating optimization approach for training Auto Encoders, treating each layer as a generalized linear model, and suggest using the stochastic normalized GD as the minimization algorithm in each phase.","An alternating minimization strategy for quasi-convex optimization. This article introduces a new method for training autoencoders with sigmoidal activation functions, which provides an efficient implementation of DANTE. This work presents a novel approach to quasi-Convex problem where the layers are trained as a set of generalized linear activation functions. The authors introduce a new variant of the SNGD that combines the sigmoid activation functions with the generalized ReLU activation functions"
"We propose an algorithmic framework to schedule constellations of small spacecraft with 3-DOF re-orientation capabilities, networked with inter-sat links. This article proposes a communication module to optimize the schedule of communication for the problem of spacecraft constellations, and compares the algorithm in distributed and centralized settings.","Combine physical models of orbital mechanics (OM), attitude control systems (ACS), and inter-satellite links (ISL) and optimizes the schedule for any satellite in a constellation to observe a known set of ground regions with rapidly changing parameters and observation requirements. This work presents an approach to optimizing the scheduler for each satellite in the constellation."
"Image captioning as a conditional GAN training with novel architectures, also study two discrete GAN training methods. An improved GAN model for image captioning that proposes a context-aware LSTM captioner, introduces a stronger co-attentive discriminator with better performance, and uses SCST for GAN training.","A novel GAN framework for image captioning based on Self-critical Sequence Training (SCST) and Gumbel Straight-Through (ST) This work introduces a new GAN-based framework that outperforms other GANs in terms of training stability and the overall performance. This work presents a novel sequence-based GAN architecture that enables better language composition, more expressive captions, and more descriptive captions."
A two-stage approach consisting of sentence selection followed by span selection can be made more robust to adversarial attacks in comparison to a single-stage model trained on full context. This work investigates an existing model and finds that a two-stage trained QA method is not more robust to adversarial attacks compared to other methods.,"A two-stage approach for question-answering with adversarial training. This paper presents a novel approach for QA that combines sentence selection and context selection to improve the robustness of both models. This work introduces a new approach to QA in which sentence selection is combined with context selection, allowing for more robust robustness to adversarial attacks."
"We use a GAN discriminator to perform an approximate rejection sampling scheme on the output of the GAN generator. Proposes a rejection sampling algorithm for sampling from the GAN generator. This study proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling, to help filter ‘good’ samples from GANs’ generator.","A new rejection sampling method for GANs that can be used to correct errors in the GAN distribution. This paper introduces a new method for solving the problem of discriminator rejection sampling, which is based on the assumption that the discriminator is biased towards a given distribution. The work presents a new rejection sample method for generating samples that are biased toward a particular class of classes."
We show how to get good representations from the point of view of Simiarity Search. Studies the impact of changing the image classification part on top of the DNN on the ability to index the descriptors with a LSH or a kd-tree algorithm. Proposes to use softmax cross-entropy loss to learn a network that tries to reduce the angles between inputs and the corresponding class vectors in a supervised framework using.,"An approximate nearest neighbor search algorithm for hyperplane neural networks. This article studies the problem of finding the correct distance between a query and its nearest neighbor in a dataset of size n. The study presents a method to find the nearest neighbor of a hyperplane NNS algorithm, based on the cosine similarity theorem."
Adversarial learning methods encourage NLI models to ignore dataset-specific biases and help models transfer across datasets. The paper proposes an adversarial setup to mitigate annotation artifacts in natural language inference data This work presents a method for removing bias of a textual entailment model through an adversarial training objective.,"An adversarial architecture for NLI models that does not require direct supervision in the form of a domain label. This work introduces a new classifier architecture that can be trained on NLI datasets without requiring direct supervision. The authors introduce two new adversarial architectures, a baseline NLI architecture and an adversarial hypothesis-only architecture."
"Object instance recognition with adversarial autoencoders was performed with a novel 'mental image' target that is canonical representation of the input image. The paper proposes a method to learn features for object recognition that is invariant to various transformations of the object, most notably object pose. This paper investigated the task of few shot recognition via a generated “mental image” as intermediate representation given the input image.",MIDCGAN is a novel autoencoder that learns features that are useful for image generation. This work introduces a new model of autoencoders for the problem of generating images that are sharper and closer to the target than a regular autoencatcher. The authors introduce a new method for generating images from mental images that can be used for general classification tasks such as image-to-image translation.
"Question answering models that model the joint distribution of questions and answers can learn more than discriminative models This paper proposes a generative approach to textual and visual QA, where a joint distribution over the question and answer space given the context is learned, which captures more complex relationships. This paper introduces a generative model for question answering and proposes to model p(q,a|c), factorized as p(a|c) * p(q|a,c). The authors proposes a generative QA model, which optimizes jointly the distribution of questions and answering given a document/context.","A generative QA model that learns to answer questions independently of the answer and document. This work introduces a generative model for answering questions using question-to-context attention. The study presents a novel QA framework for answering complex questions in a multi-sentence setting, where the question is answered independently from the document and the document."
"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet. The study explains and generalizes approaches for learning neural nets with hard activation. This article examines the problem of optimizing deep networks of hard-threshold units. The study discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it with a collection of heuristics/approximations.","A recursive mini-batch algorithm for learning deep neural networks with hard-threshold activations. This work introduces a recursive minibatch algorithm to learn deep neural network activations that includes the popular straight-through estimator as a special case. This paper presents a recursive semi-batch method for learning neural networks that includes a deep gradient descent problem, and shows that it improves classification accuracy in a number of settings, including AlexNet."
"pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images Explores explaining scenes with surfels in a neural recognition model, and demonstrate results on image reconstruction, synthesis, and mental shape rotation. Authors introduce a method to create a 3D scene model given a 2D image and a camera pose using a self-superfised model",A novel method of unsupervised learning of 3D structure from a single view of a scene. This work introduces a novel method for learning 3D structural properties of objects in a scene by using a variational encoder network. The study presents a novel approach to learn 3D representations of objects that can be viewed from a wide range of locations.
"We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games. Learning to play two-player general-sum games with state with imperfect information Specifies a trigger strategy (CCC) and corresponding algorithm, demonstrating convergence to efficient outcomes in social dilemmas without need for agents to observe each other's actions.",A new reward-ergodic strategy for cooperative and selfish agents. This article introduces a new policy gradient approach for cooperative/selfish agents in Markov games. This study presents a new reward gradient approach to cooperative/socially motivated agents in the Prisoner's Dilemma (POMG) where the agent learns to cooperate with the other agent. The authors present a novel policy gradient method for cooperating with other agents in a Markov game
We learn a fast neural solver for PDEs that has convergence guarantees. Develops a method to accelerate the finite difference method in solving PDEs and proposes a revised framework for fixed point iteration after discretization. The authors propose a linear method for speeding up PDE solvers.,"A new iterative solver that converges faster than the hand designed solver. This article introduces a new iteratively solver for deep linear convolutional networks, which can be easily obtained on different geometries and boundary conditions. The study presents a new Iterative Solver for a class of problems where the fixed point of the function is the same as that of the previous iteration."
"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with improved performance and significantly reduced cost than traditional methods. The authors propose a procedure to generate an ensemble of sparse structured models A new framework for training ensemble neural networks that uses SG-MCMC methods within deep learning, and then increases computational efficiency by group sparsity+pruning. This work explores the use of FNN and LSTMs to make bayesian model averaging more computationally feasible and improve average model performance.","A Bayesian model averaging method for LSTMs that outperforms SGD and other Bayesian inference frameworks. This work presents a Bayesian method for learning LSTM models using stochastic gradient descent (SGLD). The authors present a method to learn a Bayes-based model averaging algorithm, which can be used to improve the performance of existing Bayesian models."
"We propose and apply a meta-learning methodology based on Weak Supervision, for combining Semi-Supervised and Ensemble Learning on the task of Biomedical Relationship Extraction. A semi-supervised method for relation classification, which trains multiple base learners using a small labeled dataset and applies some of them to annotate unlabeled examples for semi-supervised learning. This paper addresses the problem of generating training data for biological relation extraction, and uses predictions from data labeled by weak classifiers as additional training data for a meta learning algorithm. This paper proposes a combination of semi-supervised learning and ensemble learning for information extraction, with experiments conducted on a biomedical relation extraction task","An ensemble learning method for improving generalization of deep neural networks on large datasets. This work introduces a new approach to Deep Neural Networks, called Majority Voting, which is an ensemble learning technique that uses a simple bi-directional Long-Short Term Memory network and a large dataset. The study presents a new method for learning a diverse set of data points in a supervised learning setting."
"we propose a regularizer that improves the classification performance of neural networks the authors propose to train a model from a point of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. Proposes to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer to maximize mutual information I(Y, \hat{T}) while constraining irrelevant information","Information Bottleneck (IB) problem is a binary supervised classification problem where the mutual information between signal X and feature T is treated as irrelevant information for classification. This article presents a binary-supervised classification problem based on the notion of mutual information in deep neural networks. The authors present a binary algorithm for classification of binary neural networks, which can be decomposed into two stages: maximization over mutual information and minimization."
"Presents new architecture which leverages information globalization power of u-nets in a deeper networks and performs well across tasks without any bells and whistles. A network architecture for semantic image segmentation, based on composing a stack of basic U-Net architectures, that reduces the number of parameters and improves results. This proposes a stacked U-Net architecture for image segmentation.","A deep net-based classification network that globalizes information while preserving resolution. This work introduces stacked u-nets, which iteratively combine features from different resolution scales while maintaining resolution. The work presents a new architecture for image classification that combines the information globalization power of u-nets with semantic segmentation and object detection tasks."
"We introduce a smoothness regularization for convolutional kernels of CNN that can help improve adversarial robustness and lead to perceptually-aligned gradients This article proposes a new regularization scheme that encourages convolutional kernels to be smoother, arguing that reducing neural network reliance on high-frequency components helps robustness against adversarial examples. The authors propose a method for learning smoother convolutional kernels, specifically, a regularizer penalizing large changes between consecutive pixels of the kernel with the intuition of penalizing the use of high-frequency input components.","A smooth regularization of deep neural networks to improve the robustness of models. This work presents a novel method for training deep neural network models that is more robust to adversarial perturbations than vanilla training loss. The authors present a novel algorithm for training neural networks with low-frequency components that can be used to improve their robustness. This study introduces a new approach to training neural network architectures with low frequency components, and shows that regularization can improve robustness significantly."
"We use meta-gradients to attack the training procedure of deep neural networks for graphs. Studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. An algorithm to alter graph structure by adding/deleting edges so as to degrade the global performance of node classification, and the idea to use meta-learning to solve the bilevel optimization problem.","A new meta-gradient approach for node classification that is more efficient to compute than adversarial examples. This article presents a meta gradient approach for the problem of node classification, which is an instance of semi-supervised classification where the attacker has full knowledge about the graph structure and all node attributes. The authors present a new meta gradient formulation for the purpose of learning nodes from unlabeled data, which can be used to train deep learning models."
We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters Proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-toend through differentiable Bayesian Filters and two different versions of the Unscented Kalman Filter Revisits Bayes filters and evaluates the benefit of training the observation and process noise models while keeping all other models fixed This work presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms. The approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated diagonal covariance.,A method to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. This work introduces a method to train a model that learns to predict the likelihood of an observation given a particle in a graph-construction context. The authors present a method for learning a heterogeneous Bayesian filter based on observation noise.
"We propose a method for learning latent dependency structure in variational autoencoders. Uses a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model. This article presents a VAE approach in which a dependency structure on the latent variable is learned during training. The authors propose to augment the latent space of a VAE with an auto-regressive structure, to improve the expressiveness of both the inference network and the latent prior",A method for learning the dependency structure between latent variables in deep latent variable models. This work introduces a framework for learning a dependency structure of a variational autoencoder in the latent variable space of a deep generative model. The study presents a framework to learn the dependency structures of a latent variable model in deep neural networks by using a Bayesian network and an inference framework.
We examine the relationship between probability density values and image content in non-invertible GANs. The authors try to estimate the probability distribution of the image with the help of GAN and develop a proper approximation to the PDFs in the latent space.,"A new GAN based on probability density that predicts the densities of images given their latent representation. This work introduces a new method for predicting the density of images in the MNIST dataset, which is based on a regression analysis of the probability density of the latent representation of the image. The authors introduce a new model of GANs, called GAN-based DNNs, which can predict the predicted density of an image given its latent representation"
A simple fast method for extracting visual features from convolutional neural networks Proposes a fast way to learn convolutional features that later can be used with any classifier by using reduced numbers of training epocs and specific schedule delays of learning rate Use a learning rate decay scheme that is fixed relative to the number of epochs used in training and extract the penultimate layer output as features to train a conventional classifier.,A method to speed up the training of convolutional neural networks with a fast learning rate. This article presents a method to train convolutionally neural networks faster by introducing a new step decay schedule that is a function of the total number of epochs predicted to train the model. The authors present a method for learning convolutionality features fast by introducing an exponential and fast decay of the learning rate of the network.
"Decompose the task of learning a generative model into learning disentangled latent factors for subsets of the data and then learning the joint over those latent factors. Locally Disentangled Factors for hierarchical latent variable generative model, which can be seen as a hierarchical variant of Adversarially Learned Inference The paper investigates the potential of hierarchical latent variable models for generating images and image sequences and proposes to train several ALI models stacked on top of each other to create a hierarchical representation of the data. The work aims to learn the hierarchies for training GAN in a hierarchical optimization schedule directly instead of being designed by a human",An unsupervised method for learning hierarchical latent variables with a resolution-based hierarchy. This article presents a novel method for training deep generative models with a hierarchical latent variable in each layer of the graph. This work introduces a new method for disentangled latent variables in the graph that combines resolution and gradients. The authors present a novel approach for learning hierarchies in a hierarchical way by using a resolution based hierarchy.
"Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE). This work proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution Introduces a variation on the Wasserstein AudoEncoders which is a novel regularized auto-encoder architecture that proposes a specific choice of the divergence penalty This work proposes the Cramer-Wold autoencoder, which uses the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.","Cramer-Wold distance from standard multivariate normal density is a generalization of AutoEncoder based generative models. This work presents a method for computing the distance between two distributions from the standard normal density in the latent space. The authors provide a method to compute the distance of a sample from the normal density of a Gaussian prior, and show that it is faster than other autoencoders."
"Learn to rank learning curves in order to stop unpromising training jobs early. Novelty: use of pairwise ranking loss to directly model the probability of improving and transfer learning across data sets to reduce required training data. The article proposes a method to rank learning curves of neural networks that can model learning curves across different datasets, achieving higher speed-ups on image classification tasks.","A method to compare the learning curve of a new configuration to the one of the currently best configuration. This article presents a method for comparing the learning curves of different configurations of a neural architecture using meta-knowledge. The authors present a method to evaluate the likelihood of improvement of a model based on its final learning curve. This paper presents a new method for evaluating the performance of neural architectures, which is based on a combination of partial and partial learning curves"
"Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve. This work shows that wider RNNs improve convergence speed when applied to NLP problems, and by extension the effect of increasing the widths in deep neural networks on the convergence of optimization This paper characterizes the impact of over-parametrization in the number of iterations it takes an algorithm to converge, and presents further empirical observations on the effects of over-parametrization in neural network training.","Convergence rate is a function of direct distance from initialization point to final point, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal."
"Generalized Graph Embedding Models A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches. Tackles the task of learning embeddings of multi-relational graphs using a neural network Proposes a new method, GEN, to compute embeddings of multirelationship graphs, particularly that so-called E-Cells and R-Cells can answer queries of the form (h,r,?),(?r,t), and (h,?,t)","A new embedding learning framework for modeling knowledge graphs. This article presents a new embeddings framework for representing knowledge graphs that is more representative and informative than previous approaches. The study presents a deep learning framework based on the embedding dictionary of Knowledge Graphs, which can be used to model different types of knowledge graphs in a wide range of tasks."
"Mean Replacement is an efficient method to improve the loss after pruning and Taylor approximation based scoring functions works better with absolute values. Proposes a simple improvement to methods for unit pruning using ""mean replacement"" This work presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning",A new pruning method for neural networks that identifies a stable set of prunable units early in the training process. This paper presents a method for pruning neural networks with a fixed number of units at each layer. The authors present a method to prune neural networks without mean propagation and show that the pruning penalty is relatively low during the training phase.
"We introduce a system called GamePad to explore the application of machine learning methods to theorem proving in the Coq proof assistant. This work describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an SVM on both tasks. Proposes that machine learning techniques be used to help build proof in the theorem prover Coq.","A deep learning algorithm for theorem proving in the Coq proof assistant. This work introduces a deep learning system called GamePad that can be used to explore the application of machine learning methods to theorem proving. This article introduces a new method for proof-of-the-art theorem proving based on Coq, which combines deep learning with machine learning to solve the problem of proving a theorem in a supervised manner."
"We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets. This article proposed ""self-ensemble label filtering"" for learning with noisy labels where the label noise is instance-independent, which yield more accurate identification of inconsistent predictions. This study proposes an algorithm for learning from data with noisy labels which alternates between updating the model and removing samples that look like they have noisy labels.","Self-ensemble filtering is a principled method against learning under noisy labels. This paper introduces a semi-supervised approach to learning from noisy labels in deep neural networks by leveraging ensemble predictions of the network's output to improve the filtering process. This work introduces a new method for learning from noise-free labeled data, called Self-Ensemble Learning, which combines ensemble predictions with a self-aware filtering strategy."
"An algorithm for training neural networks efficiently on temporally redundant data. The work describes a neural coding scheme for spike based learning in deep neural networks This paper presents a method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This paper applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network, combining the three components in a way not seen previously.","A deep neural network with spiking gradients that performs about as well as a traditional deep network trained with backpropagation. This work introduces a new neural network encoding scheme, called ""spiking"", which is used to train neural networks with a spiking gradient. The authors present a method for training a neural network using spiking and an adaptive scale of quantization to match the magnitude of the input."
A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers A method for constructing adversarial attacks that are less detectable by humans without cost in image space by changing the target class to be similar to the original class of the image.,"Identifying a target label for adversarial samples in the label space is not at the expense of huge loss in the image space. This article introduces a new attack method that can be used to identify the target label of an adversarial sample. The study presents a method for identifying the target labels of a classifier in the feature space, and then using it to generate adversarial data."
"We propose AD-VAT, where the tracker and the target object, viewed as two learnable agents, are opponents and can mutually enhance during training. This article aims to address the visual active tracking problem with a training mechanism in which the tracker and target serve as mutual opponents This study presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. Proposes a novel reward function - ""partial zero sum"", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.","A novel Adversarial Reinforcement Learning method for the Asymmetric Dueling task, i.e., a novel Dueling mechanism for ASD-VAT. This work introduces a novel adversarial reinforcement learning method that learns to control the movement of the target in a dueling/competitive manner. The authors introduce a new adversarial learning method for ASDF-Vat, which is based on a combination of RL and RL."
"Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks This study describes a large-scale experiment on human object/sematic representations and a model of such representations. This study develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images. A new approach to learn a sparse, positive, interpretable semantic space that maximizes human similarity judgements by training to specifically maximize the prediction of human similarity judgements.","A probabilistic model of the decision in a given trial based on similarity between the embedding vectors of the three concepts presented. The authors present a model for understanding the reasoning behind a decision made by a group of concepts, and show that the decision can be explained as a function of similarity between two concepts."
"We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality. Introduces progressive growing and a simple parameter-free minibatch summary statistic feature for use in GAN training to enable synthesis of high-resolution images.","A novel GAN formulation that preserves variation in the training data distribution. This work introduces a novel method for preserving variation in GANs, which is based on the idea that the generator and discriminator can be used to generate images at high resolution. The authors present a novel GAL formulation that allows for the generation of images at low resolution, and shows that the generated images are more stable than the original model."
"A fast second-order solver for deep learning that works on ImageNet-scale problems with no hyper-parameter tuning Choosing direction by using a single step of gradient descent ""towards Newton step"" from an original estimate, and then taking this direction instead of original gradient A new approximate second-order optimization method with low computational cost that replaces the computation of the Hessian matrix with a single gradient step and a warm start strategy.","A Hessian-free method for non-convex deep neural network optimisation. The authors introduce a Hessian matrix for the first-order optimization of deep neural networks, and show that it can outperform first order methods on large datasets. This work introduces a hybrid gradient descent method for deep neural nets, which achieves high convergence rates on multi-parameter optimization."
Models of source code that combine global and structural features learn more powerful representations of programs. A new method to model the source code for the bug repairing task using a sandwich model like [RNN GNN RNN] which significantly improves localization and repair accuracy.,"A new family of Graph Neural Networks for code that efficiently combine longer distance information with semantic structural information. This work introduces a new family to graph-based neural networks for code, which is based on a sequence-based embedding of code into a graph representation. The study presents a new generation of graph neural networks (GGNNs) for code translation and repair tasks, which combines the message passing information of sequence models with the semantic structure of the code."
"A novel differentiable neural architecture search framework for mixed quantization of ConvNets. The authors introduce a new method for neural architecture search which selects the precision quantization of weights at each neural network layer, and use it in the context of network compression. The study presents a new approach in network quantization by quantizing different layers with different bit-widths and introduces a new differentiable neural architecture search framework.","A combinatorial optimization problem that solves the mixed precision quantization problem with stochastic super nets. This work introduces a new architecture search framework, which is based on the idea of stochastically decomposing neural nets into a super net. The authors introduce a new algorithm called DNAS, which can be used to find the optimal architecture for a given neural net."
"We propose an novel learning method for deep sound recognition named BC learning. Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes to increase discriminitive power of the final learned network. Proposes a method to improve the performance of a generic learning method by generating ""in between class"" training samples and presents the basic intuition and necessity of the proposed technique.","BC learning is a simple and powerful method that improves various sound recognition methods and elicits the true value of large-scale networks. This study studies the effect of BC learning on sound recognition in a wide range of settings, and shows that BC learning improves the performance of many sound recognition models."
"Motivated by theories of language and communication, we introduce community-based autoencoders, in which multiple encoders and decoders collectively learn structured and reusable representations. The authors tackle the problem of representation learning, aim to build reusable and structured represenation, argue co-adaptation between encoder and decoder in traditional AE yields poor representation, and introduce community based auto-encoders. The article presents a community based autoencoder framework to address co-adaptation of encoders and decoders and aims at constructing better representations.","A new framework for representation learning that is easier to learn than traditional autoencoders and decoders. This work introduces a new approach to the representation learning problem in which the encoder and decoder are trained to encode information in a systematic way. The authors introduce a new framework of autoencoder-encoder-decoder training, where each encoder is randomly sampled to learn a representation of the input."
"This study presents a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. Proposes a framework for making predictions on sparse, irregularly sampled time-series data using an interpolation module that models the missing values in using smooth interpolation, non-smooth interpolation, and intensity. Solves the problem of supervised learning with sparse and irregularly sampled multivariate time series using a semi-parametric interpolation network followed by a prediction network.","A novel interpolation network for multivariate time series that captures the global structure of time series. This work introduces a new deep learning framework for multi-time series interpolation, which is able to capture information about time series in a multi-timescale manner. The authors introduce a new interpolation method for multivariable time series, which uses a deep learning model to capture data from multiple dimensions of a multivariate dataset."
"We propose 3D shape programs, a structured, compositional shape representation. Our model learns to infer and execute shape programs to explain 3D shapes. An approach to infer shape programs given 3D models, with architecture consisting of a recurrent network that encodes a 3D shape and outputs instructions, and a second module that renders the program to 3D. This study introduces a high-level semantic description for 3D shapes, given by the ShapeProgram.","An end-to-end neural program synthesizer for 3D image reconstruction. This work introduces a domain-specific language for image reconstruction, which combines a neural program synthesis and a neural executor. The work presents a domain specific language for 3d image reconstruction that combines a deep learning framework with a neural network to generate programs from raw images."
"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures. Introduces a new method to generate RNNs architectures using a domain-specific language for two types of generators (random and RL-based) together with a ranking function and evaluator. This work casts the search of good RNN Cell architectures as a black-box optimization problem where examples are represented as an operator tree and scored based on learnt functions or generated by a RL agent. This work investigates meta-learning strategy for automated architecture search in the context of RNN by using a DSL that specifies RNN recurrent operations.","A deep reinforcement learning architecture search framework based on the OpenNMT framework. This article presents a deep learning framework for RNNs with an extension DSL to the LSTM encoder and decoder. The authors present a deep RL framework for learning deep neural networks using open-end DSLs, which is designed to be used to train deep RL architectures."
"Deep and narrow neural networks will converge to erroneous mean or median states of the target function depending on the loss with high probability. This paper studies failure modes of deep and narrow networks, focusing on as small as possible models for which the undesired behavior occurs. This paper shows that the training of deep ReLU neural networks will converge to a constant classifier with high probability over random initialization if hidden layer widths are too small.",An approximate non-linear activation function for neural networks that can be approximated by a small 3-layer feed-forward NN. This article presents a method for approximating the nonlinear activation functions of neural networks in terms of the width of the input and output of the neural network. The authors show that a simple nonlinear activator function can be approximate by a 2-layer neural network with widths larger than the input dimension.
