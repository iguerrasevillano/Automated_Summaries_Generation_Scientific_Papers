target,abstractive_summary
"A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.","A variational Dirichlet algorithm for deep neural networks classification problem and define a higher-order uncertainty measure. This work presents a variational dirichlet framework for deep network classification problem, which can greatly widen the distance between in-and out-of-distribution data to achieve significant results on various datasets and architectures. The authors present a variable label-wise distribution generated from a lower-order distribution over the simplex S k"
"Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This study presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.",An unsupervised method for analyzing the contribution of individual neurons to NMT models. This work introduces a novel translation control technique that can be used to predict tokens inside/outside of parentheses by modifying the activation of the tense neurons from previous section TAB2. The authors present an unsupervised translation control approach for learning a combination of linguistically interpretable vectors and canonical correlation analysis (PCA) and SVCCA.
We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The article proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This work proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.,"Using a deep neural network architecture that approximates the Johson-Lindenstrauss transform by replacing large weight matrices from fully connected layers with more compact circulant layers. This study introduces a method to replace the weight matrix of a fully connected layer with a circulant layer, which can be used for dimensionality reduction."
"The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The study investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This paper describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.","An analogy-like model that learns from random candidates is perceptually plausible, so that the problem can be solved trivially by matching the correct answer to one of the answers. This work studies the problem of contrasting abstract relational structures in neural network architectures using an analogy regime and a novel approach to interpolation. The authors present a model for neural network learning from random candidate answers which uses a link between the two domains."
"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.","A new concept annotation task for medical time series data that can be used to predict and localize medical concepts. This work introduces a new concept-annotation task which can be applied to the medical data by generating medical information from the data. The authors address the problem of concept annotation for medical data, namely, when the data are not available or when the annotations are available they could be subjective and prone to human errors."
"We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This work proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines","A multi-way encoding framework that obstructs the adversarial gradient search. This work introduces a new attack for a binary model watermarking algorithm, which trains a model to misclassify at the final encoder layer. The authors present a multi-directional attack for both the source and target models, and demonstrate the strength of the approach."
"We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The article proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts This study focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up","A spherical CNN that can detect patterns regardless of how they are rotated over the sphere. This work introduces a new group correlation function on the Sphere, which acts naturally on the input space of the network. The paper presents a method for detecting patterns in a three-dimensional manifold called SO(3) 2 and provides a simple definition of the SenseCNN (S2 -CNN)."
"Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations","A new model for multi-agent learning that predicts the forward dynamics of agents' systems, and provides insights into the relational and social structures present in the data. This work introduces a new algorithm to learn how agents interact with each other on a single agent basis, and presents a novel method for modeling agents' behavior as well as an analysis tool for characterizing their actions."
"We introduce a transparent middleware for neural network acceleration, with own compiler engine, achieving up to 11.8x speed up on CPUs and 2.3x on GPUs. This work proposes a transparent middleware layer for neural network acceleration and obtains some acceleration results on basic CPU and GPU architectures","This article introduces a transparent middleware layer for neural network acceleration, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware devices. The authors address the problem of inference and inference in deep neural networks by using a compiler for deep learning that is transparent to the user and should support all hardware platforms and deep learning libraries."
"Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This paper studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This article explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.","In the literature on artificial dialogue agents, a distinction is often made between ""goal-oriented"" dialogue, where an agent is tasked with filling slots or otherwise obtaining or disseminating specified information from the user to help complete a task, and ""chit-chat"". The authors present a cluster of human-written game actions that are trained to imitate human actions given a goal (an ""inverse model"")"
"CharNMT is brittle This study investigates the impact of character-level noise on 4 different neural machine translation systems This work empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. This paper investigates the impact of noisy input on Machine Translation and tests ways to make NMT models more robust","A charCNN model that is more robust to different kinds of noise, by training on noisy texts. This work introduces a charcNN-based approach to solving the problem of typos and noise in a text-based way. The authors present a new method for learning a word-to-word translation using a combination of a single key and a two-step process to solve the problems of language-specific errors."
"Feedforward neural networks that can have weights pruned after training could have had the same weights pruned before training Shows that there exists sparse subnetworks that can be trained from scratch with good generalization performance and proposes a unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The work examines the hypothesis that randomly initialized neural networks contain sub-networks that converge equally fast or faster and can reach the same or better classification accuracy","A method for iterative pruning of a fully-connected network. This article considers the problem of training a pruned model with a small capacity. The authors present a method for learning pruned models from scratch, and use it as a proxy for the speed at which a network learns. The paper presents a strategy to improve validation accuracy in an early-stopping manner."
"A new regularization term can improve your training of wasserstein gans The study proposes a regularization scheme for Wasserstein GAN based on relaxation of the constraints on the Lipschitz constant of 1. The work deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric.","A GAN-based regularization term that penalizes the deviation of the norm of the critic function (as a function of the network's input) from one. This work presents a method to improve GAN training by reducing the infimum and minimizing the Wasserstein-1 distance, which can be achieved by alternating gradient descent updates for the generator and the 1-Lipschitz function f."
"DeFINE uses a deep, hierarchical, sparse network with new skip connections to learn better word embeddings efficiently. This work describes a new method for learning deep word-level representations efficiently by using a hierarchical structure with skip-connections for the use of low dimensional input and output layers.","This article introduces a new adaptive embedding method for learning input and output representations jointly while significantly reducing the number of parameters presented by these layers. The authors introduce a weight-tying mechanism that allows for faster, memory-efficient end-to-end training while providing similar or better benefits compared to existing post-training methods which require a hierarchical group transformation (HGT) to learn deep representations efficiently using sparse and dense connections."
"We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces. Paper's concepts work in the discrete-time formalism, use the master equation, and remove reliance on a locally quadratic approximation of the loss function or on any Gaussian asumptions of the SGD noise. The authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD and use the relations to set training schedule adaptively and analyze the loss-function landscape.","An analogous fluctuation-dissipation relation that quantitatively link the noise in mini-batched data to the observable evolution of the model performance. This work introduces a new lossfunction landscape with a two-point noise matrix, which can be used to test stationarity and delineate the shape of the landscape."
"We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training. Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network. This study proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks. Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).","This paper introduces a new method for binary classification using a gate-based quantum computation. The article presents a novel way to train binary qubits in the form of qRAM, which is based on a different, more general form of quantum computation as opposed to the latter."
"Transfer learning for estimating causal effects using neural networks. Develops algorithms to estimate conditional average treatment effect by auxiliary dataset in different environments, both with and without base learner. The authors propose methods to address a novel task of transfer learning for estimating the CATE function, and evaluate them using a synthetic setting and a real-world experimental dataset. Using neural network regression and comparing transfer learning frameworks to estimate a conditional average treatment effect under string ignorability assumptions","A method for assessing transfer learning for CATE estimation on real data. This article presents a method to evaluate transfer learning in observational studies, where the number of covariates is large. The authors introduce a new network layer that can be used to train DGPs with memory to solve forgetting during transfer by using lateral connections to existing frozen layers. This study explores the problem of learning CATEs from a set of large field experiments."
"We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization. Studies the forgetting behavior of training examples during SGD, and shows there exist ""support examples"" in neural network training across different network architectures. This study analyzes the extent to which networks learn to correctly classify specific examples and then forget these examples over the course of training. The article studies whether some examples in training neural networks are harder to learn than others. Such examples are forgotten and relearned multiple times through learning.","A method for identifying important, or most informative examples that are never forgotten once learnt. This work investigates the problem of forgetting dynamics in neural networks by studying the extent to which a neural network learns unforgettable examples. The authors present a method for learning memorable examples that can be used to identify ""important"" samples and detect outliers and examples with noisy labels."
"A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability. The authors claim that the previous art directly integrate neural networks into the graphical models as components, which renders the models uninterpretable. Proposal for a combination of neural nets and graphical models by using a deep neural net to predict the parameters of a graphical model.","A novel algorithm for image and text classification and survival analysis. This work introduces a novel algorithm to interpret explanations by using a domain-specific deep architecture. The authors present a method for learning the decision boundary of a prediction model, which can be used in a social context, and show that post-hoc approximations of CEN's decision boundary are consistent with the generated explanations."
"We proposed ""Difference-Seeking Generative Adversarial Network"" (DSGAN) model to learn the target distribution which is hard to collect training data. This paper presents DS-GAN, which aims to learn the difference between any two distributions whose samples are difficult or impossible to collect, and shows its effectiveness on semi-supervised learning and adversarial training tasks. This work considers the problem of learning a GAN to capture a target distribution with only very few training samples from that distribution available.","A GAN prone generator to output samples located in high-density areas of pd, and adversarial training requires a bad GAN for semi-supervised learning. This work introduces a GAN that learns the mixture distribution of the generator and the generator distribution by minimizing the statistical distance between the two support sets. The paper presents a method for training GANs with the ability to learn different target distributions"
"We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document to perform disease entity normalization. Uses a GRU autoencoder to represent the ""context"" (related enitities of a given disease within the span of a sentence), solving the BioNLP task with significant improvements over the best-known methods.","A combination of two sub-models which leverage both topical coherence and semantic features to perform disease normalization. This work focuses on the problem of disease normalisation, an essential step in the construction of a biomedical knowledge base as diseases are central to biomedicine."
"We present a single shot analysis of a trained neural network to remove redundancy and identify optimal network structure This study proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset This study presents a framework for optimising neural networks architectures through the identification of redundant filters across layers","This paper introduces a new method for determining the number of significant filters in a subset selection problem. The authors address the problem of pruning out a particular filter and retraining the system to reduce the dimensionality of the space that the data resides in after passing through a layer. This work presents a novel approach to identifying a network structure that has many principal filters, and uses it as a starting point to fine tune the network."
"The first deep learning approach to MFSR to solve registration, fusion, up-sampling in an end-to-end manner. This paper proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. This paper proposes a framework including recursive fusion to co-registration loss to solve the problem of super-resolution results and high-resolution labels not being pixel aligned.","This work introduces a deep-learning approach that solves the co-registration, fusion and registration-at-theloss problems in an end-to-end learning framework. The authors present a single image super-resolution system that can be paired with HighRes-net to account for pixel and sub-pixel shifts in the loss."
We introduce a technique that allows for gradient based training of quantized neural networks. Proposes a unified and general way of training neural networks with reduced precision quantized synaptic weights and activations. A new approach to quantizing activations which is state of the art or competitive on several real image problems. A method for learning neural networks with quantized weights and activations by stochastically quantizing values and replacing the resulting categotical distribution with a continuous relaxation,"A method for quantizing neural networks with a stochastic rounding that can be seen as a special case of the proposed approach. This article introduces a method to quantize neural networks by using a ""soft"" quantizer, which is used to train the network in a manner that corresponds to discretizing p(x) onto the input signal and then sampling grid points g i from it."
"Interpretation by Identifying model-learned features that serve as indicators for the task of interest. Explain model decisions by highlighting the response of these features in test data. Evaluate explanations objectively with a controlled dataset. This article proposes a method for producing visual explanations for deep neural network outputs and releases a new synthetic dataset. A method for Deep Neural Networks that identifies automatically relevant features of the set of the classes, supporting interpretation and explanation without relying on additional annotations.","A method for identifying the top-responding filters that are important for the prediction of a given class of interest. This paper introduces a method to automatically identify the network-encoded features that serve as indicators for the classification of the classes of interest to be predicted using heatmap visualizations. The authors present a novel algorithm for defining the class label of an image, a set of identified filters and a class prediction."
"A method that build representations of sequential data and its dynamics through generative models with an active process Combines neural networks and Gaussian distributions to create an architecture and generative model for images and video which minimizes the error between generated and supplied images. The study proposes a Bayesian network model, realized as a neural network, that learns different data in the form of a linear dynamical system","A hierarchical model for generative adversarial networks (GANs), where the inputs are constant and the representations have to adapt to the input over several iterations. This work introduces a hierarchic architecture that uses an encoder to determine how the observations change over time. The authors present a method for defining the dynamics of interactions in a latent space, which can be understood as the transition from Z t to Z T+1"
"Obtains state-of-the-art accuracy for quantized, shallow nets by leveraging distillation. Proposes small and low-cost models by combining distillation and quantization for vision and neural machine translation experiments This article presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression.","A method for quantized distillation and leverages distillation in the training process by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. This study introduces a method to quantize weights of the student's weights in a quantized model, and shows that quantization can provide better compression than a distillation process."
"We use question-answering to evaluate how much knowledge about the environment can agents learn by self-supervised prediction. Proposes QA as a tool to investigate what agents learn about in the world, arguing this as an intuitive method for humans which allows for arbitrary complexity. The authors propose a framework to assess representations built by predictive models that contain sufficient information to answer questions about the environment they are trained on, showing those by SimCore contained sufficient information for the LSTM to answer questions accurately.",QA decoders are trained to extract complex high-level information from the agent's internal state. This work introduces a new auxiliary network that learns to encode relevant aspects of the environment in a way that is self-supervised by a loss on the agent’s future prediction against the ground-truth egocentric observation at tk.
"We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space. This paper addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This study proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.","A new reinforcement learning algorithm to train stochastic continuous action policies with arbitrary action probability distributions. This article introduces a novel reinforcement learning approach to learn a quantile function of the action dimensions and the quantile functions of the actions. The paper presents a method for learning a state-to-state vector reward from two samples of the same network, which can be used to train a deep neural network to approximate the action probability functions."
"Learn representations for images that factor out a single attribute. This work builds on Conditional VAE GANs to allow attribute manipulation in the synthesis process. This study proposes a generative model to learn the representation which can separate the identity of an object from an attribute, and extends the autoencoder adversarial by adding an auxiliary network.","A VAE-GAN that learns a latent space representation that separates an object category from its attributes. The authors introduce a VAE architecture for image synthesis that is competitive with state of the art. This article presents a new VAE model for image attribute manipulation, where label information is partially contained in rather than solely in y."
We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification This study proposes to transfer the classifier from the model for face classification to the task of alignment and verification. The manuscript presents experiments on distilling knowledge from a face classification model to student models for face alignment and verification.,"A method to distill knowledge from the teacher network by distilling its soft-prediction, which is effective in face alignment and verification. This work presents a method for distillation of classifiers using a distillation trick to boost the distillation performance of classification. The paper introduces a new distillation method that initializes the deep layers of the student network by regressing the mid-level target of the school network."
"New state-of-the-art framework for image restoration The article proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications. This article proposes a residual non-local attention network for image restoration","This article presents a novel method for image restoration using residual non-local attention to train very deep networks by preserving more low-level features, being more suitable for image super-resolution. This work introduces a new approach for image reconstruction that can be used to improve the quality of information in the mask branch. The authors present a method for photo restoration which uses a large receptive field size to extract features from the hierarchical representations."
"We present an influence-directed approach to constructing explanations for the behavior of deep convolutional networks, and show how it can be used to answer a broad set of questions that could not be addressed by prior work. A way to measure influence that satisfies certain axioms, and a notion of influence that can be used to identify what input part is most influential for the output of a neuron in a deep neural network. This study proposes to measure the influence of single neurons with regard to a quantity of interest represented by another neuron.","A method for interpreting predictions for convolutional neural networks. This study presents a method to interpret the prediction of neural networks using an influence-directed explanation of the network's internal units. This work introduces a new approach to understanding the role of neural network activations in the classification of neurons in the input image, and shows that it is better at localizing the features used by the network in its prediction."
"In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning. A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction. Reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead is trained to generate frames that happen at any point in the future.","A novel technical approach to solve video prediction problem by reframeing the prediction problem to be time-agnostic. This work introduces a method for identifying ""bottleneck states"" across several tasks, and shows that these bottleneck states correspond to subgoals that aid in planning towards complex end goals. The authors address the problem of video prediction using video prediction as a subgoal for hierarchical planners."
We perform functional variational inference on the stochastic processes defined by Bayesian neural networks. Fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples. Presents a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors features in the literature. Presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally rather than via a prior over weights.,"A variational inference method that approximates the functional ELBO by using finite measurement sets and the spectral Stein gradient estimator. This article introduces a method for estimating the posterior dependencies of a shallow BNN, showing that under certain assumptions, the limiting distribution is a Gaussian process (GP). The authors present a technique for measuring the posterior function of an implicit distribution, which can be applied to stochastic processes."
"Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation. A method for learning deep latent-variable MRF with an optimization objective that utilizes Bethe free energy, that also solves the underlying constraints of Bethe free energy optimizations. An objective for learning latent variable MRFs based on Bethe free energy and amortized inference, different from optimizing the standard ELBO.","Using a saddlepoint objective to learn deep, undirected graphical models using the Bethe free energy approximation to the model's partition functions. This article introduces a method for learning deep-linear text HMMs with latent variables that can be optimized efficiently without sampling and outperforms other approximate inference methods in terms of held out log likelihood."
"In this work, we point to a new connection between DNNs expressivity and Sharkovsky’s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks Shows how the expressive power of NN depends on its depth and width, furthering the understanding of the benefit of deep nets for representing certain function classes. The authors derive depth-width tradeoff conditions for when relu networks are able to represent periodic functions using dynamical systems analysis.","A method for predicting periodic functions as a function of the depth. The paper presents a method to predict periodic functions that can be represented by shallow vs deep neural networks. This article introduces a technique for determining the case of periodic functions in a wide variety of layers, and shows that they can be approximated by shallow networks unless they are exponentially large."
"We propose a method that infers the time-varying data quality level for spatiotemporal forecasting without explicitly assigned labels. Introduces a new definition of data quality that relies on the notion of local variation defined in (Zhou and Scholkopf) and extends it to multiple heterogenous data sources. This article proposed a new way to evaluate the quality of different data sources with the time-vary graph model, with the quality level used as a regularization term in the objective function","Graph convolutional neural networks provide an efficient architecture to extract localized patterns from regular grids, such as images BID14. This work introduces a graph convolutionary neural network architecture that can be used to compute the data quality at vertex i. The study presents a method for generating localised patterns on graph connectivity by using a combination of two methods and a model with a different number of neighboring signals in the vertex."
"We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty in conventional networks. This work proposes using batch normalisation at test time to get the predictive uncertainty, and shows Monte Carlo prediction at test time using batch norm is better than dropout. Proposes that the regularization procedure called batch normalization can be understood as performing approximate Bayesian inference, which performs similarly to MC dropout in terms of the estimates of uncertainty that it produces.","This study presents a method for estimating uncertainty in deep neural networks by minimizing the divergence of the model prior to the mini-batch optimization. This work introduces a new algorithm for batch normalization, which can be used to standardize the distribution of each unit's input into a Bayesian setting."
"Understanding the neural network Hessian eigenvalues under the data generating distribution. This work analyzes the spectrum of the Hessian matrix of large neural networks, with an analysis of max/min eigenvalues and visualization of spectra using a Lanczos quadrature approach. This paper uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposes an efficient spectrum visualization methods.","A random matrix theory to derive analytic results for the eigenspectrum perturbations between the Empirical Hessian and False Heissian. This work introduces a method for evaluating the spectral perturbations of the True Heses by using a random-matrix theory, which is used to test the validity of the theoretical results in Section 4.2."
"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time This article proposes the Recurrent Discounted Attention (RDA), an extension to Recurrent Weighted Average (RWA) by adding a discount factor. Extends the recurrent weight average to overcome the limitation of the original method while maintaining its advantage and proposes the method of using Elman nets as the base RNN","This article introduces a Recurrent Discounted Attention Unit, which extends the RWA by allowing it to discount the attention applied to previous timesteps. This work introduces an attention matrix for each encoded state and a translated word combination that can be used to calculate the weighted average of each of them. The paper presents a method for determining where in the sequence to attend to and how much attention it assigns to multiple tasks is needed."
"How you should evaluate adversarial attacks on seq2seq The authors investigate ways of generating adversarial examples, showing that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting. The work is about meaning-preserving adversarial perturbations in the context of Seq2Seq models",A new constraint for adversarial attacks on word-based MT systems that produce more semantically similar adversarials. This study introduces a method of imbuing gradient-based word substitution attacks with simple constraints aimed at increasing the chance that the meaning is preserved (3.2). The authors present a novel defense technique for adversaries against word substitution and show that constrained substitution attacks do preserve meaning to a higher degree than unconstrained attacks.
Imposing graph structure on neural network layers for improved visual interpretability. A novel regularizer to impose graph structure upon hidden layers of a Neural Network to improve the interpretability of hidden representations. Highlights the contribution of graph spectral regularizer to the interpretability of neural networks.,"A new approach for interpreting latent representations obtained by hidden layers of neural networks. This work introduces a new set of regularizations that are defined in the spectral domain, rather than in the neuron domain, in order to directly enforce spectrral properties of the activation signal. This article presents a novel method for translating latent information into a graph topology and a generalization of the regularization used in deep learning."
A framework for learning high-quality sentence representations efficiently. Proposes a faster algorithm for learning SkipThought-style sentence representations from corpora of ordered sentences that swaps the word-level decoder for a contrastive classification loss. This article proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences,"An autoencoder model that reconstructs the surface form of a sentence, which forces the model to not only predict its semantics, but also aspects that are irrelevant to the meaning of the sentence as well. This article introduces a de-noising autoencoding approach for sentence reconstruction and encapsulates the Skip-gram approach of BID24 when words play the role of sentences."
Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly. Proposes stochastic determination methods for truncation points in backpropagation through time. A new approximation to backpropagation through time to overcome the computational and memory loads that arise when having to learn from long sequences.,"ARTBP is experimentally compared to truncated BPTT. This article introduces a method for recurrent learning, which can be used on short-term optimization, but must reflect on long-term effects once in a while. The authors present a technique for evaluating the theoretical properties of RTBP and demonstrate the soundness of the RTPB on real-world data using a stochastic gradient estimate."
"Exploration using Distributional RL and truncagted variance. Presents an RL method to manage exploration-explotation trade-offs via UCB techniques. A method to use the distribution learned by Quantile Regression DQN for exploration, in place of the usual epsilon-greedy strategy. Proposes new algorithsms (QUCB and QUCB+) to handle the exploration tradeoff in Multi-Armed Bendits and more generally in Reinforcement Learning","This article introduces a new algorithm for estimation of the arm's UCB using Hoeffdings Inequality 1 which is entirely based on counting the number of times the arm was pulled. The authors present a method to estimate the confidence bound for asymmetric distributions in the setting of multi-armed bandits and RL, showing that symmetry is in fact rare in Distributional RL."
"Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more. Describes the conditioned GAN model to generate speaker conditioned Mel spectra by augmenting the z-space corresponding to the identification This study proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitating fine-grained control over various attributes This work proposes a model that can control non-annotated attributes such as speaking style, accent, background noise, etc.",A new model for synthesis of clean speech using conditional auto-encoding. This work introduces a new latent-attribute model that incorporates the decoder inputs to include a vector inferred from the target speech and produces a conditional distribution with higher variance.
"In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model as a surrogate to the abnormal distribution. Describes a novel approach to optimising the choice of kernel towards increased testing power and shown to offer improvements over alternatives.","A deep kernel parametrization for two-sample test using auxiliary generative models, which endows a data-driven kernel via optimizing test power in Eq. This article presents a deep neural learning framework that optimizes kernels with very limited samples from the abnormal distribution Q and shows how it improves performance in time series CPD. The authors introduce a method to optimize kernels by optimizing their surrogate distributions"
"A bottom-up algorithm that expands CNNs starting with one feature per layer to architectures with sufficient representational capacity. Proposes to dynamically adjust the feature map depth of a fully convolutional neural network, formulating a measure of self-resemblance and boosting performance. Introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. Aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network.","Using a normalized pruning method to increase representational complexity by adding features to architectures that started with just one feature per layer, but large speed-ups can be introduced by adding stacks of features. This work presents a method for reducing the time evolution of the normalized cross-correlation for all weights with respect to their state at initialization and shows that this is a major detriment to the traditional DNN optimization method."
"A noval GAN framework that utilizes transformation-invariant features to learn rich representations and strong generators. Proposes a modified GAN objective consisting of a classic GAN term and an invariant encoding term. This paper presents the IVE-GAN, a model that introduces en encoder to the Generative Adversarial Network framework.","A GAN framework that tries to learn a rich representation of the covered modes of the data in their latent space. This work introduces a new GAN architecture, which extends the classical GAN by an additional encoding unit E to map samples from the true data distribution."
"Multi-view learning improves unsupervised sentence representation learning Approach uses different, complementary encoders of the input sentence and consensus maximization. The paper presents a multi-view framework for improving sentence representation in NLP tasks using generative and discriminative objective architectures. This paper shows that multi-view frameworks are more effective than using individual encoders for learning sentence representations.","A multi-view framework for learning sentence representations that leverages the functionality of both RNN and linear models. This work introduces a new framework for unsupervised learning, which uses an RNN to learn sentences from two views, and shows results on unsupervised tasks. The study presents a framework for one-view learning with a differentiable similarity function and a linear decoder in order to achieve better performance on all evaluation tasks."
"Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This work proposes finding ""meaningful"" neurons in Neural Machine Translation models by ranking based on correlation between pairs of models, different epochs, or different datasets, and proposes a controlling mechanism for the models.","A method for ranking neurons with linguistically similar properties. This study presents a novel algorithm for ranking neural networks in the context of language pairs by using a SVCCA dataset. The authors present a method to rank neural networks on a space 120 spanned by non-erased directions, and show that many of them capture common linguistic phenomena. This article introduces a new algorithm for identifying neural networks that can be used to identify specific words."
"We use graph co-attention in a paired graph training system for graph classification and regression. This work injects a multi-head co-attention mechanism in GCN that allows one drug to attend to another drug during drug side effect prediction. A method to extend graph-based learning with a co-attentional layer, which outperforms other previous ones on a pairwise graph classification task.","A graph neural network that receives pairs of graphs at once, and extends it with a co-attentional layer that allows node representations to easily exchange structural information across them. This work introduces a graph-level model for graph-structured representation learning which can be used to learn substructures from other inputs."
Devising unsupervised defense mechanisms against adversarial attacks is crucial to ensure the generalizability of the defense. This paper presents a method for detecting adversarial examples in a deep learning classification setting This study presents an unsupervised method for detecting adversarial examples of neural networks.,"This article introduces a new framework for unsupervised model assurance as well as defending against the adversaries. The work presents a novel approach to attack on defender modules by incorporating a loss function with a cross entropy loss, which is used to improve the security of data abstractions in an end-to-end way."
"We show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the DSP literature. The authors propose incorporating Residual, Highway and Masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task The authors interpret highway, residual and masking connections. The authors generate their own noisy speech by artificially adding noise from a well established noise data-set to a less know clean speech data-set.","This work presents a study of the role of residual and highway connections in deep neural networks for speech enhancement, and shows that skip connections do not necessarily improve performance with regards to the number of parameters, but they make speech enhancement models more interpretable. Presents a novel method for visualizing time-frequency cells by using a skip connection instead of subtraction, which uses a multiplicative mask M to predict the magnitude spectrum of the distortion and filter it out."
We propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. Discusses a core failing and need for I2I translation models. The paper explores the idea that an image has two components and applies an attention model where the feature masks that steer the translation process do not require semantic labels,"An adaptive instance normalization approach for multimodal image translations. This work introduces a new latent space constraint which assumes that a pair of corresponding images (x A, x B) from domains A and B can be mapped to the same representation z in a shared-latent space. This paper presents a method for translating images into images of different styles within the target domain."
A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models. A fast high performance paraphrasing based data augmentation method and a non-recurrent reading comprehension model using only convolutions and attention. This study proposes applying CNNs+self-attention modules instead of LSTMs and enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model in order to improve RC performance. This study presents a reading comprehension model using convolutions and attention and propose to augment additional training data by paraphrasing based on off-the-shelf neural machine translation,"A new feedforward model for reading comprehension that consists of only convolutional and self-attention, discarding RNNs. This article introduces a new approach to the problem of reading comprehension by using a data augmentation technique to improve the training performance of SQUAD datasets. The authors present a method for learning a novel reading comprehension model with a low level structure and a combination of self attention and context-query attention."
"We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition. This work discusses zero shot generalization into new environments, and proposes an approach with results on Grid-World, Super Mario Bros, and 3D Robotics. A method aiming to learn task-agnostic priors for zero-shot generalization, with the idea to employ a modeling approach on top of the model-based RL framework.","A method that outperforms baseline methods on gridworld, robotics tasks and video games. The study presents a method for learning dynamic priors in a grid-world environment using the model's scoring function and the dynamics model to find the best trajectory for a task T."
An autoregressive deep learning model for generating diverse point clouds. An approach for generating 3D shapes as point clouds which considers the lexicographic ordering of points according to coordinates and trains a model to predict points in order. The article introduces a generative model for point clouds using a pixel RNN-like auto-regressive model and an attention model to handle longer-range interactions.,"A self-attention module that captures the long-range dependencies between points, helping to generate plausible part configurations within 3D objects. This article introduces a self attention model for point clouds that can be used in conjunction with a non-context model. The authors present a Self-attention model for image generation using conditional PointGrow, which provides an auto-attention mechanism for generating representations of distances and convolutional coordinates."
"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies. The authors show that elimination singularities and overlap singularities impede learning in deep neural networks, and demonstrate that skip connections can reduce the prevalence of these singularities, speeding up learning. Paper examines the use of skip connections in deep networks as a way of alleviating singularities in the Hessian matrix during training.","This article introduces a novel residual architecture that introduces identity skip connections between adjacent layers and all layers above it. This study investigates the elimination, overlap and linear dependence singularities of a neural network by applying a malicious initialization scheme for the residual architecture to improve performance. The authors present a framework for avoiding overlap singularities in neural networks by adding the identity matrix from the initial weight matrices of the hidden units in the same layer."
"We formulated SGD as a Bayesian filtering problem, and show that this gives rise to RMSprop, Adam, AdamW, NAG and other features of state-of-the-art adaptive methods The work analyzes stochastic gradient descent through Bayesian filtering as a framework for analyzing adaptive methods. The authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior","An adaptive algorithm for neural network optimization that combines natural gradient variational inference and Bayesian filtering. This work introduces a novel method to connect adaptive SGD algorithms to natural gradient VI updates, which is based on the current value of the parameter, and the root-mean-square normalizer BID14."
"We show that, in continual learning settings, catastrophic forgetting can be avoided by applying off-policy RL to a mixture of new and replay experience, with a behavioral cloning loss. Proposes a particular variant of experience replay with behavior cloning as a method for continual learning.","A continual learning system that learns from a variety of different learning networks. This article introduces a novel method for reducing catastrophic forgetting in sequential training, where performance on a task decays immediately when training switches to another task. The authors consider the problem of continuous learning by combining 50-50 new-replay and combining significant reductions in catastrophic forgetting for all tasks."
"Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates. A missing data imputation network to incorporate correlation, temporal relationships, and data uncertainty for the problem of data sparsity in EHRs, which yields higher AUC on mortality rate classification tasks. The paper presented a method that combines VAE and uncertainty aware GRU for sequential missing data imputation and outcome prediction.",A deep generative model to estimate the missing values using a recurrent imputation network to exploit the temporal dynamics in conjunction with the utilization of the uncertainty. This work introduces a new method for estimating the latent representation by using VAEs and generating the uncertainty decay factor in the Eq. The authors present a method for improving the prediction performance of the data by leveraging the reconstructed data as the fidelity score.
"In a deep convolutional neural network trained with sufficient level of data augmentation, optimized by SGD, explicit regularizers (weight decay and dropout) might not provide any additional generalization improvement. This work proposes data augmentation as an alternative to commonly used regularisation techniques, and shows that for a few reference models/tasks that the same generalization performance can be achived using only data augmentation. This study presents a systematic study of data augmentation in image classification with deep neural networks, suggesting that data augmentation can replicit some common regularizers like weight decay and dropout.",A systematic analysis of the impact of data augmentation on deep neural networks with and without explicit regularization. The authors present a systematic analysis to analyze how data hausse adapts to different architectures of deep neural network architectures. This work presents a method for analyzing the role of information augmentation in deep neural models by using two different augmentation schemes as well as comparing them to the most popular regularization techniques.
"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. Introduces a method for creating mini batches for a student network by using a second learned representation space to dynamically select examples by their 'easiness and true diverseness'. Experiments the classification accuracy on MNIST, FashionMNIST, and CIFAR-10 datasets to learn a representation with curriculum learning style minibatch selection in an end-to-end framework.","A framework for self-paced learning with Adaptive Pace that learns when to introduce certain samples to a DNN during training. This work presents a framework for learning a representation space for the student CNN, which can be used to train a classifier and a classification task. The authors present a model for learning examples in a mini-batch setting using easiness and true diverseness as sample importance priors."
"We introduce an embedding space approach to constrain neural network output probability distribution. This work introduces a method to perform semi-supervised learning with deep neural networks, and the model achieves relatively high accuracy, given a small training size. This work incorporates label distribution into model learning when a limited number of training instances is available, and proposes two techniques for handling the problem of output label distribution being wrongly biased.","A novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries. This work presents a novel algorithm for learning decision boundaries on multilayer perceptron (MLP) with limited labelled examples, which can be used to train neural networks to learn the decision boundary correctly from a limited number of examples."
A comparison of five deep neural network architectures for detection of malicious domain names shows surprisingly little difference. Authors propose using five deep architectures for the cybersecurity task of domain generation algorithm detection. Applies several NN architectures to classify url's between begign and malware related URLs. This article proposes to automatically recognize domain names as malicious or benign by deep networks trained to directly classify the character sequence as such.,"A deep recurrent neural network that can detect DGAs without the need to reverse engineer malware families. This article introduces a long-term model for DGA detection, which uses an endgame embedding layer of convolutional neural networks (GRUs) to improve model performance and overcome overfitting by randomly excluding nodes during training."
A novel approach to maintain orthogonal recurrent weight matrices in a RNN. Introduces a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. Novel parametrization of RNNs allows representing orthogonal weight matrices relatively easily.,"A new activation function for orthogonal or unitary recurrents, and the full-capacity uRNN from Section 2.2. This article introduces a new activations function for scoRNN that is more efficient than other activation functions. The authors study the problem of scaling a skew-symmetric matrix with gradient descent, and show that the modReLU has improved the performance of the model."
"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization. The work discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates The authors analyze training of residual networks using large cyclic learning rates, and demonstrate fast convergence with cyclic learning rates and evidence of large learning rates acting as regularization.","A super-convergent approach for training with very large learning rates, which adds noise in the middle part of training. This work presents a method to improve SGD performance when the amount of labeled training data is limited and shows that using CLR with very small learning rates can speed up training by an order of magnitude. The authors present a new algorithm for learning rate optimization that uses a Hessian-free optimization method to estimate optimal learning rates."
"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks. This work constructs an infinite Topic Model with Variational Auto-Encoders by combining Nalisnick & Smith's stick-breaking variational auto-encoder with latent Dirichlet allocation and several inference techniques used in Miao.","A Bayesian nonparametric topic model equipped with a Variational Auto-Encoders (ITM-VAE), which uses a stick-breaking process BID35 to generate the mixture weights for a countably infinite set of topics. This work introduces an infinite Topic Model that can be approximated using either MCMC sampling or variational inference."
"Finetuning after quantization matches or exceeds full-precision state-of-the-art networks at both 8- and 4-bit quantization. This work proposes to improve the performance of low-precision models by doing quantization on pre-trained models, using large batches size, and using proper learning rate annealing with longer training time. A method for low bit quantization to enable inference on efficient hardware that achieves full accuracy on ResNet50 with 4-bit weights and activations, based on observations that fine-tuning at low precision introduces noise in the gradient.",A new state-of-the-art quantization method for fine-tuning low-precision networks with 8-bit precision exceeds the accuracy of fp32 baseline networks after one epoch of fine tuning. This work presents a method to improve the accuracy and efficiency of high-resolution deep network architectures by leveraging the availability of pretrained models.
"Translating portions of the input during training can improve cross-lingual performance. The paper proposes a cross-lingual data augmentation method to improve the language inference and question answering tasks. This article proposes to augment crosslingual data with heuristic swaps using aligned translations, like bilingual humans do in code-switching.","A cross-lingual data augmentation method that replaces a segment of the input text with its translation in another language. This work presents a method for generating word embeddings in multiple languages by using a combination of unsupervised and random initialized models. The authors present a new algorithm for combining examples in a single sentence pair and a multi-lingual dataset, which can be used to improve translation performance over a prerained model."
new GNN formalism + extensive experiments; showing differences between GGNN/GCN/GAT are smaller than thought The article proposes a new Graph Neural Network architecture that uses Feature-wise Linear Modulation to condition the source-to-target node message-passing based on the target node representation.,Graph Dynamic Convolutional Networks (RGDCN) is a GNN-FiLM model that learns to ignore graph edges based on the representation of target and source nodes. This work presents a new framework for learning to ignore message transformations in a graph domain by using a learnable function f that operates on the edge-type-dependent weights of each edge.
"We propose Hierarchical Complement Objective Training, a novel training paradigm to effectively leverage category hierarchy in the labeling space on both image classification and semantic segmentation. A method that regularizes the entropy of the posterior distribution over classes which can be useful for image classsification and segmentation tasks","An explicit hierarchical model that is trained with an objective to leverage information from a label hierarchy. This article presents a method for training neural models with the goal of penalizing the ""obviously wrong"" classes, and introduces a new hierarchic model architecture that can be used to learn hierarchically. The paper addresses the problem of hierarching in the label space by proposing a Hierarchical Approach to this problem."
We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. The paper proposes a new model to use deep models for detecting logical entailment as a product of continuous functions over possible worlds. Proposes a new model designed for machine learning with predicting logical entailment.,"This article introduces a new framework for generative models using propositional logic, which is decidable but requires a worst case of O(2 n) operations to verify entailment. The authors present a model architecture that can capture invariance by using LSTM symbols instead of symbols, and use it as a benchmark to improve the performance of syntactic models on this dataset."
"We show that training feedforward relu networks with a weak regularizer results in a maximum margin and analyze the implications of this result. Studies margin theory for neural sets and shows that max margin is monotonically increasing in size of the network This paper studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations, obtaining a generalization upper bound which does not increase with the network size.","A new method for understanding generalization on two-layer relu networks. This paper introduces a new way to understand the generalization of two layers of neural networks by making the regularization explicit. The authors present a novel method for interpreting the generalized cross-entropy loss in two layers using a linear predictor, and show that it is possible to use a single layer of neural network as a result of a multilayer neural network."
"From an incomplete RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and infer their complete object geometry. Proposes an end-to-end 3D CNN structure which combines color features and 3D features to predict the missing 3D structure of a scene from RGB-D scans. The authors propose a novel end-to-end 3D convolutional network which predicts 3D semantic instance completion as object bounding boxes, class labels and complete object geometry.","A new approach for predicting object detection and instance-level completion for an input partial 3D scan of a scene. This work introduces a new method to predict image segmentation and instance segmentation in the context of semantic scenes. The authors address the problem of query segmentation on real-world scenes, and show that it is more efficient than state-of-the-art approaches."
"We propose Convolutional CRFs a fast, powerful and trainable alternative to Fully Connected CRFs. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel and show that inference is more efficient and training is easier. Proposes to perform message passing on a truncated Gaussian kernel CRF using a defined kernel and parallelized message passing on GPU.",A new framework for fully-connected CRFs that can be implemented highly efficiently on GPUs. This work introduces a new framework of fully connected ConvCRFs which uses a Gaussian kernel to improve the performance of CNNs. The authors present a novel framework for learning convolutional neural networks by using a truncated message passing operation and implementing a large proportion of the inference step as a convolution.
"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features. This study extends SGNS with an architectural change from a bag-of-words model to a feedforward model, and contributes a new form of regularization by tying a subset of layers between different associated networks. A method to use non-linear combination of context vectors for learning vector representation of words, where the main idea is to replace each word embedding by a neural network.","A method to compute semantic (and even syntactic) similarities between the neural network representations of words. This work introduces a method for computing semantic and even syntagic similarities between word2vec and Benoulli embeddings, which can capture semantic properties of the word, but tends to neglect most of the syntaktic information to improve the quality of word representations."
Comparison of psychophysical and CNN-encoded texture representations in a one-class neural network novelty detection application. This article focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task This work considers detecting anomalies in textures and proposes original loss function. Proposes training two anomaly detectors from three different models to detect perceptual anomalies in visual textures.,A novel objective function to train one-class neural networks for novelty detection in visual surface inspection applications. This work introduces a new texture model based on a pretrained CNN and a method to detect visual anomalies when comparing reference and neural network features. The authors present a novel object-based texture model that learns the feature of a prerained convolutional neural network by learning a hyperplane separating reference data with same image statistics.
Human-like Clustering with CNNs The work validates the idea that deep convolutional neural networks could learn to cluster input data better than other clustering methods by noting their ability to interpret the context of every input point due to a large field of view. This article combines deep learning for feature representation with the task of human-like unsupervised grouping.,"Using hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offers a promising solution for solving clustering tasks. This article introduces a new framework for clustering in an effort to solve clustering problems by focusing on the compositionality of the real world structures and objects."
"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance. This article proposes new layer architectures of neural networks using a low-rank representation of tensors This study incorporates tensor decomposition and tensor regression into CNN by using a new tensor regression layer.","Using the unfolded expression of the regression weights and the core with respect to each factor can be obtained by writing: DISPLAYFORM1 The paper introduces tensor decompositions, which express the output of a neural network as a low-rank multi-linear mapping from a high-order activation ensor to the softmax layer."
"A novel network architecture to perform Deep 3D Zoom or close-ups. A method for creating a ""zoomed image"" for a given input image,and a novel back re-projection reconstruction loss that allows the network to learn underlying 3D structure and maintain a natural appearance. An algorithm for synthesizing 3D-zoom behavior when the camera is moving forward, a network structure incorporating disparity estimation in a GANs framework to synthesize novel views, and a proposed new computer vision task.",An unsupervised framework for 3D-zoom dataset of natural scenes based on the need for special equipment to ensure camera movement is restricted to the Z-axis. This work presents a novel view synthesis problem for multiple input image scenarios using an off-the-shelf structure from motion algorithm to obtain the camera pose and fixed background points of a given video sequence in combination with traditional optimization techniques to directly estimate the warping operation for each input image.
"We introduce the universal deep neural network compression scheme, which is applicable universally for compression of any models and can perform near-optimally regardless of their weight distribution. Introduces a pipeline for network compression that is similar to deep compression and uses randomized lattice quantization instead of the classical vector quantization, and uses universal source coding (bzip2) instead of Huffman coding.",A universal entropy coded vector quantization framework consisting of universal quantization and universal lossless source coding such as LempelZiv-Welch BID13 BID14 BID15 and Burrows-Wheeler transform BID17 This work introduces a universal lattice quantization method for vector quantizing in order to achieve universal performance regardless of source statistics at any rates.
"A quantitative refinement of the universal approximation theorem via an algebraic approach. The authors derive the universal approximation property proofs algebraically and assert that the results are general to other kinds of neural networks and similar learners. A new proof of Leshno's version of the universal approximation property for neural networks, and new insights into the universal approximation property.","A neural network with one hidden layer can compute a certain class of functions. This article introduces a new neural network to compute non-bias weights, and shows that the first layer has a hidden layer, where the last layer is fixed and randomly chosen from a suitable range. The paper presents a novel neural network for learning non-bipolar functions, which uses a differentiable function called bias weights (resp)."
"We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling Introducting an importance sampling distribution and using samples from distribution to compute importance-weighted estimate of the gradient This work proposes to use important sampling to optimize VAE with discrete latent variables.","The VAE is an easy latent variable model, where the observations x  p(x|z) are dependent on latent variables. This work introduces a method for training VAEs with stochastic gradient descent by using SGVB. The authors present a new VAE estimator that can be used to train them with gbvs, and show that it is useful for learning latent representations (e.g."
"Linking Wasserstein-trust region entropic policy gradients, and the heat equation. The work explores the connections between reinforcement learning and the theory of quadratic optimal transport The authors studied policy gradient with change of policies limited by a trust region of Wasserstein distance in the multi-armed bandit setting, showing that in the small steps limit, the policy dynamics are governed by the heat equation (Fokker-Planck equation).","A regularization term for entropy-regularized policy, which is a free energy functional, named by analogy with a similar quantity in statistical mechanics 1 This article considers the process of policy iteration, that is, finding a sequence of policies ( n ) converging towards the optimal policy."
We introduce a scale-invariant neural network architecture for changepoint detection in multivariate time series. The article leverages the concept of wavelet transform within a deep architecture to solve change point detection. This paper proposes a pyramid based neural net and applies it to 1D signals with underlying processes occurring at different time scales where the task is change point detection,"A deep neural network architecture that can efficiently identify abrupt and gradual changes at multiple scales. This work introduces a new model for changepoint detection, which can encode short-term and long-term temporal patterns and detect from abrupt to extremely gradual changepoints. The authors present a deep neural networks architecture that is more efficient than baselines and can be used for learning labels for different changes in real-world datasets."
"A novel graph signal processing framework for quantifying the effects of experimental perturbations in single cell biomedical data. This study introduces several methods to process experimental results on biological cells and proposes a MELD algorithm mapping hard group assignments to soft assignments, allowing relevant groups of cells to be clustered.","A novel clustering algorithm for identifying cell states that are most or least affected by an experimental perturbation. This work introduces a new clustering method for learning the Enhanced Experimental Signal (EES), which quantifies how prototypical each cell is of each experimental condition. The authors present a clustering approach to identify cells that are not affected by a given perturbation, and proposed a two-sample experiment using a graph of cellular states across experimental"
"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning. Provided a convergence analysis of Sign SGD algorithm for non-covex cases The work explores an algorithm that uses the sign of the gradients instead of actual gradients for training deep models","Using stochastic gradient evaluations to improve the performance of signSGD. This work presents a method for optimisation that deals with the one norm of the gradient vector, and shows that it can take exponential time to escape saddle points if it gets too close to them BID5. The authors present a technique for assessing the behaviour of signsSGD around saddle points by using a learning rate and a mini-batch schedule."
"Existing pruning methods fail when applied to GANs tackling complex tasks, so we present a simple and robust method to prune generators that works well for a wide variety of networks and tasks. The authors propose a modification to the classic distillation method for the task of compressing a network to address the failure of previous solutions when applied to generative adversarial networks.","A self-supervised GAN compression method for generative learning that can be used to train a discriminator to make the generator behave more like the original generator suffers from this issue. This work introduces a new discriminator, which is already well trained on the target data set, and a selectively binarize of both networks during distillation of a classification network."
We propose to solve a problem of simultaneous classification and novelty detection within the GAN framework. Proposes a GAN to unify classification and novelty detection. The study presents a method for novelty detection based on a multi-class GAN which is trained to output images generated from a mixture of the nominal and novel distributions. The study proposes a GAN for novelty detection using a mixture generator with feature matching loss,"A GAN-based semi-supervised generator capable of generating samples from a mixture of nominal and novel data distributions This study presents a method for multi-class discriminator training with a generator that generates samples from the mixed distributions of real data. The paper studies the problem of non-supervising Generative Networks (GANs) by minimizing a Feature Matching loss, showing that only a bad generator is able to improve"
"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task. This study proposes a new method for classifying nodes of a graph, which can be used in semi-supervised scenarios and on a completely new graph. The paper introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks. Provides a fair and almost comprehensive discussion of the state of art approaches to learning vector representations for the nodes of a graph.","A new attention mechanism for inductive learning that is directly applicable to graphs that are completely unseen during training. This article introduces a new attention-memory method for learning on graphs with unsupervised access to the global structure of each node by using a shared mechanism. The authors present a novel attention-message method for induction learning, which uses a single-layer feedforward neural network and a nonlinearity function."
A formal method's approach to skill composition in reinforcement learning tasks The study combines RL and constraints expressed by logical formulas by setting up an automation from scTLTL formulas. Proposes a method that helps to construct policy from learned subtasks on the topic of combining RL tasks with linear temporal logic formulas.,An approximately optimal composite policy can result from taking the average of the Q-functions of existing policies and finding a maximum of the reward of individual tasks. This article presents a method to learn a multi-task learning policy that achieves the AN D task composition' by using energy-based model DISPLAYFORM1 where E(s) is an energy function that can be represented by a function approximator.
"We created a new dataset for data interpretation over plots and also propose a baseline for the same. The authors propose a pipeline to solve the DIP problem involving learning from datasets containing triplets of the form {plot, question, answer} Proposes an algorithm that can interpret data shown in scientific plots.","A modular framework for plot question answering. This paper presents a novel test set which contains plots based on data extracted from Open Government Data as opposed to World Bank Data. The authors present a multi-staged dataset that contains triplets of the form plot, question, answer and show that plots are realistic with different scales including floating point numbers and floating point integers in a fixed range."
Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue This article proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations with sizable gains over the baselines. Proposes combining external pretrained word embeddings and pretrained word embeddings on training data by keeping them as two views. Proposes method to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to conversational datasets and applies new variants of LSTM-based model to the task of response-selection in dialogue modeling.,A new ESIM model that can be used to improve the performance of dialogue corpus and doban conversation corpus. This work introduces a novel method for learning word embedding vectors on a general text corpus by using pre-training them with semantic knowledge from ConceptNet BID25 and merging them into a common representation BID24.
Acquire states from high frequency region for search-control in Dyna. The authors propose to do sampling in the high-frequency domain to increase the sample efficiency This study proposes a new way to select states from which do do transitions in dyna algorithm.,"A simple strategy to locally measure the frequency of a function's frequency by gradient norm, and provide a theoretical justification for this approach. This work introduces a dynamic algorithm to localize the frequency in a signal's domain and provides a method to quantify the difficulty of estimation. The authors present a model-free architecture for RL that can be used to estimate the frequency on a point in the state space using a gradient norm."
"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines. Proposes a method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent Proposal for an online distillation method called co-distillation, applied at scale, where two different models are trained to match predictions of the other model in addition to minimizing its own loss. Online distillation technique is introduced to accelerate traditional algorithms for large-scaled distributed neural network training","Using ensemble distillation to reduce prediction churn is a good way of reducing the reproducibility benefits of codistillation. This work presents a novel method for training neural networks that can be used to improve the performance of recurrent models, and shows it does not lose the reproducibilities of ensembles of the same model. The authors present a method for learning weights that are only available on a small subset of the training data."
"Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark. A sequential latent variable model for knowledge selection in dialogue generation that extends the posterior attention model to the latent knowledge selection problem and achieves higher performances than previous state-of-the-art models. A novel architecture for selecting knowledge-grounded multi-turn dialogue that yields state of the art on relevant benchmarks datasets, and scores higher in human evaluations.","An open-domain model for multi-turn knowledge-grounded dialogue. This article introduces a latent variable model for knowledge selection, which uses the posterior knowledge distribution as a pseudo-label to improve the accuracy of knowledge selection and subsequently utterance generation. The authors present a new approach to multi-twin knowledge-based dialogue by decomposing it into two sub-problems."
"Neural Network Verification for Temporal Properties and Sequence Generation Models This work extends interval bound propagation to recurrent computation and auto-regressive models, introduces and extends Signal Temporal Logic for specifying temporal contraints, and provides proof that STL with bound propagation can ensure neural models conform to temporal specification. A way to train time-series regressors verifiably with respect to a set of rules defined by signal temporal logic, and work in deriving bound propagation rules for the STL language.",A simplified verification method for verifiably robust models. This article introduces a novel verification procedure for deep neural networks that can be trained without formal guarantees of their correctness and functionality. The paper considers the problem of learning a trace-valued function to conform to a specification of the form x  S. The study presents a method for verification of neural networks using over-approximations and a new model verification technique.
We propose to use explicit vector algebraic formulae projection as an alternative way to visualize embedding spaces specifically tailored for goal-oriented analysis tasks and it outperforms t-SNE in our user study. Analysis of embedding psaces in a non-parametric (example-based_ way,A better understanding of the embedded space may lead to critical insights in improving such models. This article introduces a new way of visualizing the original embedding space and topical clusters that are close in the original high-dimensional space to be close within the lower dimensional projection space.
"a joint model and gradient sparsification method for federated learning Applies variational dropout to reduce the communication cost of distributed training of neural networks, and does experiments on mnist, cifar10 and svhn datasets. The authors propose an algorithm that reduces communication costs in federated learning by sending sparse gradients from device to server and back. Combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.","A method to jointly learn a sparse model while reducing the amount of gradients exchanged during the iterative training process. The authors proposed an efficient federated learning framework that meets both model and communication constraints. This article introduces a method for sparsifying deep neural networks using variational dropout, which can be used to sparsify Bayesian neural networks by an order of magnitude without losing the prediction accuracy."
"Discrete transformer which uses hard attention to ensure that each step only depends on a fixed context. This work presents modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. This work proposes three Discrete Transformers: a discrete and stochastic Gumbel-softmax based attention module, a two-stream syntactic and semantic transformer, and sparsity regularization.","A method for learning discrete attention by modifying the attention mechanism and objective function to improve model interpretability. This work introduces a novel approach to learning discretized attention by using a sparse attention mechanism, which can be used to improve machine translation performance. The paper presents a new approach for learning soft attention models by incorporating attention as a categorical latent variable and a ""syntactic"" attention mechanism in the input structure."
We propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization. Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'Differentiable Greedy Network' (DGN). Proposes a neural network that aims to select a subset of elements (e.g. selecting k sentences that are mostly related to a claim from a set of retrieved docs),"A deep unfolding technique that combines the two extremes of generative models and deep learning models. This work presents a deep-encoder-based method to derive novel network architectures that are interpretable as inference algorithms by turning iterations of the inference algorithm into layers of a network. The authors present a method for deep learning, combining the advantages of deep learning with deep learning and a deeper feed-forward approach."
"We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs. The article proposes a regularization term for the conditional GAN objective in order to promote diverse multimodal generation and prevent mode collapse. The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction, which can be applied to various conditional synthesis frameworks for various tasks.","A Generative Adversarial Network which learns both conditional generator G and discriminator D by learning a multi-modal mapping from latent code to output by optimizing the adversarial objective of the generator. This article considers the mode-collapse problem in many conditional generative tasks, especially for high-dimensional input and output."
"A method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost A method to train a network with large capacity, only parts of which are used at inference time dependent on input, using fine-grained conditional selection and a new method of regularization, ""batch shaping.""","A new batch-shaping loss for deep neural networks that can be trained with very large capacity. This work introduces a batch Shaping and a method for training deep neural network architectures by conditionally turning parts of the architecture on or off. This article presents a novel batch-Shaping solution to the task loss problem, which is based on two fully connected layers, with only 16 neurons in the hidden layer."
"We diagnose deep neural networks for 3D point cloud processing to explore the utility of different network architectures. The article investigates different neural network architectures for 3D point cloud processing and proposes metrics for adversarial robustness, rotational robustness, and neighborhood consistency.","A method to quantify the utility of different network architectures. This article presents a method for evaluating the usefulness of different networks in the context of 3D point clouds, and shows that DNNs can learn an orientation-aware feature for each point cloud. The authors address the problem of determining how much information is discarded during the computation of an intermediate layer feature by using a distance encoding unit (i.e., Architecture 4)."
"Optimized gated deep learning architectures for sensor fusion is proposed. The authors improve upon several limitations of the baseline negated architecture by proposing a coarser-grained gated fusion architecture and a two-stage gated fusion architecture Proposes two gated deep learning architectures for sensor fusion and by having the grouped features, demonstrates improved performance, especially in the presence of random sensor noise and failures.","A new coarser-grained gated architecture which learns robustly a set of fusion weights at the group level, leading to further performance improvements. This work introduces a two-stage gating architecture that exploits both the feature-level and group-level fusionweights to address the limitations of the negated architecture in terms of inconsistency and lack of diverse fusion mechanisms."
We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization The study presents a combination of evolutionary computation and variational EM for models with binary latent variables represented via a particle-based approximation The paper makes an attempt to tightly integrate expectation-maximization training algorithms with evolutionary algorithms.,"A novel variational optimization algorithm for probabilistic generative models with binary hidden variables This article introduces a method for learning probabilistic latent states of individuals using variational expectation maximization (E-step) and an approximate maximum likelihood optimization algorithm. The authors present a novel variant of neural networks that can be used to learn the latent state of individual genomes, and show that evolutional algorithms can be applied to the variational E-step."
"We closely analyze the VAE objective function and draw novel conclusions that lead to simple enhancements. Proposes a two-stage VAE method to generate high-quality samples and avoid blurriness. This paper analyzes the Gaussian VAEs. The article provides a number of theoretical results on ""vanilla"" Gaussian Variational Auto-Encoders, which are then used to build a new algorithm called ""2 stage VAEs"".","A VAE pipeline that can produce stable FID scores, an influential recent metric for evaluating generated sample quality BID16. This work introduces a new VAE parameterization framework that can be used to evaluate the latent dimension of a high-dimensional manifold. The authors present a VAE model with low-dimensional representations that is capable of reconstructing all x   using any z drawn from q|x."
"We describe an end-to-end differentiable model for QA that learns to represent spans of text in the question as denotations in knowledge graph, by learning both neural modules for composition and the syntactic structure of the sentence. This paper presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser. Proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words","A neural network approach for question answering with a parse and a latent tree that provides the answer. This work introduces a method for query answering in which phrases are grounded in the KG, and uses a recurrent neural network to combine phrases into a span representing functions that cannot yet be grounded in KG."
"Couple the GAN based image restoration framework with another task-specific network to generate realistic image while preserving task-specific features. A novel method of Task-GAN of image coupling that couples GAN and a task-specific network, which alleviates to avoid hallucination or mode collapse. The authors propose to augment GAN-based image restoration with another task-specific branch, such as classification tasks, for further improvement.","A GAN-based image restoration framework that combines task-driven loss and discriminator network to ensure both visually plausible and more accurate image restoration. This work introduces a task-oriented loss-based approach for super-human level automatic classification/diagnosis. The article presents a new method for image restoration, which can be used for image reconstruction by using a GAN as a result of a problem-based loss function."
"Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies. A method to coordinate agent behaviour by using policies that have shared latent structure, a variational policy optimization method to optimize the coordinated policies, and a derivation of the authors' variational, hierarchical update. This study suggests an algorithmic innovation consisting of hierarchical latent variables for coordinated exploration in multi-agent settings","This paper introduces a structured probabilistic policy class that uses a hierarchy of stochastic latent variables to train the policy end-to-end. Introduces an efficient and principled algorithm using variational methods to train multi-agent policies with a large number of agents. This article presents a method for learning multiple actions from a wide range of agents in a variety of different environments, and shows how they can learn more efficiently."
"Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness This study presents an adaptation of the algorithmic robustness of Xu&Mannor'12 and presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error. Proposes a study of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness and gives bounds on generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice. The study studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context","A robustness algorithm for deep learning that is correlated with its generalization performance. This work introduces a robustness approach to the generalization of deep neural networks, which uses backpropagation to learn a probability distribution on the weights of a neural network by minimizing the expected lower bound on the marginal likelihood (or the variational free energy)."
"An architecture for tabular data, which emulates branches of decision trees and uses dense residual connectivity This paper proposes deep neural forest, an algorithm which targets tabular data and integrates strong points of gradient boosting of decision trees. A novel neural network architecture mimicking how decision forests work to tackle the general problem of training deep models for tabular data and showcasing effectiveness on par with GBDT.","A novel neural network architecture that combines decision trees and dense residual connections. This work presents a novel architecture for neural models that achieves performance on the level of GBDTs. The authors present a deep neural forests architecture that is intrinsically interpretable, as if it were a conventional decision tree induction algorithm. This article introduces a new neural model architecture that integrates decision trees into a multi-modal dataset with a single decision tree."
"An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability The authors formulate training NNs as finding an optimal controller for a discrete dynamical system, allowing them to use method of successive approximations to train a NN in a way to be more robust to adversarial attacks. This study uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness.","A dynamical neural network that is robust to all adversarial perturbations. This article introduces a dynamical system view on the adversariality of neural networks, and proposed a new approach to combat adversarially robustness in deep residual networks. The paper presents a method for assessing the robustness of a neural network by adding constraint to the optimization process, which can be used to train neural networks in a wide range of ways."
"Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice. Presents a distributed implementation of signSGD with majority vote as aggregation.","This paper introduces a new mini-batch convergence behaviour for SIGNSGD, which can be applied to large neural networks. The study presents a method for reducing the communication cost of distributed SGD by 25%, resulting in a reduction in generalisation and a higher convergence rate. The authors present a model that is more efficient than adversarial gradients, and show that the majority vote should come out correct on average."
"Few-shot learning PixelCNN The study proposes on using density estimation when the availability of training data is low by using a meta-learning model. This paper considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning The paper focuses on few shot learning with autoregressive density estimation and improves PixelCNN with neural attention and meta learning techniques.","This work presents a new study of few-shot autoregressive models and their connection to meta-learning. Presents an algorithm for neural density estimation in the context of discriminative models, which can be used to train neural networks from scratch or fine-tuning from scratch. The authors present a method for learning only few shots from scratch and show that attention can improve performance in language tasks."
"we proposed a new self-driving model which is composed of perception module for see and think and driving module for behave to acquire better generalization and accident explanation ability. Presents a multitask learning architecture for depth and segmentation map estimation and the driving prediction using a perception module and a driving decision module. A method for a modified end-to-end architecture that has better generalization and explanation ability, is more robust to a different testing setting, and has decoder output that can help with debugging the model. The authors present a multi-task convolutional neural network for end-to-end driving and provide evaluations with the CARLA open source simulator showing better generalization performance in new driving conditions than baselines","This study presents a new driving direction selection method for learning the core of intergration in unsupervised learning. The paper introduces a model that can be used to train a driver's direction instead of memorizing the solution of the problem. This article proposed a novel way to learn a driving direction selector using saliency-map based visualization methods, and shows that the perception module is able to understand the input structure of the model."
We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating. The authors introduce a gradient-based approach to minimize an objective function with an L0 sparse penalty to help learn sparse neural networks,"A method to smooth the expected L 0 normalized objective with continuous distributions in a way that can maintain the exact zeros in the parameters while still allowing for gradient based optimization. This study presents a method for optimizing parametric models by using a model compression and sparsification technique, where the probability of a gate being different from zero is equal to zero."
We develop meta-learning methods for adversarially robust few-shot learning. This study presents a method that enhances the robustness of few-shot learning by introducing adversarial query data attack in the inner-task fine-tuning phase of a meta-learning algorithm. The authors of this work propose a novel approach for training a robust few-shot model.,"In the few-shot setting, adversarial querying outperforms other robustness techniques by a wide margin in terms of both clean accuracy and adversariality robustness. This article introduces a method to learn from pre-trained feature extractors on large datasets and fine-tuned on new tasks."
This study demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set. This study presents a deep autoencoder model for rating prediction that outperforms other state-of-the-art approahces on the Netflix prize dataset. Proposes to use a deep AE to do rating prediction tasks in recommender systems. The authors present a model for more accurate Netflix recommendations demonstrating that a deep autoencoder can out-perform more complex RNN-based models that have temporal information.,This paper introduces a new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the-art models on time-split Netflix data sets. This work presents a method for training deep autoencoders by using a scaled exponential linear unit (SELUs) to bypass the natural sparseness of updates in collaborative filtering and further improves the model performance.
"We propose that training with growing sets stage-by-stage provides an optimization for neural networks. The authors compare curriculum learning to learning in a random order with stages that add a new sample of examples to the previously, randomly constructed set This article studies the influence of ordering in the Curriculum and Self paced learning, and shows that to some extent the ordering of training instances is not important.",A small training set and adding new samples in both curriculum and anti-curriculum learning improves the learning performance. This work introduces a method to add samples randomly without a meaningful order to improve the performance of Anti-Crumulum learning by adding the samples randomly to the first stage so that the number of new samples to add at each stage is equal. The authors present a technique for improving the performance with a small group of random samples
"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. In the study the authors suggest to use MERA tensorization technique for compressing neural networks. A new parameterization of linear maps for neural network use, using a hierarchical factorization of the linear map that reduces the number of parameters while still allowing for relatively complex interactions to be modelled. Studies compressing feed forward layers using low rank tensor decompositions and explore a tree like decomposition",A new neural network with two penultimate fully connected layers of the model that is replaced with tensor trains. This work presents a novel method to reduce the compression rate of fully connected neural networks by replacing them with one layer of rank-4 tree elements in order to evaluate how detrimental naive compression is to accuracy. The authors use MERA as a replacement for linear layers in a neural network used to classify the CIFAR-10 dataset.
"A new method for gradient-descent inference of permutations, with applications to latent matching inference and supervised learning of permutations with neural networks The article utilizes finite approximation of the Sinkhorn operator to describe how one can construct a neural network for learning from permutation valued training data. The work proposes a new method that approximates the discrete max-weight for learning latent permutations","A method to parameterize the hard choice of a permutation P through a square matrix X. This article introduces a method for solving the problem of entropy-regularization in Sinkhorn networks, which can be used as a solution to the linear assignment problem BID28. This work presents a new method for determining the length of the Permutation GSM network by using the sinkhorn iterations."
Compressing trained DNN models by minimizing their complexity while constraining their loss. This study proposes a method for deep neural network compression under accuracy constraints. This paper presents a loss value constrained k-means encoding method for network compression and develops an iterative algorithm for model optimization.,A method for minimizing the complexity of a trained model by eliminating or merging unnecessary centroids. This article presents a method for solving the constrained optimization problem by using a compression algorithm to solve the problem of optimizing the training loss function. The authors present a novel way of reducing the complexity in a training model and show that it is possible to learn a new loss function by combining the loss function with a single compression algorithm.
"Imitation from pixels, with sparse or no reward, using off-policy RL and a tiny adversarially-learned reward function. The article proposes to use a ""minimal adversary"" in generative adversarial imitation learning under high-dimensional visual spaces. This article aims at solving the problem of estimating sparse rewards in a high-dimensional input setting.","A state-of-the-art off-policy method for control that can take advantage of a replay buffer to store past experiences. This article introduces a new defense against the use of replay buffers in simulated robotic block stacking by using an adversary-based approach, which is able to stack faster than the dense staged reward baseline agent with the same amount of actor processes."
"Improvements to adversarial robustness, as well as provable robustness guarantees, are obtained by augmenting adversarial training with a tractable Lipschitz regularization Explores augmenting the training loss with an additional gradient regularization term to improve robustness of models against adversarial examples Uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.",A method for estimating adversarial robustness in deep neural networks by using a one-step Signed Gradient attack vector or the gradient attack vector as Total Variation regularization This paper presents a method for estimation of adversarially robustness on unsupervised data. The authors present a new method to estimate adversarials' robustness and show that it is not tractable as a Lipschitz penalty during training.
"A new state-of-the-art approach for knowledge graph embedding. Presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base. This work proposes an approach to knowledge graph embedding by modeling relations as rotations in the complex vector space. Proposes a method for graph embedding to be used for link prediction","A new model able to model and infer all the three types of relation patterns, except the symmetry pattern. This article introduces a new model for knowledge graph embeddings that can be used to infer and model all the 3 types of relationships. The authors present a novel method for inferring relations between the two types of information graphs by using a score function called DISPLAYFORM1."
"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks. This work extends Neural Architecture Search to the multi-task learning problem where a task conditioned model search controller is learned to handle multiple tasks simultaneously. In this paper, authors summarize their work on building a framework, called Multitask Neural Model Search controller, for automated neural network construction across multiple tasks simultaneously.","This paper presents Multitask Neural Model Search (MNMS), an automated model construction framework that finds the best performing models in the search space for multiple tasks simultaneously. This work introduces a multi-task neural model search framework that has been pre-trained on previous searches, thereby speeding up the search for new tasks."
Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples. This article proposes adding an additional label for detecting OOD samples and adversarial examples in CNN models. The study proposes an additional class that incorporates natural out-distribution images and interpolated images for adversarial and out-distribution samples in CNNs,"A simple and computationally efficient augmented CNN that can significantly reduce the risk of misclassifying both adversaries and samples from a broad range of over-generalized out-distribution sets. This article introduces a method to learn a ""dustbin"" sub-manifold for the dustbin class, where they are mapped to this set."
"Permutation-invariant loss function for point set prediction. Proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. This paper introduces a novel distance function between point sets, applies two other permutation distances in an end-to-end object detection task, and shows that in two dimensions all local minima of the holographic loss are global minima. Proposes permutation invariant loss functions which depend on the distance of sets.","A permutation-invariant loss function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function and a regularizer for machine learning applications. This work presents a method for training points in the form of an analytic distance function for low-valued points and shows that it can be used to train points in a wide range of ways."
"Genetic algorithms based approach for optimizing deep neural network policies The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together. This article proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks.","A new state-space crossover operator that efficiently combines two parent policies into an offspring or child policy that tries to mimic its best parent in generating similar state visitation distributions. This article introduces a new network architecture for continuous control tasks, which combines both parent and child policies to achieve comparable or higher sample efficiency. The authors present a novel way of learning a policy that maximizes the expected sum of rewards starting from the initial state."
"We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints. Proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. This study introduces an approach to compressing neural networks by looking at the correlation of filter responses in each layer via two strategies. This paper proposes a compression method based on spectral analysis","A structured pruning method for reducing the number of filters to remove in a layer. This article introduces a PFA-En algorithm that uses a spectral energy analysis algorithm to estimate the number and the size of the filters in each layer. The authors present a novel pruning technique for filter selection, which is based on a linear mapping to a lower dimensional space that maximizes the variance of the data in the dataset."
This study studies the discrimination and generalization properties of GANs when the discriminator set is a restricted function class like neural networks. Balances capacities of generator and discriminator classes in GANs by guaranteeing that induced IPMs are metrics and not pseudo metrics This paper provides a mathematical analysis of the role of the size of the adversary/discriminator set in GANs,"A new discriminator set of neural distances that is bounded Lipschitz when X and  are bound to be universally approximators in the Borel probability measures. This study studies the problem of discrimination between neural networks by assuming that the discriminator sets are infinite dimensional and large enough to guarantee that d F (, ) = 0 implies  = )."
"Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. A domain generalization approach to reveal semantic information based on a linear projection scheme from CNN and NGLCM output layers. The article proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks","A new neural network building block that resembles GLCM, which has (sub)gradient everywhere, and thus are tunable through backpropagation. This work introduces a new neural networks building block for the construction of G, where all the parameters are differentiable, and therefore all the operations used in the construction have (sub-gradient)."
"Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior The authors suggest that statistical mechanics ideas will help to understand generalization properties of deep neural networks, and give an approach that provides strong qualitative descriptions of empirical results regarding deep neural networks and learning algorithms. A set of ideas related to theoretical understanding generalization properties of multilayer neural networks, and a qualitative analogy between behaviours in deep learning and results from quantitative statistical physics analysis of single and two-layer neural networks.","NNs are robust to massive amounts of noise in the data and/or can help training, while others argue that they are quite sensitive to even a modest amount of noise. This article addresses the problem of learning DNNs by explaining how convergence to flat minimizers can be used to generalize the learning process. The authors present a method for understanding NN learning using a wide range of parameters, including the specific details of the learning algorithm, and their properties."
"This work introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative evaluation of representation learning. This paper discusses the problem of evaluating and diagnosing the represenatations learnt using a generative model. Authors present a set of criteria to categorize MNISt digists and a set of interesting perturbations to modify MNIST dataset.","A new quantitative framework for assessing representation learning in image classification. This article introduces Morpho-MNIST, a framework that aims to answer the question of what extent has my model learned to represent specific factors of variation in the data? The authors present a method for evaluating representation learning using morphometric analysis and a morpho-mNIST code base to address the problem of understanding latent variables in images."
"We show that modular structured models are the best in terms of systematic generalization and that their end-to-end versions don't generalize as well. This article evaluates systemic generalization between modular neural networks and otherwise generic models via introduction of a new, spatial reasoning dataset A targeted empirical evaluation of generalization in models for visual reasoning, focused on the problem of recognizing (object, relation, object) triples in synthetic scenes featuring letters and numbers.",A new synthetic dataset that can answer questions about all 36  35 possible objects in SQOOP. This study introduces a new dataset to address the problem of instruction-following problems for seq2seq models by adding a modularity and structure to their design to make them structurally resemble the kind of rules they are supposed to learn.
"We propose a self-monitoring agent for the Vision-and-Language Navigation task. A method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module, and performs well on standard benchmarks. This work describes a model for vision-and-language navigation with a panoramic visual attention and an auxillary progress monitoring loss, giving state-of-the-art results.","A new approach for action selection-textual grounding that can be used to estimate the completeness of instruction-following. This work introduces a new objective function for the agent to measure how well it can estimate the progress made towards the goal. The authors present an agent with the ability to identify which direction to go by relying on a grounded instruction, and show that even the attentional mechanism of the baseline does not successfully track this information through time."
"We show that ENAS with ES-optimization in RL is highly scalable, and use it to compactify neural network policies by weight sharing. The authors construct reinforcement learning policies with very few parameters by compressing a feed-forward neural network, forcing it to share weights, and using a reinforcement learning method to learn the mapping of shared weights. This paper combines ideas from ENAS and ES methods for optimisation, and introduces the chromatic network architecture, which partitions weights of the RL network into tied sub-groups.","A highly scalable algorithm that learns effective policies with over 92% reduction of the number of neural networks parameters. This article introduces a method for learning weight-sharing architectures that is more efficient than hardcoded ones. The authors present an approach to learning weights for RL, where weights are chosen randomly instead of being learned, but the topologies of connections are ultimately biased towards RL tasks under consideration."
"Gaggle, an interactive visual analytic system to help users interactively navigate model space for classification and ranking tasks. A new visual analytic system which aims to enable non-expert users to interactively navigate a model space by using a demonstration-based approach. A visual analytics system that helps novice analysts navigate model space in performing classification and ranking tasks.","A new optimal model for a single model to show at each iteration is to simplify the user interface by removing a model comparison and selection step. This study introduces a new model space that can be sampled using a combination of a learning algorithm and associated hyperparameters. The article presents a method for evaluating models for the ranking task, which uses a linear dimension reduction technique to improve the user experience."
"We propose to generate adversarial example based on generative adversarial networks in a semi-whitebox and black-box settings. Describes AdvGAN, a conditional GAN plus adversarial loss, and evaluates AdvGAN on semi-white box and black box setting, reporting state-of-art results. This study proposes a way of generating adversarial examples that fool classification systems and wins MadryLab's mnist challenge.",This article introduces a method to optimize adversarial perturbation for untargeted attacks by optimizing the distance between the prediction and the ground truth of the generated adversarials. The paper presents a new way of achieving adversarially perturbated adversaries by minimizing the distillation objective of the network distillation objectives.
We introduce a novel measure of flatness at local minima of the loss surface of deep neural networks which is invariant with respect to layer-wise reparameterizations and we connect flatness to feature robustness and generalization. The authors propose a notion of feature robustness which is invariant with respect to rescaling the weight and discuss the notion's relationship to generalization. This article defines a notion of feature-robustness and combines it with epsilon representativeness of a function to describe a connection between flatness of minima and generalization in deep neural networks.,A method to measure the robustness of a neural network's flatness. This article introduces a method for assessing the generalization properties of feature robustness in the context of ReLU activations. The paper considers the problem of determining the generalisation properties of the ReLU function by using a data-independent distribution over matrices A and a model that has a non-zero value on the diagonal.
"We introduce DPFRL, a framework for reinforcement learning under partial and complex observations with a fully differentiable discriminative particle filter Introduces ideas for training DLR agents with latent state variables, modeled as a belief distribution, so they can handle partially observed environments. This article introduces a principled method for POMDP RL: Discriminative Particle Filter Reinforcement Learning that allows for reasoning with partial observations over multiple time steps, achieving state-of-the-art on benchmarks.","This article introduces the Discriminative Particle Filter Reinforcement Learning (DPFRL), a method that learns to explicitly track a latent belief while circumventing the difficulty of generative observation modeling. This work introduces a new method for learning observation features that are relevant to decision making from particle beliefs, and shows that DPFRL outperforms GRU in most cases because of its explicit structure for belief tracking."
"We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time. Studies the problem of learning a single convolutional filter using SGD and shows that under certain conditions, SGD learns a single convolutional filter. This study extends the Gaussian distribution assumption to a more general angular smoothness assumption, which covers a wider family of input distributions",A neural network that can be optimized under stochastic gradient descent with random initialization. This work introduces a neural network optimization algorithm to learn the Gaussian distribution by using specialized analytic properties of the filter. The authors present a novel algorithm for optimizing neural networks and show that a smoother input distribution leads to faster convergence.
"We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique. An algorithm called value iteration with negative sampling to address the covariate shift problem in imitation learning.",An algorithm that learns a value function that extrapolates to unseen states and achieves near-optimal performance. This work introduces an approach to attack the optimistic extrapolation problem by learning a values function that is guaranteed to induce policies that stay close to the demonstration states.
"We investigate the merits of employing neural networks in the match prediction problem where one seeks to estimate the likelihood of a group of M items preferred over another, based on partial group comparison data. This study proposes a deep neural network solution to the set ranking problem and designs a architecture for this task inspired by previous manually designed algorithms. This article provides a technique to solve the match prediction problem using a deep learning architecture.","A new model for rank aggregation tasks that can be extended to achieve satisfactory performance on a variety of real-world datasets. This work introduces a new model called RTL, which can be used to predict the likelihood of a group of M items preferred over another. The authors present a novel algorithm for ranking aggregations and show that a single layer neural network can fit some variants of the BTL model and improve prediction accuracy."
"We investigate ReLU networks in the Fourier domain and demonstrate peculiar behaviour. Fourier analysis of ReLU network, finding that they are biased towards learning low frequency This study has theoretical and empirical contributions on topic of Fourier coefficients of neural networks","A method for learning neural networks that is biased towards expressing a subset of low frequency solutions. This article introduces a bias in neural networks, which manifests itself in the process of learning, but also in the parameterization of the model itself. The authors considers the problem of learning neural network models and show that the lower frequencies of trained neural networks are more robust with respect to random parameter perturbations."
"We refine the over-approximation results from incomplete verifiers using MILP solvers to prove more robustness properties than state-of-the-art. Introduces a verifier that obtains improvement on precision of incomplete verifiers and scalability of the complete verifiers using over-parameterization, mixed integer linear programming and linear programming relaxation. A mixed strategy to obtain better precision on robustness verifications of feed-forward neural networks with piecewise linear activation functions, achieving better precision than incomplete verifiers and more scalability than complete verifiers.","A novel heuristic verifier for neural network robustness. This work introduces a method to combine overapproximation with inexact MILP formulations of the network, which are then solved to achieve more precise results for neuron bounds. The authors present a novel verifier that uses a combination of incomplete methods, including LP relaxations, and affine transformations."
Using Wasserstein-GANs to generate realistic neural activity and to detect the most relevant features present in neural population patterns. A method for simulating spike trains from populations of neurons which match empirical data using a semi-convolutional GAN. The work proposes to use GANs for synthesizing realistic neural activity patterns,A method to model neural activity patterns in response to a given set of stimuli using supervised learning techniques. This study introduces a novel way to model the activity patterns generated by GANs by comparing them to those obtained with a maximum entropy model and showing that Spike-GAN is able to effectively mimic their underlying distribution.
This work describe a 3D authoring tool for providing AR in assembly lines of industry 4.0 The article addresses how AR authoring tools support training of assembly line systems and proposes an approach An AR guidance system for industrial assembly lines that allows for on-site authoring of AR content. Presents a system that allows factory workers to be trained more efficiently using augmented reality system.,"A novel authoring tool designed to allow users to create 3D representations of the assembly line, and to place anchors and markers for the AR guidance elements. This article introduces a new authoring method that can be used to generate 3D models of industrial components and workstations. The authors present a novel authorizing tool called WAAT, which allows users to use an operator training system to create a visual representation of the entire assembly line."
Learning Priors for Adversarial Autoencoders Proposes a simple extension of adversarial auto-encoders for conditional image generation. Focuses on adversarial autoencoders and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution,This paper introduces a new method for learning a prior over latent variables to minimize adversarial loss in data space. The article presents a method for defining a latent variable distribution in the deep generative model by learning the decoder to generate images with a distribution that matches the empirical distribution of real images in the training data. The authors introduce a variational learning technique to maximize the mutual information between the variable s and the generated image.
learn hierarchal sub-policies through end-to-end training over a distribution of tasks The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. This work proposes a novel method for inducing temporal hierarchical structure in a specialized multi-task setting.,"A method for sparse-reward learning that is efficient enough to learn in complex physics environments with long time horizons, and robust enough to transfer sub-policies towards otherwise unsolvable tasks. This article introduces a novel metalearning method to learn quickly over a large number of gradient updates in the RL setting-a regime."
"Represent each entity as a probability distribution over contexts embedded in a ground space. Proposes to construct word embeddings from a histogram over context words, instead of as point vectors, which allows for measuring distances between two words in terms of optimal transport between the histograms through a method that augments representation of an entity from standard ""point in a vector space"" to a histogram with bins located at some points in that vector space.","A point-wise embedding method for word entailment, which can be used to measure any kind of distance between words. This article presents a method for learning representations that are able to effectively capture such inherent uncertainty and polysemy. The authors show that distributional estimations capture more of this information compared to points-wise embedded vectors alone, but has largely been ignored in the past."
"precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization Investigates the problem of neural network quantization by employing an end-to-end precision highway to reduce the accumulated quantization error and enable ultra-low precision in deep neural networks. This paper studies methods to improve the performance of quantized neural networks This study proposes to keep a high activation/gradient flow in two kinds of networks structures, ResNet and LSTM.","A network-level approach to quantization, called precision highway, which can be applied to both pre-activation convolutional and recurrent neural networks. This work introduces a novel quantization method for high-precision information flow by proposing a weight-binarized AlexNet that gives the same accuracy as a fully precision one."
"Many graph classification data sets have duplicates, thus raising questions about generalization abilities and fair comparison of the models. The authors discuss isomorphism bias in graph datasets, the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model, theoretically analogous to data leakage effects.","A graph classification problem with isomorphic graphs can be interpreted as a classification problem with weighted loss. This article presents a method for graph classification that uses WL algorithm to generalize to ""harder"" instances than in the original test sets. The authors address the problem of classifying graphs in data sets using a combination of individualization and individualization-refinement paradigms, showing a drop of accuracy by up to 15%."
"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders. Alternating minimization framework for training autoencoder and encoder-decoder networks The authors explore an alternating optimization approach for training Auto Encoders, treating each layer as a generalized linear model, and suggest using the stochastic normalized GD as the minimization algorithm in each phase.","A novel method for training neural networks with sigmoid activation functions. This work introduces a new algorithm to train autoencoders by using alternating minimization and showing how DANTE can be applied to multi-layer neural networks. The paper presents a method for learning deep networks with multiple hidden layers, which uses a generalized gradient descent technique to perform quasi-convex optimization."
"We propose an algorithmic framework to schedule constellations of small spacecraft with 3-DOF re-orientation capabilities, networked with inter-sat links. This paper proposes a communication module to optimize the schedule of communication for the problem of spacecraft constellations, and compares the algorithm in distributed and centralized settings.","A physics-based agile EO scheduler that optimizes the schedule for any satellite in a given constellation to observe a set of ground regions with rapidly changing parameters and observation requirements. This work presents a novel algorithmic framework that combines physical models of orbital mechanics (OM), attitude control systems, and inter-satellite links to improve the revisit/response for the same number of satellites Copyright  2019."
"Image captioning as a conditional GAN training with novel architectures, also study two discrete GAN training methods. An improved GAN model for image captioning that proposes a context-aware LSTM captioner, introduces a stronger co-attentive discriminator with better performance, and uses SCST for GAN training.","A novel GAN-based framework for image captioning that enables better language composition, more accurate compositional alignment of image and text, and light-weight efficient training of discrete sequence GANs. This work introduces a GAN framework that combines self-critical Sequence Training (SCST) and Gumbel Straight-Through (Gumbel ST)."
A two-stage approach consisting of sentence selection followed by span selection can be made more robust to adversarial attacks in comparison to a single-stage model trained on full context. This article investigates an existing model and finds that a two-stage trained QA method is not more robust to adversarial attacks compared to other methods.,"A promising two-stage approach for QA which decomposes the task into two stages: select relevant sentences from the passage; and select a span among those sentences. This work introduces a new approach for question-answering (QA), where an adversary can fool the model into selecting the wrong span in the passage. The study presents a novel way to improve adversarial robustness in both the context selection and span selection."
"We use a GAN discriminator to perform an approximate rejection sampling scheme on the output of the GAN generator. Proposes a rejection sampling algorithm for sampling from the GAN generator. This study proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling, to help filter ‘good’ samples from GANs’ generator.","Using a rejection sampling scheme to approximately correct errors in the GAN generator distribution. This article introduces a method for generating samples from pre-trained generators, and shows that under quite strict assumptions, this scheme allows us to recover the data distribution exactly. This work presents a novel rejection sampling algorithm for GANs using a discriminator to approximate the true data distribution of the generator."
We show how to get good representations from the point of view of Simiarity Search. Studies the impact of changing the image classification part on top of the DNN on the ability to index the descriptors with a LSH or a kd-tree algorithm. Proposes to use softmax cross-entropy loss to learn a network that tries to reduce the angles between inputs and the corresponding class vectors in a supervised framework using.,"A method for determining the angle between query and database points to determine the nearest neighbor. This work studies the cosine similarity NNS problem in hyperplane LSH BID3 using an angle of  with the closest vector, and compares the two ANN implementations as a function of the angle of the query and the closest neighbor search. The paper presents a method to estimate the distance between query time and query time to find the correct neighbor."
Adversarial learning methods encourage NLI models to ignore dataset-specific biases and help models transfer across datasets. The article proposes an adversarial setup to mitigate annotation artifacts in natural language inference data This study presents a method for removing bias of a textual entailment model through an adversarial training objective.,A domain-adversarial training framework for NLI models that performs well on other NLI datasets regardless of what annotation artifacts exist in the training corpus's hypotheses. This work introduces two architectures that enable a model to perform well on non-trivial datasets without possibly learning the relationship between the two texts.
"Object instance recognition with adversarial autoencoders was performed with a novel 'mental image' target that is canonical representation of the input image. The article proposes a method to learn features for object recognition that is invariant to various transformations of the object, most notably object pose. This study investigated the task of few shot recognition via a generated “mental image” as intermediate representation given the input image.","A mental image DCGAN that learns features that are useful for recognizing entire classes of objects. This article introduces a new way to learn the mental image and adversarial loss in a multimodal autoencoder, where the discriminator can tell a real target sample quickly and the real component of the opponent. The authors present a mental image generation approach with a dual-mode learning approach"
"Question answering models that model the joint distribution of questions and answers can learn more than discriminative models This study proposes a generative approach to textual and visual QA, where a joint distribution over the question and answer space given the context is learned, which captures more complex relationships. This article introduces a generative model for question answering and proposes to model p(q,a|c), factorized as p(a|c) * p(q|a,c). The authors proposes a generative QA model, which optimizes jointly the distribution of questions and answering given a document/context.","CLEVR is a fully connected layer applied to the concatenation of word embeddings and the hidden states of each layer of the language model. This work studies the problem of over-fitting to discriminative loss functions, which saturate when simple correlations allow the question to be answered confidently, leaving no incentive for further learning on the example."
"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet. The study explains and generalizes approaches for learning neural nets with hard activation. This study examines the problem of optimizing deep networks of hard-threshold units. The article discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it with a collection of heuristics/approximations.","A discrete optimization problem for hard-threshold hidden units in order to minimize loss. This work presents a mini-batch algorithm for learning deep neural networks that includes the popular but poorly justified straight-through estimator as a special case. The authors address the problem of learning deep deep networks with gradient descent, and show that it improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet."
"pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images Explores explaining scenes with surfels in a neural recognition model, and demonstrate results on image reconstruction, synthesis, and mental shape rotation. Authors introduce a method to create a 3D scene model given a 2D image and a camera pose using a self-superfised model","A method of unsupervised learning of the 3D structure from a single 2D image. This work introduces a method to infer the structural properties of a 3D scene using a latent variable (or vector) into a viewpoint-dependent representation of surface elements that constitute the visible part of the scene. The authors present a new model for 3D rendering, which uses a gradient-based renderer and a surfel-based rendering pipeline."
"We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games. Learning to play two-player general-sum games with state with imperfect information Specifies a trigger strategy (CCC) and corresponding algorithm, demonstrating convergence to efficient outcomes in social dilemmas without need for agents to observe each other's actions.","A method for learning cooperative and selfish policies that improves the behavior of both agents. This article introduces a new policy-ergodic approach to exchangeability in the prisoner's Dilemma, which can be used as a co-operation problem. The authors address the problem of exchangeability between agents by using a pair of policies, one for each agent, and a starting state with a low rate of payoff."
We learn a fast neural solver for PDEs that has convergence guarantees. Develops a method to accelerate the finite difference method in solving PDEs and proposes a revised framework for fixed point iteration after discretization. The authors propose a linear method for speeding up PDE solvers.,"A new linear convolutional network with a fixed point and fast convergence on the class of problems of interest. This work introduces a new iterator for discretized PDE problems, where the function b is clear from the underlying physical problem. The paper presents a method to learn a linear convex operator that can be used to solve discretized problems by learning a solution governed by the same A."
"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with improved performance and significantly reduced cost than traditional methods. The authors propose a procedure to generate an ensemble of sparse structured models A new framework for training ensemble neural networks that uses SG-MCMC methods within deep learning, and then increases computational efficiency by group sparsity+pruning. This work explores the use of FNN and LSTMs to make bayesian model averaging more computationally feasible and improve average model performance.","A Bayesian model averaging is more accurate in prediction and robust to over-fitting than point estimates of parameters. This study presents a Bayes-based method for posterior sampling, which uses a SGLD learning rate to learn from the posterior. The paper considers the problem of posterior sampling with a large number of models, and suggests a way to improve performance by using a random pruned network instead of random connection pruning."
"We propose and apply a meta-learning methodology based on Weak Supervision, for combining Semi-Supervised and Ensemble Learning on the task of Biomedical Relationship Extraction. A semi-supervised method for relation classification, which trains multiple base learners using a small labeled dataset and applies some of them to annotate unlabeled examples for semi-supervised learning. This study addresses the problem of generating training data for biological relation extraction, and uses predictions from data labeled by weak classifiers as additional training data for a meta learning algorithm. This article proposes a combination of semi-supervised learning and ensemble learning for information extraction, with experiments conducted on a biomedical relation extraction task","Using an ensemble learning methodology to improve the generalization of relation extraction in a small-scale controlled experiment. This work introduces a new method for supervised learning, which can be used on large-scale (or web-scale) datasets. The authors present a detailed approach to denoising relationships with neural networks, and show that it is useful for training data from different sources."
"we propose a regularizer that improves the classification performance of neural networks the authors propose to train a model from a point of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. Proposes to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer to maximize mutual information I(Y, \hat{T}) while constraining irrelevant information","A novel information optimization problem for supervised classification:Maximize I(Y; Y) and prevent over-fitting in the learning process. This article introduces a novel algorithm to optimize neural networks, which can be decomposed into two stages: Transformation stage of the unstructured signal under the deep invertible (invertible) feature map F to become linearly separable."
"Presents new architecture which leverages information globalization power of u-nets in a deeper networks and performs well across tasks without any bells and whistles. A network architecture for semantic image segmentation, based on composing a stack of basic U-Net architectures, that reduces the number of parameters and improves results. This proposes a stacked U-Net architecture for image segmentation.","stacked u-nets, which iteratively combine features from different resolution scales while maintaining resolution, are capable of handling the complexity of natural images. This article introduces a stack of auxiliary blocks to globalize information while preserving resolution for image segmentation and object detection tasks. The authors study the problem of globalizing information in a deep network by using a small number of parameters that can be used to address semantic segmentation tasks."
"We introduce a smoothness regularization for convolutional kernels of CNN that can help improve adversarial robustness and lead to perceptually-aligned gradients This work proposes a new regularization scheme that encourages convolutional kernels to be smoother, arguing that reducing neural network reliance on high-frequency components helps robustness against adversarial examples. The authors propose a method for learning smoother convolutional kernels, specifically, a regularizer penalizing large changes between consecutive pixels of the kernel with the intuition of penalizing the use of high-frequency input components.","Using a smooth kernel to filter out the low-frequency components of an image and improve models' robustness. This work studies the robustness of neural networks by regularizing the CNN to be most sensitive to the low (or high) component of images. The authors use a variation of logit pairing loss, which penalizes the KL divergence over softmax as the distance metric."
"We use meta-gradients to attack the training procedure of deep neural networks for graphs. Studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. An algorithm to alter graph structure by adding/deleting edges so as to degrade the global performance of node classification, and the idea to use meta-learning to solve the bilevel optimization problem.","A meta-learning approach for node classification, which is based on the meta-gradient principle of meta learning. This study introduces a meta learning method to infer the classes of the unlabeled nodes and shows that under restrictive attack settings, without access to the target classifier, our attacks can render it near-useless for use in production (i.e., on test data)."
We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters Proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-toend through differentiable Bayesian Filters and two different versions of the Unscented Kalman Filter Revisits Bayes filters and evaluates the benefit of training the observation and process noise models while keeping all other models fixed This paper presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms. The approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated diagonal covariance.,A method to learn heterostochastic process noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. This work introduces a method for learning heteroscedastic filtering from data using an unscented transform to predict the mean and covariance of the state at the next timestep.
"We propose a method for learning latent dependency structure in variational autoencoders. Uses a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model. This work presents a VAE approach in which a dependency structure on the latent variable is learned during training. The authors propose to augment the latent space of a VAE with an auto-regressive structure, to improve the expressiveness of both the inference network and the latent prior","A method for learning the dependency structure between latent variables in deep generative models. The authors present a variational autoencoder framework for learning latent variable space in a Bayesian network with a learned, flexible dependency structure. This work introduces a new model that learns the dependency structure of a Latent Variable Structured Variable Scale (VAE) using a sampling procedure to generate expectations over latent variable structures."
We examine the relationship between probability density values and image content in non-invertible GANs. The authors try to estimate the probability distribution of the image with the help of GAN and develop a proper approximation to the PDFs in the latent space.,"A method to extract the probability density of an image given its latent representation. This article introduces a method for estimating densities in MNIST images, which uses the discriminator to produce samples that are likely to be real and low values to fake points. The authors study the problem of calculating density in GANs by comparing histograms of predicted distributions in the train and test datasets."
A simple fast method for extracting visual features from convolutional neural networks Proposes a fast way to learn convolutional features that later can be used with any classifier by using reduced numbers of training epocs and specific schedule delays of learning rate Use a learning rate decay scheme that is fixed relative to the number of epochs used in training and extract the penultimate layer output as features to train a conventional classifier.,A new approach to learning feature fast is to design a step decay schedule that is a function of the total number of epochs predicted to train the model. This study presents a method to learn feature fast in a neural network by using supervised Convolutional Neural Networks (CNN) to extract high-quality image features.
"Decompose the task of learning a generative model into learning disentangled latent factors for subsets of the data and then learning the joint over those latent factors. Locally Disentangled Factors for hierarchical latent variable generative model, which can be seen as a hierarchical variant of Adversarially Learned Inference The paper investigates the potential of hierarchical latent variable models for generating images and image sequences and proposes to train several ALI models stacked on top of each other to create a hierarchical representation of the data. The work aims to learn the hierarchies for training GAN in a hierarchical optimization schedule directly instead of being designed by a human","A method for unsupervised learning of hierarchical latent variables with a requisite but general prior to facilitate disentangled learning dynamics in each level. This work introduces a new approach to unsupervised models, which uses a resolution-based hierarchy framework and the inference framework. The paper presents a method to decouple level-wise training objectives which significantly accelerates the training process."
"Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE). This work proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution Introduces a variation on the Wasserstein AudoEncoders which is a novel regularized auto-encoder architecture that proposes a specific choice of the divergence penalty This article proposes the Cramer-Wold autoencoder, which uses the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.","This article introduces a method for estimating the distance between two distributions and the distance of a sample from the standard normal density. The paper presents a novel method to compute distance between different distributions by using a close analytic formula, which can be used to approximate the divergence measure in WAE-MMD. This work explores the problem of decomposition on a distribution with a closed-form obtained from a characteristic kernel"
"Learn to rank learning curves in order to stop unpromising training jobs early. Novelty: use of pairwise ranking loss to directly model the probability of improving and transfer learning across data sets to reduce required training data. The article proposes a method to rank learning curves of neural networks that can model learning curves across different datasets, achieving higher speed-ups on image classification tasks.",A method to determine the likelihood of no improvement is to compare the learning curve of a new configuration to the one of the currently best configurations. This article introduces a meta-knowledge method that models the probability that the model currently being investigated surpasses the best solution so far.
"Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve. This study shows that wider RNNs improve convergence speed when applied to NLP problems, and by extension the effect of increasing the widths in deep neural networks on the convergence of optimization This study characterizes the impact of over-parametrization in the number of iterations it takes an algorithm to converge, and presents further empirical observations on the effects of over-parametrization in neural network training.","A convergence curve characterized by direct distance from initialization point to final point, the average step size and the average angle between gradient vectors and the path that connects current weights to final wights. This study presents a convergence curve that can be characterized into a powerlaw region within which the number of gradient updates to convergence has a reciprocal relationship to model size and linear relationship to its dataset size."
"Generalized Graph Embedding Models A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches. Tackles the task of learning embeddings of multi-relational graphs using a neural network Proposes a new method, GEN, to compute embeddings of multirelationship graphs, particularly that so-called E-Cells and R-Cells can answer queries of the form (h,r,?),(?r,t), and (h,?,t)","A new embedding learning framework for modeling author connections on social networks that is more robust to global underfitting and local over-fitting. This work introduces a new embedded learning framework, which can be used to solve link prediction problems. The authors present a model for neural network embeddation that uses a hidden layer of the embedded dictionary in order to answer query."
"Mean Replacement is an efficient method to improve the loss after pruning and Taylor approximation based scoring functions works better with absolute values. Proposes a simple improvement to methods for unit pruning using ""mean replacement"" This paper presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning","A new pruning method to prune a predefined fraction of its units using the chosen criterion, and measure the pruning penalty by comparing the losses measured before and after pruning. This work introduces a new scoring function for all scoring functions and shows that the best performing scoring functions are somewhat consistent over the course of training, implying that the dead parts of the network appear during the stages of training."
"We introduce a system called GamePad to explore the application of machine learning methods to theorem proving in the Coq proof assistant. This article describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an SVM on both tasks. Proposes that machine learning techniques be used to help build proof in the theorem prover Coq.","This article introduces a new system for theorem proving in the Coq proof assistant. The paper presents a method for the application of the coq proof script to the task of premise selection, which can be used on tasks useful for the automate prediction task. The authors present a way to learn proof state by implementing a patch to Coq that uses a Coq Ltac interpreter to log the intermediate proof states."
"We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets. This study proposed ""self-ensemble label filtering"" for learning with noisy labels where the label noise is instance-independent, which yield more accurate identification of inconsistent predictions. This article proposes an algorithm for learning from data with noisy labels which alternates between updating the model and removing samples that look like they have noisy labels.","A method for self-ensemble learning under noisy labels. This work introduces a method to learn from noisy labels by using a model ensemble to improve the performance of DNNs. The authors present a framework for non-expert learning on noisy labels that uses a moving-average of model snapshots to filter out the noisy labels, and show that SELF consistently outperforms the existing methods on asymmetric and symmetric noise at all noise levels."
"An algorithm for training neural networks efficiently on temporally redundant data. The paper describes a neural coding scheme for spike based learning in deep neural networks This paper presents a method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This work applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network, combining the three components in a way not seen previously.","This work presents a new algorithm for adaptive layer quantization on all layers of a neural network. This article introduces a method for adaptively tuning the gradients on each layer of the neural network to match the running average of the magnitude of the data. The authors present a novel algorithm for encoding layers of neural networks that can be trained with backpropagation, despite only communicating discrete values between layers."
A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers A method for constructing adversarial attacks that are less detectable by humans without cost in image space by changing the target class to be similar to the original class of the image.,"An attack that makes adversarial samples less recognizable in the label space by human observers. This work introduces a method for generating adversarials to be mis-classified as the target label, and shows improvement in the attack rate is not at the expense of loss in the image space. The paper presents a new attack method that can attack a classifier with ""similar"" label to the input image's ground truth."
"We propose AD-VAT, where the tracker and the target object, viewed as two learnable agents, are opponents and can mutually enhance during training. This work aims to address the visual active tracking problem with a training mechanism in which the tracker and target serve as mutual opponents This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. Proposes a novel reward function - ""partial zero sum"", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.","A novel reversarial reinforcement learning method for VAT task, i.e., the Asymmetric Dueling mechanism (AD-VAT). This article introduces a new approach to training agents using adversarial neural networks in order to predict the tracker’s reward as an auxiliary task. The study presents a method for training agents that is more likely to compete with a target with the appropriate difficulty level when both agents are stronger simultaneously."
"Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks This work describes a large-scale experiment on human object/sematic representations and a model of such representations. This work develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images. A new approach to learn a sparse, positive, interpretable semantic space that maximizes human similarity judgements by training to specifically maximize the prediction of human similarity judgements.","A probabilistic model for a given trial with non-negativity and sparsity penalty. This study presents a novel method for estimating the differences between concept senses by using a random initialization parameter to obtain sparse embeddings. The authors present a model for two concepts that can be used to model phenomena such as judgments of typicality or similarity between concepts, or reaction times in various semantic tasks."
"We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality. Introduces progressive growing and a simple parameter-free minibatch summary statistic feature for use in GAN training to enable synthesis of high-resolution images.","A GAN formulation that does not explicitly require the entire training data distribution to be represented by the resulting generative model. This article introduces a method for measuring the degree of preserved variation in GANs, which can be used to measure the distance between the training distribution and the generated distribution. The authors present a GAN formula that uses weight initialization instead of a trivial N (0, 1) initialization and then explicitly scale the weights at runtime."
"A fast second-order solver for deep learning that works on ImageNet-scale problems with no hyper-parameter tuning Choosing direction by using a single step of gradient descent ""towards Newton step"" from an original estimate, and then taking this direction instead of original gradient A new approximate second-order optimization method with low computational cost that replaces the computation of the Hessian matrix with a single gradient step and a warm start strategy.","A method for non-convex deep network optimisation, which is useful against first order solvers. This article introduces a new Hessian-free method that can be used to improve second-order neural networks without the inner CG loop. The authors present a method for multi-task deep neural networks with no hyper-parameter tuning and show strong results on several large-scale problems."
Models of source code that combine global and structural features learn more powerful representations of programs. A new method to model the source code for the bug repairing task using a sandwich model like [RNN GNN RNN] which significantly improves localization and repair accuracy.,"A graph-based embedding of programs to predict the most likely variable at each variable-use location and generate a repair prediction using an enumerative approach. This work introduces a graph neural network architecture that integrates message passing with the semantic structural information available to the GGNN. The paper presents two new models that efficiently combine longerdistance information, such as the sequence model can represent, with the number of message passes."
"A novel differentiable neural architecture search framework for mixed quantization of ConvNets. The authors introduce a new method for neural architecture search which selects the precision quantization of weights at each neural network layer, and use it in the context of network compression. The work presents a new approach in network quantization by quantizing different layers with different bit-widths and introduces a new differentiable neural architecture search framework.","This paper introduces a new framework for quantization of neural nets, which uses the DNAS framework to solve the mixed precision quantization problem. The authors study the stochastic super net as a method for quantizing macro architectures of neural networks by constructing a super net whose macro architecture (number of layers, filter size of each layer, etc.) is the same as the target network."
"We propose an novel learning method for deep sound recognition named BC learning. Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes to increase discriminitive power of the final learned network. Proposes a method to improve the performance of a generic learning method by generating ""in between class"" training samples and presents the basic intuition and necessity of the proposed technique.","A model that learns to mix the mixed sounds of dog bark and rain with a certain ratio is proportional to the original feature distribution of class A and B. This work studies the performance of sound recognition networks on datasets and data augmentation schemes in which BC learning proves to be always beneficial. The study presents a method for improving the performance on various sound recognition network, datasets, and data-augmentation schemes."
"Motivated by theories of language and communication, we introduce community-based autoencoders, in which multiple encoders and decoders collectively learn structured and reusable representations. The authors tackle the problem of representation learning, aim to build reusable and structured represenation, argue co-adaptation between encoder and decoder in traditional AE yields poor representation, and introduce community based auto-encoders. The article presents a community based autoencoder framework to address co-adaptation of encoders and decoders and aims at constructing better representations.",A framework for autoencoders that can be used to train a community-based encoder and decoder. This article introduces a framework for learning representations that are more easily extracted and re-used for supervised learning and reinforcement learning. The authors study the problem of co-adaptation between a pair of encoders and a non-decoder classifier
"This work presents a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. Proposes a framework for making predictions on sparse, irregularly sampled time-series data using an interpolation module that models the missing values in using smooth interpolation, non-smooth interpolation, and intensity. Solves the problem of supervised learning with sparse and irregularly sampled multivariate time series using a semi-parametric interpolation network followed by a prediction network.","A deep learning model for multivariate time series that can be shared across multiple dimensions during the interpolation stage, while any standard deep-learning model can be used for the prediction network. This work introduces a deep learning architecture for multi-time series prediction which uses a semi-parametric intensity function representation to isolate information about short duration events from broader trends."
"We propose 3D shape programs, a structured, compositional shape representation. Our model learns to infer and execute shape programs to explain 3D shapes. An approach to infer shape programs given 3D models, with architecture consisting of a recurrent network that encodes a 3D shape and outputs instructions, and a second module that renders the program to 3D. This article introduces a high-level semantic description for 3D shapes, given by the ShapeProgram.","A purely end-to-end neural network synthesizer for 3D graphics. This article introduces a novel way to infer 3D shapes from raw, unannotated shapes, and shows how it can be used to reconstruct parts of objects. The authors present a method for modeling 3D images using a domain-specific language, which uses a specific language to learn more about the structure of objects and their properties."
"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures. Introduces a new method to generate RNNs architectures using a domain-specific language for two types of generators (random and RL-based) together with a ranking function and evaluator. This article casts the search of good RNN Cell architectures as a black-box optimization problem where examples are represented as an operator tree and scored based on learnt functions or generated by a RL agent. This article investigates meta-learning strategy for automated architecture search in the context of RNN by using a DSL that specifies RNN recurrent operations.","This article introduces a new algorithm for RNN architectures that uses a hidden state h t and an extension of the DSL to code. This study presents a novel method for generating RNNs by using a LSTM-encoder-decoder, which can be used to generate a dataset of candidate architectures. The authors present a technique for constructing RNN with a unique vector representation in the output node"
"Deep and narrow neural networks will converge to erroneous mean or median states of the target function depending on the loss with high probability. This work studies failure modes of deep and narrow networks, focusing on as small as possible models for which the undesired behavior occurs. This paper shows that the training of deep ReLU neural networks will converge to a constant classifier with high probability over random initialization if hidden layer widths are too small.","A simple approximately radial activation function can be approximated by a small 3-layer feed-forward NN, but it cannot be approximated by any 2-layer network with the same accuracy irrespective of the activation functions. This work presents a novel approxiation theory of a class of (possibly discontinuous) piecewise C  functions for ReLU NNs, showing that no more than O( 2(d"
