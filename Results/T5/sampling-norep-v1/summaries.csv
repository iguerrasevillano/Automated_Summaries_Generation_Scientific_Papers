target,abstractive_summary
"A new framework based variational inference for out-of-distribution detection Describes a probabilistic approach to quantifying uncertainty in DNN classification tasks that outperforms other SOTA methods in the task of out-of-distribution detection. A new framework for out-of-distribution detection, based on variaitonal inference and a prior Dirichlet distribution, that reports state of the art results on several datasets. An out-of distribution detection via a new method to approximate the confidence distribution of classification probability using variational inference of Dirichlet distribution.",A variational Dirichlet framework for deep neural network classification problem and a higher-order uncertainty measure. This work introduces a variational dirichlet algorithm to improve the accuracy of in-and out-of-distribution datasets by introducing a variable label-wise distribution P(y) over the simplex S k. The paper considers the image classification problem with image classification as x and output label as y.
"Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This article presents unsupervised approaches to discovering important neurons in neural machine translation systems and analyzes linguistic properties controlled by those neurons. Unsupervised methods for ranking neurons in machine translation where important neurons are thus identified and used to control the MT output.",An unsupervised approach for analyzing the contribution of individual neurons to NMT models. This work introduces a novel translation control method for translating neural networks into a vector space using a combination of multiple vectors and canonical correlation analysis. The authors study the role of tense neurons in the translation of linguistically interpretable neural networks by modifying the activation of the active neurons from the previous section TAB2.
We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators. The work proposes using circulant and diagonal matrices to speed up computation and reduce memory requirements in eural networks. This work proves that bounded width diagonal-circulent ReLU networks (DC-ReLU) are universal approximators.,A deep neural network architecture that can approximate a deep ReLU network in which the dense matrices are of low rank. This study studies the applicability of a Deep diagonal-circulant ReLU networks by showing that any matrix can be decomposed into products of bounded width and small depth is more useful than the one by Huhtanen & Perämäki because it can be used to approximate the first one.
"The most robust capacity for analogical reasoning is induced when networks learn analogies by contrasting abstract relational structures in their input domains. The paper investigates the ability of a neural network to learn analogy, showing that a simple neural network is able to solve certain analogy problems This article describes an approach to train neural networks for analogical reasoning tasks, specifically considering visual analogy and symbolic analogies.","A neural network that learns from contrasting abstract relational structures on an analogical problem by using an analogy-like model architecture. This study investigates the problem of semantically plausible relational representations in a neural network, and shows that it is more likely to be used to solve an ananalogical problems. The authors present a model for neural networks that learn from random candidates who choose incorrect answers from the same domain as the correct answer."
"We propose a deep Multi Instance Learning framework based on recurrent neural networks which uses pooling functions and attention mechanisms for the concept annotation tasks. The paper addresses the classification of medical time-series data and proposes to model the temporal relationship between the instances in each series using a recurrent neural network architecture. Proposes a novel Multiple Instance Learning (MIL) formulation called Relation MIL (RMIL), and discussed a number of its variants with LSTM, Bi-LSTM, S2S, etc. and explores integrating RMIL with various attention mechanisms, and demonstrates its usage on medical concept prediction from time series data.","An annotation task for medical time series data, i.e., to localize medical concepts. This work introduces the concept annotation task as the problem of predicting and localizing medical concepts by using the medical data as input. The authors present a method for generating medical information from medical data such as intubation, extubation and resuscitate."
"We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks. This paper proposes replacing the final cross-entropy layer trained on one-hot labels in classifiers by encoding each label as a high-dimensional vector and training the classifier to minimize L2 distance from the encoding of the correct class. Authors propose new method against adversarial attacks that shows significant amount of gains compared to baselines","A deep encoding framework that obstructs the adversarial gradient search. This study introduces a multi-way attack for both the source and target models, and shows that it is more efficient than one-hot (1of K) attacks. This work presents a deep learning framework that can be used to determine the direction of an input from the input to the blue class, or vice versa."
"We introduce Spherical CNNs, a convolutional network for spherical signals, and apply it to 3D model recognition and molecular energy regression. The article proposes a framework for constructing spherical convolutional networks based on a novel synthesis of several existing concepts This study focuses on how to extend convolutional neural networks to have built-in spherical invariance, and adapts tools from non-Abelian harmonic analysis to achieve this goal. The authors develop a novel scheme for representing spherical data from the ground up","A spherical CNN that can detect patterns regardless of how they are rotated over the sphere. This work introduces a new neural network to detect patterns in a three-dimensional manifold called SO(3) 2. The authors present a method for detecting patterns in the space of moves for the plane, and show that it is possible to find patterns on the plane with a 3D translation."
"Relational Forward Models for multi-agent learning make accurate predictions of agents' future behavior, they produce intepretable representations and can be used inside agents. A way of reducing variance in model free learning by having an explicit model, that uses a graph conv net-like architecture, of actions that other agents will take. Predicting multi-agent behavior using a relational forward model with a recurrent component, outperforming two baselines and two ablations","A deep learning model for multi-agent systems with a rich and orderly structure that can be leveraged to understand these systems, and to improve how artificial agents learn to operate in them. This study introduces a deep learning framework for multiagent system modeling behavior of multiple agents, and uses it to predict the forward dynamics of each agent's system. The authors present an analysis tool for characterizing interactions between different agents"
"We introduce a transparent middleware for neural network acceleration, with own compiler engine, achieving up to 11.8x speed up on CPUs and 2.3x on GPUs. This paper proposes a transparent middleware layer for neural network acceleration and obtains some acceleration results on basic CPU and GPU architectures","This paper introduces a transparent middleware layer for neural network acceleration, allowing one to combine device-specific libraries and custom optimizations while supporting numerous hardware platforms. The authors address the problem of inference and inference in deep neural networks by using a compiler for deep learning frameworks, and show that it is transparent to the user and should support all hardware platforms and deep learning libraries."
"Agents interact (speak, act) and can achieve goals in a rich world with diverse language, bridging the gap between chit-chat and goal-oriented dialogue. This paper studies a multiagent dialog task in which the learning agent aims to generate natural language actions that elicit a particular action from the other agent, and shows RL-agents can achieve higher task completion levels than imitation learning baselines. This work explores the goal-oriented dialogue setting with reinforcement learning in a Fantasy Text Adventure Game and observes that the RL approaches outperform supervised learning models.","A new algorithm for artificial dialogue agents that can be trained to imitate human actions given a goal. This work introduces a cluster of AI models, which can be used to improve the quality of the textual descriptions of the world. The authors study the problem of artificial dialogue agent training with self-chat, and show that it is possible to use a set of AI Models as a baseline for comparing them to a RL model."
"CharNMT is brittle This study investigates the impact of character-level noise on 4 different neural machine translation systems This study empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural. This study investigates the impact of noisy input on Machine Translation and tests ways to make NMT models more robust","A charCNN model that is more robust to different kinds of noise, by training on noisy texts. This study studies the robustness of text translations in a Wrot-like language, and shows that it can be used to address typos and noise. The authors introduce a charcNN model which uses a keyboard type to detect errors in the first and last letters, and show that it performs well on Key in French, but not elsewhere."
"Feedforward neural networks that can have weights pruned after training could have had the same weights pruned before training Shows that there exists sparse subnetworks that can be trained from scratch with good generalization performance and proposes a unpruned, randomly initialized NNs contain subnetworks that can be trained from scratch with similar generalization accuracy. The article examines the hypothesis that randomly initialized neural networks contain sub-networks that converge equally fast or faster and can reach the same or better classification accuracy","A method to train a pruned network with a small capacity. The authors use iterative pruning as a proxy for the speed at which a network learns, but not after re-initializing the pruned layers and retraining them. This article considers the problem of training a fully connected network on a pruning task like MNIST."
"A new regularization term can improve your training of wasserstein gans The paper proposes a regularization scheme for Wasserstein GAN based on relaxation of the constraints on the Lipschitz constant of 1. The study deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric.","A new training method for generating GANs by increasing the loss by regularization term that penalizes the deviation of the norm of the critic function (as a function of the network's input) from one. This work presents a new training technique to improve GAN training, with the aim of reducing the infimum by minimizing the Wasserstein-1 distance on Earth-Mover distance."
"DeFINE uses a deep, hierarchical, sparse network with new skip connections to learn better word embeddings efficiently. This work describes a new method for learning deep word-level representations efficiently by using a hierarchical structure with skip-connections for the use of low dimensional input and output layers.","This work introduces a new layer-to-layer approach for learning input and output representations jointly while significantly reducing the number of network parameters. The authors address the problem of learning representations from different subsets of the first layer using sparse and dense connections, allowing HGT to learn deep representations efficiently by learning a hierarchical link between the input layers and the output layers."
"We prove fluctuation-dissipation relations for SGD, which can be used to (i) adaptively set learning rates and (ii) probe loss surfaces. Paper's concepts work in the discrete-time formalism, use the master equation, and remove reliance on a locally quadratic approximation of the loss function or on any Gaussian asumptions of the SGD noise. The authors derive the stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in SGD and use the relations to set training schedule adaptively and analyze the loss-function landscape.","This work introduces an analogy of fluctuation-dissipation relations that quantitatively link the noise in mini-batched data to the evolution of the model performance and facilitates the learning process. The authors present a method for determining the properties of the lossfunction landscape, including the strength of its Hessian counterpart and the degree of anharmonicity."
"We show that NN parameter and hyperparameter cost landscapes can be generated as quantum states using a single quantum circuit and that these can be used for training and meta-training. Describes a method where a deep learning framework can be quantised by considering the two state form of a Bloch sphere/qubit and creating a quantum binary neural network. This work proposes quantum amplitude amplification, a new algorithm for training and model selection in binary neural networks. Proposes a novel idea of outputting a quantum state that represents a complete cost landscape of all parameters for a given binary neural network, by constructing a quantum binary neural network (QBNN).","A quantum annealing device that takes in a given set of weights and an empty register, and outputs the resultant accuracy according to the chosen neural network. This work introduces a new gate-based quantum computation method for binary qubits, which is used to perform binary classification, and shows that it can be used to train binary quebits by using a reversibility gate."
"Transfer learning for estimating causal effects using neural networks. Develops algorithms to estimate conditional average treatment effect by auxiliary dataset in different environments, both with and without base learner. The authors propose methods to address a novel task of transfer learning for estimating the CATE function, and evaluate them using a synthetic setting and a real-world experimental dataset. Using neural network regression and comparing transfer learning frameworks to estimate a conditional average treatment effect under string ignorability assumptions","A method for evaluating CATE estimation on real data that can be used in observational studies. This work presents a method for learning CATEs from the perspective of transfer learning by combining them with non-parametric methods. The authors introduce a new algorithm to learn features from the label presented in the image X, and show that it is possible to train CATE models using a set of different input functions."
"We show that catastrophic forgetting occurs within what is considered to be a single task and find that examples that are not prone to forgetting can be removed from the training set without loss of generalization. Studies the forgetting behavior of training examples during SGD, and shows there exist ""support examples"" in neural network training across different network architectures. This study analyzes the extent to which networks learn to correctly classify specific examples and then forget these examples over the course of training. The paper studies whether some examples in training neural networks are harder to learn than others. Such examples are forgotten and relearned multiple times through learning.","An implicit regularization method for learning unforgettable examples, based on forgetting dynamics, to identify important or most informative examples. This study investigates the problem of learning outliers with noisy labels in a neural network and shows that it is possible to find out how many memorable examples are forgotten once learnt. The authors address the issue of learning unimportant examples as a result of a linearly separable learning process."
"A class of networks that generate simple models on the fly (called explanations) that act as a regularizer and enable consistent model diagnostics and interpretability. The authors claim that the previous art directly integrate neural networks into the graphical models as components, which renders the models uninterpretable. Proposal for a combination of neural nets and graphical models by using a deep neural net to predict the parameters of a graphical model.","Using a domain-specific deep architecture for image and text classification and survival analysis, our results demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support. This work introduces a class of probabilistic models to be used in a social context, demonstrating that post-hoc approximations of CEN's decision boundary are consistent with the generated explanations."
"We proposed ""Difference-Seeking Generative Adversarial Network"" (DSGAN) model to learn the target distribution which is hard to collect training data. This article presents DS-GAN, which aims to learn the difference between any two distributions whose samples are difficult or impossible to collect, and shows its effectiveness on semi-supervised learning and adversarial training tasks. This work considers the problem of learning a GAN to capture a target distribution with only very few training samples from that distribution available.","A Generative Approach for learning data distribution from its samples and thereafter produce novel and high-dimensional samples from its learned distributions, such as image and speech synthesis. This work introduces a GAN method to learn the generator distribution from mixed distributions and shows that it is more stable to train DSGAN in a semi-supervised environment."
"We present NormCo, a deep coherence model which considers the semantics of an entity mention, as well as the topical coherence of the mentions within a single document to perform disease entity normalization. Uses a GRU autoencoder to represent the ""context"" (related enitities of a given disease within the span of a sentence), solving the BioNLP task with significant improvements over the best-known methods.","A biomedical knowledge base with a combination of two sub-models which leverage both topical coherence and semantic features to perform disease normalization. This work presents a novel method for addressing the problem of disease normization by combining two datasets of distantly supervised data, extracted through two corpora of scattered datasets created for non-normalization purposes."
"We present a single shot analysis of a trained neural network to remove redundancy and identify optimal network structure This paper proposes a set of heuristics for identifying a good neural network architecture, based on PCA of unit activations over the dataset This work presents a framework for optimising neural networks architectures through the identification of redundant filters across layers","A new neural network with a number of principal filters and retrain from scratch. This work explores the problem of pruning out a particular filter to reduce the dimensionality of the space that the data resides in after passing through a layer, and shows that the number of significant filters is an intrinsic property of a specific network structure."
"The first deep learning approach to MFSR to solve registration, fusion, up-sampling in an end-to-end manner. This work proposes an end-to-end multi-frame super-resolution algorithm, that relies on a pair-wise co-registrations and fusing blocks (convolutional residual blocks), embedded in a encoder-decoder network 'HighRes-net' that estimates the super-resolution image. This work proposes a framework including recursive fusion to co-registration loss to solve the problem of super-resolution results and high-resolution labels not being pixel aligned.","This paper introduces a deep-learning approach that solves the co-registration, fusion and registration-at-theloss problems in an end-to-end learning framework. The authors address the problem of single image super-resolution (SISR) with a new loss mechanism that can be used to learn high-frequency representations during training."
We introduce a technique that allows for gradient based training of quantized neural networks. Proposes a unified and general way of training neural networks with reduced precision quantized synaptic weights and activations. A new approach to quantizing activations which is state of the art or competitive on several real image problems. A method for learning neural networks with quantized weights and activations by stochastically quantizing values and replacing the resulting categotical distribution with a continuous relaxation,A method for quantizing neural networks with a stochastic rounding that can be seen as a special case of the proposed approach. This study presents a method to quantize neural networks by reducing the precision of the arithmetic operations in the network. The paper introduces a novel quantization method for determining the distribution of the input signal and then sampling grid points from it.
"Interpretation by Identifying model-learned features that serve as indicators for the task of interest. Explain model decisions by highlighting the response of these features in test data. Evaluate explanations objectively with a controlled dataset. This study proposes a method for producing visual explanations for deep neural network outputs and releases a new synthetic dataset. A method for Deep Neural Networks that identifies automatically relevant features of the set of the classes, supporting interpretation and explanation without relying on additional annotations.","A method to automatically identifie the network-encoded features that are important for the prediction of a given class. This work introduces a method for identifying the class of interest in an image, a set of identified relevant filters, and a class prediction using heatmap visualizations of the top-responding relevant filters for the predicted class."
"A method that build representations of sequential data and its dynamics through generative models with an active process Combines neural networks and Gaussian distributions to create an architecture and generative model for images and video which minimizes the error between generated and supplied images. The article proposes a Bayesian network model, realized as a neural network, that learns different data in the form of a linear dynamical system","A new model for generative adversarial networks that is capable of adapting to changing scenarios. This work introduces a hierarchical architecture where the activations of layers at the same level of a coder and a generator are laterally used to send information about the last prediction during inference, and about the inference for the generation of data."
"Obtains state-of-the-art accuracy for quantized, shallow nets by leveraging distillation. Proposes small and low-cost models by combining distillation and quantization for vision and neural machine translation experiments This paper presents a framework of using the teacher model to help the compression for the deep learning model in the context of model compression.","A method for incorporating distillation loss, expressed with respect to the teacher, into the training of a student network. This study introduces a method for quantizing and leveraged distillation in the training process by incorporating it into the learning process. The authors present two methods for combining full-precision and deep neural networks with compression in terms of depth, and show that quantized shallow students can reach similar accuracy levels to full-presence and deeper teacher models."
"We use question-answering to evaluate how much knowledge about the environment can agents learn by self-supervised prediction. Proposes QA as a tool to investigate what agents learn about in the world, arguing this as an intuitive method for humans which allows for arbitrary complexity. The authors propose a framework to assess representations built by predictive models that contain sufficient information to answer questions about the environment they are trained on, showing those by SimCore contained sufficient information for the LSTM to answer questions accurately.","QA performance is indicative of the agent's ability to capture global environment structure and semantics solely through egocentric prediction. This work introduces a novel decoder network that learns to encode relevant aspects of the environment in a representation amenable to easy decoding into symbols. The authors use question-answering as a general purpose answer for higher-order conceptual knowledge, which can be used to solve complex questions about the environment."
"We train with state aligned vector rewards an agent predicting state changes from action distributions, using a new reinforcement learning technique inspired by quantile regression. Presents algorithm that aims to speed up reinforcement learning in situations where the reward is aligned with the state space. This work addresses RL in the continuous action space, using a re-parametrised policy and a novel vector-based training objective. This work proposes to mix distributional RL with a net in charge of modeling the evolution of the world in terms of quantiles, claiming improvements in sample efficiency.",A deep reinforcement learning algorithm to train stochastic continuous action policies with arbitrary action probability distributions. This work introduces a new reinforcement learning architecture that learns the quantile function of the action dimensions and state space in order to maximize the expected (discounted) reward. The authors extend the reinforcement learning paradigm to allow for faster training based on more informative state aligned vector rewards.
"Learn representations for images that factor out a single attribute. This study builds on Conditional VAE GANs to allow attribute manipulation in the synthesis process. This study proposes a generative model to learn the representation which can separate the identity of an object from an attribute, and extends the autoencoder adversarial by adding an auxiliary network.","This paper introduces a VAE-GAN for image synthesis that is more competitive with state of the art. The authors introduce a new convolutional GAN, DCGAN, architecture to improve classification results. This work introduces an IFcVAE-gAN which can be trained in a latent space representation that separates an object category from its attributes and performs better than fine-grain image manipulation."
We take face recognition as a breaking point and propose model distillation with knowledge transfer from face classification to alignment and verification This article proposes to transfer the classifier from the model for face classification to the task of alignment and verification. The manuscript presents experiments on distilling knowledge from a face classification model to student models for face alignment and verification.,"A method to distill the knowledge from the teacher network by learning its soft-prediction. This work introduces a new distillation method for face classification, which uses a distillation trick to improve the distillation performance of classifying tasks. The authors present a method for distillation of the knowledge in face alignment and verification by transferring it from the classifier to the instructor network. Presents an initialization method for class classification tasks with the same data and identity labels."
"New state-of-the-art framework for image restoration The article proposes a convolutional neural network architecture that includes blocks for local and non-local attention mechanisms, which are claimed to be responsible for achieving excellent results in four image restoration applications. This article proposes a residual non-local attention network for image restoration","Using residual non-local attention learning to train very deep networks by preserving more low-level features, being more suitable for image restoration. This work presents a novel method for capturing information from hierarchical features in the context of images that can be used to improve image super-resolution. The authors present a new approach to extract information from the mask branches of the mask branch and show that it's possible to obtain more sophisticated attention map in mask branches."
"We present an influence-directed approach to constructing explanations for the behavior of deep convolutional networks, and show how it can be used to answer a broad set of questions that could not be addressed by prior work. A way to measure influence that satisfies certain axioms, and a notion of influence that can be used to identify what input part is most influential for the output of a neuron in a deep neural network. This paper proposes to measure the influence of single neurons with regard to a quantity of interest represented by another neuron.","A method to interpret predictions for convolutional neural networks by focusing on the essence of a class, and identifying influential neurons over the entire population. This work introduces a method for interpreting predictions for an image that is more relevant to the outcomes of the neurons in the input model. The authors present a new way of understanding the effect of influence on a slice of the network with two natural invariance properties."
"In visual prediction tasks, letting your predictive model choose which times to predict does two things: (i) improves prediction quality, and (ii) leads to semantically coherent ""bottleneck state"" predictions, which are useful for planning. A method on prediction of frames in a video, the approach including that target prediction is floating, resolved by a minimum on the error of prediction. Reformulates the task of video prediction/interpolation so that a predictor is not forced to generate frames at fixed time intervals, but instead is trained to generate frames that happen at any point in the future.","A novel technical approach to solve video prediction problem. This work introduces a method for reframeing the prediction problem to be time-agnostic, and shows that bottleneck states correspond to subgoals that aid in planning towards complex end goals. The authors consider the use of bottleneck predictions as subgoals for a hierarchical planner, which is loosely related to options."
We perform functional variational inference on the stochastic processes defined by Bayesian neural networks. Fitting of variational Bayesian Neural Network approximations in functional form and considering matching to a stochastic process prior implicitly via samples. Presents a novel ELBO objective for training BNNs which allows for more meaningful priors to be encoded in the model rather than the less informative weight priors features in the literature. Presents a new variational inference algorithm for Bayesian neural network models where the prior is specified functionally rather than via a prior over weights.,"This work introduces a variational inference method that approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. The authors present a method for estimating posterior dependencies of a shallow BNN, showing that an implicit distribution is a Gaussian process (GP). This study presents a technique to estimate the posterior function of an implicit distributed by combining finite measurements and the Stein gradient estimation method."
"Learning deep latent variable MRFs with a saddle-point objective derived from the Bethe partition function approximation. A method for learning deep latent-variable MRF with an optimization objective that utilizes Bethe free energy, that also solves the underlying constraints of Bethe free energy optimizations. An objective for learning latent variable MRFs based on Bethe free energy and amortized inference, different from optimizing the standard ELBO.","A method for learning undirected graphical models with discrete latent variables that can be optimized efficiently without sampling. This work introduces an approach to learning neural text HMMs using the Bethe free energy approximation, which makes use of a saddlepoint objective to improve the expressiveness of MRFs with arbitrary pairwise factors in terms of held out log likelihood."
"In this work, we point to a new connection between DNNs expressivity and Sharkovsky’s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks Shows how the expressive power of NN depends on its depth and width, furthering the understanding of the benefit of deep nets for representing certain function classes. The authors derive depth-width tradeoff conditions for when relu networks are able to represent periodic functions using dynamical systems analysis.","A method to predict periodic functions as a function of the depth. This work presents a method for predicting periodic functions that can be represented by DNNs, i.e., by some particular choice of weights on their edges (and for a wide variety of standard activation units in their layers). The authors show that the composition of t(x; 2) with itself k times will create exponentially many oscillations."
"We propose a method that infers the time-varying data quality level for spatiotemporal forecasting without explicitly assigned labels. Introduces a new definition of data quality that relies on the notion of local variation defined in (Zhou and Scholkopf) and extends it to multiple heterogenous data sources. This article proposed a new way to evaluate the quality of different data sources with the time-vary graph model, with the quality level used as a regularization term in the objective function","A graph convolutional neural network architecture that can be used to extract localized patterns from regular grids, such as images BID14. This work introduces a graph connectivity architecture for generating localized signals at vertex i with the help of data quality. The study presents a computational framework for estimating the local variation of a vertex signal at the vertex's vertex in a way that is less fluctuated by the number of neighbors."
"We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models, and we demonstrate how this finding allows us to make useful estimates of the model uncertainty in conventional networks. This study proposes using batch normalisation at test time to get the predictive uncertainty, and shows Monte Carlo prediction at test time using batch norm is better than dropout. Proposes that the regularization procedure called batch normalization can be understood as performing approximate Bayesian inference, which performs similarly to MC dropout in terms of the estimates of uncertainty that it produces.",A deep-net-based model-wise operation to normalize the distribution of each unit's input into a batch normalized network. This work introduces a new method for estimating uncertainty in deep neural networks by using a Bayesian approach. The authors present a deep learning method that can be used as a mini-batch optimization method and provide a better understanding of the problem of quantifiable variables in a wide range of tasks.
"Understanding the neural network Hessian eigenvalues under the data generating distribution. This paper analyzes the spectrum of the Hessian matrix of large neural networks, with an analysis of max/min eigenvalues and visualization of spectra using a Lanczos quadrature approach. This paper uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposes an efficient spectrum visualization methods.","A method to analyse spectral perturbations between the Empirical Hessian and False Shepard. This study uses random matrix theory to derive an analysis of the eigenspectrum differences between the True Hesid and Fal-Emergency Hepard, which can be used to determine the perturbation between the two models."
"We introduce the Recurrent Discounted Unit which applies attention to any length sequence in linear time This study proposes the Recurrent Discounted Attention (RDA), an extension to Recurrent Weighted Average (RWA) by adding a discount factor. Extends the recurrent weight average to overcome the limitation of the original method while maintaining its advantage and proposes the method of using Elman nets as the base RNN","This article introduces a Recurrent Discounted Attention Unit, which extends the RWA by allowing it to discount the attention applied to previous timesteps. This work introduces an attention matrix for each encoded state and translated word combination in order to learn where in the sequence to attend to."
"How you should evaluate adversarial attacks on seq2seq The authors investigate ways of generating adversarial examples, showing that adversarial training with the attack most consistent with the introduced meaning-preservation criteria results in improved robustness to this type of attack without degradation in the non-adversarial setting. The study is about meaning-preserving adversarial perturbations in the context of Seq2Seq models","A new constraint for adversarial attacks on word-based MT systems, and a method of imbuing gradient-based word substitution attacks with simple constraints aimed at increasing the chance that the meaning is preserved. This work presents a novel defense technique for constrained words substitution attacks that replace one word in a sentence to maximize an adversarially loss function L adv, similar to the substitution attacks proposed in BID1."
Imposing graph structure on neural network layers for improved visual interpretability. A novel regularizer to impose graph structure upon hidden layers of a Neural Network to improve the interpretability of hidden representations. Highlights the contribution of graph spectral regularizer to the interpretability of neural networks.,"A new approach for generating human interpretable patterns in the latent representation obtained by hidden layers of neural networks. This work introduces a new set of regularizations that are defined in the spectral domain, rather than in the neuron domain, in order to directly enforce specral properties of the activation signal."
A framework for learning high-quality sentence representations efficiently. Proposes a faster algorithm for learning SkipThought-style sentence representations from corpora of ordered sentences that swaps the word-level decoder for a contrastive classification loss. This study proposes a framework for unsupervised learning of sentence representations by maximizing a model of the probability of true context sentences relative to random candidate sentences,"An autoencoder model that learns to reconstruct the surface form of a sentence, which forces the model to not only predict its semantics, but aspects that are irrelevant to the meaning of the sentence as well. This work introduces a novel de-noising autoencoding approach for identifying the correct context sentence instead of neighboring sentences. The authors present a framework for encoding sentence representations that learn sentence concatenation functions."
Provides an unbiased version of truncated backpropagation by sampling truncation lengths and reweighting accordingly. Proposes stochastic determination methods for truncation points in backpropagation through time. A new approximation to backpropagation through time to overcome the computational and memory loads that arise when having to learn from long sequences.,"ARTBP is experimentally compared to truncated BPTT. This study introduces a novel gradient estimation method that can be used for training sequences with short-term effects, but requires a reflection on the long-term effect. The authors present a method for recurrent learning algorithms, and show that it is possible to perform a linear gradient estimate using a parametric dynamical system instead of a priori."
"Exploration using Distributional RL and truncagted variance. Presents an RL method to manage exploration-explotation trade-offs via UCB techniques. A method to use the distribution learned by Quantile Regression DQN for exploration, in place of the usual epsilon-greedy strategy. Proposes new algorithsms (QUCB and QUCB+) to handle the exploration tradeoff in Multi-Armed Bendits and more generally in Reinforcement Learning",A quantitative estimation of the arm's UCB is performed via Hoeffdings Inequality 1 which is entirely based on counting the number of times the arm was pulled. This article presents a method to estimate quantiles for multi-armed bandits and RL by comparing them to asymmetric distributions. The authors present a quantitative estimation algorithm that estimates quantile numbers for each arm using the highest mean plus standard deviation.
"Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more. Describes the conditioned GAN model to generate speaker conditioned Mel spectra by augmenting the z-space corresponding to the identification This article proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitating fine-grained control over various attributes This study proposes a model that can control non-annotated attributes such as speaking style, accent, background noise, etc.",A new model for synthesis of clean speech using conditional auto-encoders. This work introduces a new language-token approach to synthesizing clean audio by using a room simulator to add background noise and reverberation to the synthesized speech. The authors study the problem of synthesising clean audio from a multi-speaker English corpus
"In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model as a surrogate to the abnormal distribution. Describes a novel approach to optimising the choice of kernel towards increased testing power and shown to offer improvements over alternatives.","A deep kernel parametrization framework which endows a data-driven kernel for the kernel two-sample test. This work presents a deep neural learning framework that optimizes kernel selection with very limited samples from the abnormal distribution Q. The authors introduce a method for optimizing kernel selection using surrogate distribution G to improve test power when samples from Q are insufficient, and show that optimization of kernels with only limited samples of Q can reduce performance degradation."
"A bottom-up algorithm that expands CNNs starting with one feature per layer to architectures with sufficient representational capacity. Proposes to dynamically adjust the feature map depth of a fully convolutional neural network, formulating a measure of self-resemblance and boosting performance. Introduces a simple correlation-based metric to measure whether filters in neural networks are being used effectively, as a proxy for effective capacity. Aims to address the deep learning architecture search problem via incremental addition and removal of channels in intermediate layers of the network.","A method to improve representational complexity of deep neural networks by adding features to existing architectures. This work presents a method for optimizing deep neural network models, which can be used to train large architectures with less than perchance expected computational overhead. The authors present a way to reduce the cost of training deep neural systems by adding additional features on top of a DNN optimization procedure, and show that it is possible to learn more complex sub-sampling functions."
"A noval GAN framework that utilizes transformation-invariant features to learn rich representations and strong generators. Proposes a modified GAN objective consisting of a classic GAN term and an invariant encoding term. This study presents the IVE-GAN, a model that introduces en encoder to the Generative Adversarial Network framework.","A novel GAN framework that tries to capture the distribution of a given dataset to map from an arbitrary latent space to new synthetic data points, and a discriminative model that attempts to distinguish between samples from the generator and the true data. This work introduces a new GAN architecture that extends the classical GAN structure by an additional encoding unit E to map samples from each individual sample x to the original source."
"Multi-view learning improves unsupervised sentence representation learning Approach uses different, complementary encoders of the input sentence and consensus maximization. The work presents a multi-view framework for improving sentence representation in NLP tasks using generative and discriminative objective architectures. This article shows that multi-view frameworks are more effective than using individual encoders for learning sentence representations.","A multi-view framework for learning sentence representations that outperforms existing unsupervised learning models. This work introduces a multi-vision framework to learn sentences from two views, and shows results on all unsupervised tasks. The authors present a model for learning sentences from sentences using a linear decoder and a non-loglinear model with a differentiable similarity function in each view."
"Unsupervised methods for finding, analyzing, and controlling important neurons in NMT This article proposes finding ""meaningful"" neurons in Neural Machine Translation models by ranking based on correlation between pairs of models, different epochs, or different datasets, and proposes a controlling mechanism for the models.","A new method for ranking neurons with linguistically similar directions. This work introduces a new algorithm to rank neural networks on language pairs, and shows that many of them capture common linguistic phenomena. The authors study the problem of ranking neurons in a single language pairs by using a SVCCA-based coder-decoder model, and show that most of them do not require any external supervision."
"We use graph co-attention in a paired graph training system for graph classification and regression. This study injects a multi-head co-attention mechanism in GCN that allows one drug to attend to another drug during drug side effect prediction. A method to extend graph-based learning with a co-attentional layer, which outperforms other previous ones on a pairwise graph classification task.","A graph neural network that receives pairs of graphs at a time, and extends it with a co-attentional layer that allows node representations to easily exchange structural information across them. This work introduces a new model for graph-level representation learning, which can be used to learn substructures from other inputs. The authors present a pairwise graph classification task in which the two representations of each other are represented by atoms."
Devising unsupervised defense mechanisms against adversarial attacks is crucial to ensure the generalizability of the defense. This work presents a method for detecting adversarial examples in a deep learning classification setting This study presents an unsupervised method for detecting adversarial examples of neural networks.,"This study introduces an end-to-end framework for unsupervised model assurance as well as defending against the adversaries. The authors present a novel attack approach to verify the legitimacy of data abstractions in each DL layer, and show that parallel checkpointing modules can be used to improve the performance of the victim model. This work presents a method for training defender modules to checkpoint the data representations of data points within the rarely explored regions of the data points."
"We show how using skip connections can make speech enhancement models more interpretable, as it makes them use similar mechanisms that have been explored in the DSP literature. The authors propose incorporating Residual, Highway and Masking blocks inside a fully convolutional pipeline in order to understand how iterative inference of the output and the masking is performed in a speech enhancement task The authors interpret highway, residual and masking connections. The authors generate their own noisy speech by artificially adding noise from a well established noise data-set to a less know clean speech data-set.","This paper presents a new model for speech enhancement using skip connections, which performs operations between the input and the output of each block. The authors show that skip connections do not necessarily improve performance with regards to the number of parameters, but they also make speech enhancement models more interpretable. This study studies the role of residual and highway connections in neural networks for speech denoising"
We propose the Exemplar Guided & Semantically Consistent Image-to-image Translation (EGSC-IT) network which conditions the translation process on an exemplar image in the target domain. Discusses a core failing and need for I2I translation models. The work explores the idea that an image has two components and applies an attention model where the feature masks that steer the translation process do not require semantic labels,This paper introduces a method to translate images from different styles in the target domain into images of different styles. The authors present an approach to translating images from a latent space to a single image in the source domain using Adaptive Instance Normalization (AdaIN) and a shared-latent space constraint that can be applied to the translation process.
A simple architecture consisting of convolutions and attention achieves results on par with the best documented recurrent models. A fast high performance paraphrasing based data augmentation method and a non-recurrent reading comprehension model using only convolutions and attention. This study proposes applying CNNs+self-attention modules instead of LSTMs and enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model in order to improve RC performance. This paper presents a reading comprehension model using convolutions and attention and propose to augment additional training data by paraphrasing based on off-the-shelf neural machine translation,"A feedforward model that consists of only convolutional and self-attention, discarding RNNs, which are used by most of the existing models. This work introduces a backtranslation model for reading comprehension with a combination of self attention and embedding layers to improve the performance of the training data. The authors present a new approach to the problem of refining an RNN model by using a non-convolutional neural network"
"We learn dense scores and dynamics model as priors from exploration data and use them to induce a good policy in new tasks in zero-shot condition. This work discusses zero shot generalization into new environments, and proposes an approach with results on Grid-World, Super Mario Bros, and 3D Robotics. A method aiming to learn task-agnostic priors for zero-shot generalization, with the idea to employ a modeling approach on top of the model-based RL framework.","A method that learns to move to the right in the upper level of Figure 1 would fail to transfer the priors to the lower level and further play the game in the new level because change of configurations and background. This work presents a method for learning a trajectory from a grid-world example to a robotics Blocked-Reach environment, which can outperform baseline methods on a wide range of applications including gridworld."
An autoregressive deep learning model for generating diverse point clouds. An approach for generating 3D shapes as point clouds which considers the lexicographic ordering of points according to coordinates and trains a model to predict points in order. The study introduces a generative model for point clouds using a pixel RNN-like auto-regressive model and an attention model to handle longer-range interactions.,"A self-attention module that captures the long-range dependencies between points, helping to generate plausible part configurations within 3D objects. This work introduces a self attention module for modeling point clouds, which can be used in conjunction with an unconditional model. The authors present a Self-attention framework for generating representations of point clouds by using a discrete coordinate and a convolutional layer layer to create a coherent representation of the point cloud."
"Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies. The authors show that elimination singularities and overlap singularities impede learning in deep neural networks, and demonstrate that skip connections can reduce the prevalence of these singularities, speeding up learning. Paper examines the use of skip connections in deep networks as a way of alleviating singularities in the Hessian matrix during training.","This work introduces a hyper-residual architecture that introduces identity skip connections between adjacent layers and all layers above it. This study investigates the elimination, overlap and linear dependence singularities of the hidden units in the same layer, and how they can be used to improve the training accuracy. The authors present a nonlinearity network with no skip connections, and show that there is a direct correlation between elimination and linear dependency singularities."
"We formulated SGD as a Bayesian filtering problem, and show that this gives rise to RMSprop, Adam, AdamW, NAG and other features of state-of-the-art adaptive methods The paper analyzes stochastic gradient descent through Bayesian filtering as a framework for analyzing adaptive methods. The authors attempt to unify existing adaptive gradient methods under the Bayesian filtering framework with the dynamical prior",An adaptive algorithm for neural network optimization that uses Bayesian filtering to optimize parameters of BID23 BID30 BID24 BID25 BID7 BID22. This work introduces a novel method for optimizing neural networks by combining the state-of-the-art adaptive SGD algorithms with bayesian filters and natural gradient variational inference.
"We show that, in continual learning settings, catastrophic forgetting can be avoided by applying off-policy RL to a mixture of new and replay experience, with a behavioral cloning loss. Proposes a particular variant of experience replay with behavior cloning as a method for continual learning.","A continuul learning system that learns new skills that are related to old ones faster than it would have de novo, a property known as constructive interference or positive transfer. This study presents a novel method for reducing catastrophic forgetting on sequential training tasks, and introduces a new strategy to reduce the catastrophic forgett. The authors address the problem of continual learning by using experience replay buffers to minimize the impact of rare experiences in RL."
"Our variational-recurrent imputation network (V-RIN) takes into account the correlated features, temporal dynamics, and further utilizes the uncertainty to alleviate the risk of biased missing values estimates. A missing data imputation network to incorporate correlation, temporal relationships, and data uncertainty for the problem of data sparsity in EHRs, which yields higher AUC on mortality rate classification tasks. The work presented a method that combines VAE and uncertainty aware GRU for sequential missing data imputation and outcome prediction.","A deep generative model to estimate the missing values and the uncertainty in the latent space, yielding the uncertainty decay factor. This work introduces a new method for estimating the uncertain information by using a recurrent imputation network as a secondary problem but major concern in the task performance. The authors use a deep-generative model combined with a robust generative network to extract the uncertainty of the uncertainty and the temporal dynamics together."
"In a deep convolutional neural network trained with sufficient level of data augmentation, optimized by SGD, explicit regularizers (weight decay and dropout) might not provide any additional generalization improvement. This article proposes data augmentation as an alternative to commonly used regularisation techniques, and shows that for a few reference models/tasks that the same generalization performance can be achived using only data augmentation. This study presents a systematic study of data augmentation in image classification with deep neural networks, suggesting that data augmentation can replicit some common regularizers like weight decay and dropout.","A systematic analysis of the impact of data augmentation on deep neural networks compared to the most popular regularization techniques. This work presents a method for analyzing the role of explicit regularization in deep neural network architectures. The authors present an analysis of how data hausse adapts to different architectures of different depths of the network, and show that it can be used as a substitute for explicitly regularization."
"LEAP combines the strength of adaptive sampling with that of mini-batch online learning and adaptive representation learning to formulate a representative self-paced strategy in an end-to-end DNN training protocol. Introduces a method for creating mini batches for a student network by using a second learned representation space to dynamically select examples by their 'easiness and true diverseness'. Experiments the classification accuracy on MNIST, FashionMNIST, and CIFAR-10 datasets to learn a representation with curriculum learning style minibatch selection in an end-to-end framework.","A framework for self-paced learning with Adaptive Pace that learns when to introduce certain samples to the DNN during training. This work presents a framework for learning sample selection in a mini-batch setting using easiness and true diverseness as sample importance priors. The authors present a method for learning representations from multiple clusters of clusters, including CNNs and CNNs."
"We introduce an embedding space approach to constrain neural network output probability distribution. This study introduces a method to perform semi-supervised learning with deep neural networks, and the model achieves relatively high accuracy, given a small training size. This article incorporates label distribution into model learning when a limited number of training instances is available, and proposes two techniques for handling the problem of output label distribution being wrongly biased.",A novel algorithm that jointly optimizes output probability distribution on a clustered embedding space to make neural networks draw effective decision boundaries. This work presents a novel algorithm for learning the decision boundary of a multilayer perceptron (MLP) classifier with limited examples. The authors investigate the problem of learning the choice boundary from a limited number of labels in a hidden layer of an MLP trained with 50 labelled MNIST data.
A comparison of five deep neural network architectures for detection of malicious domain names shows surprisingly little difference. Authors propose using five deep architectures for the cybersecurity task of domain generation algorithm detection. Applies several NN architectures to classify url's between begign and malware related URLs. This paper proposes to automatically recognize domain names as malicious or benign by deep networks trained to directly classify the character sequence as such.,"A deep neural network architecture that can detect DGAs without the need to reverse engineer new, emerging malware families. This work introduces a deep-recurrent neural network classifier for DGA detection which uses an embedding layer to detect malicious and benign domain names in order to avoid overfitting. The authors present a long-term recurrent model of LSTMs that uses a bidirectional GRU algorithm to improve model performance."
A novel approach to maintain orthogonal recurrent weight matrices in a RNN. Introduces a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This article suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. Novel parametrization of RNNs allows representing orthogonal weight matrices relatively easily.,"A new activation function for orthogonal or unitary recurrent neural networks. This work introduces a method to improve the performance of the modReLU over other activation functions, such as ReLU. The authors study the problem of scaling the recurrence matrix by scaling the diagonal matrix by a negative one eigenvalue and using a gradient descent algorithm to reduce the complexity of the gradient descent steps."
"Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization. The work discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates The authors analyze training of residual networks using large cyclic learning rates, and demonstrate fast convergence with cyclic learning rates and evidence of large learning rates acting as regularization.","A super-convergent approach for training with very large learning rates, which adds noise in the middle part of training. The authors show that using CLR with very small learning rates can speed up training by an order of magnitude. This work presents a new method to improve the generalization performance of SGD by reducing the noise level and improving the performance of the training rate."
"A Bayesian Nonparametric Topic Model with Variational Auto-Encoders which achieves the state-of-the-arts on public benchmarks in terms of perplexity, topic coherence and retrieval tasks. This study constructs an infinite Topic Model with Variational Auto-Encoders by combining Nalisnick & Smith's stick-breaking variational auto-encoder with latent Dirichlet allocation and several inference techniques used in Miao.","A Bayesian nonparametric topic model equipped with a Variational Auto-Encoders (ITM-VAE). This work introduces an infinite Topic Model, which uses a stick-breaking process BID35 to generate the mixture weights for a countably infinite set of topics and can grow the number of parameters with the amount of training data."
"Finetuning after quantization matches or exceeds full-precision state-of-the-art networks at both 8- and 4-bit quantization. This article proposes to improve the performance of low-precision models by doing quantization on pre-trained models, using large batches size, and using proper learning rate annealing with longer training time. A method for low bit quantization to enable inference on efficient hardware that achieves full accuracy on ResNet50 with 4-bit weights and activations, based on observations that fine-tuning at low precision introduces noise in the gradient.",A new state-of-the-art method for training low-precision networks with integer constraints. Presents a new state of the art method to combat gradient noise introduced by weight quantization. This work presents a novel method for learning low precision networks that exceeds the accuracy of the full-presence baseline networks after just one epoch of fine-tuning.
"Translating portions of the input during training can improve cross-lingual performance. The paper proposes a cross-lingual data augmentation method to improve the language inference and question answering tasks. This work proposes to augment crosslingual data with heuristic swaps using aligned translations, like bilingual humans do in code-switching.","A cross-lingual data augmentation method that replaces a segment of the input text with its translation in a way that each example is solely in one language. The authors introduce a new algorithm for multilingual word embeddings, which can be used to train examples in multiple languages without having to explicitly further align them. This study studies the effectiveness of cross-linguistic neural networks as cross-language augmentors"
new GNN formalism + extensive experiments; showing differences between GGNN/GCN/GAT are smaller than thought The article proposes a new Graph Neural Network architecture that uses Feature-wise Linear Modulation to condition the source-to-target node message-passing based on the target node representation.,A GNN-FiLM model for molecular graphs that outperforms baseline methods on a regression task and performs competitively on other tasks. This work introduces a new approach for learning to ignore graph edges by replacing the learnable parameters of the model with an edge-type-dependent weight based on the message transformation of target nodes.
"We propose Hierarchical Complement Objective Training, a novel training paradigm to effectively leverage category hierarchy in the labeling space on both image classification and semantic segmentation. A method that regularizes the entropy of the posterior distribution over classes which can be useful for image classsification and segmentation tasks",An explicit hierarchical model that is trained to penalize the obviously wrong classes at different granularity levels. This work introduces a training paradigm for deep neural models using an objective and a complement objective to leverage information from a label hierarchy. The study presents a method of training neural models with an objective that focuses on maximizing the probability value of the ground truth and the parental category of ground truth in order to minimize the predicted probability of the wrong class.
We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. The study proposes a new model to use deep models for detecting logical entailment as a product of continuous functions over possible worlds. Proposes a new model designed for machine learning with predicting logical entailment.,"This article introduces a new model architecture that can capture invariance without needing to understand the structure of the problem. The authors present a method for generating propositional variables, which is useful for solving syntactic problems. This work presents a framework for entailment modeling using propositional logic, and discusses the problem of generating representations by using LSTM RNNs as a base-to-linear model."
"We show that training feedforward relu networks with a weak regularizer results in a maximum margin and analyze the implications of this result. Studies margin theory for neural sets and shows that max margin is monotonically increasing in size of the network This study studies the implicit bias of minimizers of a regularized cross entropy loss of a two-layer network with ReLU activations, obtaining a generalization upper bound which does not increase with the network size.","A new method for understanding the generalization of two-layer relu networks. This work introduces a new way to explain the regularization of neural networks by making it explicit. The authors present a method for proving the normalization in two layer relu network with quadratic activations, and show that there is an analogy between the two layers of the relu feature and the standard kernel SVM with relu features."
"From an incomplete RGB-D scan of a scene, we aim to detect the individual object instances comprising the scene and infer their complete object geometry. Proposes an end-to-end 3D CNN structure which combines color features and 3D features to predict the missing 3D structure of a scene from RGB-D scans. The authors propose a novel end-to-end 3D convolutional network which predicts 3D semantic instance completion as object bounding boxes, class labels and complete object geometry.",An end-to-end approach that outperforms state-of-the-art scan completion on SUNCG as well as on real-world ScanNet benchmarks. This work introduces a new method for predicting image segmentation and instance segmentation in the context of a scene by using a convolution layer to output objectness scores for each anchor.
"We propose Convolutional CRFs a fast, powerful and trainable alternative to Fully Connected CRFs. The authors replace the large filtering step in the permutohedral lattice with a spatially varying convolutional kernel and show that inference is more efficient and training is easier. Proposes to perform message passing on a truncated Gaussian kernel CRF using a defined kernel and parallelized message passing on GPU.","A new framework for fully-connected CRFs, which can be implemented efficiently on GPUs. This work introduces a new framework of fully connected CRF with a Gaussian kernel that can be used to improve the performance of CNNs. The authors address the problem of learning a large part of the inference step to be a convolution with truncated gaussian core"
"Word2net is a novel method for learning neural network representations of words that can use syntactic information to learn better semantic features. This paper extends SGNS with an architectural change from a bag-of-words model to a feedforward model, and contributes a new form of regularization by tying a subset of layers between different associated networks. A method to use non-linear combination of context vectors for learning vector representation of words, where the main idea is to replace each word embedding by a neural network.","A method to compute syntactic similarities between the neural network representations of the words. This work introduces a new neural network that can capture semantic properties of the word, but they tend to neglect most of it. The authors show that parameter sharing in word2net performs better than applying word2vec or standard Benoulli embeddings on the augmented vocabulary of word/tag pairs."
Comparison of psychophysical and CNN-encoded texture representations in a one-class neural network novelty detection application. This paper focuses on novelty detection and shows that psychophysical representations can outperform VGG-encoder features in some part of this task This work considers detecting anomalies in textures and proposes original loss function. Proposes training two anomaly detectors from three different models to detect perceptual anomalies in visual textures.,This work introduces a novel objective function to train one-class neural networks for novelty detection in visual surface inspection applications. The authors introduce a new objective function that explicitly learns an hyperplane separating reference data and data with the same image statistics. This work presents a model for visual surface inspectors using a neural network to detect visual texture anomalies by comparing features of a pretrained CNN.
Human-like Clustering with CNNs The work validates the idea that deep convolutional neural networks could learn to cluster input data better than other clustering methods by noting their ability to interpret the context of every input point due to a large field of view. This paper combines deep learning for feature representation with the task of human-like unsupervised grouping.,"A deep learning model for clustering and a hierarchical framework that progressively builds complex patterns on top of the simpler ones. This work explores the compositionality of real-world structures and objects in a clustering process, and suggests a new approach to clustering algorithms. The authors address the problem of clustering in neural networks by proposing a differentiating the structure of a large dataset of unlabeled patterns into meaningful clusters."
"We propose tensor contraction and low-rank tensor regression layers to preserve and leverage the multi-linear structure throughout the network, resulting in huge space savings with little to no impact on performance. This paper proposes new layer architectures of neural networks using a low-rank representation of tensors This work incorporates tensor decomposition and tensor regression into CNN by using a new tensor regression layer.",A deep convolutional neural network with a low-rank multi-linear mapping from a high-order activation tensor to the softmax layer. This work introduces a deep recognising neural network (CNN) which expresses the output of a neural network as a lower-rank Multi-Linear Mapping between the inputs and the activations of convolution layers.
"A novel network architecture to perform Deep 3D Zoom or close-ups. A method for creating a ""zoomed image"" for a given input image,and a novel back re-projection reconstruction loss that allows the network to learn underlying 3D structure and maintain a natural appearance. An algorithm for synthesizing 3D-zoom behavior when the camera is moving forward, a network structure incorporating disparity estimation in a GANs framework to synthesize novel views, and a proposed new computer vision task.","An unsupervised framework for learning a 3D-zoom dataset of natural scenes due to the need for special equipment to ensure camera movement is restricted to the Z-axis. This study introduces a novel view synthesis problem for multiple input image scenarios, using an off-the-shelf structure from motion algorithm to obtain the camera pose and fixed background points of a given video sequence in combination with traditional optimization techniques to directly estimate the warping operation."
"We introduce the universal deep neural network compression scheme, which is applicable universally for compression of any models and can perform near-optimally regardless of their weight distribution. Introduces a pipeline for network compression that is similar to deep compression and uses randomized lattice quantization instead of the classical vector quantization, and uses universal source coding (bzip2) instead of Huffman coding.","A universal lossy compression framework consisting of universal quantization and universal lossless source coding such as LempelZiv-Welch BID13 BID14 BID15 and Burrows-Wheeler transform BID16 BID17. This study introduces a universal DNN framework for entropy coded vector quantization, where the rate is infinity and the distortion diminishes."
"A quantitative refinement of the universal approximation theorem via an algebraic approach. The authors derive the universal approximation property proofs algebraically and assert that the results are general to other kinds of neural networks and similar learners. A new proof of Leshno's version of the universal approximation property for neural networks, and new insights into the universal approximation property.","A neural network with a hidden layer that can compute a certain class of functions: R n  R m, where  =  W is the class of non-linear functions. This study introduces a neural network to compute non-bias weights in the first layer, and shows that it holds if they are not fixed and randomly chosen from a suitable range."
"We propose an easy method to train Variational Auto Encoders (VAE) with discrete latent representations, using importance sampling Introducting an importance sampling distribution and using samples from distribution to compute importance-weighted estimate of the gradient This work proposes to use important sampling to optimize VAE with discrete latent variables.","The VAE is an easy latent variable model, where the observations x  p(x|z) are dependent on latent variables z  P(z). This work introduces a method to train VAEs using stochastic gradient descent on a training data set, which they call Stochastic Gradient Variational Bayes (SGVB)."
"Linking Wasserstein-trust region entropic policy gradients, and the heat equation. The paper explores the connections between reinforcement learning and the theory of quadratic optimal transport The authors studied policy gradient with change of policies limited by a trust region of Wasserstein distance in the multi-armed bandit setting, showing that in the small steps limit, the policy dynamics are governed by the heat equation (Fokker-Planck equation).","A regularization term for policy iteration that aims at finding a sequence of policies converging towards the optimal policy  *. This study introduces a new method for iterating policy gradients in the context of entropy-regularized rewards, which can be used as a regularizing term to improve policy diffusion and avoid early convergence to suboptimal policies."
We introduce a scale-invariant neural network architecture for changepoint detection in multivariate time series. The work leverages the concept of wavelet transform within a deep architecture to solve change point detection. This work proposes a pyramid based neural net and applies it to 1D signals with underlying processes occurring at different time scales where the task is change point detection,"A deep neural network architecture that can efficiently identify abrupt and gradual changes at multiple scales. This work introduces a deep neural networks architecture to detect abrupt and gradually changes in real-world datasets. The authors show that changepoint detection can be treated as a supervised learning problem, and demonstrate that the proposed architecture can encode short-term and long-term temporal patterns and can detect from abrupt to extremely gradual changes over a wide range of timescales."
"A novel graph signal processing framework for quantifying the effects of experimental perturbations in single cell biomedical data. This paper introduces several methods to process experimental results on biological cells and proposes a MELD algorithm mapping hard group assignments to soft assignments, allowing relevant groups of cells to be clustered.","A novel clustering algorithm for cell states that are prototypical of experimental or control conditions. This work introduces a clustering approach for learning the EES of TCR activation by filtering the noisy categorical experimental label in the graph frequency domain to recover a smooth signal with continuous values. The authors study the differences between experimental and control conditions, and proposed a new clustering method for identifying cells most or least affected by an experimental perturbation."
"We prove a non-convex convergence rate for the sign stochastic gradient method. The algorithm has links to algorithms like Adam and Rprop, as well as gradient quantisation schemes used in distributed machine learning. Provided a convergence analysis of Sign SGD algorithm for non-covex cases The work explores an algorithm that uses the sign of the gradients instead of actual gradients for training deep models","Using stochastic gradient evaluations to improve the performance of signSGD. This work introduces a method for avoiding saddle points where the objective function has a lower bound, which means that it is possible to avoid large gradient steps downhill indefinitely. The authors investigate the problem of escaping saddle points with a higher bound than gradient descent, and show that there is a better way to do this than SGD on a regular basis."
"Existing pruning methods fail when applied to GANs tackling complex tasks, so we present a simple and robust method to prune generators that works well for a wide variety of networks and tasks. The authors propose a modification to the classic distillation method for the task of compressing a network to address the failure of previous solutions when applied to generative adversarial networks.",A self-supervised GAN compression method for generative training of a classification network. This work introduces a new discriminator that is already well trained on the target data set and can be used to train a discriminator to make the generator behave more like the original generator suffers from this issue. The authors use a GAN to guide pruning in order to achieve compelling compression rates with little change in the quality of the compressed generator's ouput.
We propose to solve a problem of simultaneous classification and novelty detection within the GAN framework. Proposes a GAN to unify classification and novelty detection. The work presents a method for novelty detection based on a multi-class GAN which is trained to output images generated from a mixture of the nominal and novel distributions. The paper proposes a GAN for novelty detection using a mixture generator with feature matching loss,"A GAN-based mixture discriminator that generates samples from a mixture of nominal and novel data distributions is the optimal novelty detector. This work presents a method for semi-supervised classification where only a small fraction of real examples have labels, and the bulk of the real data is unlabeled. The authors introduce a GAN framework to train a multi-class discriminator which is capable of generating samples from the unknown distribution."
"A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks and an inductive protein-protein interaction task. This article proposes a new method for classifying nodes of a graph, which can be used in semi-supervised scenarios and on a completely new graph. The study introduces a neural network architecture to operate on graph-structured data named Graph Attention Networks. Provides a fair and almost comprehensive discussion of the state of art approaches to learning vector representations for the nodes of a graph.","A new attention architecture for inductive learning that uses a single-layer feedforward neural network, and is directly applicable to tasks where the model has to generalize to completely unseen graphs. This work introduces a new approach to self-attention by introducing a multi-head approach to the use of a shared attention mechanism to all edges of the graph."
A formal method's approach to skill composition in reinforcement learning tasks The article combines RL and constraints expressed by logical formulas by setting up an automation from scTLTL formulas. Proposes a method that helps to construct policy from learned subtasks on the topic of combining RL tasks with linear temporal logic formulas.,"A composite policy that achieves the AN D task composition (the composite policy maximizes the average reward of individual tasks) with little to no constraints on task distribution at learning time. This work introduces a new strategy for multi-task learning, which can be achieved by using energy-based model DISPLAYFORM1 where the energy function is represented by a function approximator."
"We created a new dataset for data interpretation over plots and also propose a baseline for the same. The authors propose a pipeline to solve the DIP problem involving learning from datasets containing triplets of the form {plot, question, answer} Proposes an algorithm that can interpret data shown in scientific plots.","A multi-staged modular framework with various sub-components to extract relevant data from the plot and convert it to a semi-structured table. This work introduces a novel test set for question answering using a plot of triplets of the form plot, question, answer as well as a new dataset which contains plots generated from synthetic data with limited (i) $x-y$ axes variables."
Combine information between pre-built word embedding and task-specific word representation to address out-of-vocabulary issue This study proposes an approach to improve the out-of-vocabulary embedding prediction for the task of modeling dialogue conversations with sizable gains over the baselines. Proposes combining external pretrained word embeddings and pretrained word embeddings on training data by keeping them as two views. Proposes method to extend the coverage of pre-trained word embeddings to deal with the OOV problem that arises when applying them to conversational datasets and applies new variants of LSTM-based model to the task of response-selection in dialogue modeling.,A deep neural network model for addressing out-of-vocabulary word issues that can be used to enrich word representations. This study introduces a method to combine pre-trained word embedding vectors with those generated on a general text corpus and the existing model with structured knowledge from semantic network ConceptNet BID25 and merge them into a common representation BID24.
Acquire states from high frequency region for search-control in Dyna. The authors propose to do sampling in the high-frequency domain to increase the sample efficiency This article proposes a new way to select states from which do do transitions in dyna algorithm.,"A method to locally measure the frequency of a function's point in the domain and provide a theoretical justification for this approach. This study introduces a method to localize the number of samples required for its reconstruction in an online RL setting. The authors present a new strategy to locally estimate the frequency on a state space by using a gradient norm, which can be used to determine how much it is needed to learn from the state space."
"We perform large scale experiments to show that a simple online variant of distillation can help us scale distributed neural network training to more machines. Proposes a method to scale distributed training beyond the current limits of mini-batch stochastic gradient descent Proposal for an online distillation method called co-distillation, applied at scale, where two different models are trained to match predictions of the other model in addition to minimizing its own loss. Online distillation technique is introduced to accelerate traditional algorithms for large-scaled distributed neural network training","Using ensemble distillation to reduce prediction churn in training neural networks. This study studies the reproducibility benefits of codistillation, and shows that it does not lose the reproducibility benefits of ensembles of neural networks, reducing churin in the predictions of different retrains of the same model. The authors present a novel method for reducing prediction loss by using stale predictions instead of up-to-date predictions."
"Our approach is the first attempt to leverage a sequential latent variable model for knowledge selection in the multi-turn knowledge-grounded dialogue. It achieves the new state-of-the-art performance on Wizard of Wikipedia benchmark. A sequential latent variable model for knowledge selection in dialogue generation that extends the posterior attention model to the latent knowledge selection problem and achieves higher performances than previous state-of-the-art models. A novel architecture for selecting knowledge-grounded multi-turn dialogue that yields state of the art on relevant benchmarks datasets, and scores higher in human evaluations.",A sequential latent variable model for multi-turn knowledge-grounded dialogue. This work presents a novel approach to multi-open-domain knowledge-based dialogue by decomposing it into two sub-problems: first selecting knowledge from a large pool of candidates and generating a response based on the selected knowledge and context. The authors present a method for understanding the history of knowledge selection with and without knowing the response in previous turns.
"Neural Network Verification for Temporal Properties and Sequence Generation Models This work extends interval bound propagation to recurrent computation and auto-regressive models, introduces and extends Signal Temporal Logic for specifying temporal contraints, and provides proof that STL with bound propagation can ensure neural models conform to temporal specification. A way to train time-series regressors verifiably with respect to a set of rules defined by signal temporal logic, and work in deriving bound propagation rules for the STL language.","A new verification method for deep neural networks that performs well in terms of test error or reward. This study introduces a novel verification method to verify the robustness of neural networks, and uses it as a specification language to provide guarantees on temporal specifications. The authors consider the problem of learning a trace-value function to verifiably satisfy a specific specification of the form x  S."
We propose to use explicit vector algebraic formulae projection as an alternative way to visualize embedding spaces specifically tailored for goal-oriented analysis tasks and it outperforms t-SNE in our user study. Analysis of embedding psaces in a non-parametric (example-based_ way,"A better understanding of the embedded space may lead to critical insights in improving such models. This study presents a novel way to learn how embeddings relate to each other with respect to dimensions of variability, and shows that it's possible to use a set of axes to identify differences between the two dimensions of variation. The authors present a method for learning a wide range of variables in a large-scale dataset"
"a joint model and gradient sparsification method for federated learning Applies variational dropout to reduce the communication cost of distributed training of neural networks, and does experiments on mnist, cifar10 and svhn datasets. The authors propose an algorithm that reduces communication costs in federated learning by sending sparse gradients from device to server and back. Combines distributed optimization algorithm with variational dropout to sparsify the gradients sent to master server from local learners.",An efficient federated learning framework that meets both communication and communication constraints. This work introduces a method to jointly learn a sparse model while reducing the amount of gradients exchanged during the iterative training process. The authors present a novel method for sparsifying deep neural networks by pruning the weights whose learned dropout rates are high while still achieving comparable predictive accuracies compared to the unpruned ones.
"Discrete transformer which uses hard attention to ensure that each step only depends on a fixed context. This paper presents modifications to the standard transformer architecture with the goal of improving interpretability while retaining performance in NLP tasks. This article proposes three Discrete Transformers: a discrete and stochastic Gumbel-softmax based attention module, a two-stream syntactic and semantic transformer, and sparsity regularization.","An approach for learning a discrete attention mechanism by modifying the attention mechanism and objective function to improve model interpretability. This study introduces a novel approach to learn a neural network model using attention as a categorical latent variable and a ""syntactic"" semantic variable to get discrete decisions. The authors present a method for learning more discrete inputs in a machine translation dataset"
We propose a subset selection algorithm that is trainable with gradient based methods yet achieves near optimal performance via submodular optimization. Proposes a neural network based model that integrates submodular function by combining gradient based optimization technique with submodular framework named 'Differentiable Greedy Network' (DGN). Proposes a neural network that aims to select a subset of elements (e.g. selecting k sentences that are mostly related to a claim from a set of retrieved docs),"A deep learning technique that combines the two extremes of generative models and deep learning models. This work presents deep unfolding BID9, a method to derive novel network architectures that are interpretable as inference algorithms by turning them into layers of a network. The authors present a deep learning approach for deep learning with an emphasis on the importance of modeling dependence between sentences and submodular objective functions."
"We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs. The paper proposes a regularization term for the conditional GAN objective in order to promote diverse multimodal generation and prevent mode collapse. The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction, which can be applied to various conditional synthesis frameworks for various tasks.","A Generative Adversarial Network that learns both conditional generator G and discriminator D by optimizing the adversarial objective of the generator. This work introduces a multi-modal mapping method for mode-collapse, which can be used to solve the mode colapse problem in many conditional generative tasks, especially for high-dimensional inputs and outputs."
"A method that trains large capacity neural networks with significantly improved accuracy and lower dynamic computational cost A method to train a network with large capacity, only parts of which are used at inference time dependent on input, using fine-grained conditional selection and a new method of regularization, ""batch shaping.""","This work introduces a new batch-shaping loss that can be used to train neural networks with very large capacity while keeping the computational overhead small. The authors present a method for training neural networks without the need for sparsification by using a feed-forward design of two fully connected layers, with only 16 neurons in the hidden layer. This study studies the effectiveness of batch-Shaping losses in deep ResNet-BAS networks"
"We diagnose deep neural networks for 3D point cloud processing to explore the utility of different network architectures. The article investigates different neural network architectures for 3D point cloud processing and proposes metrics for adversarial robustness, rotational robustness, and neighborhood consistency.","A method for calculating the utility of different network architectures. This work presents a method for learning an intermediate layer feature that is discarded during the computation of a 3D point cloud. The authors address the problem of determining how much information is discarded during computation of an intermediate-layer feature, and show that it is possible to learn a new orientation-aware feature for each point in N(i)."
"Optimized gated deep learning architectures for sensor fusion is proposed. The authors improve upon several limitations of the baseline negated architecture by proposing a coarser-grained gated fusion architecture and a two-stage gated fusion architecture Proposes two gated deep learning architectures for sensor fusion and by having the grouped features, demonstrates improved performance, especially in the presence of random sensor noise and failures.","A new coarser-grained gated architecture which learns robustly a set of fusion weights at the (feature) group level, leading to further performance improvements. This work introduces a two-stage gating architecture that exploits both the feature-level and group-level fusionweights to address the limitations of the negated architecture in terms of inconsistency and lack of diverse fusion mechanisms."
We present Evolutionary EM as a novel algorithm for unsupervised training of generative models with binary latent variables that intimately connects variational EM with evolutionary optimization The paper presents a combination of evolutionary computation and variational EM for models with binary latent variables represented via a particle-based approximation The work makes an attempt to tightly integrate expectation-maximization training algorithms with evolutionary algorithms.,A novel probabilistic generative model with binary hidden variables that can be used for the variational optimization loop. This work introduces a novel method for learning probabilistic models with binary-hidden variables and shows that evolutional algorithms can be applied to the latent states as genomes of individuals. This study studies the applicability and scalability of probability generative models using variational expectation maximization (E-step).
"We closely analyze the VAE objective function and draw novel conclusions that lead to simple enhancements. Proposes a two-stage VAE method to generate high-quality samples and avoid blurriness. This article analyzes the Gaussian VAEs. The work provides a number of theoretical results on ""vanilla"" Gaussian Variational Auto-Encoders, which are then used to build a new algorithm called ""2 stage VAEs"".","A VAE pipeline that can produce stable FID scores, an influential recent metric for evaluating generated sample quality BID16. This work introduces a VAE model with low-dimensional representations that performs well in the context of high-dimensional space. The authors present a novel VAE parameterization framework that generates reliable FIDs, and show that it is possible to achieve the global optimum by using a Gaussian assumption."
"We describe an end-to-end differentiable model for QA that learns to represent spans of text in the question as denotations in knowledge graph, by learning both neural modules for composition and the syntactic structure of the sentence. This study presents a model for visual question answering that can learn both parameters and structure predictors for a modular neural network, without supervised structures or assistance from a syntactic parser. Proposes for training a question answering model from answers only and a KB by learning latent trees that capture the syntax and learn the semantic of words","A neural network approach for question answering with a span of text that is represented by a denotation in a knowledge graph, and a vector that captures ungrounded aspects of meaning. This work explores the problem of question answering using a tree of interpretable expressions, and uses a recurrent neural network to build a representation for a question sentence."
"Couple the GAN based image restoration framework with another task-specific network to generate realistic image while preserving task-specific features. A novel method of Task-GAN of image coupling that couples GAN and a task-specific network, which alleviates to avoid hallucination or mode collapse. The authors propose to augment GAN-based image restoration with another task-specific branch, such as classification tasks, for further improvement.","A new task-based image restoration framework for realizing and preserving information important to the downstream tasks. This work introduces a task-driven loss network to ensure both visually plausible and more accurate (medical/face) image restoration. The authors present a method for super-human level automatic classification/diagnosis using GANs, which provides a generalization of the proposed method for image restoration in vivo clinical medical imaging datasets."
"Make deep reinforcement learning in large state-action spaces more efficient using structured exploration with deep hierarchical policies. A method to coordinate agent behaviour by using policies that have shared latent structure, a variational policy optimization method to optimize the coordinated policies, and a derivation of the authors' variational, hierarchical update. This study suggests an algorithmic innovation consisting of hierarchical latent variables for coordinated exploration in multi-agent settings","A structured probabilistic policy class that uses a hierarchy of stochastic latent variables to train the policy end-to-end. This work presents a method for learning multi-agent environments that explicitly require team coordination, and feature competitive pressures that are characteristic of many coordinated decision problems. The authors present a model for multi-Agent environments with a large number of agents, and show that learned latent structures correlate with meaningful co-ordination patterns."
"Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness This article presents an adaptation of the algorithmic robustness of Xu&Mannor'12 and presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error. Proposes a article of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness and gives bounds on generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice. The work studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context",A deep learning algorithm that is robust to perturbed samples. This work presents a robustness approach to the generalization of deep neural networks by minimizing the expected lower bound on the variational free energy. The authors present a deep learning model with robustness and a method to learn a probability distribution on the weights of a neural network using backpropagation to minimize the expected low bound for the variable free energy in order to improve performance.
"An architecture for tabular data, which emulates branches of decision trees and uses dense residual connectivity This article proposes deep neural forest, an algorithm which targets tabular data and integrates strong points of gradient boosting of decision trees. A novel neural network architecture mimicking how decision forests work to tackle the general problem of training deep models for tabular data and showcasing effectiveness on par with GBDT.","A novel deep neural network architecture that combines decision trees and dense residual connections. This work introduces a new deep neural forests architecture which combines the decision trees with dense residual links. The authors present a deep neural model architecture for tabular data that is intrinsically interpretable, as if it were a conventional decision tree induction algorithm. This study presents a method for building neural models that achieve performance on the level of GBDTs."
"An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability The authors formulate training NNs as finding an optimal controller for a discrete dynamical system, allowing them to use method of successive approximations to train a NN in a way to be more robust to adversarial attacks. This article uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness.","A dynamical system view on the adversarial robustness of the models, and a new method that significantly defenses adversarially attacks. This work introduces a dynamical network view for deep neural networks that is robust to all (adversarial) perturbations. The authors present an optimization problem for Hamiltonian maximization, which can be solved efficiently by adding constraints in gradient descent based algorithms."
"Workers send gradient signs to the server, and the update is decided by majority vote. We show that this algorithm is convergent, communication efficient and fault tolerant, both in theory and in practice. Presents a distributed implementation of signSGD with majority vote as aggregation.","A mini-batch convergence algorithm for distributed SGD with hundreds of millions of parameters. This work introduces a new algorithm that can be used to improve the performance of large neural networks. The study presents a method for reducing the computational cost of distributed SIGNSGD by 25%, resulting in a small loss in generalisation and a reduction in the number of simultaneous parallels between the two algorithms."
"Few-shot learning PixelCNN The study proposes on using density estimation when the availability of training data is low by using a meta-learning model. This study considers the problem of one/few-shot density estimation, using metalearning techniques that have been applied to one/few-shot supervised learning The work focuses on few shot learning with autoregressive density estimation and improves PixelCNN with neural attention and meta learning techniques.","This work introduces a meta-learning framework for few-shot autoregressive models, which can be used to train neural networks from scratch or fine-tuning from scratch. The authors present a new set of self-regressive model models that can be trained with attention on a wide range of target distributions, and show how attention can improve performance in language tasks."
"we proposed a new self-driving model which is composed of perception module for see and think and driving module for behave to acquire better generalization and accident explanation ability. Presents a multitask learning architecture for depth and segmentation map estimation and the driving prediction using a perception module and a driving decision module. A method for a modified end-to-end architecture that has better generalization and explanation ability, is more robust to a different testing setting, and has decoder output that can help with debugging the model. The authors present a multi-task convolutional neural network for end-to-end driving and provide evaluations with the CARLA open source simulator showing better generalization performance in new driving conditions than baselines","This study presents a method for learning the core of intergration to solve driving direction selection problem in unseen test town. The article introduces a new model that can be used to learn and segment the driver's input using a saliency-map based visualization method. The authors present a novel way to improve the performance of driving models by improving the perception module weights, and show that it has a different structure than training town's."
We show how to optimize the expected L_0 norm of parametric models with gradient descent and introduce a new distribution that facilitates hard gating. The authors introduce a gradient-based approach to minimize an objective function with an L0 sparse penalty to help learn sparse neural networks,A method to smooth the expected L 0 normalized objective with continuous distributions in a way that can maintain the exact zero in the parameters while still allowing for efficient gradient based optimization. This work introduces a compression and sparsification method for optimizing parametric models by using a model compression and a generalization method.
We develop meta-learning methods for adversarially robust few-shot learning. This paper presents a method that enhances the robustness of few-shot learning by introducing adversarial query data attack in the inner-task fine-tuning phase of a meta-learning algorithm. The authors of this paper propose a novel approach for training a robust few-shot model.,"In the few-shot setting, adversarial querying outperforms other robustness techniques by a wide margin in terms of clean accuracy and adversariality robustness. This study introduces a novel method for retrieving data from a neural network using a pre-trained feature extractor that is trained on a large corpus of data sets and fine-tuning on new tasks."
This article demonstrates how to train deep autoencoders end-to-end to achieve SoA results on time-split Netflix data set. This article presents a deep autoencoder model for rating prediction that outperforms other state-of-the-art approahces on the Netflix prize dataset. Proposes to use a deep AE to do rating prediction tasks in recommender systems. The authors present a model for more accurate Netflix recommendations demonstrating that a deep autoencoder can out-perform more complex RNN-based models that have temporal information.,A new model for the rating prediction task in recommender systems which significantly outperforms previous state-of-the-art models on time-split Netflix data set. This work introduces a new model to train deep autoencoders that is more sparse than shallow ones. The authors present a method for training autoencoding deep models with non-linear activation functions and a novel output re-feeding training algorithm.
"We propose that training with growing sets stage-by-stage provides an optimization for neural networks. The authors compare curriculum learning to learning in a random order with stages that add a new sample of examples to the previously, randomly constructed set This work studies the influence of ordering in the Curriculum and Self paced learning, and shows that to some extent the ordering of training instances is not important.",A small training set and adding new samples to both curriculum and anti-curriculum learning makes these methods better. The authors show that adding samples randomly without a meaningful order improves the learning performance. This study investigates the problem of finding out how to add samples to the training set by adding them randomly in the first stage of the training process. The study presents a method for improving the performance of training with a group of random samples
"We replace the fully connected layers of a neural network with the multi-scale entanglement renormalization ansatz, a type of quantum operation which describes long range correlations. In the work the authors suggest to use MERA tensorization technique for compressing neural networks. A new parameterization of linear maps for neural network use, using a hierarchical factorization of the linear map that reduces the number of parameters while still allowing for relatively complex interactions to be modelled. Studies compressing feed forward layers using low rank tensor decompositions and explore a tree like decomposition","A neural network with two penultimate fully connected layers of the model replaced with MERA layers. This work presents a novel compression method for neural networks that uses a tree-like connectivity to capture correlations between inputs in a region determined by its height in the tree. The authors present a new layer of a neural network using tensor trains and a set of rank-4 tree elements, followed by a single tree element."
"A new method for gradient-descent inference of permutations, with applications to latent matching inference and supervised learning of permutations with neural networks The article utilizes finite approximation of the Sinkhorn operator to describe how one can construct a neural network for learning from permutation valued training data. The work proposes a new method that approximates the discrete max-weight for learning latent permutations","A method to parameterize the hard choice of a permutation P through a square matrix X. This work introduces a method for learning a mapping from scrambled objectsX to actual, non-scrambled X by using the Sinkhorn iterations in order to solve the matching problem. The authors present a technique for determining the length of the Permutation Equations"
Compressing trained DNN models by minimizing their complexity while constraining their loss. This study proposes a method for deep neural network compression under accuracy constraints. This article presents a loss value constrained k-means encoding method for network compression and develops an iterative algorithm for model optimization.,A method for minimizing the complexity of a trained model by eliminating or merging unnecessary centroids. This work presents a method to solve the constrained optimization problem by eliminating the centroids and reducing the number of computations. The authors present a novel loss function algorithm that eliminates centroids in the training process and then calculates M based on the optimal value of the loss function.
"Imitation from pixels, with sparse or no reward, using off-policy RL and a tiny adversarially-learned reward function. The paper proposes to use a ""minimal adversary"" in generative adversarial imitation learning under high-dimensional visual spaces. This article aims at solving the problem of estimating sparse rewards in a high-dimensional input setting.",A deep-distributed Deterministic Policy Gradients (D4PG) agent that does off-policy training with experience replay with buffer B. This work introduces a deep-deterministic policy-gradient approach to block stacking in simulated robot robots using only demonstrations and a sparse binary reward indicating whether or not the stack has completed.
"Improvements to adversarial robustness, as well as provable robustness guarantees, are obtained by augmenting adversarial training with a tractable Lipschitz regularization Explores augmenting the training loss with an additional gradient regularization term to improve robustness of models against adversarial examples Uses a trick to simplify the adversarial loss by one in which the adversarial perturbation appears in closed form.",A method for estimating adversarial robustness in deep neural networks that is not tractable as a Lipschitz penalty. This work presents a method to estimate adversarially robustness on unseen data drawn from the same distribution using either the one-step Signed Gradient attack vector or the gradient attack vector as Total Variation regularization.
"A new state-of-the-art approach for knowledge graph embedding. Presents a neural link prediction scoring function that can infer symmetry, anti-symmetry, inversion and composition patterns of relations in a knowledge base. This work proposes an approach to knowledge graph embedding by modeling relations as rotations in the complex vector space. Proposes a method for graph embedding to be used for link prediction","A new knowledge graph embedding model that can model and infer all the three types of relation patterns. This work introduces a new method for inferring relation patterns, which is able to infer the patterns of (or between) the relations. The authors present an approach to inducing relation patterns by defining the distance function of each relation as a rotation in the complex vector spaces."
"We present Multitask Neural Model Search, a Meta-learner that can design models for multiple tasks simultaneously and transfer learning to unseen tasks. This paper extends Neural Architecture Search to the multi-task learning problem where a task conditioned model search controller is learned to handle multiple tasks simultaneously. In this paper, authors summarize their work on building a framework, called Multitask Neural Model Search controller, for automated neural network construction across multiple tasks simultaneously.","This work presents Multitask Neural Model Search (MNMS), an automated model construction framework that finds the best performing models in the search space for multiple tasks simultaneously. The authors present a multi-task neural model search framework that has been pre-trained on previous searches, thus speeding up the search for new tasks."
Properly training CNNs with dustbin class increase their robustness to adversarial attacks and their capacity to deal with out-distribution samples. This paper proposes adding an additional label for detecting OOD samples and adversarial examples in CNN models. The work proposes an additional class that incorporates natural out-distribution images and interpolated images for adversarial and out-distribution samples in CNNs,"A simple and computationally efficient solution for reducing the risk of misclassifying both adversaries and samples from a broad range of over-generalized out-distribution sets. This study introduces a method to learn a ""dustbin"" sub-manifold that can be used to train adversarial examples instead of unsupervised dustbins."
"Permutation-invariant loss function for point set prediction. Proposes a new loss for points registration (aligning two point sets) with preferable permutation invariant property. This work introduces a novel distance function between point sets, applies two other permutation distances in an end-to-end object detection task, and shows that in two dimensions all local minima of the holographic loss are global minima. Proposes permutation invariant loss functions which depend on the distance of sets.","An analytic distance function for moderately sized point sets of known cardinality that is shown to have very desirable properties, both as a loss function and a regularizer for machine learning applications. This study introduces a permutation-invariant loss function for low-value points in the context of machine learning and shows that it has favorable properties."
"Genetic algorithms based approach for optimizing deep neural network policies The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together. This article proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks.","A new state-space crossover operator that efficiently combines two parent policies into an offspring or child policy that takes advantage of both parents. This work introduces a new policy gradient operator for continuous control tasks, where the policy is evaluated by a deep neural network with a probability distribution over the parameter space and evaluates the objective function on the candidates with the high fitness."
"We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints. Proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. This work introduces an approach to compressing neural networks by looking at the correlation of filter responses in each layer via two strategies. This work proposes a compression method based on spectral analysis","A closed-form algorithm based on spectral energy analysis for suggesting the number of filters to remove in a layer. This work introduces a new set of filters that can be pruned to a lower dimensional space by using a random initialization to obtain the highest test accuracy. The authors present a method for filter selection, which is more accurate than Scratch and CIFAR-100."
This article studies the discrimination and generalization properties of GANs when the discriminator set is a restricted function class like neural networks. Balances capacities of generator and discriminator classes in GANs by guaranteeing that induced IPMs are metrics and not pseudo metrics This article provides a mathematical analysis of the role of the size of the adversary/discriminator set in GANs,"A bounded Lipschitz distance for non-decreasing homogenous activation functions, and a stronger neural distance. This paper considers the problem of discriminating against non-parametric and infinite dimensional representations of the discriminator sets. The authors show that discriminating between the two sets of discriminators is based on the fact that the discrepancies between the three sets are invariantly equal to those of the other sets."
"Building on previous work on domain generalization, we hope to produce a classifier that will generalize to previously unseen domains, even when domain identifiers are not available during training. A domain generalization approach to reveal semantic information based on a linear projection scheme from CNN and NGLCM output layers. The paper proposes an unsupervised approach to identify image features that are not meaningful for image classification tasks","A new neural network building block that resembles GLCM but has (sub)gradient everywhere and thus are tunable with backpropagation. This work introduces a new neural networks building block for the construction of a neural network, where all the parameters are differentiable, and thus can be projected with a projection matrix constructed by a residual maker matrix."
"Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior The authors suggest that statistical mechanics ideas will help to understand generalization properties of deep neural networks, and give an approach that provides strong qualitative descriptions of empirical results regarding deep neural networks and learning algorithms. A set of ideas related to theoretical understanding generalization properties of multilayer neural networks, and a qualitative analogy between behaviours in deep learning and results from quantitative statistical physics analysis of single and two-layer neural networks.","An approach to generalization of NN/DNN learning by focusing heavily on the specific details of the model, the detailed properties of the data and their noise, and so on. This article considers the problem of convergence to flat minimizers in statistical learning, shows that convergence to sharp minima can be used to improve prediction quality by 1%)."
"This paper introduces Morpho-MNIST, a collection of shape metrics and perturbations, in a step towards quantitative evaluation of representation learning. This work discusses the problem of evaluating and diagnosing the represenatations learnt using a generative model. Authors present a set of criteria to categorize MNISt digists and a set of interesting perturbations to modify MNIST dataset.","A new quantitative framework for assessing representation learning that has well understood and easily measurable factors of variation in the data. This work introduces a new framework to evaluate representation learning by adding a morphometric analysis, enabling quantitative comparison of trained models, and identification of roles of latent variables, and characterisation of sample diversity."
"We show that modular structured models are the best in terms of systematic generalization and that their end-to-end versions don't generalize as well. This study evaluates systemic generalization between modular neural networks and otherwise generic models via introduction of a new, spatial reasoning dataset A targeted empirical evaluation of generalization in models for visual reasoning, focused on the problem of recognizing (object, relation, object) triples in synthetic scenes featuring letters and numbers.","A synthetic dataset for learning a generalization task that learns rules on how to compose words and fail spectacularly when asked to interpret ""jump"", ""run twice"" and ""walk twice"". This work introduces a new dataset called Spatial Queries On Object Pairs (SQOOP), in which a model has to perform spatial relational reasoning about pairs of randomly scattered letters and digits in the image."
"We propose a self-monitoring agent for the Vision-and-Language Navigation task. A method for vision+language navigation which tracks progress on the instruction using a progress monitor and a visual-textual co-grounding module, and performs well on standard benchmarks. This work describes a model for vision-and-language navigation with a panoramic visual attention and an auxillary progress monitoring loss, giving state-of-the-art results.","A new objective function for the agent to measure how well it can estimate the completeness of instruction-following. This work introduces a new approach for action selection-textual grounding, which is used to measure the progress made towards the goal. The authors present a method for determining which direction to go by relying on a grounded instruction, and show that the attentional mechanism of the baseline does not successfully track this information through time."
"We show that ENAS with ES-optimization in RL is highly scalable, and use it to compactify neural network policies by weight sharing. The authors construct reinforcement learning policies with very few parameters by compressing a feed-forward neural network, forcing it to share weights, and using a reinforcement learning method to learn the mapping of shared weights. This study combines ideas from ENAS and ES methods for optimisation, and introduces the chromatic network architecture, which partitions weights of the RL network into tied sub-groups.","A highly scalable algorithm that learns effective policies with over 92% reduction of the number of neural networks parameters. This work introduces a method for learning weight-sharing architectures that are more complex than hardcoded ones. The authors study the problem of learning weight sharing architectures in RL by using ES to improve the efficiency of the controller, and proposed a way to learn weight sharing mechanisms which can be used to measure the accuracy of the input data."
"Gaggle, an interactive visual analytic system to help users interactively navigate model space for classification and ranking tasks. A new visual analytic system which aims to enable non-expert users to interactively navigate a model space by using a demonstration-based approach. A visual analytics system that helps novice analysts navigate model space in performing classification and ranking tasks.",A new optimal model for a single iteration. This study introduces a new model space that can be sampled using a combination of a learning algorithm and hyperparameters. The authors present a model space for the task of classifying data objects in a linear way by removing a comparison and selection step to ensure they do not change labels in future iterations.
"We propose to generate adversarial example based on generative adversarial networks in a semi-whitebox and black-box settings. Describes AdvGAN, a conditional GAN plus adversarial loss, and evaluates AdvGAN on semi-white box and black box setting, reporting state-of-art results. This paper proposes a way of generating adversarial examples that fool classification systems and wins MadryLab's mnist challenge.",A deep neural network that generates adversarial examples against an untargeted image. This work combines re-ranking loss and a L 2 norm loss to constrain the generated instance to be close to the original one in terms of L 2. The authors present a method for maximizing the distance between the prediction and the ground truth of the generated adversarials.
We introduce a novel measure of flatness at local minima of the loss surface of deep neural networks which is invariant with respect to layer-wise reparameterizations and we connect flatness to feature robustness and generalization. The authors propose a notion of feature robustness which is invariant with respect to rescaling the weight and discuss the notion's relationship to generalization. This paper defines a notion of feature-robustness and combines it with epsilon representativeness of a function to describe a connection between flatness of minima and generalization in deep neural networks.,"This article considers the generalization properties of feature robustness in a neural network. The authors introduce a new method to measure the flatness of a function f =  •  (e.g., an unnormalized neural network) towards local changes in the feature space. This study presents a method for evaluating the generalisation properties of features robustness from a set of distributions to a particular distribution."
"We introduce DPFRL, a framework for reinforcement learning under partial and complex observations with a fully differentiable discriminative particle filter Introduces ideas for training DLR agents with latent state variables, modeled as a belief distribution, so they can handle partially observed environments. This work introduces a principled method for POMDP RL: Discriminative Particle Filter Reinforcement Learning that allows for reasoning with partial observations over multiple time steps, achieving state-of-the-art on benchmarks.","This work introduces a new method for learning to explicitly track a latent belief, while circumventing the difficulty of generative observation modeling. The authors present a method for re-learning observation features that are irrelevant for belief tracking and decision making by using a discriminative Particle Filter Reinforcement Learning framework. This study presents a novel method for understanding beliefs in RL with partial observations, which is more flexible than a generative model."
"We prove randomly initialized (stochastic) gradient descent learns a convolutional filter in polynomial time. Studies the problem of learning a single convolutional filter using SGD and shows that under certain conditions, SGD learns a single convolutional filter. This work extends the Gaussian distribution assumption to a more general angular smoothness assumption, which covers a wider family of input distributions","A neural network optimization algorithm that can recover the filter in polynomial time. This work introduces a new method for optimizing neural networks with stochastic gradient descent to improve the performance of the filter. The authors study the problem of learning Filter Filters, and show that a smoother input distribution leads to faster convergence and a more efficient filter."
"We introduce a notion of conservatively-extrapolated value functions, which provably lead to policies that can self-correct to stay close to the demonstration states, and learn them with a novel negative sampling technique. An algorithm called value iteration with negative sampling to address the covariate shift problem in imitation learning.",An algorithm that learns a value function that extrapolates to unseen states and achieves near-optimal performance. This work presents a method for attacking the optimistic extrapolation problem by learning a values function that is guaranteed to induce policies that stay close to the demonstration states.
"We investigate the merits of employing neural networks in the match prediction problem where one seeks to estimate the likelihood of a group of M items preferred over another, based on partial group comparison data. This article proposes a deep neural network solution to the set ranking problem and designs a architecture for this task inspired by previous manually designed algorithms. This work provides a technique to solve the match prediction problem using a deep learning architecture.","Using a single layer neural network to improve prediction accuracy on real-world online game datasets. This work introduces a BTL model that can be applied to rank aggregation tasks, and shows that it is adaptable for other tasks. The authors present a new model for match prediction in the context of a group of M items chosen by a collection of n items selected from a given dataset."
"We investigate ReLU networks in the Fourier domain and demonstrate peculiar behaviour. Fourier analysis of ReLU network, finding that they are biased towards learning low frequency This study has theoretical and empirical contributions on topic of Fourier coefficients of neural networks","A neural network that is biased towards expressing a subset of such solutions, namely those that are low frequency. This study presents a neural network bias that manifests itself in the process of learning, but also in the parameterization of the model itself. The authors show that the lower frequencies of trained neural networks are more robust with respect to random parameter perturbations."
"We refine the over-approximation results from incomplete verifiers using MILP solvers to prove more robustness properties than state-of-the-art. Introduces a verifier that obtains improvement on precision of incomplete verifiers and scalability of the complete verifiers using over-parameterization, mixed integer linear programming and linear programming relaxation. A mixed strategy to obtain better precision on robustness verifications of feed-forward neural networks with piecewise linear activation functions, achieving better precision than incomplete verifiers and more scalability than complete verifiers.","A novel verifier for deep neural network robustness, combining overapproximation with an inexact MILP formulation. This work introduces a method for solving deep neural networks without adversarial examples by using a combination of state-of-the-art techniques and a novel heuristic that points to neural networks whose approximated bounds should be refined."
Using Wasserstein-GANs to generate realistic neural activity and to detect the most relevant features present in neural population patterns. A method for simulating spike trains from populations of neurons which match empirical data using a semi-convolutional GAN. The paper proposes to use GANs for synthesizing realistic neural activity patterns,"A new neural network that can be used to model a set of stimuli using supervised learning techniques. This study investigates the application of deep learning methods to real datasets and shows that Spike-GAN is not only memorizing but also mimicking their underlying distribution. The authors present a novel method for modeling neural activity in response to a given set of stimulations, and show that it is able to emulate the current distribution of the generated samples."
This study describe a 3D authoring tool for providing AR in assembly lines of industry 4.0 The study addresses how AR authoring tools support training of assembly line systems and proposes an approach An AR guidance system for industrial assembly lines that allows for on-site authoring of AR content. Presents a system that allows factory workers to be trained more efficiently using augmented reality system.,"A new authoring tool designed to allow users to create a 3D representation of the assembly line, and to place anchors and markers for the AR guidance elements. This article introduces a new Authoring Tool that can be used to create 3D models of industrial components and workstations. The article presents a method for creating 3D modeling objects using an operator training system, which allows users to use their own information on the instructions of the machine learning process."
Learning Priors for Adversarial Autoencoders Proposes a simple extension of adversarial auto-encoders for conditional image generation. Focuses on adversarial autoencoders and introduces a code generator network to transform a simple prior into one that together with the generator can better fit the data distribution,"A deep generative model that can be learned from the code generator to minimize adversarial loss in data space. This work introduces a neural network for generating images with a latent distribution that is capable of learning from the decoder. The authors present a variational learning method to maximize the mutual information I(s) between the variable and the generated image, and show that it is possible to learn a prior over the latent variables during training."
learn hierarchal sub-policies through end-to-end training over a distribution of tasks The authors consider the problem of learning a useful set of ‘sub policies’ that can be shared between tasks so as to jump start learning on new tasks drawn from the task distribution. This work proposes a novel method for inducing temporal hierarchical structure in a specialized multi-task setting.,"A new master policy that is optimized for sparse-reward tasks. This work introduces a new model of sub-policies that can be used to train agents in complex physics environments with long time horizons. The authors show that the master policy is efficient enough to learn in a large number of gradient updates, and that it is robust enough to transfer them towards otherwise unsolvable non-solvable tasks."
"Represent each entity as a probability distribution over contexts embedded in a ground space. Proposes to construct word embeddings from a histogram over context words, instead of as point vectors, which allows for measuring distances between two words in terms of optimal transport between the histograms through a method that augments representation of an entity from standard ""point in a vector space"" to a histogram with bins located at some points in that vector space.","A method for optimal transport between documents and contexts that can capture such inherent uncertainty and polysemy. This work explores the problem of co-occurrence information required to build the distributions as a first step of point-wise embedding methods, but has been ignored in the past. The authors present a method for determining the distance between documents by defining a suitable underlying cost on the movement of contexts."
"precision highway; a generalized concept of high-precision information flow for sub 4-bit quantization Investigates the problem of neural network quantization by employing an end-to-end precision highway to reduce the accumulated quantization error and enable ultra-low precision in deep neural networks. This study studies methods to improve the performance of quantized neural networks This paper proposes to keep a high activation/gradient flow in two kinds of networks structures, ResNet and LSTM.","A network-level approach to quantization, called precision highway, for high-precision information flow. This work introduces a novel quantization method that can be applied to the pre-activation convolutional and recurrent neural networks. The authors present a method for quantization using weight/activation quantization with no accuracy loss and an analysis of the energy and memory overhead of the network."
"Many graph classification data sets have duplicates, thus raising questions about generalization abilities and fair comparison of the models. The authors discuss isomorphism bias in graph datasets, the overfitting effect in learning networks whenever graph isomorphism features are incorporated within the model, theoretically analogous to data leakage effects.","A graph classification algorithm for isomorphic graphs that has repeating instances which cause the problem of a bias in the training data set. This work presents a graph classification problem with mismatched target labels that are different from the target label, and shows that it is not expressive enough to map the structure of the graphs to the target labels correctly. The authors present a method to classify graphs using a single number that uniquely identifies an individual graph."
"We utilize the alternating minimization principle to provide an effective novel technique to train deep autoencoders. Alternating minimization framework for training autoencoder and encoder-decoder networks The authors explore an alternating optimization approach for training Auto Encoders, treating each layer as a generalized linear model, and suggest using the stochastic normalized GD as the minimization algorithm in each phase.","A method for training neural networks with sigmoid activation functions that can be applied to quasi-convex optimization problems. This work introduces alternating minimization strategy, DANTE, which can be used to train multi-layer networks with multiple hidden layers. The authors present a method to train autoencoders and show how to use the generalized ReLU function to train deep neural networks without further finetuning/cross-validation."
"We propose an algorithmic framework to schedule constellations of small spacecraft with 3-DOF re-orientation capabilities, networked with inter-sat links. This work proposes a communication module to optimize the schedule of communication for the problem of spacecraft constellations, and compares the algorithm in distributed and centralized settings.","A new algorithmic scheduler that optimizes the schedule for any satellite in a given constellation to observe a known set of ground regions with rapidly changing parameters and observation requirements. This work introduces an algorithmic framework for combining satellite and subsystem characteristics with a network-wide scheduler, which is expected to run onboard every satellite from one GP to another."
"Image captioning as a conditional GAN training with novel architectures, also study two discrete GAN training methods. An improved GAN model for image captioning that proposes a context-aware LSTM captioner, introduces a stronger co-attentive discriminator with better performance, and uses SCST for GAN training.","A GAN-based framework for image captioning that enables better language composition, more accurate compositional alignment of image and text, and light-weight efficient training of discrete sequence GANs. This work studies the viability of self-critical Sequence Training (SCST) and Gumbel Straight-Through (Gumbel ST)."
A two-stage approach consisting of sentence selection followed by span selection can be made more robust to adversarial attacks in comparison to a single-stage model trained on full context. This article investigates an existing model and finds that a two-stage trained QA method is not more robust to adversarial attacks compared to other methods.,A new approach for two-stage QA that decomposes the task into two stages: select relevant sentences from the passage; and select a span among those sentences. This work presents a novel way to make adversarial models more robust to adversarially attacks. The authors present a method for learning a fixed-length question representation that is then used to score potential spans in the context selection process.
"We use a GAN discriminator to perform an approximate rejection sampling scheme on the output of the GAN generator. Proposes a rejection sampling algorithm for sampling from the GAN generator. This study proposed a post-processing rejection sampling scheme for GANs, named Discriminator Rejection Sampling, to help filter ‘good’ samples from GANs’ generator.","A method to correct errors in the GAN generator distribution. This work introduces a new rejection sampling scheme using the discriminator to completely minimize its own loss yields DISPLAYFORM. The authors present a method for resolving the problem of reject sampling and show that under quite strict assumptions, this scheme allows us to recover the data distribution exactly."
We show how to get good representations from the point of view of Simiarity Search. Studies the impact of changing the image classification part on top of the DNN on the ability to index the descriptors with a LSH or a kd-tree algorithm. Proposes to use softmax cross-entropy loss to learn a network that tries to reduce the angles between inputs and the corresponding class vectors in a supervised framework using.,A new dataset of n = 10 6 random unit vectors and plant queries at a given angle  from a randomly selected subset of database points. The authors study the cosine similarity problem in hyperplane LSH BID3 and show that the probability of finding the closest vector is at least 99.33%.
Adversarial learning methods encourage NLI models to ignore dataset-specific biases and help models transfer across datasets. The article proposes an adversarial setup to mitigate annotation artifacts in natural language inference data This paper presents a method for removing bias of a textual entailment model through an adversarial training objective.,"A domain-adversarial training framework for neural networks that can perform well on other NLI datasets regardless of what annotation artifacts exist in the training corpus' hypotheses. This work presents two architectures that enable a model to perform well against hidden biases, while in fact they are still hidden in the representation. The authors present an adversarial loss function that reduces the risk of false impressions of success."
"Object instance recognition with adversarial autoencoders was performed with a novel 'mental image' target that is canonical representation of the input image. The work proposes a method to learn features for object recognition that is invariant to various transformations of the object, most notably object pose. This article investigated the task of few shot recognition via a generated “mental image” as intermediate representation given the input image.","A new mental image DCGAN that learns features that are useful for recognizing entire classes of objects. This work introduces a new approach to learning mental images from the perspective of an object generation model and a discriminator-like representation of the object in the form of a bottleneck feature vector, of length n. The authors describe a novel way of learning mental image recognition by learning a range of classification features from the input to the target distribution."
"Question answering models that model the joint distribution of questions and answers can learn more than discriminative models This paper proposes a generative approach to textual and visual QA, where a joint distribution over the question and answer space given the context is learned, which captures more complex relationships. This work introduces a generative model for question answering and proposes to model p(q,a|c), factorized as p(a|c) * p(q|a,c). The authors proposes a generative QA model, which optimizes jointly the distribution of questions and answering given a document/context.","A discriminative language model that can identify the only producer, and ignore the rest of the question. This work introduces a discriminative loss function which saturates when simple correlations allow the question to be answered confidently, leaving no incentive for further learning on the example. The authors address the problem of discriminative word embedding by using a hidden state of each layer of a language model with a trainable vector of size d."
"We learn deep networks of hard-threshold units by setting hidden-unit targets using combinatorial optimization and weights by convex optimization, resulting in improved performance on ImageNet. The work explains and generalizes approaches for learning neural nets with hard activation. This study examines the problem of optimizing deep networks of hard-threshold units. The study discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it with a collection of heuristics/approximations.","A mini-batch algorithm for learning deep hard-threshold networks that includes the popular but poorly justified straight-through estimator as a special case. The authors address the problem of learning deep neural networks with hidden units in order to minimize loss, and show that it improves classification accuracy in a number of settings, including for AlexNet and ResNet-18 on ImageNet."
"pix2scene: a deep generative based approach for implicitly modelling the geometrical properties of a 3D scene from images Explores explaining scenes with surfels in a neural recognition model, and demonstrate results on image reconstruction, synthesis, and mental shape rotation. Authors introduce a method to create a 3D scene model given a 2D image and a camera pose using a self-superfised model","A method of unsupervised learning of the 3D structure in a 3D scene using a latent variable. This work studies the structural properties of a 2D scene and shows that only a small fraction of the entities are perceivable from the camera. The authors present a method to learn a surfel-based representation of the visible part of the scene by learning a viewpoint-dependent representation of surface elements, similar to surfels BID24"
"We show how to use deep RL to construct agents that can solve social dilemmas beyond matrix games. Learning to play two-player general-sum games with state with imperfect information Specifies a trigger strategy (CCC) and corresponding algorithm, demonstrating convergence to efficient outcomes in social dilemmas without need for agents to observe each other's actions.","A method for learning cooperative and selfish policies that improves game-to-game performance. The authors address the problem of defection in the Prisoner's Dilemma by introducing a new policy gradient technique to improve the rate of payoff between agents. This article introduces a novel approach to learn cooperative policies, which can be used to train agents with a different level of co-operation and a higher rate of return on their own."
We learn a fast neural solver for PDEs that has convergence guarantees. Develops a method to accelerate the finite difference method in solving PDEs and proposes a revised framework for fixed point iteration after discretization. The authors propose a linear method for speeding up PDE solvers.,A new linear convolutional network that can be used to solve a class of PDE problems governed by the same A. This work introduces a new iterator for discretized PDE problem solving with a fixed point and fast convergence on the class of problems of interest. The authors address the problem of learning an iterative solver that is able to approximate T (I) from a single point of view.
"Propose a novel method by integrating SG-MCMC sampling, group sparse prior and network pruning to learn Sparse Structured Ensemble (SSE) with improved performance and significantly reduced cost than traditional methods. The authors propose a procedure to generate an ensemble of sparse structured models A new framework for training ensemble neural networks that uses SG-MCMC methods within deep learning, and then increases computational efficiency by group sparsity+pruning. This work explores the use of FNN and LSTMs to make bayesian model averaging more computationally feasible and improve average model performance.","A Bayesian inference framework that averaging more models than other traditional methods of learning ensembles can improve performance. This study studies the problem of posterior sampling in LSTM with an emphasis on discretization error, and shows that SGLD does not have a Metropolis-Hastings correction step. The authors present a method for evaluating posterior sampling as a result of a lack of regularization in the case of DNNs."
"We propose and apply a meta-learning methodology based on Weak Supervision, for combining Semi-Supervised and Ensemble Learning on the task of Biomedical Relationship Extraction. A semi-supervised method for relation classification, which trains multiple base learners using a small labeled dataset and applies some of them to annotate unlabeled examples for semi-supervised learning. This article addresses the problem of generating training data for biological relation extraction, and uses predictions from data labeled by weak classifiers as additional training data for a meta learning algorithm. This study proposes a combination of semi-supervised learning and ensemble learning for information extraction, with experiments conducted on a biomedical relation extraction task","This study introduces a new ensemble learning methodology for relation extraction, which can be adapted and generalized to large-scale (or web-scale) datasets. The authors present a method for ensemble learning with deep neural networks that uses an unlabeled data point to generate a label or abstain from voting. This work explores the problem of ensemble learning in a small-scale controlled environment by using a multi-modal network"
"we propose a regularizer that improves the classification performance of neural networks the authors propose to train a model from a point of maximizing mutual information between the predictions and the true outputs, with a regularization term that minimizes irrelevant information while learning. Proposes to decompose the parameters into an invertible feature map F and a linear transformation w in the last layer to maximize mutual information I(Y, \hat{T}) while constraining irrelevant information","A novel information optimization problem for the discrete prediction random variable Y in deep neural network. This work introduces a new algorithm to predict the mutual information between signal X and feature T, which can be decomposed into two stages: Transformation stage, transformation stage, and classification stage. The authors present a novel algorithm for the classification of deep neural networks that is robust against invertible features of Y and prevents over-fitting in the learning process."
"Presents new architecture which leverages information globalization power of u-nets in a deeper networks and performs well across tasks without any bells and whistles. A network architecture for semantic image segmentation, based on composing a stack of basic U-Net architectures, that reduces the number of parameters and improves results. This proposes a stacked U-Net architecture for image segmentation.",A deep-net architecture capable of handling the complexity of natural images that leverages information globalization power of u-nets in a deeper net-work architecture. This study presents a deep network for image segmentation and object detection tasks that requires global information about all pixels in an image. The authors present a wide range of net- work architectures that combine features from different resolution scales to achieve higher resolution on semantic segmentation tasks.
"We introduce a smoothness regularization for convolutional kernels of CNN that can help improve adversarial robustness and lead to perceptually-aligned gradients This paper proposes a new regularization scheme that encourages convolutional kernels to be smoother, arguing that reducing neural network reliance on high-frequency components helps robustness against adversarial examples. The authors propose a method for learning smoother convolutional kernels, specifically, a regularizer penalizing large changes between consecutive pixels of the kernel with the intuition of penalizing the use of high-frequency input components.","Using a smooth kernel to filter out the low-frequency components of an image and improve the robustness of models. This work introduces a new definition of the high-frequency component of a neural network by using a variation of logit pairing loss, which penalizes the KL divergence over softmax as a distance metric."
"We use meta-gradients to attack the training procedure of deep neural networks for graphs. Studies the problem of learning a better poisoned graph parameters that can maximize the loss of a graph neural network. An algorithm to alter graph structure by adding/deleting edges so as to degrade the global performance of node classification, and the idea to use meta-learning to solve the bilevel optimization problem.","A meta-learning approach for non-targeted node classification, which is based on a meta-gradient that learns to avoid poisoning attackers. This work introduces a new meta gradient method for the detection of attacks in a multi-task learning setting, and shows that the meta gradient achieves similar results to the bi-level formulation in Eq. The authors show that meta learning is more efficient than meta learning."
We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters Proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-toend through differentiable Bayesian Filters and two different versions of the Unscented Kalman Filter Revisits Bayes filters and evaluates the benefit of training the observation and process noise models while keeping all other models fixed This study presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms. The approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated diagonal covariance.,A method to learn heterostochastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. This work presents a method for learning heteroscedastic process noise models by learning the observation noise from data and using it as a filtering method. The authors present a way to learn a heterosceptic process noise model that predicts higher uncertainty for the position and heading of the car.
"We propose a method for learning latent dependency structure in variational autoencoders. Uses a matrix of binary random variables to capture dependencies between latent variables in a hierarchical deep generative model. This paper presents a VAE approach in which a dependency structure on the latent variable is learned during training. The authors propose to augment the latent space of a VAE with an auto-regressive structure, to improve the expressiveness of both the inference network and the latent prior","A method for learning the dependency structure between latent variables in deep generative models. This work introduces a variational autoencoder framework that learns the depend structure of a latent variable model and combines it with a learned, flexible dependency structure. The authors present a method to learn a dependency structure in a Bayesian network using a sampling procedure to achieve higher expectations over latent variable structures."
We examine the relationship between probability density values and image content in non-invertible GANs. The authors try to estimate the probability distribution of the image with the help of GAN and develop a proper approximation to the PDFs in the latent space.,"A method to extract the probability density value of an image given its latent representation. This work introduces a method for estimating densities of image distributions by using a discriminator to produce samples that are likely to be real and low values to fake points. The authors present a way to estimate density in images of high density, and show that it is possible to use a GAN to generate samples from the discriminator classifying them as real."
A simple fast method for extracting visual features from convolutional neural networks Proposes a fast way to learn convolutional features that later can be used with any classifier by using reduced numbers of training epocs and specific schedule delays of learning rate Use a learning rate decay scheme that is fixed relative to the number of epochs used in training and extract the penultimate layer output as features to train a conventional classifier.,A new approach to learning feature fast is to design a step decay schedule that is based on the total number of epochs predicted to train the model. This work introduces a method to learn feature fast by using supervised Convolutional Neural Networks (CNNs) to extract high-quality image features Fast. The authors address the problem of building feature fast models with exponential and fast decay of the learning rate.
"Decompose the task of learning a generative model into learning disentangled latent factors for subsets of the data and then learning the joint over those latent factors. Locally Disentangled Factors for hierarchical latent variable generative model, which can be seen as a hierarchical variant of Adversarially Learned Inference The study investigates the potential of hierarchical latent variable models for generating images and image sequences and proposes to train several ALI models stacked on top of each other to create a hierarchical representation of the data. The work aims to learn the hierarchies for training GAN in a hierarchical optimization schedule directly instead of being designed by a human",A method to learn hierarchical latent variables with a requisite but general prior to disentangled learning. This study presents a method for unsupervised modeling models of high-dimensional observed variables and shows that it is possible to achieve decoupled training of complicated generative models. This work introduces a new approach to the disentanglement of hierarchy models by using a resolution hierarchial framework.
"Inspired by prior work on Sliced-Wasserstein Autoencoders (SWAE) and kernel smoothing we construct a new generative model – Cramer-Wold AutoEncoder (CWAE). This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution Introduces a variation on the Wasserstein AudoEncoders which is a novel regularized auto-encoder architecture that proposes a specific choice of the divergence penalty This article proposes the Cramer-Wold autoencoder, which uses the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.",A method for estimating distance between two distributions to one dimensional calculation. This work introduces a method for measuring distance between samples and the distance of a sample from the standard normal density. The authors investigate the problem of divergence measurement in the absence of an analytic formula that would enable the computation of the distance between different distributions by using the Cramer-Wold Theorem BID3 and Radon Transform BID4.
"Learn to rank learning curves in order to stop unpromising training jobs early. Novelty: use of pairwise ranking loss to directly model the probability of improving and transfer learning across data sets to reduce required training data. The paper proposes a method to rank learning curves of neural networks that can model learning curves across different datasets, achieving higher speed-ups on image classification tasks.","A method to determine the likelihood of no improvement is to compare the learning curve of a new configuration to the one of the currently best configurations. This work introduces a ranking model that models the probability that the current best solution surpasses the best solution so far. The authors use meta-knowledge to explicitly denote the entire learning curve, y 1 and y max."
"Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve. This study shows that wider RNNs improve convergence speed when applied to NLP problems, and by extension the effect of increasing the widths in deep neural networks on the convergence of optimization This study characterizes the impact of over-parametrization in the number of iterations it takes an algorithm to converge, and presents further empirical observations on the effects of over-parametrization in neural network training.","A convergence curve characterized by direct distance from initialization point to final point, the average step size grows with a power-law-like relationship, and the average angle between gradient vectors becomes more aligned with each other during traversal. This study presents a convergence rate model that can be characterized into a grid region within which the number of gradient updates to convergence has a reciprocal relationship to model size and linear relationship to dataset size."
"Generalized Graph Embedding Models A generalized knowledge graph embedding approach which learns the embeddings based on three different simultaneous objectives, and performs on par or even outperforms existing state-of-the art approaches. Tackles the task of learning embeddings of multi-relational graphs using a neural network Proposes a new method, GEN, to compute embeddings of multirelationship graphs, particularly that so-called E-Cells and R-Cells can answer queries of the form (h,r,?),(?r,t), and (h,?,t)","The proposed embedding learning framework for modeling author connections on social networks is completely unsupervised, which is distinctly different from previous works. This study considers the problem of embeddering neural networks using a hidden layer of the embedded dictionary and shows that it outperforms when given 50% of the data in a wide variety of artificial intelligence tasks. The authors present a model with a non-supervised label, which can be used to solve link prediction problems."
"Mean Replacement is an efficient method to improve the loss after pruning and Taylor approximation based scoring functions works better with absolute values. Proposes a simple improvement to methods for unit pruning using ""mean replacement"" This work presents a mean-replacement pruning strategy and utilizes the absolute-valued Taylor expansion as the scoring function for the pruning","A pruning method to prune a predefined fraction of its units using a chosen criterion and measure the pruning penalty. This work introduces a pruning strategy for prunable networks, where the best performing scoring functions can be used to reduce bias propagation in training. The authors present a new pruning strategy that measures the loss as a function with activations and a first order approximation of the absolute change in the loss."
"We introduce a system called GamePad to explore the application of machine learning methods to theorem proving in the Coq proof assistant. This paper describes a system for applying machine learning to interactive theorem proving, focuses on tasks of tactic prediction and position evaluation, and shows that a neural model outperforms an SVM on both tasks. Proposes that machine learning techniques be used to help build proof in the theorem prover Coq.","This paper introduces a new gamepad system for theorem proving in the Coq proof assistant. The authors present a method to learn proof states in a coq proof script, and show that it can be used to develop a pre-proved proof script. This work presents a framework for the application of machine learning to the orem provers on tasks useful for the task of premise selection which operates with little to no supervision."
"We propose a self-ensemble framework to train more robust deep learning models under noisy labeled datasets. This study proposed ""self-ensemble label filtering"" for learning with noisy labels where the label noise is instance-independent, which yield more accurate identification of inconsistent predictions. This article proposes an algorithm for learning from data with noisy labels which alternates between updating the model and removing samples that look like they have noisy labels.","A method for self-ensemble filtering and semi-supervised model ensembles that learns from noisy labeled data. This work introduces a method to improve the performance on noisy labels by reducing the noise ratio of noisy labels. The authors present a new filtering framework for learning from noisy labels using a non-expert dataset and a multi-task approach to filter out noisy labels, resulting in a more robust and robust manner."
"An algorithm for training neural networks efficiently on temporally redundant data. The article describes a neural coding scheme for spike based learning in deep neural networks This paper presents a method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data. This work applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network, combining the three components in a way not seen previously.","This work presents a method for adaptively tuning layers of neural network activations, which can be implemented on every layer of a neural network. The authors introduce a new algorithm to improve the performance of neural networks on Temporal MNIST by implementing a ""sparse"" and ""event-based"" quantization scheme. This study presents an algorithm for encoding gradient activations on each layer of the neural network"
A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers A method for constructing adversarial attacks that are less detectable by humans without cost in image space by changing the target class to be similar to the original class of the image.,"In the image space, adversarial samples generated by our method are less recognizable by human observers than other attacks. This work introduces a method for attacking an input image to be misclassified in the label space and shows that it is not at the expense of the loss in the attack rate or attack rate. The authors present a new attack method for generating adversarials generated by using a probability model, then move the input to the target label."
"We propose AD-VAT, where the tracker and the target object, viewed as two learnable agents, are opponents and can mutually enhance during training. This work aims to address the visual active tracking problem with a training mechanism in which the tracker and target serve as mutual opponents This paper presents a simple multi-agent Deep RL task where a moving tracker tries to follow a moving target. Proposes a novel reward function - ""partial zero sum"", which only encourages the tracker-target competition when they are close and penalizes whey they are too far.","A novel re-enforcement learning method for multi-agent training. This work introduces a neural network to train agents using adversarial training in a dueling/competitive manner. The authors present a new tracker that learns to predict their reward as an auxiliary task, and show that the tracker is more likely to compete with a target with the appropriate difficulty level when both agents are stronger simultaneously."
"Human behavioral judgments are used to obtain sparse and interpretable representations of objects that generalize to other tasks This study describes a large-scale experiment on human object/sematic representations and a model of such representations. This article develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images. A new approach to learn a sparse, positive, interpretable semantic space that maximizes human similarity judgements by training to specifically maximize the prediction of human similarity judgements.","Using sparse representations to model phenomena such as judgments of typicality or similarity between concepts, or reaction times in various semantic tasks. This work introduces a method for estimating vectors for each of the different concept senses (synsets) of a concept sense, and a novel way to model situations where the decision is close to being deterministic."
"We train generative adversarial networks in a progressive fashion, enabling us to generate high-resolution images with high quality. Introduces progressive growing and a simple parameter-free minibatch summary statistic feature for use in GAN training to enable synthesis of high-resolution images.","A GAN formulation that does not explicitly require the entire training data distribution to be represented by the result of the resulting generative model. This work introduces a GAN formula that can be used to measure the distance between the training distribution and the generated distribution, and provides a better quality version of the CELEBA dataset. The authors address the problem of mode collapse in GANs, and show that it is possible to do so over a dozen minibatches."
"A fast second-order solver for deep learning that works on ImageNet-scale problems with no hyper-parameter tuning Choosing direction by using a single step of gradient descent ""towards Newton step"" from an original estimate, and then taking this direction instead of original gradient A new approximate second-order optimization method with low computational cost that replaces the computation of the Hessian matrix with a single gradient step and a warm start strategy.","A new method for non-convex deep network optimisation, which can be used to improve second-order optimisation. This work introduces a new algorithm that uses the Hessian loop to solve non-consequence deep neural networks with no hyper-parameter tuning. The authors present a method for multi-task deep learning, using a surrogate matrix and a recursive gradient descent step in order to avoid saddle-points"
Models of source code that combine global and structural features learn more powerful representations of programs. A new method to model the source code for the bug repairing task using a sandwich model like [RNN GNN RNN] which significantly improves localization and repair accuracy.,"A graph-based model for code that uses message-passing to represent relations in code, which makes them de facto local due to the high cost of iteration. This work introduces a new family of models that efficiently combine longerdistance information, such as the sequence model can represent, with the semantic structural information available to the GGNN."
"A novel differentiable neural architecture search framework for mixed quantization of ConvNets. The authors introduce a new method for neural architecture search which selects the precision quantization of weights at each neural network layer, and use it in the context of network compression. The article presents a new approach in network quantization by quantizing different layers with different bit-widths and introduces a new differentiable neural architecture search framework.","Using the DNAS framework to solve the mixed precision quantization problem by constructing a super net whose macro architecture (number of layers, filter size of each layer, etc.) is the same as the target network. This work introduces a new model for quantization of neural nets with a stochastic super net that represents the architecture space A. The authors present a novel method for quantizing the loss function of a neural network"
"We propose an novel learning method for deep sound recognition named BC learning. Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes to increase discriminitive power of the final learned network. Proposes a method to improve the performance of a generic learning method by generating ""in between class"" training samples and presents the basic intuition and necessity of the proposed technique.","A new model for sound recognition and data augmentation in the feature space, and a regularization of the positional relationship between the feature distributions of the classes. This work introduces a new model to learn mixed sounds of dog bark and rain when the mixing ratio is within the range of 0.45 -0.8. The study presents a novel model for learning mixed sounds from two different parts of the features of the class."
"Motivated by theories of language and communication, we introduce community-based autoencoders, in which multiple encoders and decoders collectively learn structured and reusable representations. The authors tackle the problem of representation learning, aim to build reusable and structured represenation, argue co-adaptation between encoder and decoder in traditional AE yields poor representation, and introduce community based auto-encoders. The study presents a community based autoencoder framework to address co-adaptation of encoders and decoders and aims at constructing better representations.","A framework for learning representations that are more easily extracted and re-used for a different task, such as supervised learning or reinforcement learning. This work introduces a framework for autoencoders (CbAEs), in which the encoder and decoder can learn to reconstruct information from the input. The authors study the problem of co-adaptation between a pair of encoders and a latent vector model."
"This study presents a new deep learning architecture for addressing the problem of supervised learning with sparse and irregularly sampled multivariate time series. Proposes a framework for making predictions on sparse, irregularly sampled time-series data using an interpolation module that models the missing values in using smooth interpolation, non-smooth interpolation, and intensity. Solves the problem of supervised learning with sparse and irregularly sampled multivariate time series using a semi-parametric interpolation network followed by a prediction network.","A deep learning model for multivariate time series that can be used to isolate information about short duration events from broader trends. This work presents an interpolation network architecture that explicitly leverages a separate information channel related to patterns of medical event point processes. The authors present a deep learning framework for multi-time series prediction and class classification tasks with sparse and irregularly sampled time series, and a new representation of the input time series."
"We propose 3D shape programs, a structured, compositional shape representation. Our model learns to infer and execute shape programs to explain 3D shapes. An approach to infer shape programs given 3D models, with architecture consisting of a recurrent network that encodes a 3D shape and outputs instructions, and a second module that renders the program to 3D. This article introduces a high-level semantic description for 3D shapes, given by the ShapeProgram.","A purely end-to-end neural network synthesizer for 3D graphics. This work introduces a new way of learning to infer 3D shapes from raw, unannotated shapes, and then execute them directly from an RGB image. The authors present a method for modeling 3D images using a domain-specific language, which can be used to reconstruct parts of a table top and a square layer that is more accurate and physically plausible."
"We define a flexible DSL for RNN architecture generation that allows RNNs of varying size and complexity and propose a ranking function that represents RNNs as recursive neural networks, simulating their performance to decide on the most promising architectures. Introduces a new method to generate RNNs architectures using a domain-specific language for two types of generators (random and RL-based) together with a ranking function and evaluator. This work casts the search of good RNN Cell architectures as a black-box optimization problem where examples are represented as an operator tree and scored based on learnt functions or generated by a RL agent. This paper investigates meta-learning strategy for automated architecture search in the context of RNN by using a DSL that specifies RNN recurrent operations.","This study introduces a novel RNN architecture that uses a hidden state h t and an extension of the DSL to code by traversing the tree from the source nodes towards the final node. The authors present a new architecture definition for RNNs, which can be used to estimate the next batch of candidate architectures using a search-based optimization algorithm."
"Deep and narrow neural networks will converge to erroneous mean or median states of the target function depending on the loss with high probability. This paper studies failure modes of deep and narrow networks, focusing on as small as possible models for which the undesired behavior occurs. This article shows that the training of deep ReLU neural networks will converge to a constant classifier with high probability over random initialization if hidden layer widths are too small.","A simple approximately radial activation function can be approximated by a small 3-layer feed-forward NN, but it is impossible to approximate by any 2-layer network with the same accuracy, unless its width is exponential in the dimension. The authors study approximation theory of a class of (possibly discontinuous) piecewise C  functions for ReLU NNs and show that no more than O( 2(d1)/"
